{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import division\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['wine', 'default_credit', 'compas', 'lsat']\n",
    "bnn_widths = [200, 200, 200, 200]\n",
    "bnn_depths = [2, 2, 2, 2]\n",
    "\n",
    "vae_widths = [300, 300, 300, 300] # [200, 200, 200, 200]\n",
    "vae_depths = [3, 3, 3, 3] # We go deeper because we are using residual models\n",
    "vae_latent_dims = [6, 8, 4, 4]\n",
    "\n",
    "# For automatic explainer generation\n",
    "\n",
    "regression_bools = [True, False, False, True]\n",
    "gauss_cat_vae_bools = [False, True, True, True]\n",
    "flat_vae_bools = [False, False, False, False]\n",
    "\n",
    "var_names = {}\n",
    "var_names_flat = {}\n",
    "\n",
    "var_names['wine'] = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide',\n",
    "            'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n",
    "var_names_flat['wine'] = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide',\n",
    "            'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n",
    "\n",
    "var_names['default_credit'] = ['Given credit', 'Gender', 'Education', 'Marital status', 'Age', 'Payment delay 1', 'Payment delay 2',\n",
    "            'Payment delay 3', 'Payment delay 4', 'Payment delay 5', 'Payment delay 6', 'Bill statement 1', 'Bill statement 2',\n",
    "            'Bill statement 3', 'Bill statement 4', 'Bill statement 5', 'Bill statement 6', 'Previous payment 1', 'Previous payment 2',\n",
    "            'Previous payment 3', 'Previous payment 4', 'Previous payment 5', 'Previous payment 6']\n",
    "var_names_flat['default_credit'] = ['Given credit', 'Gender M', 'Gender F', 'Education grad', 'Education under', 'Education HS', 'Education Other',\n",
    "                 'Marital status M', 'Marital status S', 'Marital status Other', 'Age', 'Payment delay 1', 'Payment delay 2',\n",
    "            'Payment delay 3', 'Payment delay 4', 'Payment delay 5', 'Payment delay 6', 'Bill statement 1', 'Bill statement 2',\n",
    "            'Bill statement 3', 'Bill statement 4', 'Bill statement 5', 'Bill statement 6', 'Previous payment 1', 'Previous payment 2',\n",
    "            'Previous payment 3', 'Previous payment 4', 'Previous payment 5', 'Previous payment 6']\n",
    "\n",
    "var_names['compas'] = [\"age_cat\", \"race\", \"sex\", \"c_charge_degree\", 'is_recid', 'priors_count', 'time_served']\n",
    "var_names_flat['compas'] = ['25 - 45', 'Greater than 45', 'Less than 25', 'African-American', 'Asian', 'Caucasian', 'Hispanic', 'Native American', 'Other', 'Female', 'Male', 'Felony', 'misdemeanour', 'not_recid', 'is_recid', 'priors_count', 'time_served']\n",
    "\n",
    "var_names['lsat'] = ['LSAT', 'UGPA', 'race', 'sex']\n",
    "var_names_flat['lsat'] = ['LSAT', 'UGPA', 'amerind', 'mexican', 'other', 'black', 'asian', 'puerto', 'hisp', 'white', 'female', 'male']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## choose dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dname = 'compas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_UCI(dset_name, splits=10, seed=0, separate_targets=True, save_dir='data/'):\n",
    "    mkdir(save_dir)\n",
    "\n",
    "    if dset_name == 'boston':\n",
    "        if not os.path.isfile(save_dir+'housing.data'):\n",
    "            urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\",\n",
    "                               filename=save_dir+'housing.data')\n",
    "        data = pd.read_csv(save_dir + 'housing.data', header=0, delimiter=\"\\s+\").values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'concrete':\n",
    "        if not os.path.isfile(save_dir+'Concrete_Data.xls'):\n",
    "            urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete_Data.xls\",\n",
    "                               filename=save_dir+'Concrete_Data.xls')\n",
    "        data = pd.read_excel(save_dir+ 'Concrete_Data.xls', header=0, delimiter=\"\\s+\").values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'energy':\n",
    "        if not os.path.isfile(save_dir+'ENB2012_data.xlsx'):\n",
    "            urllib.urlretrieve(\"http://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx\",\n",
    "                               filename=save_dir+'ENB2012_data.xlsx')\n",
    "        data = pd.read_excel(save_dir+'ENB2012_data.xlsx', header=0, delimiter=\"\\s+\").values\n",
    "        y_idx = [-2, -1]\n",
    "\n",
    "    elif dset_name == 'power':\n",
    "        if not os.path.isfile(save_dir+'CCPP.zip'):\n",
    "            urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00294/CCPP.zip\",\n",
    "                               filename=save_dir+'CCPP.zip')\n",
    "        zipped = zipfile.ZipFile(save_dir+\"CCPP.zip\")\n",
    "        data = pd.read_excel(zipped.open('CCPP/Folds5x2_pp.xlsx'), header=0, delimiter=\"\\t\").values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'wine':\n",
    "        if not os.path.isfile(save_dir+'winequality-red.csv'):\n",
    "            urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\",\n",
    "                               filename=save_dir+'winequality-red.csv')\n",
    "        data = pd.read_csv(save_dir+'winequality-red.csv', header=1, delimiter=';').values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'yatch':\n",
    "        if not os.path.isfile(save_dir+'yacht_hydrodynamics.data'):\n",
    "            urllib.urlretrieve(\"http://archive.ics.uci.edu/ml/machine-learning-databases/00243/yacht_hydrodynamics.data\",\n",
    "                               filename=save_dir+'yacht_hydrodynamics.data')\n",
    "        data = pd.read_csv(save_dir+'yacht_hydrodynamics.data', header=1, delimiter='\\s+').values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'kin8nm':\n",
    "        if not os.path.isfile(save_dir+'dataset_2175_kin8nm.csv'):\n",
    "            urllib.urlretrieve(\"https://www.openml.org/data/get_csv/3626/dataset_2175_kin8nm.csv\",\n",
    "                               filename=save_dir+'dataset_2175_kin8nm.csv')\n",
    "        data = pd.read_csv(save_dir+'dataset_2175_kin8nm.csv', header=1, delimiter=',').values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'kin8nm':\n",
    "        if not os.path.isfile(save_dir+'dataset_2175_kin8nm.csv'):\n",
    "            urllib.urlretrieve(\"https://www.openml.org/data/get_csv/3626/dataset_2175_kin8nm.csv\",\n",
    "                               filename=save_dir+'dataset_2175_kin8nm.csv')\n",
    "        data = pd.read_csv(save_dir+'dataset_2175_kin8nm.csv', header=1, delimiter=',').values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'naval':\n",
    "        if not os.path.isfile(save_dir + 'UCI%20CBM%20Dataset.zip'):\n",
    "            urllib.urlretrieve(\"http://archive.ics.uci.edu/ml/machine-learning-databases/00316/UCI%20CBM%20Dataset.zip\",\n",
    "                               filename=save_dir + 'UCI%20CBM%20Dataset.zip')\n",
    "        zipped = zipfile.ZipFile(save_dir + \"UCI%20CBM%20Dataset.zip\")\n",
    "        data = pd.read_csv(zipped.open('UCI CBM Dataset/data.txt'), header='infer', delimiter=\"\\s+\").values\n",
    "        y_idx = [-2, -1]\n",
    "\n",
    "    elif dset_name == 'protein':\n",
    "        if not os.path.isfile(save_dir+'CASP.csv'):\n",
    "            urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00265/CASP.csv\",\n",
    "                               filename=save_dir+'CASP.csv')\n",
    "        data = pd.read_csv(save_dir+'CASP.csv', header=1, delimiter=',').values\n",
    "        y_idx = [0]\n",
    "\n",
    "    elif dset_name == 'default_credit':\n",
    "        if not os.path.isfile(save_dir + 'default of credit card clients.xls'):\n",
    "            urllib.request.urlretrieve(\n",
    "                \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\",\n",
    "                filename=save_dir + 'default of credit card clients.xls')\n",
    "        data = pd.read_excel(save_dir + 'default of credit card clients.xls', header=[0, 1], index_col=0, # delimiter=\"\\s+\"\n",
    "                             ).values\n",
    "        y_idx = [-1]  # OK\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise Exception('Dataset name doesnt match any known datasets.')\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    data = data[np.random.permutation(np.arange(len(data)))]\n",
    "\n",
    "    kf = KFold(n_splits=splits)\n",
    "    for j, (train_index, test_index) in enumerate(kf.split(data)):\n",
    "\n",
    "        if separate_targets:\n",
    "            x_idx = list(range(data.shape[1]))\n",
    "            for e in y_idx:\n",
    "                x_idx.remove(x_idx[e])\n",
    "\n",
    "            x_idx = np.array(x_idx)\n",
    "            y_idx = np.array(y_idx)\n",
    "            x_train, y_train = data[train_index, :], data[train_index, :]\n",
    "            x_train, y_train = x_train[:, x_idx], y_train[:, y_idx]\n",
    "            x_test, y_test = data[test_index, :], data[test_index, :]\n",
    "            x_test, y_test = x_test[:, x_idx], y_test[:, y_idx]\n",
    "\n",
    "            x_means, x_stds = x_train.mean(axis=0), x_train.std(axis=0)\n",
    "            y_means, y_stds = y_train.mean(axis=0), y_train.std(axis=0)\n",
    "\n",
    "            y_stds[y_stds < 1e-10] = 1\n",
    "            x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "            x_train = ((x_train - x_means) / x_stds).astype(np.float32)\n",
    "            y_train = ((y_train - y_means) / y_stds).astype(np.float32)\n",
    "\n",
    "            x_test = ((x_test - x_means) / x_stds).astype(np.float32)\n",
    "            y_test = ((y_test - y_means) / y_stds).astype(np.float32)\n",
    "\n",
    "            return x_train, x_test, x_means, x_stds, y_train, y_test, y_means, y_stds\n",
    "\n",
    "        else:\n",
    "            x_train, x_test = data[train_index, :], data[test_index, :]\n",
    "            x_means, x_stds = x_train.mean(axis=0), x_train.std(axis=0)\n",
    "\n",
    "            x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "            x_train = ((x_train - x_means) / x_stds).astype(np.float32)\n",
    "            x_test = ((x_test - x_means) / x_stds).astype(np.float32)\n",
    "\n",
    "            return x_train, x_test, x_means, x_stds\n",
    "\n",
    "def check_data_file(fname):\n",
    "    files = os.listdir(\".\")  # get the current directory listing\n",
    "    print\n",
    "    \"Looking for file '%s' in the current directory...\" % fname\n",
    "\n",
    "    if fname not in files:\n",
    "        print\n",
    "        \"'%s' not found! Downloading from GitHub...\" % fname\n",
    "        addr = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
    "        \n",
    "        response = urllib.request.urlopen(addr)\n",
    "\n",
    "        data = response.read()\n",
    "        fileOut = open(fname, \"wb\")\n",
    "        fileOut.write(data)\n",
    "        fileOut.close()\n",
    "        print\n",
    "        \"'%s' download and saved locally..\" % fname\n",
    "    else:\n",
    "        print\n",
    "        \"File found in current directory..\"\n",
    "\n",
    "\n",
    "\n",
    "def get_my_COMPAS(rseed=0, separate_test=True, test_ratio=0.2, save_dir='../data/'):\n",
    "    \"\"\"\n",
    "        The adult dataset can be obtained from: https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\n",
    "        The code will look for the data file in the present directory, if it is not found, it will download them from GitHub.\n",
    "    \"\"\"\n",
    "\n",
    "    SEED = rseed\n",
    "    seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    their_FEATURES_CLASSIFICATION = [\"age_cat\", \"race\", \"sex\", \"priors_count\", \"c_charge_degree\"]\n",
    "    FEATURES_CLASSIFICATION = [\"age_cat\", \"race\", \"sex\", \"c_charge_degree\", \"is_recid\", \"priors_count\",\n",
    "                               \"time_served\"]  # features to be used for classification\n",
    "    CONT_VARIABLES = [\"priors_count\",\n",
    "                      \"time_served\"]  # continuous features, will need to be handled separately from categorical features, categorical features will be encoded using one-hot\n",
    "    CLASS_FEATURE = \"two_year_recid\"  # the decision variable\n",
    "\n",
    "\n",
    "    COMPAS_INPUT_FILE = save_dir + \"compas-scores-two-years.csv\"\n",
    "    check_data_file(COMPAS_INPUT_FILE)\n",
    "\n",
    "    # load the data and get some stats\n",
    "    df = pd.read_csv(COMPAS_INPUT_FILE)\n",
    "    df = df.dropna(subset=[\"days_b_screening_arrest\"])  # dropping missing vals\n",
    "\n",
    "    # convert to np array\n",
    "    data = df.to_dict('list')\n",
    "    for k in data.keys():\n",
    "        data[k] = np.array(data[k])\n",
    "\n",
    "    dates_in = data['c_jail_in']\n",
    "    dates_out = data['c_jail_out']\n",
    "    # this measures time in Jail\n",
    "    time_served = []\n",
    "    for i in range(len(dates_in)):\n",
    "        di = datetime.datetime.strptime(dates_in[i], '%Y-%m-%d %H:%M:%S')\n",
    "        do = datetime.datetime.strptime(dates_out[i], '%Y-%m-%d %H:%M:%S')\n",
    "        time_served.append((do - di).days)\n",
    "    time_served = np.array(time_served)\n",
    "    time_served[time_served < 0] = 0\n",
    "    data[\"time_served\"] = time_served\n",
    "\n",
    "    \"\"\" Filtering the data \"\"\"\n",
    "\n",
    "    # These filters are the same as propublica (refer to https://github.com/propublica/compas-analysis)\n",
    "    # If the charge date of a defendants Compas scored crime was not within 30 days from when the person was arrested, we assume that because of data quality reasons, that we do not have the right offense.\n",
    "    idx = np.logical_and(data[\"days_b_screening_arrest\"] <= 30, data[\"days_b_screening_arrest\"] >= -30)\n",
    "\n",
    "    # We coded the recidivist flag -- is_recid -- to be -1 if we could not find a compas case at all.\n",
    "    idx = np.logical_and(idx, data[\"is_recid\"] != -1)\n",
    "\n",
    "    # In a similar vein, ordinary traffic offenses -- those with a c_charge_degree of 'O' -- will not result in Jail time are removed (only two of them).\n",
    "    idx = np.logical_and(idx, data[\"c_charge_degree\"] != \"O\")  # F: felony, M: misconduct\n",
    "\n",
    "    # We filtered the underlying data from Broward county to include only those rows representing people who had either recidivated in two years, or had at least two years outside of a correctional facility.\n",
    "    idx = np.logical_and(idx, data[\"score_text\"] != \"NA\")\n",
    "\n",
    "    # select the examples that satisfy this criteria\n",
    "    for k in data.keys():\n",
    "        data[k] = data[k][idx]\n",
    "\n",
    "    y = data[CLASS_FEATURE]\n",
    "    \"\"\" Feature normalization and one hot encoding \"\"\"\n",
    "\n",
    "    print\n",
    "    \"\\nNumber of people recidivating within two years\"\n",
    "    print\n",
    "    pd.Series(y).value_counts()\n",
    "    print\n",
    "    \"\\n\"\n",
    "\n",
    "    X = []  # empty array with num rows same as num examples, will hstack the features to it\n",
    "    X_dims = []\n",
    "\n",
    "    feature_names = []\n",
    "    for attr in FEATURES_CLASSIFICATION:\n",
    "        vals = data[attr]\n",
    "        if attr in CONT_VARIABLES:\n",
    "            vals = [float(v) for v in vals]\n",
    "            # vals = preprocessing.scale(vals, axis=0, with_mean=True, with_std=True)  # 0 mean and 1 variance\n",
    "            vals = np.reshape(vals, (len(y), -1))  # convert from 1-d arr to a 2-d arr with one col\n",
    "            X_dims.append(1)\n",
    "\n",
    "        else:  # for binary categorical variables, the label binarizer uses just one var instead of two\n",
    "            enc = preprocessing.OneHotEncoder(categories='auto', handle_unknown='error')\n",
    "            enc.fit(vals.reshape(-1, 1))\n",
    "            vals = enc.transform(vals.reshape(-1, 1)).todense()\n",
    "            X_dims += [vals.shape[1]]*vals.shape[1]\n",
    "\n",
    "        # add to learnable features\n",
    "        X.append(vals)\n",
    "\n",
    "        if attr in CONT_VARIABLES:  # continuous feature, just append the name\n",
    "            feature_names.append(attr)\n",
    "        else:  # categorical features\n",
    "            if vals.shape[1] == 1:  # binary features that passed through lib binarizer\n",
    "                feature_names.append(attr)\n",
    "            else:\n",
    "                for k in enc.categories_:  # non-binary categorical features, need to add the names for each cat\n",
    "                    feature_names.append(attr + \"_\" + str(k))\n",
    "\n",
    "    X = np.array(np.concatenate(list(X), axis=1))\n",
    "    X_dims = np.array(X_dims)\n",
    "\n",
    "    if separate_test:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=rseed, shuffle=True)\n",
    "\n",
    "        x_means, x_stds = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "        x_means[X_dims>1] = 0\n",
    "        x_stds[X_dims>1] = 1\n",
    "        x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "        x_train = ((X_train - x_means) / x_stds).astype(np.float32)\n",
    "        x_test = ((X_test - x_means) / x_stds).astype(np.float32)\n",
    "\n",
    "        return x_train, x_test, x_means, x_stds, y_train, y_test, feature_names, X_dims\n",
    "    else:\n",
    "        x_means, x_stds = X.mean(axis=0), X.std(axis=0)\n",
    "        print(X_dims.shape, x_means.shape)\n",
    "        x_means[:,X_dims>1] = 0\n",
    "        x_stds[:,X_dims>1] = 1\n",
    "        x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "        x_train = ((X - x_means) / x_stds).astype(np.float32)\n",
    "\n",
    "        return x_train, x_means, x_stds, y, feature_names, X_dims\n",
    "\n",
    "\n",
    "def mkdir(paths):\n",
    "    if not isinstance(paths, (list, tuple)):\n",
    "        paths = [paths]\n",
    "    for path in paths:\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path, mode=0o777)\n",
    "            \n",
    "def unnormalise_cat_vars(x, x_means, x_stds, input_dim_vec):\n",
    "    input_dim_vec = np.array(input_dim_vec)\n",
    "    unnorm_x = np.multiply(x, x_stds) + x_means\n",
    "\n",
    "    fixed_unnorm = unnorm_x.round()\n",
    "    # -=\n",
    "    fixed_unnorm = fixed_unnorm - fixed_unnorm.min(axis=0).reshape([1, fixed_unnorm.shape[1]])  # this sets all mins to 0\n",
    "    for idx, dims in enumerate(input_dim_vec):\n",
    "        if dims > 1:\n",
    "            vec = fixed_unnorm[:, idx]\n",
    "            vec[vec > dims - 1] = dims - 1\n",
    "            fixed_unnorm[:, idx] = vec\n",
    "\n",
    "    x[:, input_dim_vec > 1] = fixed_unnorm[:, input_dim_vec > 1]\n",
    "    return x\n",
    "\n",
    "def gauss_cat_to_flat(x, input_dim_vec):\n",
    "    output = []\n",
    "    for idx, dim in enumerate(input_dim_vec):\n",
    "        if dim == 1:\n",
    "            output.append(x[:, idx].unsqueeze(1))\n",
    "        elif dim > 1:\n",
    "            oh_vec = torch_onehot(x[:, idx], dim).type(x.type())\n",
    "            output.append(oh_vec)\n",
    "        else:\n",
    "            raise ValueError('Error, invalid dimension value')\n",
    "    return torch.cat(output, dim=1)\n",
    "\n",
    "\n",
    "def X_dims_to_input_dim_vec(X_dims):\n",
    "    \"\"\"This is for our cat_Gauss VAE model\"\"\"\n",
    "    input_dim_vec = []\n",
    "    i = 0\n",
    "    while i < len(X_dims):\n",
    "        input_dim_vec.append(X_dims[i])\n",
    "        i += X_dims[i]\n",
    "    return np.array(input_dim_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compas (5554, 17) (618, 17)\n",
      "[3 6 2 2 2 1 1]\n"
     ]
    }
   ],
   "source": [
    "# from src.UCI_loader import load_UCI, unnormalise_cat_vars\n",
    "# from src.compas_loader import get_my_COMPAS, X_dims_to_input_dim_vec\n",
    "# from src.LSAT_loader import get_my_LSAT\n",
    "# from src.gauss_cat import gauss_cat_to_flat\n",
    "\n",
    "\n",
    "if dname == 'wine':\n",
    "    x_train, x_test, x_means, x_stds, y_train, y_test, y_means, y_stds = \\\n",
    "    load_UCI(dset_name='wine', splits=10, seed=42, separate_targets=True, save_dir='../data/')\n",
    "    \n",
    "elif dname == 'default_credit':\n",
    "    # Note that this dataset is given without flattening\n",
    "    x_train, x_test, x_means, x_stds, y_train, y_test, y_means, y_stds = \\\n",
    "    load_UCI(dset_name='default_credit', splits=10, seed=42, separate_targets=True, save_dir='../data/')\n",
    "    print('Credit', x_train.shape, x_test.shape)\n",
    "    input_dim_vec = [1, 2, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ,1 ,1 ,1 ,1 ,1]\n",
    "    \n",
    "    x_train = unnormalise_cat_vars(x_train, x_means, x_stds, input_dim_vec)\n",
    "    x_test = unnormalise_cat_vars(x_test, x_means, x_stds, input_dim_vec)\n",
    "    # target unnormalisation\n",
    "    y_train = unnormalise_cat_vars(y_train, y_means, y_stds, [2])\n",
    "    y_test = unnormalise_cat_vars(y_test, y_means, y_stds, [2])\n",
    "    \n",
    "    x_train = gauss_cat_to_flat(torch.Tensor(x_train), input_dim_vec)\n",
    "    x_test = gauss_cat_to_flat(torch.Tensor(x_test), input_dim_vec)\n",
    "\n",
    "    print(input_dim_vec)\n",
    "\n",
    "elif dname == 'compas':\n",
    "    x_train, x_test, x_means, x_stds, y_train, y_test, feature_names, X_dims = \\\n",
    "    get_my_COMPAS(rseed=42, separate_test=True, test_ratio=0.1, save_dir='../data/')\n",
    "    input_dim_vec = X_dims_to_input_dim_vec(X_dims)\n",
    "    print('Compas', x_train.shape, x_test.shape)\n",
    "    print(input_dim_vec)\n",
    "    \n",
    "elif dname == 'lsat':\n",
    "    x_train, x_test, x_means, x_stds, y_train, y_test, y_means, y_stds, my_data_keys, input_dim_vec = \\\n",
    "    get_my_LSAT(save_dir='../data/')\n",
    "    print('LSAT', x_train.shape, x_test.shape)\n",
    "    print(input_dim_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "class Datafeed(data.Dataset):\n",
    "\n",
    "    def __init__(self, x_train, y_train=None, transform=None):\n",
    "        self.data = x_train\n",
    "        self.targets = y_train\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.data[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.targets is not None:\n",
    "            return img, self.targets[index]\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5554, 17) (5554,) (618, 17) (618,)\n"
     ]
    }
   ],
   "source": [
    "# from src.utils import Datafeed\n",
    "\n",
    "trainset = Datafeed(x_train, y_train, transform=None)\n",
    "valset = Datafeed(x_test, y_test, transform=None)\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BNN LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP_gauss(nn.Module):\n",
    "    def __init__(self, input_dim, width, depth, output_dim, flatten_image):\n",
    "        super(MLP_gauss, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.flatten_image = flatten_image\n",
    "\n",
    "#         layers = [nn.Linear(input_dim, width), nn.ReLU()]\n",
    "#         for i in range(depth - 1):\n",
    "#             layers.append(nn.Linear(width, width))\n",
    "#             layers.append(nn.ReLU())\n",
    "#         layers.append(nn.Linear(width, 2*output_dim))\n",
    "        \n",
    "        layers = [nn.Linear(input_dim,2*output_dim)]\n",
    "\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # if self.flatten_image:\n",
    "        #     x = x.view(-1, self.input_dim)\n",
    "        x = self.block(x)\n",
    "        mu = x[:, :self.output_dim]\n",
    "        sigma = F.softplus(x[:, self.output_dim:])\n",
    "        return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNet(object):\n",
    "    def __init__(self):\n",
    "        cprint('c', '\\nNet:')\n",
    "\n",
    "    def get_nb_parameters(self):\n",
    "        return np.sum(np.fromiter((p.numel() for p in self.model.parameters()),int))\n",
    "#         return np.sum(np.fromiter(p.numel() for p in self.model.parameters(),dtype=int))\n",
    "#         k = np.sum()\n",
    "#         print(type(k))\n",
    "        return np.sum(p.numel() for p in self.model.parameters())\n",
    "\n",
    "    def set_mode_train(self, train=True):\n",
    "        if train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "    def update_lr(self, epoch, gamma=0.99):\n",
    "        self.epoch += 1\n",
    "        if self.schedule is not None:\n",
    "            if len(self.schedule) == 0 or epoch in self.schedule:\n",
    "                self.lr *= gamma\n",
    "                print('learning rate: %f  (%d)\\n' % (self.lr, epoch))\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = self.lr\n",
    "\n",
    "#     def save(self, filename):\n",
    "#         cprint('c', 'Writting %s\\n' % filename)\n",
    "#         torch.save({\n",
    "#             'epoch': self.epoch,\n",
    "#             'lr': self.lr,\n",
    "#             'model': self.model,\n",
    "#             'optimizer': self.optimizer}, filename)\n",
    "    def save(self,filename):\n",
    "        cprint('c', 'Writting %s\\n' % filename)\n",
    "        torch.save({\n",
    "            'epoch': self.epoch,\n",
    "            'lr': self.lr,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, filename)\n",
    "\n",
    "#     def load(self, filename):\n",
    "#         cprint('c', 'Reading %s\\n' % filename)\n",
    "#         state_dict = torch.load(filename)\n",
    "#         self.epoch = state_dict['epoch']\n",
    "#         self.lr = state_dict['lr']\n",
    "#         self.model = state_dict['model']\n",
    "#         self.optimizer = state_dict['optimizer']\n",
    "#         print('  restoring epoch: %d, lr: %f' % (self.epoch, self.lr))\n",
    "#         return self.epoch\n",
    "    def load(self,filename):\n",
    "        cprint('c', 'Reading %s\\n' % filename)\n",
    "        state_dict = torch.load(filename)\n",
    "        self.epoch = state_dict['epoch']\n",
    "        self.lr = state_dict['lr']\n",
    "        # self.model = state_dict['model_state_dict']\n",
    "        self.model.load_state_dict(state_dict['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(state_dict['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer\n",
    "\n",
    "\n",
    "class H_SA_SGHMC(Optimizer):\n",
    "    \"\"\" Stochastic Gradient Hamiltonian Monte-Carlo Sampler that uses a burn-in\n",
    "        procedure to adapt its own hyperparameters during the initial stages\n",
    "        of sampling.\"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-2, base_C=0.05, gauss_sig=0.1, alpha0=10, beta0=10):\n",
    "        \"\"\" Set up a SGHMC Optimizer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : iterable\n",
    "            Parameters serving as optimization variable.\n",
    "        lr: float, optional\n",
    "            Base learning rate for this optimizer.\n",
    "            Must be tuned to the specific function being minimized.\n",
    "            Default: `1e-2`.\n",
    "        base_C:float, optional\n",
    "            (Constant) momentum decay per time-step.\n",
    "            Default: `0.05`.\n",
    "        \"\"\"\n",
    "\n",
    "        self.eps = 1e-6\n",
    "        self.alpha0 = alpha0\n",
    "        self.beta0 = beta0\n",
    "\n",
    "        if gauss_sig == 0:\n",
    "            self.weight_decay = 0\n",
    "        else:\n",
    "            self.weight_decay = 1 / (gauss_sig ** 2)\n",
    "\n",
    "        if self.weight_decay <= 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if base_C < 0:\n",
    "            raise ValueError(\"Invalid friction term: {}\".format(base_C))\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            base_C=base_C,\n",
    "        )\n",
    "        super(H_SA_SGHMC, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, burn_in=False, resample_momentum=False, resample_prior=False):\n",
    "        \"\"\"Simulate discretized Hamiltonian dynamics for one step\"\"\"\n",
    "        loss = None\n",
    "\n",
    "        for group in self.param_groups:  # iterate over blocks -> the ones defined in defaults. We dont use groups.\n",
    "            for p in group[\"params\"]:  # these are weight and bias matrices\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state = self.state[p]  # define dict for each individual param\n",
    "                if len(state) == 0:\n",
    "                    state[\"iteration\"] = 0\n",
    "                    state[\"tau\"] = torch.ones_like(p)\n",
    "                    state[\"g\"] = torch.ones_like(p)\n",
    "                    state[\"V_hat\"] = torch.ones_like(p)\n",
    "                    state[\"v_momentum\"] = torch.zeros_like(\n",
    "                        p)  # p.data.new(p.data.size()).normal_(mean=0, std=np.sqrt(group[\"lr\"])) #\n",
    "                    state['weight_decay'] = self.weight_decay\n",
    "\n",
    "                state[\"iteration\"] += 1  # this is kind of useless now but lets keep it provisionally\n",
    "\n",
    "                if resample_prior:\n",
    "                    alpha = self.alpha0 + p.data.nelement() / 2\n",
    "                    beta = self.beta0 + (p.data ** 2).sum().item() / 2\n",
    "                    gamma_sample = gamma(shape=alpha, scale=1 / (beta), size=None)\n",
    "                    #                     print('std', 1/np.sqrt(gamma_sample))\n",
    "                    state['weight_decay'] = gamma_sample\n",
    "\n",
    "                base_C, lr = group[\"base_C\"], group[\"lr\"]\n",
    "                weight_decay = state[\"weight_decay\"]\n",
    "                tau, g, V_hat = state[\"tau\"], state[\"g\"], state[\"V_hat\"]\n",
    "\n",
    "                d_p = p.grad.data\n",
    "                if weight_decay != 0:\n",
    "                    d_p.add_(weight_decay, p.data)\n",
    "\n",
    "                # update parameters during burn-in\n",
    "                if burn_in:  # We update g first as it makes most sense\n",
    "                    tau.add_(-tau * (g ** 2) / (\n",
    "                                V_hat + self.eps) + 1)  # specifies the moving average window, see Eq 9 in [1] left\n",
    "                    tau_inv = 1. / (tau + self.eps)\n",
    "                    g.add_(-tau_inv * g + tau_inv * d_p)  # average gradient see Eq 9 in [1] right\n",
    "                    V_hat.add_(-tau_inv * V_hat + tau_inv * (d_p ** 2))  # gradient variance see Eq 8 in [1]\n",
    "\n",
    "                V_sqrt = torch.sqrt(V_hat)\n",
    "                V_inv_sqrt = 1. / (V_sqrt + self.eps)  # preconditioner\n",
    "\n",
    "                if resample_momentum:  # equivalent to var = M under momentum reparametrisation\n",
    "                    state[\"v_momentum\"] = torch.normal(mean=torch.zeros_like(d_p),\n",
    "                                                       std=torch.sqrt((lr ** 2) * V_inv_sqrt))\n",
    "                v_momentum = state[\"v_momentum\"]\n",
    "\n",
    "                noise_var = (2. * (lr ** 2) * V_inv_sqrt * base_C - (lr ** 4))\n",
    "                noise_std = torch.sqrt(torch.clamp(noise_var, min=1e-16))\n",
    "                # sample random epsilon\n",
    "                noise_sample = torch.normal(mean=torch.zeros_like(d_p), std=torch.ones_like(d_p) * noise_std)\n",
    "\n",
    "                # update momentum (Eq 10 right in [1])\n",
    "                v_momentum.add_(- (lr ** 2) * V_inv_sqrt * d_p - base_C * v_momentum + noise_sample)\n",
    "\n",
    "                # update theta (Eq 10 left in [1])\n",
    "                p.data.add_(v_momentum)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_object(filename):\n",
    "    with open(filename, 'rb') as input:\n",
    "        \n",
    "        from collections import OrderedDict\n",
    "        \n",
    "        samples = pickle.load(input, encoding='bytes')\n",
    "        new_samples = []\n",
    "        for i,weight_dict in enumerate(samples):\n",
    "            new_weight_dict = OrderedDict()\n",
    "            for key in weight_dict:\n",
    "                new_weight_dict[key.decode(\"utf-8\")] = weight_dict[key]\n",
    "            new_samples.append(new_weight_dict)\n",
    "        return new_samples\n",
    "#         try:\n",
    "#             return pickle.load(input)\n",
    "#         except Exception as e:\n",
    "#             try:\n",
    "#                 print(e)\n",
    "#                 return pickle.load(input, encoding=\"latin1\")\n",
    "#             except Exception as a:\n",
    "#                 print(a)\n",
    "#                 return pickle.load(input, encoding='bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch.backends.cudnn as cudnn\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BNN_cat(BaseNet):  # for categorical distributions\n",
    "    def __init__(self, model, N_train, lr=1e-2, cuda=True, grad_std_mul=30, seed=None):\n",
    "        super(BNN_cat, self).__init__()\n",
    "\n",
    "        cprint('y', 'BNN categorical output')\n",
    "        self.lr = lr\n",
    "        self.model = model\n",
    "        self.cuda = cuda\n",
    "        self.seed = seed\n",
    "\n",
    "        self.N_train = N_train\n",
    "        self.create_net()\n",
    "        self.create_opt()\n",
    "        self.schedule = None  # [] #[50,200,400,600]\n",
    "        self.epoch = 0\n",
    "\n",
    "        self.grad_buff = []\n",
    "        self.max_grad = 1e20\n",
    "        self.grad_std_mul = grad_std_mul\n",
    "\n",
    "        self.weight_set_samples = []\n",
    "\n",
    "    def create_net(self):\n",
    "        if self.seed is None:\n",
    "            torch.manual_seed(42)\n",
    "        else:\n",
    "            torch.manual_seed(self.seed)\n",
    "        if self.cuda:\n",
    "            if self.seed is None:\n",
    "                torch.cuda.manual_seed(42)\n",
    "            else:\n",
    "                torch.cuda.manual_seed(self.seed)\n",
    "        if self.cuda:\n",
    "            self.model.cuda()\n",
    "            cudnn.benchmark = True\n",
    "\n",
    "        print('    Total params: %.2fM' % (self.get_nb_parameters() / 1000000.0))\n",
    "\n",
    "    def create_opt(self):\n",
    "        \"\"\"This optimiser incorporates the gaussian prior term automatically. The prior variance is gibbs sampled from\n",
    "        its posterior using a gamma hyper-prior.\"\"\"\n",
    "        self.optimizer = H_SA_SGHMC(params=self.model.parameters(), lr=self.lr, base_C=0.05, gauss_sig=0.1)  # this last parameter does nothing\n",
    "\n",
    "    def fit(self, x, y, burn_in=False, resample_momentum=False, resample_prior=False):\n",
    "        self.set_mode_train(train=True)\n",
    "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "        self.optimizer.zero_grad()\n",
    "        out = self.model(x)\n",
    "        loss = F.cross_entropy(out, y, reduction='mean')\n",
    "        loss = loss * self.N_train  # We use mean because we treat as an estimation of whole dataset\n",
    "        loss.backward()\n",
    "\n",
    "        if len(self.grad_buff) > 1000:\n",
    "            self.max_grad = np.mean(self.grad_buff) + self.grad_std_mul * np.std(self.grad_buff)\n",
    "            self.grad_buff.pop(0)\n",
    "\n",
    "        self.grad_buff.append(nn.utils.clip_grad_norm_(parameters=self.model.parameters(),\n",
    "                                                       max_norm=self.max_grad, norm_type=2))\n",
    "        if self.grad_buff[-1] >= self.max_grad:\n",
    "            print(self.max_grad, self.grad_buff[-1])\n",
    "            self.grad_buff.pop()\n",
    "        self.optimizer.step(burn_in=burn_in, resample_momentum=resample_momentum, resample_prior=resample_prior)\n",
    "\n",
    "        # out: (batch_size, out_channels, out_caps_dims)\n",
    "        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
    "        err = pred.ne(y.data).sum()\n",
    "\n",
    "        return loss.data * x.shape[0] / self.N_train, err\n",
    "\n",
    "    def eval(self, x, y, train=False):\n",
    "        self.set_mode_train(train=False)\n",
    "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "\n",
    "        out = self.model(x)\n",
    "        loss = F.cross_entropy(out, y, reduction='sum')\n",
    "        probs = F.softmax(out, dim=1).data.cpu()\n",
    "\n",
    "        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
    "        err = pred.ne(y.data).sum()\n",
    "\n",
    "        return loss.data, err, probs\n",
    "\n",
    "    def save_sampled_net(self, max_samples):\n",
    "\n",
    "        if len(self.weight_set_samples) >= max_samples:\n",
    "            self.weight_set_samples.pop(0)\n",
    "\n",
    "        self.weight_set_samples.append(copy.deepcopy(self.model.state_dict()))\n",
    "\n",
    "        cprint('c', ' saving weight samples %d/%d' % (len(self.weight_set_samples), max_samples))\n",
    "        return None\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.set_mode_train(train=False)\n",
    "        x, = to_variable(var=(x, ), cuda=self.cuda)\n",
    "        out = self.model(x)\n",
    "        probs = F.softmax(out, dim=1).data.cpu()\n",
    "        return probs.data\n",
    "\n",
    "    def sample_predict(self, x, Nsamples, grad=False):\n",
    "        \"\"\"return predictions using multiple samples from posterior\"\"\"\n",
    "        self.set_mode_train(train=False)\n",
    "        if Nsamples == 0:\n",
    "            Nsamples = len(self.weight_set_samples)\n",
    "        x, = to_variable(var=(x, ), cuda=self.cuda)\n",
    "\n",
    "        if grad:\n",
    "            self.optimizer.zero_grad()\n",
    "            if not x.requires_grad:\n",
    "                x.requires_grad = True\n",
    "\n",
    "        out = x.data.new(Nsamples, x.shape[0], self.model.output_dim)\n",
    "\n",
    "        # iterate over all saved weight configuration samples\n",
    "        for idx, weight_dict in enumerate(self.weight_set_samples):\n",
    "            if idx == Nsamples:\n",
    "                break\n",
    "            self.model.load_state_dict(weight_dict)\n",
    "            out[idx] = self.model(x)\n",
    "\n",
    "        out = out[:idx]\n",
    "        prob_out = F.softmax(out, dim=2)\n",
    "\n",
    "        if grad:\n",
    "            return prob_out\n",
    "        else:\n",
    "            return prob_out.data\n",
    "\n",
    "    def get_weight_samples(self, Nsamples=0):\n",
    "        \"\"\"return weight samples from posterior in a single-column array\"\"\"\n",
    "        weight_vec = []\n",
    "\n",
    "        if Nsamples == 0 or Nsamples > len(self.weight_set_samples):\n",
    "            Nsamples = len(self.weight_set_samples)\n",
    "\n",
    "        for idx, state_dict in enumerate(self.weight_set_samples):\n",
    "            if idx == Nsamples:\n",
    "                break\n",
    "\n",
    "            for key in state_dict.keys():\n",
    "                if 'weight' in key:\n",
    "                    weight_mtx = state_dict[key].cpu().data\n",
    "                    for weight in weight_mtx.view(-1):\n",
    "                        weight_vec.append(weight)\n",
    "\n",
    "        return np.array(weight_vec)\n",
    "\n",
    "    def save_weights(self, filename):\n",
    "        save_object(self.weight_set_samples, filename)\n",
    "\n",
    "    def load_weights(self, filename, subsample=1):\n",
    "        self.weight_set_samples = load_object(filename)\n",
    "        self.weight_set_samples = self.weight_set_samples[::subsample]\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, width, depth, output_dim, flatten_image):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.flatten_image = flatten_image\n",
    "\n",
    "        layers = [nn.Linear(input_dim, width), nn.ReLU(inplace=True)]\n",
    "        for i in range(depth - 1):\n",
    "            layers.append(nn.Linear(width, width))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        layers.append(nn.Linear(width, output_dim))\n",
    "\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # if self.flatten_image:\n",
    "        #    x = x.view(-1, self.input_dim)\n",
    "        return self.block(x)\n",
    "\n",
    "def cprint(color, text, **kwargs):\n",
    "    if color[0] == '*':\n",
    "        pre_code = '1;'\n",
    "        color = color[1:]\n",
    "    else:\n",
    "        pre_code = ''\n",
    "    code = {\n",
    "        'a': '30',\n",
    "        'r': '31',\n",
    "        'g': '32',\n",
    "        'y': '33',\n",
    "        'b': '34',\n",
    "        'p': '35',\n",
    "        'c': '36',\n",
    "        'w': '37'\n",
    "    }\n",
    "    print(\"\\x1b[%s%sm%s\\x1b[0m\" % (pre_code, code[color], text), **kwargs)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\n",
      "Net:\u001b[0m\n",
      "\u001b[33mBNN categorical output\u001b[0m\n",
      "    Total params: 0.04M\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import division\n",
    "# %matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "# from BNN.models import MLP_gauss\n",
    "# from BNN.wrapper import BNN_gauss, BNN_cat, MLP\n",
    "import numpy as np\n",
    "\n",
    "if regression_bools[names.index(dname)]:\n",
    "    \n",
    "    input_dim = x_train.shape[1]\n",
    "    width = bnn_widths[names.index(dname)]\n",
    "    depth = bnn_depths[names.index(dname)]\n",
    "    output_dim = y_train.shape[1]\n",
    "    model = MLP_gauss(input_dim, width, depth, output_dim, flatten_image=False)\n",
    "\n",
    "    N_train = x_train.shape[0]\n",
    "    lr = 1e-2\n",
    "    cuda = torch.cuda.is_available()\n",
    "    BNN = BNN_gauss(model, N_train, lr=lr, cuda=cuda)\n",
    "    \n",
    "else:\n",
    "    N_train = x_train.shape[0]\n",
    "    input_dim = x_train.shape[1]\n",
    "    width = bnn_widths[names.index(dname)]\n",
    "    depth = bnn_depths[names.index(dname)]\n",
    "    output_dim = 2\n",
    "    model = MLP(input_dim, width, depth, output_dim, flatten_image=False)\n",
    "\n",
    "    lr = 1e-2\n",
    "    cuda = torch.cuda.is_available()\n",
    "    BNN = BNN_cat(model, N_train, lr=lr, cuda=cuda)\n",
    "\n",
    "\n",
    "\n",
    "save_dir = '../saves/fc_BNN_NEW_' + dname\n",
    "\n",
    "BNN.load_weights(save_dir + '_models/state_dicts.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dir = '../saves/fc_BNN_NEW_' + dname\n",
    "\n",
    "# BNN.load_weights(save_dir + '_models/state_dicts.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_set_samples = load_object(filename)\n",
    "\n",
    "# Nsamples = len(weight_set_samples)\n",
    "\n",
    "# BNN.sample_predict(x, Nsamples, grad=False):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_set_samples = load_object(save_dir + '_models/state_dicts.pkl')\n",
    "# Nsamples = len(weight_set_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = \"../saves/fc_BNN_NEW_compas_models/state_dicts.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.load(open(filename, 'rb'), encoding='bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_object(filename):\n",
    "#     with open(filename, 'rb') as input:\n",
    "        \n",
    "#         from collections import OrderedDict\n",
    "        \n",
    "#         samples = pickle.load(input, encoding='bytes')\n",
    "#         new_samples = []\n",
    "#         for i,weight_dict in enumerate(samples):\n",
    "#             new_weight_dict = OrderedDict()\n",
    "#             for key in weight_dict:\n",
    "#                 new_weight_dict[key.decode(\"utf-8\")] = weight_dict[key]\n",
    "#             new_samples.append(new_weight_dict)\n",
    "#         return new_samples\n",
    "#         # return pickle.load(input, encoding='bytes')\n",
    "# #         try:\n",
    "# #             return pickle.load(input)\n",
    "# #         except Exception as e:\n",
    "# #             try:\n",
    "# #                 print(e)\n",
    "# #                 return pickle.load(input, encoding=\"latin1\")\n",
    "# #             except Exception as a:\n",
    "# #                 print(a)\n",
    "# #                 return pickle.load(input, encoding='bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = load_object(filename)\n",
    "# samples = samples[::1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = BNN.weight_set_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### It's because you're opening the file in bytes mode, and so you're calling bytes.startswith() and not str.startswith()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import OrderedDict\n",
    "\n",
    "# new_weight_dict = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import OrderedDict\n",
    "\n",
    "# new_samples = [] # samples.copy()\n",
    "# for i,weight_dict in enumerate(samples):\n",
    "#     new_weight_dict = OrderedDict() #weight_dict.copy()\n",
    "#     for key in weight_dict:\n",
    "#         new_weight_dict[key.decode(\"utf-8\")] = weight_dict[key] #new_weight_dict.pop(key)\n",
    "#     new_samples.append(new_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ADDED STUFF\n",
    "\n",
    "def selective_softmax(x, input_dim_vec, grad=False, cat_probs=False, prob_sample=False, eps=1e-20):\n",
    "    \"\"\"Applies softmax operation to specified dimensions. Gradient estimator is optional.\n",
    "    cat_probs returns probability vectors over categorical variables instead of maxing\n",
    "    if cat_probs is activated with prob sample, a one-hot vector will be sampled (reparametrisable)\"\"\"\n",
    "    output = torch.zeros_like(x)\n",
    "    cum_dims = 0\n",
    "    for idx, dim in enumerate(input_dim_vec):\n",
    "        if dim == 1:\n",
    "            output[:, cum_dims] = x[:, cum_dims]\n",
    "            if prob_sample:  # this assumes an rms loss when training\n",
    "                noise = x.new_zeros(x.shape[0]).normal_(mean=0, std=1)\n",
    "                output[:, cum_dims] = output[:, cum_dims] + noise\n",
    "            cum_dims += 1\n",
    "        elif dim > 1:\n",
    "            if not cat_probs:\n",
    "                if not grad:\n",
    "                    y = x[:, cum_dims:cum_dims + dim].max(dim=1)[1]\n",
    "                    y_vec = torch_onehot(y, dim).type(x.type())\n",
    "                    output[:, cum_dims:cum_dims + dim] = y_vec\n",
    "                else:\n",
    "                    x_cat = x[:, cum_dims:cum_dims + dim]\n",
    "                    probs = F.softmax(x_cat, dim=1)\n",
    "                    y_hard = x[:, cum_dims:cum_dims + dim].max(dim=1)[1]\n",
    "                    y_oh = torch_onehot(y_hard, dim).type(x.type())\n",
    "                    output[:, cum_dims:cum_dims + dim] = (y_oh - probs).detach() + probs\n",
    "            else:\n",
    "                x_cat = x[:, cum_dims:cum_dims + dim]\n",
    "                probs = F.softmax(x_cat, dim=1)\n",
    "\n",
    "                if prob_sample:  # we are going to use gumbel trick here\n",
    "                    log_probs = torch.log(probs)\n",
    "                    u = log_probs.new(log_probs.shape).uniform_(0, 1)\n",
    "                    g = -torch.log(-torch.log(u + eps) + eps)\n",
    "                    cat_samples = (log_probs + g).max(dim=1)[1]\n",
    "                    hard_samples = torch_onehot(cat_samples, dim).type(x.type())\n",
    "                    output[:, cum_dims:cum_dims + dim] = hard_samples\n",
    "                else:\n",
    "                    output[:, cum_dims:cum_dims + dim] = probs\n",
    "\n",
    "\n",
    "            cum_dims += dim\n",
    "        else:\n",
    "            raise ValueError('Error, invalid dimension value')\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss,CrossEntropyLoss\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "\n",
    "def torch_onehot(y, Nclass):\n",
    "    if y.is_cuda:\n",
    "        y = y.type(torch.cuda.LongTensor)\n",
    "    else:\n",
    "        y = y.type(torch.LongTensor)\n",
    "    y_onehot = torch.zeros((y.shape[0], Nclass)).type(y.type())\n",
    "    # In your for loop\n",
    "    y_onehot.scatter_(1, y.unsqueeze(1), 1)\n",
    "    return y_onehot\n",
    "\n",
    "\n",
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = math.sqrt(\n",
    "                            (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (\n",
    "                                        N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss\n",
    "\n",
    "class rms_cat_loglike(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim_vec, reduction='none'):\n",
    "        super(rms_cat_loglike, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        self.input_dim_vec = input_dim_vec\n",
    "        self.mse = MSELoss(reduction='none')  # takes(input, target)\n",
    "        self.ce = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def forward(self, x, y):\n",
    "\n",
    "        log_prob_vec = []\n",
    "        cum_dims = 0\n",
    "        for idx, dims in enumerate(self.input_dim_vec):\n",
    "            if dims == 1:\n",
    "                # Gaussian_case\n",
    "                log_prob_vec.append(-self.mse(x[:, cum_dims], y[:, idx]).unsqueeze(1))\n",
    "                cum_dims += 1\n",
    "\n",
    "            elif dims > 1:\n",
    "                if x.shape[1] == y.shape[1]:\n",
    "                    raise Exception('Input and target seem to be in flat format. Need integer cat targets.')\n",
    "                                \n",
    "                if y.is_cuda:\n",
    "                    tget = y[:, idx].type(torch.cuda.LongTensor)\n",
    "                else:\n",
    "                    tget = y[:, idx].type(torch.LongTensor)\n",
    "\n",
    "                log_prob_vec.append(-self.ce(x[:, cum_dims:cum_dims + dims], tget).unsqueeze(1))\n",
    "                cum_dims += dims\n",
    "\n",
    "            else:\n",
    "                raise ValueError('Error, invalid dimension value')\n",
    "\n",
    "        log_prob_vec = torch.cat(log_prob_vec, dim=1)\n",
    "\n",
    "        if self.reduction == 'none':\n",
    "            return log_prob_vec\n",
    "        elif self.reduction == 'sum':\n",
    "            return log_prob_vec.sum()\n",
    "        elif self.reduction == 'average':\n",
    "            return log_prob_vec.mean()\n",
    "\n",
    "class MLP_preact_generator_net(nn.Module):\n",
    "    def __init__(self, input_dim, width, depth, latent_dim):\n",
    "        super(MLP_preact_generator_net, self).__init__()\n",
    "        # input layer\n",
    "        generative_layers = [nn.Linear(latent_dim, width), nn.LeakyReLU(inplace=False), nn.BatchNorm1d(num_features=width)]\n",
    "        # body\n",
    "        for i in range(depth-1):\n",
    "            generative_layers.append(\n",
    "                    # skip-connection from prior network to generative network\n",
    "                    preact_leaky_MLPBlock(width))  ## *dependency\n",
    "        # output layer\n",
    "        generative_layers.extend([\n",
    "            nn.Linear(width,\n",
    "                      input_dim),\n",
    "        ])\n",
    "        self.block = nn.Sequential(*generative_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class SkipConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    Skip-connection over the sequence of layers in the constructor.\n",
    "    The module passes input data sequentially through these layers\n",
    "    and then adds original data to the result.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        super(SkipConnection, self).__init__()\n",
    "        self.inner_net = nn.Sequential(*args)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input + self.inner_net(input)\n",
    "\n",
    "def preact_leaky_MLPBlock(width):\n",
    "    return SkipConnection(\n",
    "        nn.LeakyReLU(inplace=False),\n",
    "        nn.BatchNorm1d(num_features=width),\n",
    "        nn.Linear(width, width),\n",
    "        )\n",
    "\n",
    "class MLP_preact_recognition_net(nn.Module):\n",
    "    def __init__(self, input_dim, width, depth, latent_dim):\n",
    "        super(MLP_preact_recognition_net, self).__init__()\n",
    "        # input layer\n",
    "        proposal_layers = [nn.Linear(input_dim, width)]\n",
    "        # body\n",
    "        for i in range(depth-1):\n",
    "            proposal_layers.append(\n",
    "                preact_leaky_MLPBlock(width)) ## *dependency\n",
    "        # output layer\n",
    "        proposal_layers.extend(\n",
    "            [nn.LeakyReLU(inplace=False), nn.BatchNorm1d(num_features=width),\n",
    "            nn.Linear(width, latent_dim * 2)])\n",
    "\n",
    "        self.block = nn.Sequential(*proposal_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class VAE_gauss_cat(nn.Module):\n",
    "    def __init__(self, input_dim_vec, width, depth, latent_dim, pred_sig=False):\n",
    "        super(VAE_gauss_cat, self).__init__()\n",
    "\n",
    "        input_dim = 0\n",
    "        self.input_dim_vec = input_dim_vec\n",
    "        for e in input_dim_vec:\n",
    "            input_dim += e\n",
    "        \n",
    "        self.encoder = MLP_preact_recognition_net(input_dim, width, depth, latent_dim) ## *dependency\n",
    "        if pred_sig:\n",
    "            raise NotImplementedError()\n",
    "            # self.decoder = generator_net(2*input_dim, width, depth, latent_dim)\n",
    "            # self.rec_loglike = GaussianLoglike(min_sigma=1e-2)\n",
    "        else:\n",
    "            self.decoder = MLP_preact_generator_net(input_dim, width, depth, latent_dim)\n",
    "            self.rec_loglike = rms_cat_loglike(self.input_dim_vec, reduction='none') ## *dependency\n",
    "        self.pred_sig = pred_sig\n",
    "        \n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Works with flattened representATION\"\"\"\n",
    "        approx_post_params = self.encoder(x)\n",
    "        approx_post = normal_parse_params(approx_post_params, 1e-3)\n",
    "        return approx_post\n",
    "\n",
    "    def decode(self, z_sample):\n",
    "        \"\"\"Works with flattened representATION\"\"\"\n",
    "        rec_params = self.decoder(z_sample)\n",
    "        return rec_params\n",
    "\n",
    "    def vlb(self, prior, approx_post, x, rec_params):\n",
    "        \"\"\"Works with flattened representATION\"\"\"\n",
    "        if self.pred_sig:\n",
    "            pass\n",
    "        else:\n",
    "            rec = self.rec_loglike(rec_params, x).view(x.shape[0], -1).sum(-1)\n",
    "        kl = kl_divergence(approx_post, prior).view(x.shape[0], -1).sum(-1)\n",
    "        return rec - kl\n",
    "\n",
    "    def iwlb(self, prior, approx_post, x, K=50):\n",
    "        estimates = []\n",
    "        for i in range(K):\n",
    "            latent = approx_post.rsample()\n",
    "            rec_params = self.decode(latent)\n",
    "            if self.pred_sig:\n",
    "                pass\n",
    "            else:\n",
    "                rec_loglike = self.rec_loglike(rec_params, x).view(x.shape[0], -1).sum(-1)\n",
    "\n",
    "            prior_log_prob = prior.log_prob(latent)\n",
    "            prior_log_prob = prior_log_prob.view(x.shape[0], -1)\n",
    "            prior_log_prob = prior_log_prob.sum(-1)\n",
    "\n",
    "            proposal_log_prob = approx_post.log_prob(latent)\n",
    "            proposal_log_prob = proposal_log_prob.view(x.shape[0], -1)\n",
    "            proposal_log_prob = proposal_log_prob.sum(-1)\n",
    "\n",
    "            estimate = rec_loglike + prior_log_prob - proposal_log_prob\n",
    "            estimates.append(estimate[:, None])\n",
    "\n",
    "        return torch.logsumexp(torch.cat(estimates, 1), 1) - np.log(K)\n",
    "\n",
    "class VAE_gauss_cat_net(BaseNet):\n",
    "    def __init__(self, input_dim_vec, width, depth, latent_dim, pred_sig=False, lr=1e-3, cuda=True, flatten=True):\n",
    "        super(VAE_gauss_cat_net, self).__init__()\n",
    "        cprint('y', 'VAE_gauss_net')\n",
    "\n",
    "        self.cuda = cuda\n",
    "        self.input_dim = 0\n",
    "        self.input_dim_vec = input_dim_vec\n",
    "        for e in self.input_dim_vec:\n",
    "            self.input_dim += e\n",
    "        self.flatten = flatten\n",
    "        if not self.flatten:\n",
    "            pass\n",
    "            # raise Exception('Error calculation not supported without flattening')\n",
    "\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.latent_dim = latent_dim\n",
    "        self.lr = lr\n",
    "        self.pred_sig = pred_sig\n",
    "        \n",
    "        # Here create the network\n",
    "        self.create_net()\n",
    "        \n",
    "        # Here create the optimizer\n",
    "        self.create_opt()\n",
    "        \n",
    "        self.epoch = 0\n",
    "        self.schedule = None\n",
    "\n",
    "        if self.cuda:\n",
    "            self.prior = self.prior = Normal(loc=torch.zeros(latent_dim).cuda(), scale=torch.ones(latent_dim).cuda())\n",
    "        else:\n",
    "            self.prior = Normal(loc=torch.zeros(latent_dim), scale=torch.ones(latent_dim))\n",
    "        self.vlb_scale = 1 / len(self.input_dim_vec)  # scale for dimensions of input so we can use same LR always\n",
    "\n",
    "    def create_net(self):\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed(42)\n",
    "        self.model = VAE_gauss_cat(self.input_dim_vec, self.width, self.depth, self.latent_dim, self.pred_sig)\n",
    "        if self.cuda:\n",
    "            self.model = self.model.cuda()\n",
    "            cudnn.benchmark = True\n",
    "        print('    Total params: %.2fM' % (self.get_nb_parameters() / 1000000.0))\n",
    "\n",
    "    def create_opt(self):\n",
    "        self.optimizer = RAdam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def fit(self, x):\n",
    "        self.set_mode_train(train=True)\n",
    "\n",
    "        if self.flatten:\n",
    "            x_flat = gauss_cat_to_flat(x, self.input_dim_vec)\n",
    "        else:\n",
    "            x_flat = x\n",
    "            x = flat_to_gauss_cat(x, self.input_dim_vec)\n",
    "\n",
    "        x, x_flat = to_variable(var=(x, x_flat), cuda=self.cuda)\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        approx_post = self.model.encode(x_flat)\n",
    "        z_sample = approx_post.rsample()\n",
    "        rec_params = self.model.decode(z_sample)\n",
    "\n",
    "        vlb = self.model.vlb(self.prior, approx_post, x, rec_params)\n",
    "        loss = (- vlb * self.vlb_scale).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return vlb.mean().item(), rec_params\n",
    "\n",
    "    def eval(self, x, sample=False):\n",
    "        self.set_mode_train(train=False)\n",
    "\n",
    "        if self.flatten:\n",
    "            x_flat = gauss_cat_to_flat(x, self.input_dim_vec)\n",
    "        else:\n",
    "            x_flat = x\n",
    "            x = flat_to_gauss_cat(x, self.input_dim_vec)\n",
    "\n",
    "        x, x_flat = to_variable(var=(x, x_flat), cuda=self.cuda)\n",
    "        approx_post = self.model.encode(x_flat)\n",
    "        if sample:\n",
    "            z_sample = approx_post.sample()\n",
    "        else:\n",
    "            z_sample = approx_post.loc\n",
    "        rec_params = self.model.decode(z_sample)\n",
    "\n",
    "        vlb = self.model.vlb(self.prior, approx_post, x, rec_params)\n",
    "\n",
    "        return vlb.mean().item(), rec_params\n",
    "\n",
    "    def eval_iw(self, x, k=50):\n",
    "        self.set_mode_train(train=False)\n",
    "        if self.flatten:\n",
    "            x_flat = gauss_cat_to_flat(x, self.input_dim_vec)\n",
    "        else:\n",
    "            x_flat = x\n",
    "            x = flat_to_gauss_cat(x, self.input_dim_vec)\n",
    "\n",
    "        x, x_flat = to_variable(var=(x, x_flat), cuda=self.cuda)\n",
    "\n",
    "        approx_post = self.model.recognition_encode(x)\n",
    "\n",
    "        iw_lb = self.model.iwlb(self.prior, approx_post, x, k)\n",
    "        return iw_lb.mean().item()\n",
    "\n",
    "    def recongnition(self, x, grad=False, flatten=None):\n",
    "        if flatten is None:\n",
    "            flatten = self.flatten\n",
    "        if flatten and grad:\n",
    "            raise Exception('flatten and grad options are not compatible')\n",
    "        self.set_mode_train(train=False)\n",
    "        if flatten:\n",
    "            x = gauss_cat_to_flat(x, self.input_dim_vec)\n",
    "        if grad:\n",
    "            if not x.requires_grad:\n",
    "                x.requires_grad = True\n",
    "        else:\n",
    "            x, = to_variable(var=(x,), volatile=True, cuda=self.cuda)\n",
    "        approx_post = self.model.encode(x)\n",
    "        return approx_post\n",
    "\n",
    "    def regenerate(self, z, grad=False, unflatten=False):\n",
    "        if unflatten and grad:\n",
    "            raise Exception('flatten and grad options are not compatible')\n",
    "        self.set_mode_train(train=False)\n",
    "        if grad:\n",
    "            if not z.requires_grad:\n",
    "                z.requires_grad = True\n",
    "        else:\n",
    "            z, = to_variable(var=(z,), volatile=True, cuda=self.cuda)\n",
    "        out = self.model.decode(z)\n",
    "\n",
    "        if unflatten:\n",
    "            out = flat_to_gauss_cat(out, self.input_dim_vec)\n",
    "        else:\n",
    "            out = selective_softmax(out, self.input_dim_vec, grad=grad)\n",
    "\n",
    "        if self.pred_sig:\n",
    "            raise Exception('Not implemented')\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gauss_cat\n",
      "\u001b[36m\n",
      "Net:\u001b[0m\n",
      "\u001b[33mVAE_gauss_net\u001b[0m\n",
      "    Total params: 0.38M\n",
      "\u001b[36mReading ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# from src.UCI_loader import load_UCI\n",
    "# import torch\n",
    "# from torchvision import datasets, transforms\n",
    "# from VAE.fc_gauss import VAE_gauss_net\n",
    "# from VAE.fc_gauss_cat import VAE_gauss_cat_net\n",
    "# from VAE.train import train_VAE\n",
    "# from src.utils import Datafeed\n",
    "\n",
    "\n",
    "width = vae_widths[names.index(dname)]\n",
    "depth = vae_depths[names.index(dname)] # number of hidden layers\n",
    "latent_dim = vae_latent_dims[names.index(dname)]\n",
    "\n",
    "if not gauss_cat_vae_bools[names.index(dname)]:\n",
    "    input_dim = x_train.shape[1]\n",
    "    lr = 1e-4\n",
    "    cuda = torch.cuda.is_available()\n",
    "    VAE = VAE_gauss_net(input_dim, width, depth, latent_dim, pred_sig=False, lr=lr, cuda=cuda)\n",
    "    \n",
    "else:\n",
    "    print('gauss_cat')\n",
    "    cuda = torch.cuda.is_available()\n",
    "    lr = 1e-4\n",
    "    VAE = VAE_gauss_cat_net(input_dim_vec, width, depth, latent_dim, pred_sig=False,\n",
    "                            lr=lr, cuda=cuda, flatten=flat_vae_bools[names.index(dname)])\n",
    "\n",
    "save_dir = '../saves/fc_preact_VAE_NEW(300)_' + dname\n",
    "# VAE.load(save_dir + '_models/theta_best.dat')\n",
    "VAE.load(save_dir + '_models/theta_best.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map to latent space + get uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.nn.functional import softplus\n",
    "\n",
    "def decompose_entropy_cat(probs, eps=1e-10):\n",
    "    # probs (Nsamples, batch_size, classes)\n",
    "    posterior_preds = probs.mean(dim=0, keepdim=False)\n",
    "    total_entropy = -(posterior_preds * torch.log(posterior_preds + eps)).sum(dim=1, keepdim=False)\n",
    "\n",
    "    sample_preds_entropy = -(probs * torch.log(probs + eps)).sum(dim=2, keepdim=False)\n",
    "    aleatoric_entropy = sample_preds_entropy.mean(dim=0, keepdim=False)\n",
    "\n",
    "    epistemic_entropy = total_entropy - aleatoric_entropy\n",
    "\n",
    "    # returns (batch_size)\n",
    "    return total_entropy, aleatoric_entropy, epistemic_entropy\n",
    "\n",
    "\n",
    "def latent_project_cat(BNN, VAE, dset, batch_size=1024, cuda=True, prob_BNN=True):\n",
    "    if cuda:\n",
    "        loader = torch.utils.data.DataLoader(dset, batch_size=batch_size, shuffle=False, pin_memory=True,\n",
    "                                             num_workers=3)\n",
    "    else:\n",
    "        loader = torch.utils.data.DataLoader(dset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                             num_workers=3)\n",
    "    z_train = []\n",
    "    y_train = []\n",
    "    x_train = []\n",
    "    tr_aleatoric_vec = []\n",
    "    tr_epistemic_vec = []\n",
    "\n",
    "    for j, (x, y_l) in enumerate(loader):\n",
    "        zz = VAE.recongnition(x).loc.data.cpu().numpy()\n",
    "\n",
    "        # print(x.shape)\n",
    "        if prob_BNN:\n",
    "            probs = BNN.sample_predict(x, 0, False)\n",
    "            total_entropy, aleatoric_entropy, epistemic_entropy = decompose_entropy_cat(probs)\n",
    "        else:\n",
    "            probs = BNN.predict(x, grad=False)\n",
    "            total_entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=1, keepdim=False)\n",
    "            aleatoric_entropy = total_entropy\n",
    "            epistemic_entropy = total_entropy*0\n",
    "\n",
    "        tr_epistemic_vec.append(epistemic_entropy.data)\n",
    "        tr_aleatoric_vec.append(aleatoric_entropy.data)\n",
    "\n",
    "        z_train.append(zz)\n",
    "        y_train.append(y_l.numpy())\n",
    "        x_train.append(x.numpy())\n",
    "\n",
    "    tr_aleatoric_vec = torch.cat(tr_aleatoric_vec).cpu().numpy()\n",
    "    tr_epistemic_vec = torch.cat(tr_epistemic_vec).cpu().numpy()\n",
    "    z_train = np.concatenate(z_train)\n",
    "    x_train = np.concatenate(x_train)\n",
    "    y_train = np.concatenate(y_train)\n",
    "\n",
    "    return tr_aleatoric_vec, tr_epistemic_vec, z_train, x_train, y_train\n",
    "\n",
    "def to_variable(var=(), cuda=True, volatile=False):\n",
    "    out = []\n",
    "    for v in var:\n",
    "        if isinstance(v, np.ndarray):\n",
    "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
    "        if not v.is_cuda and cuda:\n",
    "            v = v.cuda()\n",
    "        if not isinstance(v, Variable):\n",
    "            v = Variable(v, volatile=volatile)\n",
    "        out.append(v)\n",
    "    return out\n",
    "\n",
    "def normal_parse_params(params, min_sigma=1e-3):\n",
    "    \"\"\"\n",
    "    Take a Tensor (e. g. neural network output) and return\n",
    "    torch.distributions.Normal distribution.\n",
    "    This Normal distribution is component-wise independent,\n",
    "    and its dimensionality depends on the input shape.\n",
    "    First half of channels is mean of the distribution,\n",
    "    the softplus of the second half is std (sigma), so there is\n",
    "    no restrictions on the input tensor.\n",
    "    min_sigma is the minimal value of sigma. I. e. if the above\n",
    "    softplus is less than min_sigma, then sigma is clipped\n",
    "    from below with value min_sigma. This regularization\n",
    "    is required for the numerical stability and may be considered\n",
    "    as a neural network architecture choice without any change\n",
    "    to the probabilistic model.\n",
    "    \"\"\"\n",
    "    n = params.shape[0]\n",
    "    d = params.shape[1]\n",
    "    mu = params[:, :d // 2]\n",
    "    sigma_params = params[:, d // 2:]\n",
    "    sigma = softplus(sigma_params)\n",
    "    sigma = sigma.clamp(min=min_sigma)\n",
    "    distr = Normal(mu, sigma)\n",
    "    return distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Uncertainty')"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x320 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from interpret.visualization_tools import latent_map_2d_gauss, latent_project_gauss, latent_project_cat\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if regression_bools[names.index(dname)]:\n",
    "    \n",
    "    tr_aleatoric_vec, tr_epistemic_vec, z_train, x_train, y_train = \\\n",
    "        latent_project_gauss(BNN, VAE, dset=trainset, batch_size=2048, cuda=cuda)\n",
    "    \n",
    "    tr_uncertainty_vec = tr_aleatoric_vec + tr_epistemic_vec\n",
    "\n",
    "    te_aleatoric_vec, te_epistemic_vec, z_test, x_test, y_test = \\\n",
    "        latent_project_gauss(BNN, VAE, dset=valset, batch_size=2048, cuda=cuda)\n",
    "    \n",
    "    te_uncertainty_vec = (te_aleatoric_vec**2 + te_epistemic_vec**2)**(1.0/2)\n",
    "    \n",
    "else:\n",
    "    tr_aleatoric_vec, tr_epistemic_vec, z_train, x_train, y_train = \\\n",
    "        latent_project_cat(BNN, VAE, dset=trainset, batch_size=2048, cuda=cuda)\n",
    "    tr_uncertainty_vec = tr_aleatoric_vec + tr_epistemic_vec\n",
    "    \n",
    "    te_aleatoric_vec, te_epistemic_vec, z_test, x_test, y_test = \\\n",
    "        latent_project_cat(BNN, VAE, dset=valset, batch_size=2048, cuda=cuda)\n",
    "    \n",
    "    te_uncertainty_vec = te_aleatoric_vec + te_epistemic_vec\n",
    "    \n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "uncertainty_idxs_sorted = np.flipud(np.argsort(te_uncertainty_vec))\n",
    "aleatoric_idxs_sorted = np.flipud(np.argsort(te_aleatoric_vec))\n",
    "epistemic_idxs_sorted = np.flipud(np.argsort(te_epistemic_vec))\n",
    "\n",
    "plt.figure(dpi=80)\n",
    "plt.hist(te_uncertainty_vec, density=True, alpha=0.5)\n",
    "plt.hist(tr_uncertainty_vec, density=True, alpha=0.5)\n",
    "plt.legend(['test', 'train'])\n",
    "plt.ylabel('Uncertainty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['25 - 45', 'Greater than 45', 'Less than 25', 'African-American', 'Asian', 'Caucasian', 'Hispanic', 'Native American', 'Other', 'Female', 'Male', 'Felony', 'misdemeanour', 'not_recid', 'is_recid', 'priors_count', 'time_served']\n"
     ]
    }
   ],
   "source": [
    "print(var_names_flat[dname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview uncertainty histogram per dataset dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x480 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x480 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "var_N = 11\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, dpi=120)\n",
    "# plt.figure(dpi=80)\n",
    "axes[0].hist(x_train[:,var_N], density=True, alpha=0.5)\n",
    "axes[0].hist(x_test[:,var_N], density=True, alpha=0.5)\n",
    "axes[0].legend(['train', 'test'])\n",
    "axes[0].set_title(var_names_flat[dname][var_N])\n",
    "\n",
    "bins = [-5, -4, -3, -2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2, 3, 4, 5]\n",
    "center_bins = ((np.array([0] + bins) + np.array(bins + [0]))/2)[1:]\n",
    "center_bins[-1] = bins[-1]\n",
    "\n",
    "bin_idx = np.digitize(x_train[:, var_N], bins, right=False)\n",
    "bin_means = []\n",
    "bin_stds = []\n",
    "aleatoric_mean = []\n",
    "aleatoric_stds = []\n",
    "epistemic_mean = []\n",
    "epistemic_stds = []\n",
    "\n",
    "for n_bin, bin_start in enumerate(bins):\n",
    "    y_select = y_train[bin_idx==n_bin]\n",
    "    aleatoric_select = tr_aleatoric_vec[bin_idx==n_bin]\n",
    "    epistemic_select = tr_epistemic_vec[bin_idx==n_bin]\n",
    "    if len(y_select) == 0:\n",
    "        bin_means.append(np.nan)\n",
    "        bin_stds.append(np.nan)\n",
    "        aleatoric_mean.append(np.nan)\n",
    "        aleatoric_stds.append(np.nan)\n",
    "        epistemic_mean.append(np.nan)\n",
    "        epistemic_stds.append(np.nan)\n",
    "    else:\n",
    "        bin_means.append(y_select.mean())\n",
    "        bin_stds.append(y_select.std())\n",
    "        aleatoric_mean.append(aleatoric_select.mean())\n",
    "        aleatoric_stds.append(aleatoric_select.std())\n",
    "        epistemic_mean.append(epistemic_select.mean())\n",
    "        epistemic_stds.append(epistemic_select.std())\n",
    "    \n",
    "# plt.figure(dpi=80)\n",
    "axes[1].errorbar(center_bins, bin_means, yerr=bin_stds, fmt='o')\n",
    "axes[1].set_xlabel(var_names_flat[dname][var_N])\n",
    "axes[1].set_ylabel('target var')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, dpi=120)\n",
    "# plt.figure(dpi=80)\n",
    "axes[0].errorbar(center_bins, aleatoric_mean, yerr=aleatoric_stds, fmt='o')\n",
    "axes[0].set_xlabel(var_names_flat[dname][var_N])\n",
    "axes[0].set_ylabel('Aleatoric')\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(dpi=80)\n",
    "axes[1].errorbar(center_bins, epistemic_mean, yerr=epistemic_stds, fmt='o')\n",
    "axes[1].set_xlabel(var_names_flat[dname][var_N])\n",
    "axes[1].set_ylabel('Epistemic')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_index = uncertainty_idxs_sorted \n",
    "\n",
    "Nbatch = 512\n",
    "z_init_batch = z_test[use_index[:Nbatch]]\n",
    "x_init_batch = x_test[use_index[:Nbatch]]\n",
    "y_init_batch = y_test[use_index[:Nbatch]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run base CLUE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "class CLUE(BaseNet):\n",
    "    \"\"\"This will be a general class for CLUE, etc.\n",
    "    A propper optimiser will be used instead of my manually designed one.\"\"\"\n",
    "\n",
    "    def __init__(self, VAE, BNN, original_x, uncertainty_weight, aleatoric_weight, epistemic_weight, prior_weight, distance_weight,\n",
    "                 latent_L2_weight, prediction_similarity_weight,\n",
    "                 lr, desired_preds=None, cond_mask=None, distance_metric=None, z_init=None, norm_MNIST=False, flatten_BNN=False,\n",
    "                 regression=False, prob_BNN=True, cuda=True):\n",
    "\n",
    "        \"\"\"Option specification:\n",
    "        MNIST: boolean, specifies whether to apply normalisation to VAE outputs before being passed on to the BNN\"\"\"\n",
    "         # Load models\n",
    "        self.VAE = VAE\n",
    "        self.BNN = BNN\n",
    "        self.BNN.set_mode_train(train=False)\n",
    "        self.VAE.set_mode_train(train=False)\n",
    "\n",
    "        # Objective function definition\n",
    "        self.uncertainty_weight = uncertainty_weight\n",
    "        self.aleatoric_weight = aleatoric_weight\n",
    "        self.epistemic_weight = epistemic_weight\n",
    "        self.prior_weight = prior_weight\n",
    "        self.distance_weight = distance_weight\n",
    "        self.distance_metric = distance_metric\n",
    "\n",
    "        self.latent_L2_weight = latent_L2_weight\n",
    "        self.prediction_similarity_weight = prediction_similarity_weight\n",
    "        self.desired_preds = desired_preds\n",
    "        # if self.desired_preds is not None:\n",
    "        #     self.desired_preds = torch.Tensor(self.desired_preds)\n",
    "            # if not regression:\n",
    "            #     self.desired_preds = self.desired_preds#.type(torch.LongTensor)\n",
    "\n",
    "        # Other CLUE parameters\n",
    "        self.regression = regression\n",
    "        # self.vae_sig = vae_sig  # We dont actually use this in our CLUE generative models as it performs worse\n",
    "        self.flatten_BNN = flatten_BNN\n",
    "        self.norm_MNIST = norm_MNIST\n",
    "        self.original_x = torch.Tensor(original_x)\n",
    "\n",
    "\n",
    "        self.prob_BNN = prob_BNN\n",
    "        self.cuda = cuda\n",
    "        if self.cuda:\n",
    "            self.original_x = self.original_x.cuda()\n",
    "            # self.z_init = self.z_init.cuda()\n",
    "            if self.desired_preds is not None:\n",
    "                self.desired_preds = self.desired_preds.cuda()\n",
    "        self.cond_mask = cond_mask\n",
    "\n",
    "        # Trainable params\n",
    "        self.trainable_params = list()\n",
    "\n",
    "        if self.VAE is None:  # this will be for ablation test: sensitivity analisys\n",
    "            self.trainable_params.append(nn.Parameter(original_x))\n",
    "        else:\n",
    "            self.z_dim = VAE.latent_dim\n",
    "            if z_init is not None:\n",
    "                self.z_init = torch.Tensor(z_init)\n",
    "                if cuda:\n",
    "                    self.z_init = self.z_init.cuda()\n",
    "                self.z = nn.Parameter(self.z_init)\n",
    "                self.trainable_params.append(self.z)\n",
    "            else:\n",
    "                self.z_init = torch.zeros(self.z_dim).unsqueeze(0).repeat(original_x.shape[0],1)\n",
    "                if self.cuda:\n",
    "                    self.z_init = self.z_init.cuda()\n",
    "                self.z = nn.Parameter(self.z_init)\n",
    "                self.trainable_params.append(self.z)\n",
    "\n",
    "        # Optimiser\n",
    "        self.optimizer = Adam(self.trainable_params, lr=lr)\n",
    "        # SGD(self.trainable_params, lr=lr, momentum=0.5, nesterov=True)\n",
    "\n",
    "    def randomise_z_init(self, std):\n",
    "        # assert (self.z.data == self.z_init).all()\n",
    "        eps = torch.randn(self.z.shape).type(self.z.type())\n",
    "        self.z.data = std * eps + self.z_init\n",
    "        return None\n",
    "\n",
    "    def pred_dist(self, preds):\n",
    "        # We dont implement for now as we could just use VAEAC with class conditioning\n",
    "        assert self.desired_preds is not None\n",
    "\n",
    "        if self.regression:\n",
    "            dist = F.mse_loss(preds, self.desired_preds, reduction='none')\n",
    "        else:\n",
    "\n",
    "            if len(self.desired_preds.shape) == 1 or self.desired_preds.shape[1] == 1:\n",
    "                dist = F.nll_loss(preds, self.desired_preds, reduction='none')\n",
    "            else:  # Soft cross entropy loss\n",
    "                dist = -(torch.log(preds) * self.desired_preds).sum(dim=1)\n",
    "        return dist\n",
    "\n",
    "    def uncertainty_from_z(self):\n",
    "        # if self.vae_sig:\n",
    "        #     x = self.VAE.regenerate(self.z, grad=True).loc\n",
    "        # else:\n",
    "        # We dont use unflatten option because BNNs always take flattened input and unflatten doesnt support grad\n",
    "        x = self.VAE.regenerate(self.z, grad=True)\n",
    "\n",
    "        if self.flatten_BNN:\n",
    "            to_BNN = x.view(x.shape[0], -1)\n",
    "        else:\n",
    "            to_BNN = x\n",
    "\n",
    "        if self.norm_MNIST:\n",
    "            to_BNN = MNIST_mean_std_norm(to_BNN)\n",
    "\n",
    "        if self.prob_BNN:\n",
    "            if self.regression:\n",
    "                mu_vec, std_vec = self.BNN.sample_predict(to_BNN, Nsamples=0, grad=True)\n",
    "                total_uncertainty, aleatoric_uncertainty, epistemic_uncertainty = decompose_std_gauss(mu_vec, std_vec)\n",
    "                preds = mu_vec.mean(dim=0)\n",
    "            else:\n",
    "                probs = self.BNN.sample_predict(to_BNN, Nsamples=0, grad=True)\n",
    "                total_uncertainty, aleatoric_uncertainty, epistemic_uncertainty = decompose_entropy_cat(probs)\n",
    "                preds = probs.mean(dim=0)\n",
    "        else:\n",
    "            if self.regression:\n",
    "                mu, std = self.BNN.predict(to_BNN, grad=True)\n",
    "                total_uncertainty = std.squeeze(1)\n",
    "                aleatoric_uncertainty = total_uncertainty\n",
    "                epistemic_uncertainty = total_uncertainty*0\n",
    "                preds = mu\n",
    "            else:\n",
    "                probs = self.BNN.predict(to_BNN, grad=True)\n",
    "                total_uncertainty = -(probs * torch.log(probs + 1e-10)).sum(dim=1, keepdim=False)\n",
    "                aleatoric_uncertainty = total_uncertainty\n",
    "                epistemic_uncertainty = total_uncertainty*0\n",
    "                preds = probs\n",
    "\n",
    "        return total_uncertainty, aleatoric_uncertainty, epistemic_uncertainty, x, preds\n",
    "\n",
    "    def get_objective(self, x, total_uncertainty, aleatoric_uncertainty, epistemic_uncertainty, preds):\n",
    "        # Put objectives together\n",
    "        objective = self.uncertainty_weight * total_uncertainty + self.aleatoric_weight * aleatoric_uncertainty + \\\n",
    "                    self.epistemic_weight * epistemic_uncertainty\n",
    "\n",
    "        if self.VAE is not None and self.cond_mask is None and self.prior_weight > 0:\n",
    "            try:\n",
    "                prior_loglike = self.VAE.prior.log_prob(self.z).sum(dim=1)\n",
    "            except:  # This mode is just for CondCLUE but the objective method is inherited\n",
    "                prior_loglike = self.VAEAC.get_prior(self.original_x, self.cond_mask, flatten=False).log_prob(self.z).sum(dim=1)\n",
    "            objective += self.prior_weight * prior_loglike\n",
    "\n",
    "        if self.latent_L2_weight != 0 and self.latent_L2_weight is not None:\n",
    "            # print('latent_L2_weight is not implement correctly as distances are computed across batches')\n",
    "            latent_dist = F.mse_loss(self.z, self.z_init, reduction='none').view(x.shape[0], -1).sum(dim=1)\n",
    "            objective += self.latent_L2_weight * latent_dist\n",
    "\n",
    "        if self.desired_preds is not None:\n",
    "            pred_dist = self.pred_dist(preds).view(preds.shape[0], -1).sum(dim=1)\n",
    "            objective += self.prediction_similarity_weight * pred_dist\n",
    "\n",
    "        if self.distance_metric is not None:\n",
    "            dist = self.distance_metric(x, self.original_x).view(x.shape[0], -1).sum(dim=1)\n",
    "            objective += self.distance_weight * dist\n",
    "\n",
    "            return objective, self.distance_weight*dist\n",
    "        else:\n",
    "            return objective, 0\n",
    "\n",
    "    def optimise(self, min_steps=3, max_steps=25,\n",
    "                 n_early_stop=3):\n",
    "        # Vectors to capture changes for this minibatch\n",
    "        z_vec = [self.z.data.cpu().numpy()]\n",
    "        x_vec = []\n",
    "        uncertainty_vec = np.zeros((max_steps, self.z.shape[0]))\n",
    "        aleatoric_vec = np.zeros((max_steps, self.z.shape[0]))\n",
    "        epistemic_vec = np.zeros((max_steps, self.z.shape[0]))\n",
    "        dist_vec = np.zeros((max_steps, self.z.shape[0]))\n",
    "        cost_vec = np.zeros((max_steps, self.z.shape[0]))  # this one doesnt consider the prior\n",
    "\n",
    "        it_mask = np.zeros(self.z.shape[0])\n",
    "\n",
    "        for step_idx in range(max_steps):\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            total_uncertainty, aleatoric_uncertainty, epistemic_uncertainty, x, preds = self.uncertainty_from_z()\n",
    "            objective, w_dist = self.get_objective(x, total_uncertainty, aleatoric_uncertainty, epistemic_uncertainty, preds)\n",
    "            # We sum over features and over batch size in order to make dz invariant of batch (used to average over batch size)\n",
    "            print(\"Hi\")\n",
    "            objective.sum(dim=0).backward()  # backpropagate\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # save vectors\n",
    "            uncertainty_vec[step_idx, :] = total_uncertainty.data.cpu().numpy()\n",
    "            aleatoric_vec[step_idx, :] = aleatoric_uncertainty.data.cpu().numpy()\n",
    "            epistemic_vec[step_idx, :] = epistemic_uncertainty.data.cpu().numpy()\n",
    "            dist_vec[step_idx, :] = (w_dist.data.cpu().numpy())\n",
    "            cost_vec[step_idx, :] = (objective.data.cpu().numpy())\n",
    "            x_vec.append(x.data)  # we dont convert to numpy yet because we need x0 for L1\n",
    "            z_vec.append(self.z.data.cpu().numpy())  # this one is after gradient update while x is before\n",
    "\n",
    "            it_mask = CLUE.update_stopvec(cost_vec, it_mask, step_idx, n_early_stop, min_steps)\n",
    "\n",
    "        #  Generate final (or resulting s sample)\n",
    "\n",
    "        x = self.VAE.regenerate(self.z, grad=False).data\n",
    "        x_vec.append(x)\n",
    "        x_vec = [i.cpu().numpy() for i in x_vec]  # convert x to numpy\n",
    "        x_vec = np.stack(x_vec)\n",
    "        z_vec = np.stack(z_vec)\n",
    "\n",
    "        # Recover correct indexes using mask\n",
    "        uncertainty_vec, epistemic_vec, aleatoric_vec, dist_vec, cost_vec, z_vec, x_vec = CLUE.apply_stopvec(it_mask,\n",
    "                      uncertainty_vec, epistemic_vec, aleatoric_vec, dist_vec, cost_vec, z_vec, x_vec,\n",
    "                      n_early_stop)\n",
    "        return z_vec, x_vec, uncertainty_vec, epistemic_vec, aleatoric_vec, cost_vec, dist_vec\n",
    "\n",
    "    @staticmethod\n",
    "    def update_stopvec(cost_vec, it_mask, step_idx, n_early_stop, min_steps):\n",
    "        # TODO: can go in a CLUE parent class\n",
    "        asymptotic_rel = np.abs(cost_vec[step_idx - n_early_stop, :] - cost_vec[step_idx, :]) < cost_vec[0, :] * 1e-2\n",
    "        asymptotic_abs = np.abs(cost_vec[step_idx - n_early_stop, :] - cost_vec[step_idx, :]) < 1e-3\n",
    "\n",
    "        if step_idx > min_steps:\n",
    "            condition_sum = asymptotic_rel + asymptotic_abs\n",
    "        else:\n",
    "            condition_sum = np.array([0])\n",
    "\n",
    "        stop_vec = condition_sum.clip(max=1, min=0)\n",
    "\n",
    "        to_mask = (it_mask == 0).astype(int) * stop_vec\n",
    "        it_mask[to_mask == 1] = step_idx\n",
    "\n",
    "        if (it_mask == 0).sum() == 0 and n_early_stop > 0:\n",
    "            print('it %d, all conditions met, stopping' % step_idx)\n",
    "        return it_mask\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_stopvec(it_mask, uncertainty_vec, epistemic_vec, aleatoric_vec, dist_vec, cost_vec, z_vec, x_vec, n_early_stop):\n",
    "        # uncertainty_vec[step_idx, batch_size]\n",
    "        it_mask = (it_mask - n_early_stop + 1).astype(int)\n",
    "        for i in range(uncertainty_vec.shape[1]):\n",
    "            if it_mask[i] > 0 and n_early_stop > 0:\n",
    "                uncertainty_vec[it_mask[i]:, i] = uncertainty_vec[it_mask[i], i]\n",
    "                epistemic_vec[it_mask[i]:, i] = epistemic_vec[it_mask[i], i]\n",
    "                aleatoric_vec[it_mask[i]:, i] = aleatoric_vec[it_mask[i], i]\n",
    "                cost_vec[it_mask[i]:, i] = cost_vec[it_mask[i], i]\n",
    "                dist_vec[it_mask[i]:, i] = dist_vec[it_mask[i], i]\n",
    "                z_vec[it_mask[i]:, i] = z_vec[it_mask[i], i]\n",
    "                x_vec[it_mask[i]:, i] = x_vec[it_mask[i], i]\n",
    "        return uncertainty_vec, epistemic_vec, aleatoric_vec, dist_vec, cost_vec, z_vec, x_vec\n",
    "\n",
    "\n",
    "    def sample_explanations(self, n_explanations, init_std=0.15, min_steps=3, max_steps=25,\n",
    "                                    n_early_stop=3):\n",
    "    # This creates a new first axis and stacks outputs there\n",
    "        full_x_vec = []\n",
    "        full_z_vec = []\n",
    "        full_uncertainty_vec = []\n",
    "        full_aleatoric_vec = []\n",
    "        full_epistemic_vec = []\n",
    "        full_dist_vec = []\n",
    "        full_cost_vec = []\n",
    "\n",
    "        for i in range(n_explanations):\n",
    "\n",
    "            self.randomise_z_init(std=init_std)\n",
    "\n",
    "            torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "            # clue_instance.optimizer = SGD(self.trainable_params, lr=lr, momentum=0.5, nesterov=True)\n",
    "            z_vec, x_vec, uncertainty_vec, epistemic_vec, aleatoric_vec, cost_vec, dist_vec = self.optimise(\n",
    "                                                                    min_steps=min_steps, max_steps=max_steps,\n",
    "                                                                    n_early_stop=n_early_stop)\n",
    "\n",
    "            full_x_vec.append(x_vec)\n",
    "            full_z_vec.append(z_vec)\n",
    "            full_uncertainty_vec.append(uncertainty_vec)\n",
    "            full_aleatoric_vec.append(aleatoric_vec)\n",
    "            full_epistemic_vec.append(epistemic_vec)\n",
    "            full_dist_vec.append(dist_vec)\n",
    "            full_cost_vec.append(cost_vec)\n",
    "\n",
    "        full_x_vec = np.concatenate(np.expand_dims(full_x_vec, axis=0), axis=0)\n",
    "        full_z_vec = np.concatenate(np.expand_dims(full_z_vec, axis=0), axis=0)\n",
    "        full_cost_vec = np.concatenate(np.expand_dims(full_cost_vec, axis=0), axis=0)\n",
    "        full_dist_vec = np.concatenate(np.expand_dims(full_dist_vec, axis=0), axis=0)\n",
    "        full_uncertainty_vec = np.concatenate(np.expand_dims(full_uncertainty_vec, axis=0), axis=0)\n",
    "        full_aleatoric_vec = np.concatenate(np.expand_dims(full_aleatoric_vec, axis=0), axis=0)\n",
    "        full_epistemic_vec = np.concatenate(np.expand_dims(full_epistemic_vec, axis=0), axis=0)\n",
    "\n",
    "        return full_x_vec, full_z_vec, full_uncertainty_vec, full_aleatoric_vec, full_epistemic_vec, full_dist_vec, full_cost_vec\n",
    "\n",
    "    @classmethod\n",
    "    def batch_optimise(cls, VAE, BNN, original_x, uncertainty_weight, aleatoric_weight, epistemic_weight, prior_weight,\n",
    "                       distance_weight, latent_L2_weight, prediction_similarity_weight, lr, min_steps=3, max_steps=25,\n",
    "                       n_early_stop=3, batch_size=256, cond_mask=None, desired_preds=None,\n",
    "                       distance_metric=None, z_init=None, norm_MNIST=False, flatten_BNN=False, regression=False,\n",
    "                       prob_BNN=True, cuda=True):\n",
    "    # This stacks outputs along the first (batch_size) axis\n",
    "        full_x_vec = []\n",
    "        full_z_vec = []\n",
    "        full_uncertainty_vec = []\n",
    "        full_aleatoric_vec = []\n",
    "        full_epistemic_vec = []\n",
    "        full_dist_vec = []\n",
    "        full_cost_vec = []\n",
    "\n",
    "        idx_iterator = generate_ind_batch(original_x.shape[0], batch_size=batch_size, random=False, roundup=True)\n",
    "        for train_idx in idx_iterator:\n",
    "\n",
    "            if z_init is not None:\n",
    "                z_init_use = z_init[train_idx]\n",
    "            else:\n",
    "                z_init_use = z_init\n",
    "\n",
    "            if desired_preds is not None:\n",
    "                desired_preds_use = desired_preds[train_idx].data\n",
    "            else:\n",
    "                desired_preds_use = desired_preds\n",
    "\n",
    "            CLUE_runner = cls(VAE, BNN, original_x[train_idx], uncertainty_weight, aleatoric_weight, epistemic_weight, prior_weight, distance_weight,\n",
    "                              latent_L2_weight, prediction_similarity_weight, lr, cond_mask=cond_mask, distance_metric=distance_metric,\n",
    "                              z_init=z_init_use, norm_MNIST=norm_MNIST, desired_preds=desired_preds_use,\n",
    "                              flatten_BNN=flatten_BNN, regression=regression, prob_BNN=prob_BNN, cuda=cuda)\n",
    "\n",
    "            z_vec, x_vec, uncertainty_vec, epistemic_vec, aleatoric_vec, cost_vec, dist_vec = \\\n",
    "                CLUE_runner.optimise(min_steps=min_steps, max_steps=max_steps, n_early_stop=n_early_stop)\n",
    "\n",
    "            full_x_vec.append(x_vec)\n",
    "            full_z_vec.append(z_vec)\n",
    "            full_uncertainty_vec.append(uncertainty_vec)\n",
    "            full_aleatoric_vec.append(aleatoric_vec)\n",
    "            full_epistemic_vec.append(epistemic_vec)\n",
    "            full_dist_vec.append(dist_vec)\n",
    "            full_cost_vec.append(cost_vec)\n",
    "\n",
    "        full_x_vec = np.concatenate(full_x_vec, axis=1)\n",
    "        full_z_vec = np.concatenate(full_z_vec, axis=1)\n",
    "        full_cost_vec = np.concatenate(full_cost_vec, axis=1)\n",
    "        full_dist_vec = np.concatenate(full_dist_vec, axis=1)\n",
    "        full_uncertainty_vec = np.concatenate(full_uncertainty_vec, axis=1)\n",
    "        full_aleatoric_vec = np.concatenate(full_aleatoric_vec, axis=1)\n",
    "        full_epistemic_vec = np.concatenate(full_epistemic_vec, axis=1)\n",
    "\n",
    "        return full_x_vec, full_z_vec, full_uncertainty_vec, full_aleatoric_vec, full_epistemic_vec, full_dist_vec, full_cost_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ln_distance(nn.Module):\n",
    "    \"\"\"If dims is None Compute across all dimensions except first\"\"\"\n",
    "    def __init__(self, n, dim=None):\n",
    "        super(Ln_distance, self).__init__()\n",
    "        self.n = n\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        d = x - y\n",
    "        if self.dim is None:\n",
    "            self.dim = list(range(1, len(d.shape)))\n",
    "        return torch.abs(d).pow(self.n).sum(dim=self.dim).pow(1./float(self.n))\n",
    "\n",
    "def decompose_std_gauss(mu, sigma, sum_dims=True):\n",
    "    # probs (Nsamples, batch_size, output_sims)\n",
    "    aleatoric_var = (sigma**2).mean(dim=0)\n",
    "    epistemic_var = ((mu ** 2).mean(dim=0) - mu.mean(dim=0) ** 2)\n",
    "    total_var = aleatoric_var + epistemic_var\n",
    "    if sum_dims:\n",
    "        aleatoric_var = aleatoric_var.sum(dim=1)\n",
    "        epistemic_var = epistemic_var.sum(dim=1)\n",
    "        total_var = total_var.sum(dim=1)\n",
    "    return total_var.sqrt(), aleatoric_var.sqrt(), epistemic_var.sqrt()\n",
    "\n",
    "def decompose_entropy_cat(probs, eps=1e-10):\n",
    "    # probs (Nsamples, batch_size, classes)\n",
    "    posterior_preds = probs.mean(dim=0, keepdim=False)\n",
    "    total_entropy = -(posterior_preds * torch.log(posterior_preds + eps)).sum(dim=1, keepdim=False)\n",
    "\n",
    "    sample_preds_entropy = -(probs * torch.log(probs + eps)).sum(dim=2, keepdim=False)\n",
    "    aleatoric_entropy = sample_preds_entropy.mean(dim=0, keepdim=False)\n",
    "\n",
    "    epistemic_entropy = total_entropy - aleatoric_entropy\n",
    "\n",
    "    # returns (batch_size)\n",
    "    return total_entropy, aleatoric_entropy, epistemic_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W python_anomaly_mode.cpp:104] Warning: Error detected in AddmmBackward0. Traceback of forward call that caused the error:\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/asyncio/events.py\", line 81, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 457, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 446, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 353, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 648, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 353, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2947, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3172, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3364, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_11786/25044258.py\", line 38, in <module>\n",
      "    z_vec, x_vec, uncertainty_vec, epistemic_vec, aleatoric_vec, cost_vec, dist_vec = CLUE_explainer.optimise(\n",
      "  File \"/tmp/ipykernel_11786/1623288118.py\", line 182, in optimise\n",
      "    total_uncertainty, aleatoric_uncertainty, epistemic_uncertainty, x, preds = self.uncertainty_from_z()\n",
      "  File \"/tmp/ipykernel_11786/1623288118.py\", line 118, in uncertainty_from_z\n",
      "    probs = self.BNN.sample_predict(to_BNN, Nsamples=0, grad=True)\n",
      "  File \"/tmp/ipykernel_11786/1391810931.py\", line 125, in sample_predict\n",
      "    out[idx] = self.model(x)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/tmp/ipykernel_11786/1391810931.py\", line 182, in forward\n",
      "    return self.block(x)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/container.py\", line 141, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/linear.py\", line 103, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/functional.py\", line 1848, in linear\n",
      "    return torch._C._nn.linear(input, weight, bias)\n",
      " (function _print_stack)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [200, 2]], which is output 0 of AsStridedBackward0, is at version 601; expected version 600 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11786/25044258.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m z_vec, x_vec, uncertainty_vec, epistemic_vec, aleatoric_vec, cost_vec, dist_vec = CLUE_explainer.optimise(\n\u001b[0m\u001b[1;32m     39\u001b[0m                                                         \u001b[0mmin_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                                                         n_early_stop=3)\n",
      "\u001b[0;32m/tmp/ipykernel_11786/1623288118.py\u001b[0m in \u001b[0;36moptimise\u001b[0;34m(self, min_steps, max_steps, n_early_stop)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;31m# We sum over features and over batch size in order to make dz invariant of batch (used to average over batch size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# backpropagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [200, 2]], which is output 0 of AsStridedBackward0, is at version 601; expected version 600 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "# from interpret.CLUE import CLUE\n",
    "# from src.utils import Ln_distance\n",
    "# from src.probability import decompose_std_gauss, decompose_entropy_cat\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "dist = Ln_distance(n=1, dim=(1))\n",
    "x_dim = x_init_batch.reshape(x_init_batch.shape[0], -1).shape[1]\n",
    "\n",
    "\n",
    "aleatoric_weight = 0\n",
    "epistemic_weight = 0\n",
    "uncertainty_weight = 1\n",
    "\n",
    "distance_weight = 2 / x_dim\n",
    "prediction_similarity_weight = 0\n",
    "\n",
    "\n",
    "if regression_bools[names.index(dname)]:\n",
    "    mu_vec, std_vec = BNN.sample_predict(x_init_batch, Nsamples=0, grad=False)\n",
    "    o_uncert, o_aleatoric, o_epistemic = decompose_std_gauss(mu_vec, std_vec)\n",
    "    desired_preds = mu_vec.mean(dim=0).cpu().numpy()\n",
    "else:\n",
    "    o_preds = BNN.sample_predict(x_init_batch, Nsamples=0, grad=False)\n",
    "    o_uncert, o_aleatoric, o_epistemic = decompose_entropy_cat(o_preds)\n",
    "    desired_preds = o_preds.mean(dim=0).cpu().numpy()\n",
    "\n",
    "\n",
    "CLUE_explainer = CLUE(VAE, BNN, x_init_batch, uncertainty_weight=uncertainty_weight, aleatoric_weight=aleatoric_weight, epistemic_weight=epistemic_weight,\n",
    "                      prior_weight=0, distance_weight=distance_weight,\n",
    "                 latent_L2_weight=0, prediction_similarity_weight=prediction_similarity_weight,\n",
    "                 lr=1e-2, desired_preds=None, cond_mask=None, distance_metric=dist,\n",
    "                 z_init=z_init_batch, norm_MNIST=False,\n",
    "                 flatten_BNN=False, regression=regression_bools[names.index(dname)], cuda=True)\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "z_vec, x_vec, uncertainty_vec, epistemic_vec, aleatoric_vec, cost_vec, dist_vec = CLUE_explainer.optimise(\n",
    "                                                        min_steps=3, max_steps=50,\n",
    "                                                        n_early_stop=3)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, dpi=130)\n",
    "axes[0].plot(cost_vec.mean(axis=1))\n",
    "axes[0].set_title('mean Cost')\n",
    "axes[0].set_xlabel('iterations')\n",
    "\n",
    "axes[1].plot(uncertainty_vec.mean(axis=1))\n",
    "axes[1].set_title('mean Total Entropy')\n",
    "axes[1].set_xlabel('iterations')\n",
    "\n",
    "axes[2].plot(dist_vec.mean(axis=1))\n",
    "axes[2].set_title('mean Ln Cost')\n",
    "axes[2].set_xlabel('iterations')\n",
    "\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.5, hspace=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BNN.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with autograd.detect_anomaly():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
