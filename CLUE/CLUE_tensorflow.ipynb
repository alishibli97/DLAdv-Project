{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['wine', 'default_credit', 'compas', 'lsat']\n",
    "bnn_widths = [200, 200, 200, 200]\n",
    "bnn_depths = [2, 2, 2, 2]\n",
    "\n",
    "vae_widths = [300, 300, 300, 300] # [200, 200, 200, 200]\n",
    "vae_depths = [3, 3, 3, 3] # We go deeper because we are using residual models\n",
    "vae_latent_dims = [6, 8, 4, 4]\n",
    "\n",
    "# For automatic explainer generation\n",
    "\n",
    "regression_bools = [True, False, False, True]\n",
    "gauss_cat_vae_bools = [False, True, True, True]\n",
    "flat_vae_bools = [False, False, False, False]\n",
    "\n",
    "var_names = {}\n",
    "var_names_flat = {}\n",
    "\n",
    "var_names['wine'] = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide',\n",
    "            'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n",
    "var_names_flat['wine'] = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide',\n",
    "            'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n",
    "\n",
    "var_names['default_credit'] = ['Given credit', 'Gender', 'Education', 'Marital status', 'Age', 'Payment delay 1', 'Payment delay 2',\n",
    "            'Payment delay 3', 'Payment delay 4', 'Payment delay 5', 'Payment delay 6', 'Bill statement 1', 'Bill statement 2',\n",
    "            'Bill statement 3', 'Bill statement 4', 'Bill statement 5', 'Bill statement 6', 'Previous payment 1', 'Previous payment 2',\n",
    "            'Previous payment 3', 'Previous payment 4', 'Previous payment 5', 'Previous payment 6']\n",
    "var_names_flat['default_credit'] = ['Given credit', 'Gender M', 'Gender F', 'Education grad', 'Education under', 'Education HS', 'Education Other',\n",
    "                 'Marital status M', 'Marital status S', 'Marital status Other', 'Age', 'Payment delay 1', 'Payment delay 2',\n",
    "            'Payment delay 3', 'Payment delay 4', 'Payment delay 5', 'Payment delay 6', 'Bill statement 1', 'Bill statement 2',\n",
    "            'Bill statement 3', 'Bill statement 4', 'Bill statement 5', 'Bill statement 6', 'Previous payment 1', 'Previous payment 2',\n",
    "            'Previous payment 3', 'Previous payment 4', 'Previous payment 5', 'Previous payment 6']\n",
    "\n",
    "var_names['compas'] = [\"age_cat\", \"race\", \"sex\", \"c_charge_degree\", 'is_recid', 'priors_count', 'time_served']\n",
    "var_names_flat['compas'] = ['25 - 45', 'Greater than 45', 'Less than 25', 'African-American', 'Asian', 'Caucasian', 'Hispanic', 'Native American', 'Other', 'Female', 'Male', 'Felony', 'misdemeanour', 'not_recid', 'is_recid', 'priors_count', 'time_served']\n",
    "\n",
    "var_names['lsat'] = ['LSAT', 'UGPA', 'race', 'sex']\n",
    "var_names_flat['lsat'] = ['LSAT', 'UGPA', 'amerind', 'mexican', 'other', 'black', 'asian', 'puerto', 'hisp', 'white', 'female', 'male']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dname = 'compas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_UCI(dset_name, splits=10, seed=0, separate_targets=True, save_dir='data/'):\n",
    "    mkdir(save_dir)\n",
    "\n",
    "    if dset_name == 'boston':\n",
    "        if not os.path.isfile(save_dir+'housing.data'):\n",
    "            urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\",\n",
    "                               filename=save_dir+'housing.data')\n",
    "        data = pd.read_csv(save_dir + 'housing.data', header=0, delimiter=\"\\s+\").values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'concrete':\n",
    "        if not os.path.isfile(save_dir+'Concrete_Data.xls'):\n",
    "            urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete_Data.xls\",\n",
    "                               filename=save_dir+'Concrete_Data.xls')\n",
    "        data = pd.read_excel(save_dir+ 'Concrete_Data.xls', header=0, delimiter=\"\\s+\").values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'energy':\n",
    "        if not os.path.isfile(save_dir+'ENB2012_data.xlsx'):\n",
    "            urllib.urlretrieve(\"http://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx\",\n",
    "                               filename=save_dir+'ENB2012_data.xlsx')\n",
    "        data = pd.read_excel(save_dir+'ENB2012_data.xlsx', header=0, delimiter=\"\\s+\").values\n",
    "        y_idx = [-2, -1]\n",
    "\n",
    "    elif dset_name == 'power':\n",
    "        if not os.path.isfile(save_dir+'CCPP.zip'):\n",
    "            urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00294/CCPP.zip\",\n",
    "                               filename=save_dir+'CCPP.zip')\n",
    "        zipped = zipfile.ZipFile(save_dir+\"CCPP.zip\")\n",
    "        data = pd.read_excel(zipped.open('CCPP/Folds5x2_pp.xlsx'), header=0, delimiter=\"\\t\").values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'wine':\n",
    "        if not os.path.isfile(save_dir+'winequality-red.csv'):\n",
    "            urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\",\n",
    "                               filename=save_dir+'winequality-red.csv')\n",
    "        data = pd.read_csv(save_dir+'winequality-red.csv', header=1, delimiter=';').values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'yatch':\n",
    "        if not os.path.isfile(save_dir+'yacht_hydrodynamics.data'):\n",
    "            urllib.urlretrieve(\"http://archive.ics.uci.edu/ml/machine-learning-databases/00243/yacht_hydrodynamics.data\",\n",
    "                               filename=save_dir+'yacht_hydrodynamics.data')\n",
    "        data = pd.read_csv(save_dir+'yacht_hydrodynamics.data', header=1, delimiter='\\s+').values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'kin8nm':\n",
    "        if not os.path.isfile(save_dir+'dataset_2175_kin8nm.csv'):\n",
    "            urllib.urlretrieve(\"https://www.openml.org/data/get_csv/3626/dataset_2175_kin8nm.csv\",\n",
    "                               filename=save_dir+'dataset_2175_kin8nm.csv')\n",
    "        data = pd.read_csv(save_dir+'dataset_2175_kin8nm.csv', header=1, delimiter=',').values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'kin8nm':\n",
    "        if not os.path.isfile(save_dir+'dataset_2175_kin8nm.csv'):\n",
    "            urllib.urlretrieve(\"https://www.openml.org/data/get_csv/3626/dataset_2175_kin8nm.csv\",\n",
    "                               filename=save_dir+'dataset_2175_kin8nm.csv')\n",
    "        data = pd.read_csv(save_dir+'dataset_2175_kin8nm.csv', header=1, delimiter=',').values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'naval':\n",
    "        if not os.path.isfile(save_dir + 'UCI%20CBM%20Dataset.zip'):\n",
    "            urllib.urlretrieve(\"http://archive.ics.uci.edu/ml/machine-learning-databases/00316/UCI%20CBM%20Dataset.zip\",\n",
    "                               filename=save_dir + 'UCI%20CBM%20Dataset.zip')\n",
    "        zipped = zipfile.ZipFile(save_dir + \"UCI%20CBM%20Dataset.zip\")\n",
    "        data = pd.read_csv(zipped.open('UCI CBM Dataset/data.txt'), header='infer', delimiter=\"\\s+\").values\n",
    "        y_idx = [-2, -1]\n",
    "\n",
    "    elif dset_name == 'protein':\n",
    "        if not os.path.isfile(save_dir+'CASP.csv'):\n",
    "            urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00265/CASP.csv\",\n",
    "                               filename=save_dir+'CASP.csv')\n",
    "        data = pd.read_csv(save_dir+'CASP.csv', header=1, delimiter=',').values\n",
    "        y_idx = [0]\n",
    "\n",
    "    elif dset_name == 'default_credit':\n",
    "        if not os.path.isfile(save_dir + 'default of credit card clients.xls'):\n",
    "            urllib.request.urlretrieve(\n",
    "                \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\",\n",
    "                filename=save_dir + 'default of credit card clients.xls')\n",
    "        data = pd.read_excel(save_dir + 'default of credit card clients.xls', header=[0, 1], index_col=0, # delimiter=\"\\s+\"\n",
    "                             ).values\n",
    "        y_idx = [-1]  # OK\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise Exception('Dataset name doesnt match any known datasets.')\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    data = data[np.random.permutation(np.arange(len(data)))]\n",
    "\n",
    "    kf = KFold(n_splits=splits)\n",
    "    for j, (train_index, test_index) in enumerate(kf.split(data)):\n",
    "\n",
    "        if separate_targets:\n",
    "            x_idx = list(range(data.shape[1]))\n",
    "            for e in y_idx:\n",
    "                x_idx.remove(x_idx[e])\n",
    "\n",
    "            x_idx = np.array(x_idx)\n",
    "            y_idx = np.array(y_idx)\n",
    "            x_train, y_train = data[train_index, :], data[train_index, :]\n",
    "            x_train, y_train = x_train[:, x_idx], y_train[:, y_idx]\n",
    "            x_test, y_test = data[test_index, :], data[test_index, :]\n",
    "            x_test, y_test = x_test[:, x_idx], y_test[:, y_idx]\n",
    "\n",
    "            x_means, x_stds = x_train.mean(axis=0), x_train.std(axis=0)\n",
    "            y_means, y_stds = y_train.mean(axis=0), y_train.std(axis=0)\n",
    "\n",
    "            y_stds[y_stds < 1e-10] = 1\n",
    "            x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "            x_train = ((x_train - x_means) / x_stds).astype(np.float32)\n",
    "            y_train = ((y_train - y_means) / y_stds).astype(np.float32)\n",
    "\n",
    "            x_test = ((x_test - x_means) / x_stds).astype(np.float32)\n",
    "            y_test = ((y_test - y_means) / y_stds).astype(np.float32)\n",
    "\n",
    "            return x_train, x_test, x_means, x_stds, y_train, y_test, y_means, y_stds\n",
    "\n",
    "        else:\n",
    "            x_train, x_test = data[train_index, :], data[test_index, :]\n",
    "            x_means, x_stds = x_train.mean(axis=0), x_train.std(axis=0)\n",
    "\n",
    "            x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "            x_train = ((x_train - x_means) / x_stds).astype(np.float32)\n",
    "            x_test = ((x_test - x_means) / x_stds).astype(np.float32)\n",
    "\n",
    "            return x_train, x_test, x_means, x_stds\n",
    "\n",
    "\n",
    "def get_my_COMPAS(rseed=0, separate_test=True, test_ratio=0.2, save_dir='../data/'):\n",
    "    \"\"\"\n",
    "        The adult dataset can be obtained from: https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\n",
    "        The code will look for the data file in the present directory, if it is not found, it will download them from GitHub.\n",
    "    \"\"\"\n",
    "\n",
    "    SEED = rseed\n",
    "    seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    their_FEATURES_CLASSIFICATION = [\"age_cat\", \"race\", \"sex\", \"priors_count\", \"c_charge_degree\"]\n",
    "    FEATURES_CLASSIFICATION = [\"age_cat\", \"race\", \"sex\", \"c_charge_degree\", \"is_recid\", \"priors_count\",\n",
    "                               \"time_served\"]  # features to be used for classification\n",
    "    CONT_VARIABLES = [\"priors_count\",\n",
    "                      \"time_served\"]  # continuous features, will need to be handled separately from categorical features, categorical features will be encoded using one-hot\n",
    "    CLASS_FEATURE = \"two_year_recid\"  # the decision variable\n",
    "\n",
    "\n",
    "    COMPAS_INPUT_FILE = save_dir + \"compas-scores-two-years.csv\"\n",
    "    check_data_file(COMPAS_INPUT_FILE)\n",
    "\n",
    "    # load the data and get some stats\n",
    "    df = pd.read_csv(COMPAS_INPUT_FILE)\n",
    "    df = df.dropna(subset=[\"days_b_screening_arrest\"])  # dropping missing vals\n",
    "\n",
    "    # convert to np array\n",
    "    data = df.to_dict('list')\n",
    "    for k in data.keys():\n",
    "        data[k] = np.array(data[k])\n",
    "\n",
    "    dates_in = data['c_jail_in']\n",
    "    dates_out = data['c_jail_out']\n",
    "    # this measures time in Jail\n",
    "    time_served = []\n",
    "    for i in range(len(dates_in)):\n",
    "        di = datetime.datetime.strptime(dates_in[i], '%Y-%m-%d %H:%M:%S')\n",
    "        do = datetime.datetime.strptime(dates_out[i], '%Y-%m-%d %H:%M:%S')\n",
    "        time_served.append((do - di).days)\n",
    "    time_served = np.array(time_served)\n",
    "    time_served[time_served < 0] = 0\n",
    "    data[\"time_served\"] = time_served\n",
    "\n",
    "    \"\"\" Filtering the data \"\"\"\n",
    "\n",
    "    # These filters are the same as propublica (refer to https://github.com/propublica/compas-analysis)\n",
    "    # If the charge date of a defendants Compas scored crime was not within 30 days from when the person was arrested, we assume that because of data quality reasons, that we do not have the right offense.\n",
    "    idx = np.logical_and(data[\"days_b_screening_arrest\"] <= 30, data[\"days_b_screening_arrest\"] >= -30)\n",
    "\n",
    "    # We coded the recidivist flag -- is_recid -- to be -1 if we could not find a compas case at all.\n",
    "    idx = np.logical_and(idx, data[\"is_recid\"] != -1)\n",
    "\n",
    "    # In a similar vein, ordinary traffic offenses -- those with a c_charge_degree of 'O' -- will not result in Jail time are removed (only two of them).\n",
    "    idx = np.logical_and(idx, data[\"c_charge_degree\"] != \"O\")  # F: felony, M: misconduct\n",
    "\n",
    "    # We filtered the underlying data from Broward county to include only those rows representing people who had either recidivated in two years, or had at least two years outside of a correctional facility.\n",
    "    idx = np.logical_and(idx, data[\"score_text\"] != \"NA\")\n",
    "\n",
    "    # select the examples that satisfy this criteria\n",
    "    for k in data.keys():\n",
    "        data[k] = data[k][idx]\n",
    "\n",
    "    y = data[CLASS_FEATURE]\n",
    "    \"\"\" Feature normalization and one hot encoding \"\"\"\n",
    "\n",
    "    print\n",
    "    \"\\nNumber of people recidivating within two years\"\n",
    "    print\n",
    "    pd.Series(y).value_counts()\n",
    "    print\n",
    "    \"\\n\"\n",
    "\n",
    "    X = []  # empty array with num rows same as num examples, will hstack the features to it\n",
    "    X_dims = []\n",
    "\n",
    "    feature_names = []\n",
    "    for attr in FEATURES_CLASSIFICATION:\n",
    "        vals = data[attr]\n",
    "        if attr in CONT_VARIABLES:\n",
    "            vals = [float(v) for v in vals]\n",
    "            # vals = preprocessing.scale(vals, axis=0, with_mean=True, with_std=True)  # 0 mean and 1 variance\n",
    "            vals = np.reshape(vals, (len(y), -1))  # convert from 1-d arr to a 2-d arr with one col\n",
    "            X_dims.append(1)\n",
    "\n",
    "        else:  # for binary categorical variables, the label binarizer uses just one var instead of two\n",
    "            enc = preprocessing.OneHotEncoder(categories='auto', handle_unknown='error')\n",
    "            enc.fit(vals.reshape(-1, 1))\n",
    "            vals = enc.transform(vals.reshape(-1, 1)).todense()\n",
    "            X_dims += [vals.shape[1]]*vals.shape[1]\n",
    "\n",
    "        # add to learnable features\n",
    "        X.append(vals)\n",
    "\n",
    "        if attr in CONT_VARIABLES:  # continuous feature, just append the name\n",
    "            feature_names.append(attr)\n",
    "        else:  # categorical features\n",
    "            if vals.shape[1] == 1:  # binary features that passed through lib binarizer\n",
    "                feature_names.append(attr)\n",
    "            else:\n",
    "                for k in enc.categories_:  # non-binary categorical features, need to add the names for each cat\n",
    "                    feature_names.append(attr + \"_\" + str(k))\n",
    "\n",
    "    X = np.array(np.concatenate(list(X), axis=1))\n",
    "    X_dims = np.array(X_dims)\n",
    "\n",
    "    if separate_test:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=rseed, shuffle=True)\n",
    "\n",
    "        x_means, x_stds = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "        x_means[X_dims>1] = 0\n",
    "        x_stds[X_dims>1] = 1\n",
    "        x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "        x_train = ((X_train - x_means) / x_stds).astype(np.float32)\n",
    "        x_test = ((X_test - x_means) / x_stds).astype(np.float32)\n",
    "\n",
    "        return x_train, x_test, x_means, x_stds, y_train, y_test, feature_names, X_dims\n",
    "    else:\n",
    "        x_means, x_stds = X.mean(axis=0), X.std(axis=0)\n",
    "        print(X_dims.shape, x_means.shape)\n",
    "        x_means[:,X_dims>1] = 0\n",
    "        x_stds[:,X_dims>1] = 1\n",
    "        x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "        x_train = ((X - x_means) / x_stds).astype(np.float32)\n",
    "\n",
    "        return x_train, x_means, x_stds, y, feature_names, X_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_file(fname):\n",
    "    files = os.listdir(\".\")  # get the current directory listing\n",
    "    print\n",
    "    \"Looking for file '%s' in the current directory...\" % fname\n",
    "\n",
    "    if fname not in files:\n",
    "        print\n",
    "        \"'%s' not found! Downloading from GitHub...\" % fname\n",
    "        addr = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
    "        \n",
    "        response = urllib.request.urlopen(addr)\n",
    "\n",
    "        data = response.read()\n",
    "        fileOut = open(fname, \"wb\")\n",
    "        fileOut.write(data)\n",
    "        fileOut.close()\n",
    "        print\n",
    "        \"'%s' download and saved locally..\" % fname\n",
    "    else:\n",
    "        print\n",
    "        \"File found in current directory..\"\n",
    "\n",
    "def mkdir(paths):\n",
    "    if not isinstance(paths, (list, tuple)):\n",
    "        paths = [paths]\n",
    "    for path in paths:\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path, mode=0o777)\n",
    "            \n",
    "def unnormalise_cat_vars(x, x_means, x_stds, input_dim_vec):\n",
    "    input_dim_vec = np.array(input_dim_vec)\n",
    "    unnorm_x = np.multiply(x, x_stds) + x_means\n",
    "\n",
    "    fixed_unnorm = unnorm_x.round()\n",
    "    # -=\n",
    "    fixed_unnorm = fixed_unnorm - fixed_unnorm.min(axis=0).reshape([1, fixed_unnorm.shape[1]])  # this sets all mins to 0\n",
    "    for idx, dims in enumerate(input_dim_vec):\n",
    "        if dims > 1:\n",
    "            vec = fixed_unnorm[:, idx]\n",
    "            vec[vec > dims - 1] = dims - 1\n",
    "            fixed_unnorm[:, idx] = vec\n",
    "\n",
    "    x[:, input_dim_vec > 1] = fixed_unnorm[:, input_dim_vec > 1]\n",
    "    return x\n",
    "\n",
    "def X_dims_to_input_dim_vec(X_dims):\n",
    "    \"\"\"This is for our cat_Gauss VAE model\"\"\"\n",
    "    input_dim_vec = []\n",
    "    i = 0\n",
    "    while i < len(X_dims):\n",
    "        input_dim_vec.append(X_dims[i])\n",
    "        i += X_dims[i]\n",
    "    return np.array(input_dim_vec)\n",
    "\n",
    "def gauss_cat_to_flat(x, input_dim_vec):\n",
    "    output = []\n",
    "    for idx, dim in enumerate(input_dim_vec):\n",
    "        if dim == 1:\n",
    "            output.append(tf.expand_dims(x[:, idx], axis=1))\n",
    "        elif dim > 1:            \n",
    "            oh_vec = tf.one_hot(x[:, idx], dim)\n",
    "            output.append(oh_vec)\n",
    "        else:\n",
    "            raise ValueError('Error, invalid dimension value')\n",
    "    return tf.concat(output, axis=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compas (5554, 17) (618, 17)\n",
      "[3 6 2 2 2 1 1]\n"
     ]
    }
   ],
   "source": [
    "if dname == 'wine':\n",
    "    x_train, x_test, x_means, x_stds, y_train, y_test, y_means, y_stds = \\\n",
    "    load_UCI(dset_name='wine', splits=10, seed=42, separate_targets=True, save_dir='../data/')\n",
    "    \n",
    "elif dname == 'default_credit':\n",
    "    # Note that this dataset is given without flattening\n",
    "    x_train, x_test, x_means, x_stds, y_train, y_test, y_means, y_stds = \\\n",
    "    load_UCI(dset_name='default_credit', splits=10, seed=42, separate_targets=True, save_dir='../data/')\n",
    "    print('Credit', x_train.shape, x_test.shape)\n",
    "    input_dim_vec = [1, 2, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ,1 ,1 ,1 ,1 ,1]\n",
    "    \n",
    "    x_train = unnormalise_cat_vars(x_train, x_means, x_stds, input_dim_vec)\n",
    "    x_test = unnormalise_cat_vars(x_test, x_means, x_stds, input_dim_vec)\n",
    "    # target unnormalisation\n",
    "    y_train = unnormalise_cat_vars(y_train, y_means, y_stds, [2])\n",
    "    y_test = unnormalise_cat_vars(y_test, y_means, y_stds, [2])\n",
    "    \n",
    "    x_train = gauss_cat_to_flat(torch.Tensor(x_train), input_dim_vec)\n",
    "    x_test = gauss_cat_to_flat(torch.Tensor(x_test), input_dim_vec)\n",
    "\n",
    "    print(input_dim_vec)\n",
    "\n",
    "elif dname == 'compas':\n",
    "    x_train, x_test, x_means, x_stds, y_train, y_test, feature_names, X_dims = \\\n",
    "    get_my_COMPAS(rseed=42, separate_test=True, test_ratio=0.1, save_dir='../data/')\n",
    "    input_dim_vec = X_dims_to_input_dim_vec(X_dims)\n",
    "    print('Compas', x_train.shape, x_test.shape)\n",
    "    print(input_dim_vec)\n",
    "    \n",
    "elif dname == 'lsat':\n",
    "    x_train, x_test, x_means, x_stds, y_train, y_test, y_means, y_stds, my_data_keys, input_dim_vec = \\\n",
    "    get_my_LSAT(save_dir='../data/')\n",
    "    print('LSAT', x_train.shape, x_test.shape)\n",
    "    print(input_dim_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = ['wine', 'default_credit', 'compas', 'lsat']\n",
    "# widths = [300, 300, 300, 300] # [200, 200, 200, 200]\n",
    "# depths = [3, 3, 3, 3] # We go deeper because we are using residual models\n",
    "# latent_dims = [6, 8, 4, 4]\n",
    "\n",
    "\n",
    "names = ['wine', 'default_credit', 'compas', 'lsat']\n",
    "bnn_widths = [200, 200, 200, 200]\n",
    "bnn_depths = [2, 2, 2, 2]\n",
    "\n",
    "vae_widths = [300, 300, 300, 300] # [200, 200, 200, 200]\n",
    "vae_depths = [3, 3, 3, 3] # We go deeper because we are using residual models\n",
    "vae_latent_dims = [6, 8, 4, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BNN Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from BNN.models import MLP_gauss\n",
    "\n",
    "\"\"\"\n",
    "BNN/models.py for tf\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "class MLP_gauss(tf.keras.Model):\n",
    "    def __init__(self, input_dim, width, depth, output_dim, flatten_image):\n",
    "        super(MLP_gauss, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.flatten_image = flatten_image\n",
    "        self.block = tf.keras.Sequential()\n",
    "\n",
    "        for i in range(depth):\n",
    "            self.block.add(tf.keras.layers.Dense(width, activation=tf.nn.relu))\n",
    "        self.block.add(tf.keras.layers.Dense(2*output_dim))\n",
    "\n",
    "    def call(self, x):\n",
    "        if self.flatten_image:\n",
    "            x = tf.reshape(x,[-1, self.input_dim])\n",
    "        x = self.block(x)\n",
    "        mu = x[:, :self.output_dim]\n",
    "        sigma = tf.math.softplus(x[:, self.output_dim:])\n",
    "        return mu, sigma\n",
    "\n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self, input_dim, width, depth, output_dim, flatten_image):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.flatten_image = flatten_image\n",
    "        self.block = tf.keras.Sequential()\n",
    "\n",
    "        for i in range(depth):\n",
    "            self.block.add(tf.keras.layers.Dense(width, activation=tf.nn.relu))\n",
    "        self.block.add(tf.keras.layers.Dense(output_dim))\n",
    "\n",
    "    def call(self, x):\n",
    "        if self.flatten_image:\n",
    "            x = tf.reshape(x,[-1, self.input_dim])\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from BNN.sampler import H_SA_SGHMC\n",
    "\n",
    "from numpy.random import gamma\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\"\"\"\n",
    "BNN/sampler.py for tf\n",
    "\"\"\"\n",
    "\n",
    "class H_SA_SGHMC(tf.keras.optimizers.Optimizer):\n",
    "    \"\"\" Stochastic Gradient Hamiltonian Monte-Carlo Sampler that uses a burn-in\n",
    "        procedure to adapt its own hyperparameters during the initial stages\n",
    "        of sampling.\"\"\"\n",
    "\n",
    "    #def __init__(self, params, lr=1e-2, base_C=0.05, gauss_sig=0.1, alpha0=10, beta0=10):\n",
    "    def __init__(self, learning_rate=1e-2, base_C=0.5, gauss_sig=0.1,alpha0=10, beta0=10,name=\"SA-SHMC\", **kwargs):\n",
    "        \"\"\"Call super().__init__() and use _set_hyper() to store hyperparameters\"\"\"\n",
    "\n",
    "        \"\"\" Set up a SGHMC Optimizer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : iterable\n",
    "            Parameters serving as optimization variable.\n",
    "        lr: float, optional\n",
    "            Base learning rate for this optimizer.\n",
    "            Must be tuned to the specific function being minimized.\n",
    "            Default: `1e-2`.\n",
    "        base_C:float, optional\n",
    "            (Constant) momentum decay per time-step.\n",
    "            Default: `0.05`.\n",
    "        \"\"\"\n",
    "        super().__init__(name, **kwargs)\n",
    "        #self._set_hyper(\"learning_rate\", kwargs.get(\"lr\", learning_rate)) # handle lr=learning_rate\n",
    "        #self._set_hyper(\"decay\", self._initial_decay) #\n",
    "        #self._set_hyper(\"momentum\", momentum)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.base_C = base_C\n",
    "\n",
    "        self.eps = 1e-6\n",
    "        self.alpha0 = alpha0\n",
    "        self.beta0 = beta0\n",
    "\n",
    "        if gauss_sig == 0:\n",
    "            self.weight_decay = 0\n",
    "        else:\n",
    "            self.weight_decay = 1 / (gauss_sig ** 2)\n",
    "\n",
    "        if self.weight_decay <= 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "        if self.learning_rate < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if self.base_C < 0:\n",
    "            raise ValueError(\"Invalid friction term: {}\".format(base_C))\n",
    "        self.resample_prior = False\n",
    "        self.burn_in = True # We update g first as it makes most sense\n",
    "        self.resample_momentum = False  # e\n",
    "        #defaults = dict(\n",
    "        #    lr=lr,\n",
    "        #    base_C=base_C,\n",
    "        #)\n",
    "        #super(H_SA_SGHMC, self).__init__(params, defaults)\n",
    "\n",
    "\n",
    "    def _create_slots(self, var_list):\n",
    "        \"\"\"For each model variable, create the optimizer variable associated with it.\n",
    "        TensorFlow calls these optimizer variables \"slots\".\n",
    "        For momentum optimization, we need one momentum slot per model variable.\n",
    "        \"\"\"\n",
    "        for var in var_list:\n",
    "            #self.add_slot(var, \"pv\") #previous variable i.e. weight or bias\n",
    "            #pv = self.get_slot(var, \"pv\")\n",
    "            #pv.assign\n",
    "            self.add_slot(var, \"g\")\n",
    "            g = self.get_slot(var, \"g\")\n",
    "            g.assign(tf.ones(g.shape))\n",
    "            self.add_slot(var, \"weight_decay\", shape = (1,)) #w_decay\n",
    "            wd = self.get_slot(var, \"weight_decay\")\n",
    "            wd.assign(tf.ones(wd.shape)*self.weight_decay)\n",
    "            self.add_slot(var, \"tau\")\n",
    "            tau = self.get_slot(var, \"tau\")\n",
    "            tau.assign(tf.ones(tau.shape))\n",
    "            self.add_slot(var, \"v_hat\")\n",
    "            v_hat = self.get_slot(var, \"v_hat\")\n",
    "            v_hat.assign(tf.ones(v_hat.shape))\n",
    "            self.add_slot(var, \"v_momentum\")\n",
    "            v_m = self.get_slot(var, \"v_momentum\")\n",
    "            v_m.assign(tf.zeros(v_m.shape))\n",
    "\n",
    "    @tf.function\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        \"\"\"Update the slots and perform one optimization step for one model variable\n",
    "        (model variable == bias or weight mtx)\n",
    "        \"\"\"\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        #lr_t = self._decayed_lr(var_dtype) # handle learning rate decay\n",
    "        lr_t = self.learning_rate\n",
    "        #pv_var = self.get_slot(var, \"pv\")\n",
    "        #pg_var = self.get_slot(var, \"pg\")\n",
    "        g = self.get_slot(var, \"g\")\n",
    "        wd = self.get_slot(var, \"weight_decay\")\n",
    "        tau = self.get_slot(var, \"tau\")\n",
    "        V_hat = self.get_slot(var, \"v_hat\")\n",
    "        v_m = self.get_slot(var, \"v_momentum\")\n",
    "\n",
    "        #if self._is_first:\n",
    "        #    self._is_first = False\n",
    "        #    new_var = new_var_m\n",
    "        #else:\n",
    "        \"\"\"\n",
    "        new_var_m = var - grad * lr_t\n",
    "        cond = grad*pg_var >= 0\n",
    "        print(cond)\n",
    "        avg_weights = (pv_var + var)/2.0\n",
    "        new_var = tf.where(cond, new_var_m, avg_weights)\n",
    "        pv_var.assign(var)\n",
    "        pg_var.assign(grad)\n",
    "        var.assign(new_var)\n",
    "        \"\"\"\n",
    "        if self.resample_prior:\n",
    "            alpha = tf.cast(self.alpha0 + tf.size(var)/2, tf.float32)\n",
    "            beta = tf.cast(self.beta0 + tf.reduce_sum(var ** 2)/2, tf.float32)\n",
    "            gamma_sample = tfp.distributions.Gamma(concentration=alpha, rate=beta).sample()# scale=1 / (beta), size=None)#TODO!!! (nosierto xk hay np)\n",
    "            wd.assign(tf.ones(wd.shape)*gamma_sample)\n",
    "        d_p = grad\n",
    "        if self.weight_decay != 0:\n",
    "            d_p += self.weight_decay*var\n",
    "\n",
    "        # update parameters during burn-in\n",
    "        if self.burn_in:  # We update g first as it makes most sense\n",
    "            tau.assign(tau - tau * (g ** 2) / (V_hat + self.eps) + 1)  # specifies the moving average window, see Eq 9 in [1] left\n",
    "            tau_inv = 1. / (tau + self.eps)\n",
    "            g.assign(g -tau_inv * g + tau_inv * d_p)  # average gradient see Eq 9 in [1] right\n",
    "            V_hat.assign(V_hat - tau_inv * V_hat + tau_inv * (d_p ** 2))  # gradient variance see Eq 8 in [1]\n",
    "\n",
    "        V_sqrt = tf.sqrt(V_hat)\n",
    "        V_inv_sqrt = 1. / (V_sqrt + self.eps)  # preconditioner\n",
    "\n",
    "        #TODO: tf.normalllll\n",
    "        if self.resample_momentum:  # equivalent to var = M under momentum reparametrisation\n",
    "            v_m.assign(tfp.distributions.Normal(loc=tf.zeros(d_p.shape),scale=tf.sqrt((self.learning_rate ** 2) * V_inv_sqrt)).sample())\n",
    "        #v_momentum = v_m\n",
    "        noise_var = (2. * (self.learning_rate ** 2) * V_inv_sqrt * self.base_C - (self.learning_rate ** 4))\n",
    "        noise_std = tf.sqrt(tf.math.maximum(noise_var, 1e-16))\n",
    "        # sample random epsilon\n",
    "        noise_sample = tfp.distributions.Normal(loc=tf.zeros(d_p.shape), scale=tf.ones(d_p.shape) * noise_std).sample()\n",
    "\n",
    "        # update momentum (Eq 10 right in [1])\n",
    "        v_m.assign(v_m - (lr_t ** 2) * V_inv_sqrt * d_p - self.base_C * v_m + noise_sample)\n",
    "\n",
    "        # update theta (Eq 10 left in [1])\n",
    "        var.assign(var + v_m)\n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BNN/wraper.py for tf\n",
    "\"\"\"\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "from src.probability import diagonal_gauss_loglike, get_rms, get_loglike\n",
    "from src.utils import BaseNet, cprint, save_object, load_object#,to_variable\n",
    "\n",
    "class BNN_cat(BaseNet):  # for categorical distributions\n",
    "    def __init__(self, model, N_train, lr=1e-2, cuda=True, grad_std_mul=30, seed=None):\n",
    "        super(BNN_cat, self).__init__()\n",
    "        cprint('y', 'BNN categorical output')\n",
    "        self.lr = lr\n",
    "        self.model = model\n",
    "        self.cuda = cuda\n",
    "        self.seed = seed\n",
    "\n",
    "        self.N_train = N_train\n",
    "        self.create_net()\n",
    "        self.create_opt()\n",
    "        self.schedule = None  # [] #[50,200,400,600]\n",
    "        self.epoch = 0\n",
    "\n",
    "        self.grad_buff = []\n",
    "        self.max_grad = 1e20\n",
    "        self.grad_std_mul = grad_std_mul\n",
    "        self.cce = tf.keras.losses.CategoricalCrossentropy(from_logits = True)\n",
    "        self.weight_set_samples = []\n",
    "\n",
    "    def create_net(self):\n",
    "        if self.seed is None:\n",
    "            tf.random.set_seed(42)\n",
    "        else:\n",
    "            tf.random.set_seed(self.seed)\n",
    "        #if self.cuda:\n",
    "        #    if self.seed is None:\n",
    "        #        torch.cuda.manual_seed(42)\n",
    "        #    else:\n",
    "        #        torch.cuda.manual_seed(self.seed)\n",
    "        #if self.cuda:\n",
    "        #    self.model.cuda()\n",
    "        #    cudnn.benchmark = True\n",
    "        nparams = self.get_nb_parameters()\n",
    "        print('    Total params: %.2fK' % (tf.cast(nparams, tf.float32)/ 1000.0))\n",
    "\n",
    "    def create_opt(self):\n",
    "        \"\"\"This optimiser incorporates the gaussian prior term automatically. The prior variance is gibbs sampled from\n",
    "        its posterior using a gamma hyper-prior.\"\"\"\n",
    "        self.optimizer = H_SA_SGHMC(lr=self.lr, base_C=0.05, gauss_sig=0.1)\n",
    "\n",
    "    @tf.function\n",
    "    def fit(self, x, y, burn_in=False, resample_momentum=False, resample_prior=False):\n",
    "        #self.set_mode_train(train=True)\n",
    "        #one optimization step\n",
    "        #x, y = to_variable(var=(x, y), cuda=self.cuda)\n",
    "        #self.optimizer.zero_grad()\n",
    "        #cce = tf.keras.losses.CategoricalCrossentropy(from_logits = True)\n",
    "        #y1 = tf.one_hot(tf.cast(y,tf.int32), 2)\n",
    "        #y1 = tf.reshape(y1,[y1.shape[0], y1.shape[-1]])\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            out = self.model(x)\n",
    "            #loss = cce(out, y1)\n",
    "            #loss = self.cce(out, y)\n",
    "            loss = tf.nn.softmax_cross_entropy_with_logits(tf.one_hot(y,2), out)\n",
    "            loss = self.N_train*tf.reduce_mean(loss)  # We use mean because we treat as an estimation of whole dataset\n",
    "\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        #set optimizer hyperparams\n",
    "        self.optimizer.burn_in = burn_in\n",
    "        self.optimizer.resample_momentum = resample_momentum\n",
    "        self.optimizer.resample_prior = resample_prior\n",
    "\n",
    "        #loss.backward()\n",
    "        if len(self.grad_buff) > 1000:\n",
    "            self.max_grad = np.mean(self.grad_buff) + self.grad_std_mul * np.std(self.grad_buff)\n",
    "            self.grad_buff.pop(0)\n",
    "\n",
    "\n",
    "        for g in gradients:\n",
    "            g = tf.clip_by_norm(g, clip_norm=self.max_grad)\n",
    "        \"\"\"\n",
    "          self.grad_buff.append(g) \n",
    "          max_norm=self.max_grad, norm_type=2))\n",
    "        if self.grad_buff[-1] >= self.max_grad:\n",
    "            print(self.max_grad, self.grad_buff[-1])\n",
    "            self.grad_buff.pop()\n",
    "        \"\"\"\n",
    "\n",
    "        #self.optimizer.step(burn_in=burn_in, resample_momentum=resample_momentum, resample_prior=resample_prior)\n",
    "        self.optimizer.apply_gradients(\n",
    "                  (grad, var) for (grad, var) in zip(gradients, self.model.trainable_variables))\n",
    "\n",
    "        # out: (batch_size, out_channels, out_caps_dims)\n",
    "        pred = tf.math.argmax(out, axis=1)  # get the index of the max log-probability\n",
    "        err = tf.math.equal(tf.math.equal(pred, tf.cast(y, tf.int64)), tf.constant(False))\n",
    "        err = tf.reduce_sum(tf.cast(err, tf.float32))\n",
    "\n",
    "        return loss * x.shape[0] / self.N_train, err\n",
    "\n",
    "    def eval(self, x, y, train=False):\n",
    "        #self.set_mode_train(train=False)\n",
    "        #x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "        #cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        #y1 = tf.one_hot(tf.cast(y,tf.int32), 2)\n",
    "        #y1 = tf.reshape(y1,[y1.shape[0], y1.shape[-1]])\n",
    "        out = self.model(x)\n",
    "        #loss = cce(out, y1)\n",
    "        loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(tf.one_hot(y, 2), out))\n",
    "        probs = tf.nn.softmax(out, axis=1)\n",
    "\n",
    "        pred = tf.math.argmax(out, axis=1)  # get the index of the max log-probability\n",
    "        err = tf.math.equal(tf.math.equal(pred, y), tf.constant(False))\n",
    "        err = tf.reduce_sum(tf.cast(err, tf.float32))\n",
    "\n",
    "        return loss, err, probs\n",
    "\n",
    "    def save_sampled_net(self, max_samples):\n",
    "\n",
    "        if len(self.weight_set_samples) >= max_samples:\n",
    "            self.weight_set_samples.pop(0)\n",
    "\n",
    "        self.weight_set_samples.append(self.model.get_weights())\n",
    "\n",
    "        cprint('c', ' saving weight samples %d/%d' % (len(self.weight_set_samples), max_samples))\n",
    "        return None\n",
    "\n",
    "    def predict(self, x):\n",
    "        #self.set_mode_train(train=False)\n",
    "        #x, = to_variable(var=(x, ), cuda=self.cuda)\n",
    "        out = self.model(x)\n",
    "        probs = tf.nn.softmax(out, axis=1)\n",
    "        return probs\n",
    "\n",
    "    def sample_predict(self, x, Nsamples, grad=False):\n",
    "        \"\"\"return predictions using multiple samples from posterior\"\"\"\n",
    "        #self.set_mode_train(train=False)\n",
    "        if Nsamples == 0:\n",
    "            Nsamples = len(self.weight_set_samples)\n",
    "\n",
    "        out = []\n",
    "\n",
    "        for idx, weight_dict in enumerate(self.weight_set_samples):\n",
    "            if idx == Nsamples:\n",
    "                break\n",
    "            self.model.set_weights(weight_dict)\n",
    "            out.append(self.model(x))\n",
    "\n",
    "        out = out[:idx]\n",
    "        prob_out = tf.nn.softmax(tf.stack(out, axis=0), axis = 2)\n",
    "        return prob_out\n",
    "\n",
    "    def get_weight_samples(self, Nsamples=0):\n",
    "        \"\"\"return weight samples from posterior in a single-column array\"\"\"\n",
    "        \"\"\"\n",
    "        weight_vec = []\n",
    "\n",
    "        if Nsamples == 0 or Nsamples > len(self.weight_set_samples):\n",
    "            Nsamples = len(self.weight_set_samples)\n",
    "\n",
    "        for idx, state_dict in enumerate(self.weight_set_samples):\n",
    "            if idx == Nsamples:\n",
    "                break\n",
    "\n",
    "            for key in state_dict.keys():\n",
    "                if 'weight' in key:\n",
    "                    weight_mtx = state_dict[key].cpu().data\n",
    "                    for weight in weight_mtx.view(-1):\n",
    "                        weight_vec.append(weight)\n",
    "\n",
    "        return np.array(weight_vec)\n",
    "        \"\"\"\n",
    "        return self.weight_set_samples\n",
    "        \n",
    "    def save_weights(self, filename):\n",
    "        save_object(self.weight_set_samples, filename)\n",
    "\n",
    "    def load_weights(self, filename, subsample=1):\n",
    "        self.weight_set_samples = load_object(filename)\n",
    "        self.weight_set_samples = self.weight_set_samples[::subsample]\n",
    "\n",
    "class BNN_gauss(BaseNet):\n",
    "    def __init__(self, model, N_train, lr=1e-2, cuda=True, eps=1e-3, grad_std_mul=20, seed = None):\n",
    "        super(BNN_gauss, self).__init__()\n",
    "        cprint('y', ' Creating Net!! ')\n",
    "        cprint('y', 'BNN gaussian output')\n",
    "        self.lr = lr\n",
    "        self.model = model\n",
    "        self.cuda = cuda\n",
    "        self.seed = seed\n",
    "        self.N_train = N_train\n",
    "        self.create_net()\n",
    "        self.create_opt()\n",
    "        self.schedule = None  # [] #[50,200,400,600]\n",
    "        self.epoch = 0\n",
    "\n",
    "        self.grad_buff = []\n",
    "        self.grad_std_mul = grad_std_mul\n",
    "        self.max_grad = 1e20\n",
    "        self.eps = eps\n",
    "\n",
    "        self.weight_set_samples = []\n",
    "\n",
    "    def create_net(self):\n",
    "        if self.seed is None:\n",
    "            tf.random.set_seed(42)\n",
    "        else:\n",
    "            tf.random.set_seed(self.seed)\n",
    "        #if self.cuda:\n",
    "        #    if self.seed is None:\n",
    "        #        torch.cuda.manual_seed(42)\n",
    "        #    else:\n",
    "        #        torch.cuda.manual_seed(self.seed)\n",
    "\n",
    "        #if self.cuda:\n",
    "        #    self.model.cuda()\n",
    "        #    cudnn.benchmark = True\n",
    "        nparams = self.get_nb_parameters()\n",
    "        print('    Total params: %.2fK' % (tf.cast(nparams, tf.float32)/ 1000.0))\n",
    "\n",
    "    def create_opt(self):\n",
    "        self.optimizer = H_SA_SGHMC(lr=self.lr, base_C=0.05, gauss_sig=0.1)\n",
    "\n",
    "    @tf.function\n",
    "    def fit(self, x, y, burn_in=False, resample_momentum=False, resample_prior=False):#TODO\n",
    "        #self.set_mode_train(train=True)\n",
    "        #one optimization step\n",
    "        #x, y = to_variable(var=(x, y), cuda=self.cuda)\n",
    "\n",
    "        #self.optimizer.zero_grad()\n",
    "        #print(\"x an y!!!! cheipsss\")\n",
    "        #print(x.shape)\n",
    "        #print(y.shape)\n",
    "        with tf.GradientTape() as tape:\n",
    "            mu, sigma = self.model(x)\n",
    "            sigma = tf.math.maximum(sigma, self.eps)\n",
    "            loss = tf.reduce_mean(-diagonal_gauss_loglike(y, mu, sigma), axis=0)* self.N_train\n",
    "\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        #set optimizer hyperparams\n",
    "        self.optimizer.burn_in = burn_in\n",
    "        self.optimizer.resample_momentum = resample_momentum\n",
    "        self.optimizer.resample_prior = resample_prior\n",
    "\n",
    "        #loss.backward()\n",
    "        if len(self.grad_buff) > 100:\n",
    "            self.max_grad = np.mean(self.grad_buff) + self.grad_std_mul * np.std(self.grad_buff)\n",
    "            self.grad_buff.pop(0)\n",
    "\n",
    "        #clippin grads\n",
    "\n",
    "        for g in gradients:\n",
    "          g = tf.clip_by_norm(g, clip_norm=self.max_grad)\n",
    "        \"\"\"\n",
    "          self.grad_buff.append(g)\n",
    "        print(self.grad_buff)\n",
    "        print(len(self.grad_buff))\n",
    "        if self.grad_buff[-1] >= self.max_grad:\n",
    "            print(self.max_grad, self.grad_buff[-1])\n",
    "            self.grad_buff.pop()\n",
    "        \"\"\"\n",
    "        #self.optimizer.step(burn_in=burn_in, resample_momentum=resample_momentum, resample_prior=resample_prior)\n",
    "        self.optimizer.apply_gradients(\n",
    "                  (grad, var) for (grad, var) in zip(gradients, self.model.trainable_variables))\n",
    "\n",
    "        return loss * x.shape[0] / self.N_train, mu, sigma\n",
    "\n",
    "    def eval(self, x, y):\n",
    "        #self.set_mode_train(train=False)\n",
    "        #x, y = to_variable(var=(x, y), cuda=self.cuda)\n",
    "        mu, sigma = self.model(x)\n",
    "        sigma = tf.math.maximum(sigma, self.eps)\n",
    "        loss = tf.reduce_mean(-diagonal_gauss_loglike(y, mu, sigma), axis=0) * self.N_train\n",
    "\n",
    "        return loss * x.shape[0] / self.N_train, mu, sigma\n",
    "\n",
    "    @staticmethod\n",
    "    def unnormalised_eval(pred_mu, pred_std, y, y_mu, y_std, gmm=False):\n",
    "        ll = get_loglike(pred_mu, pred_std, y, y_mu, y_std, gmm=gmm)  # this already computes sum\n",
    "        if gmm:\n",
    "            pred_mu = tf.reduce_mean(pred_mu, axis=0)\n",
    "        rms = get_rms(pred_mu, y, y_mu, y_std)  # this already computes sum\n",
    "        return rms, ll\n",
    "\n",
    "    def predict(self, x):\n",
    "        #self.set_mode_train(train=False)\n",
    "        #x, = to_variable(var=(x,), cuda=self.cuda)\n",
    "        mu, sigma = self.model(x)\n",
    "        return mu, sigma\n",
    "\n",
    "    def save_sampled_net(self, max_samples):\n",
    "\n",
    "        if len(self.weight_set_samples) >= max_samples:\n",
    "            self.weight_set_samples.pop(0)\n",
    "\n",
    "        self.weight_set_samples.append(self.model.get_weights())\n",
    "\n",
    "        cprint('c', ' saving weight samples %d/%d' % (len(self.weight_set_samples), max_samples))\n",
    "\n",
    "        return None\n",
    "\n",
    "    def sample_predict(self, x, Nsamples, grad=False):\n",
    "        #self.set_mode_train(train=False)\n",
    "        if Nsamples == 0:\n",
    "            Nsamples = len(self.weight_set_samples)\n",
    "        #x, = to_variable(var=(x, ), cuda=self.cuda)\n",
    "\n",
    "        mu_vec = []\n",
    "        std_vec = []\n",
    "\n",
    "        # iterate over all saved weight configuration samples\n",
    "        for idx, weight_dict in enumerate(self.weight_set_samples):\n",
    "            if idx == Nsamples:\n",
    "                break\n",
    "            self.model.set_weights(weight_dict)\n",
    "            mu_vec, std_vec = self.model(x)\n",
    "            mu_vec.append(mu)\n",
    "            std_vec.append(std)\n",
    "\n",
    "        return tf.stack(mu_vec[:idx], axis=0), tf.stack(std_vec[:idx], axis=0)\n",
    "\n",
    "    def get_weight_samples(self, Nsamples=0):\n",
    "        \"\"\"\n",
    "        weight_vec = []\n",
    "\n",
    "        if Nsamples == 0 or Nsamples > len(self.weight_set_samples):\n",
    "            Nsamples = len(self.weight_set_samples)\n",
    "\n",
    "        for idx, weights in enumerate(self.weight_set_samples):\n",
    "            if idx == Nsamples:\n",
    "                break\n",
    "            weight_vec.append(weight)\n",
    "            #for key in state_dict.keys():\n",
    "            #    if 'weight' in key:\n",
    "            #        weight_mtx = state_dict[key].cpu().data\n",
    "            #        for weight in weight_mtx.view(-1):\n",
    "             #           weight_vec.append(weight)\n",
    "        return np.array(weight_vec)\n",
    "        \"\"\"\n",
    "        return self.weight_set_samples\n",
    "        \n",
    "\n",
    "    def save_weights(self, filename):\n",
    "        save_object(self.weight_set_samples, filename)\n",
    "\n",
    "    def load_weights(self, filename):\n",
    "        self.weight_set_samples = load_object(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\n",
      "Net:\u001b[0m\n",
      "\u001b[33mBNN categorical output\u001b[0m\n",
      "    Total params: 44.20K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 23:47:32.126908: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2021-12-10 23:47:32.126943: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ali-Predator-PH315-52\n",
      "2021-12-10 23:47:32.126948: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ali-Predator-PH315-52\n",
      "2021-12-10 23:47:32.127016: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 495.29.5\n",
      "2021-12-10 23:47:32.127035: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 495.29.5\n",
      "2021-12-10 23:47:32.127039: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 495.29.5\n",
      "2021-12-10 23:47:32.127413: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#NOTE: RUN ALL THE NOTEBOOK AT LEAST UNTIL THE \"LOAD DATASETS CELL\" (BEFORE \"Compas Classification BNN\" section)\n",
    "\n",
    "# from BNN.wrapper import BNN_gauss, BNN_cat, MLP\n",
    "import numpy as np\n",
    "\n",
    "main_dir = '.'\n",
    "\n",
    "save_dir = main_dir + '/saves/fc_BNN_NEW_' + dname\n",
    "\n",
    "\n",
    "N_train = x_train.shape[0]\n",
    "input_dim = x_train.shape[1]\n",
    "width = bnn_widths[names.index(dname)]\n",
    "depth = bnn_depths[names.index(dname)]\n",
    "output_dim = 2\n",
    "model = MLP(input_dim, width, depth, output_dim, flatten_image=False)\n",
    "model.call(x_train[:10])\n",
    "\n",
    "lr = 1e-2\n",
    "cuda = False#torch.cuda.is_available()\n",
    "BNN = BNN_cat(model, N_train, lr=lr, cuda=cuda)\n",
    "\n",
    "\n",
    "\n",
    "save_dir = main_dir + '/saves/fc_BNN_NEW_' + dname\n",
    "\n",
    "BNN.load_weights(save_dir + '_models/state_dicts.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def create_encoder(input_dim_vec,width,depth,latent_dim):\n",
    "    inputs = keras.Input(shape=(sum(input_dim_vec),))\n",
    "    input = layers.Dense(width)(inputs)\n",
    "\n",
    "    for i in range(depth-1):\n",
    "\n",
    "        x = layers.LeakyReLU()(input)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(width, kernel_initializer=tf.keras.initializers.Zeros())(x)\n",
    "\n",
    "        x = x + input\n",
    "\n",
    "        input = x\n",
    "\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    outputs = layers.Dense(latent_dim*2,kernel_initializer=tf.keras.initializers.Zeros())(x)\n",
    "\n",
    "    encoder = keras.Model(inputs=inputs, outputs=outputs, name=\"encoder_model\")\n",
    "\n",
    "    # encoder.summary()\n",
    "\n",
    "    # keras.utils.plot_model(encoder, \"encoder.png\")\n",
    "    keras.utils.plot_model(encoder, \"encoder.png\", show_shapes=True)\n",
    "    \n",
    "    return encoder\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def create_decoder(input_dim_vec,width,depth,latent_dim):\n",
    "\n",
    "    inputs = keras.Input(shape=(latent_dim,))\n",
    "    input = layers.Dense(width,kernel_initializer=tf.keras.initializers.Zeros())(inputs)\n",
    "    input = layers.LeakyReLU()(input)\n",
    "    input = layers.BatchNormalization()(input)\n",
    "\n",
    "    for i in range(depth-1):\n",
    "\n",
    "        x = layers.LeakyReLU()(input)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(width,kernel_initializer=tf.keras.initializers.Zeros())(x)\n",
    "\n",
    "        x = x + input\n",
    "\n",
    "        input = x\n",
    "\n",
    "    outputs = layers.Dense(sum(input_dim_vec),kernel_initializer=tf.keras.initializers.Zeros())(x)\n",
    "\n",
    "    decoder = keras.Model(inputs=inputs, outputs=outputs, name=\"decoder_model\")\n",
    "\n",
    "    # decoder.summary()\n",
    "\n",
    "    # keras.utils.plot_model(model, \"decoder_model.png\")\n",
    "    keras.utils.plot_model(decoder, \"decoder.png\", show_shapes=True)\n",
    "    \n",
    "    return decoder\n",
    "\n",
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, input_dim_vec,width,depth,latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = create_encoder(input_dim_vec,width,depth,latent_dim)\n",
    "        self.decoder = create_decoder(input_dim_vec,width,depth,latent_dim)\n",
    "\n",
    "    @tf.function\n",
    "    def sample(self, eps=None):\n",
    "        if eps is None:\n",
    "            eps = tf.random.normal(shape=(100, self.latent_dim))\n",
    "        return self.decode(eps, apply_sigmoid=True)\n",
    "\n",
    "    def encode(self, x):\n",
    "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "    \n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        logits = self.decoder(z)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits\n",
    "    \n",
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(-.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),axis=raxis)\n",
    "\n",
    "def compute_loss(model, x):\n",
    "    mean, logvar = model.encode(x)\n",
    "    z = model.reparameterize(mean, logvar)\n",
    "    x_logit = model.decode(z)\n",
    "    cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
    "    logpx_z = -tf.reduce_sum(cross_ent)#, axis=[1, 2, 3])\n",
    "    logpz = log_normal_pdf(z, 0., 0.)\n",
    "    logqz_x = log_normal_pdf(z, mean, logvar)\n",
    "    return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, x, optimizer):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(model, x)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "def train_VAE(model):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        start_time = time.time()\n",
    "        print(epoch,end=\"\\n\")\n",
    "\n",
    "        ## Training\n",
    "        for i,x in enumerate(x_train):\n",
    "            print(f\"Finished {i}/{len(x_train)}\",end=\"\\r\")\n",
    "\n",
    "            x = x.reshape(1,len(x)) # reshape(1,23)\n",
    "            x_flat = gauss_cat_to_flat(x, input_dim_vec)\n",
    "            train_step(model, x_flat, optimizer)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        ## Testing\n",
    "        loss = tf.keras.metrics.Mean()\n",
    "        for x in x_test:\n",
    "            x = x.reshape(1,len(x)) # reshape(1,23)\n",
    "            x_flat = gauss_cat_to_flat(x, input_dim_vec)\n",
    "            loss(compute_loss(model, x_flat))\n",
    "        elbo = -loss.result()\n",
    "\n",
    "    #     display.clear_output(wait=False)\n",
    "        print(f'Epoch: {epoch}, Test set ELBO: {elbo}, time elapse for current epoch: {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = vae_widths[names.index(dname)]\n",
    "depth = vae_depths[names.index(dname)] # number of hidden layers\n",
    "latent_dim = vae_latent_dims[names.index(dname)]\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "vae = VAE(input_dim_vec,width,depth,latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f692463acd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = \"../VAE/checkpoints/vae\"\n",
    "vae.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# map to latent space + get uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5554, 17) (5554,) (618, 17) (618,)\n"
     ]
    }
   ],
   "source": [
    "# todo\n",
    "## tf.data.Dataset.range(8)\n",
    "## dataset = dataset.batch(3)\n",
    "\n",
    "import torch.utils.data as data\n",
    "\n",
    "class Datafeed(data.Dataset):\n",
    "\n",
    "    def __init__(self, x_train, y_train=None, transform=None):\n",
    "        self.data = x_train\n",
    "        self.targets = y_train\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.data[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.targets is not None:\n",
    "            return img, self.targets[index]\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# from src.utils import Datafeed\n",
    "\n",
    "trainset = Datafeed(x_train, y_train, transform=None)\n",
    "valset = Datafeed(x_test, y_test, transform=None)\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn.functional import softplus\n",
    "\n",
    "def decompose_entropy_cat(probs, eps=1e-10):\n",
    "    # probs (Nsamples, batch_size, classes)\n",
    "    \n",
    "    # 1 total entropy\n",
    "    posterior_preds = tf.math.reduce_mean(probs, axis=0, keepdims=False, name=None)\n",
    "    total_entropy = -(posterior_preds * tf.math.log(posterior_preds + eps))\n",
    "    total_entropy = tf.math.reduce_sum(total_entropy, axis=1, keepdims=False, name=None)\n",
    "\n",
    "    sample_preds_entropy = -tf.math.reduce_sum((probs * tf.math.log(probs + eps)),axis=2,keepdims=False,name=None)\n",
    "\n",
    "    # 2 aleatoric entropy\n",
    "    aleatoric_entropy = tf.math.reduce_sum(sample_preds_entropy,axis=0,keepdims=False,name=None)\n",
    "\n",
    "    # 3 epistemic entropy\n",
    "    epistemic_entropy = total_entropy - aleatoric_entropy\n",
    "\n",
    "    return total_entropy, aleatoric_entropy, epistemic_entropy\n",
    "\n",
    "\n",
    "def latent_project_cat(BNN, VAE, dset, batch_size=1024, cuda=True, prob_BNN=True):\n",
    "    import torch\n",
    "    if cuda:\n",
    "        loader = torch.utils.data.DataLoader(dset, batch_size=batch_size, shuffle=False, pin_memory=True,\n",
    "                                             num_workers=3)\n",
    "    else:\n",
    "        loader = torch.utils.data.DataLoader(dset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                             num_workers=3)\n",
    "    z_train = []\n",
    "    y_train = []\n",
    "    x_train = []\n",
    "    tr_aleatoric_vec = []\n",
    "    tr_epistemic_vec = []\n",
    "\n",
    "    #for j in range(len(dset[0])):\n",
    "    for j, (x, y_l) in enumerate(loader):\n",
    "        print(f\"Finished {j}/{len(loader)}\",end=\"\\r\")\n",
    "        \n",
    "        # print(f\"Finished {j}/{len(dset[0])}\",end=\"\\r\")\n",
    "        #x = dset[0][j].reshape(1,len(dset[0][j]))\n",
    "        #y_l = dset[1][j]\n",
    "        \n",
    "        zz,__ = VAE.encode(x.numpy())\n",
    "                \n",
    "        if prob_BNN:\n",
    "            probs = BNN.sample_predict(x.numpy(), 0, False)\n",
    "            total_entropy, aleatoric_entropy, epistemic_entropy = decompose_entropy_cat(probs)\n",
    "        else:\n",
    "            probs = BNN.predict(x, grad=False)\n",
    "            total_entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=1, keepdim=False)\n",
    "            aleatoric_entropy = total_entropy\n",
    "            epistemic_entropy = total_entropy*0\n",
    "\n",
    "        tr_epistemic_vec.append(epistemic_entropy)\n",
    "        tr_aleatoric_vec.append(aleatoric_entropy)\n",
    "\n",
    "        z_train.append(zz)\n",
    "        y_train.append(y_l.numpy())#.numpy())\n",
    "        x_train.append(x.numpy())#.numpy())\n",
    "        \n",
    "    print()\n",
    "        \n",
    "    tr_aleatoric_vec = tf.concat(tr_aleatoric_vec, axis=0, name='concat1').numpy()\n",
    "    tr_epistemic_vec = tf.concat(tr_epistemic_vec, axis=0, name='concat2').numpy()\n",
    "        \n",
    "    z_train = np.concatenate(z_train)\n",
    "    x_train = np.concatenate(x_train)    \n",
    "    y_train = np.concatenate(y_train)\n",
    "\n",
    "    return tr_aleatoric_vec, tr_epistemic_vec, z_train, x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 2/3\n",
      "Finished 0/1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Uncertainty')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x320 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from interpret.visualization_tools import latent_map_2d_gauss, latent_project_gauss, latent_project_cat\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if regression_bools[names.index(dname)]:\n",
    "    \n",
    "    tr_aleatoric_vec, tr_epistemic_vec, z_train, x_train, y_train = \\\n",
    "        latent_project_gauss(BNN, VAE, dset=trainset, batch_size=2048, cuda=cuda)\n",
    "    \n",
    "    tr_uncertainty_vec = tr_aleatoric_vec + tr_epistemic_vec\n",
    "\n",
    "    te_aleatoric_vec, te_epistemic_vec, z_test, x_test, y_test = \\\n",
    "        latent_project_gauss(BNN, VAE, dset=valset, batch_size=2048, cuda=cuda)\n",
    "    \n",
    "    te_uncertainty_vec = (te_aleatoric_vec**2 + te_epistemic_vec**2)**(1.0/2)\n",
    "    \n",
    "else:\n",
    "    tr_aleatoric_vec, tr_epistemic_vec, z_train, x_train, y_train = \\\n",
    "        latent_project_cat(BNN, vae, dset=trainset, batch_size=2048, cuda=cuda) # x_train,y_train\n",
    "    \n",
    "    tr_uncertainty_vec = tr_aleatoric_vec + tr_epistemic_vec\n",
    "    \n",
    "    te_aleatoric_vec, te_epistemic_vec, z_test, x_test, y_test = \\\n",
    "        latent_project_cat(BNN, vae, dset=valset, batch_size=2048, cuda=cuda) # x_test,y_test\n",
    "    \n",
    "    te_uncertainty_vec = te_aleatoric_vec + te_epistemic_vec\n",
    "\n",
    "uncertainty_idxs_sorted = np.flipud(np.argsort(te_uncertainty_vec))\n",
    "aleatoric_idxs_sorted = np.flipud(np.argsort(te_aleatoric_vec))\n",
    "epistemic_idxs_sorted = np.flipud(np.argsort(te_epistemic_vec))\n",
    "\n",
    "plt.figure(dpi=80)\n",
    "plt.hist(te_uncertainty_vec, density=True, alpha=0.5)\n",
    "plt.hist(tr_uncertainty_vec, density=True, alpha=0.5)\n",
    "plt.legend(['test', 'train'])\n",
    "plt.ylabel('Uncertainty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['25 - 45', 'Greater than 45', 'Less than 25', 'African-American', 'Asian', 'Caucasian', 'Hispanic', 'Native American', 'Other', 'Female', 'Male', 'Felony', 'misdemeanour', 'not_recid', 'is_recid', 'priors_count', 'time_served']\n"
     ]
    }
   ],
   "source": [
    "print(var_names_flat[dname])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview uncertainty histogram per dataset dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Felony')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x480 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "var_N = 11\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, dpi=120)\n",
    "# plt.figure(dpi=80)\n",
    "axes[0].hist(x_train[:,var_N], density=True, alpha=0.5)\n",
    "axes[0].hist(x_test[:,var_N], density=True, alpha=0.5)\n",
    "axes[0].legend(['train', 'test'])\n",
    "axes[0].set_title(var_names_flat[dname][var_N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [-5, -4, -3, -2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2, 3, 4, 5]\n",
    "center_bins = ((np.array([0] + bins) + np.array(bins + [0]))/2)[1:]\n",
    "center_bins[-1] = bins[-1]\n",
    "\n",
    "bin_idx = np.digitize(x_train[:, var_N], bins, right=False)\n",
    "bin_means = []\n",
    "bin_stds = []\n",
    "aleatoric_mean = []\n",
    "aleatoric_stds = []\n",
    "epistemic_mean = []\n",
    "epistemic_stds = []\n",
    "\n",
    "for n_bin, bin_start in enumerate(bins):\n",
    "    y_select = y_train[bin_idx==n_bin]\n",
    "    aleatoric_select = tr_aleatoric_vec[bin_idx==n_bin]\n",
    "    epistemic_select = tr_epistemic_vec[bin_idx==n_bin]\n",
    "    if len(y_select) == 0:\n",
    "        bin_means.append(np.nan)\n",
    "        bin_stds.append(np.nan)\n",
    "        aleatoric_mean.append(np.nan)\n",
    "        aleatoric_stds.append(np.nan)\n",
    "        epistemic_mean.append(np.nan)\n",
    "        epistemic_stds.append(np.nan)\n",
    "    else:\n",
    "        bin_means.append(y_select.mean())\n",
    "        bin_stds.append(y_select.std())\n",
    "        aleatoric_mean.append(aleatoric_select.mean())\n",
    "        aleatoric_stds.append(aleatoric_select.std())\n",
    "        epistemic_mean.append(epistemic_select.mean())\n",
    "        epistemic_stds.append(epistemic_select.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x480 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "axes[1].errorbar(center_bins, bin_means, yerr=bin_stds, fmt='o')\n",
    "axes[1].set_xlabel(var_names_flat[dname][var_N])\n",
    "axes[1].set_ylabel('target var')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, dpi=120)\n",
    "axes[0].errorbar(center_bins, aleatoric_mean, yerr=aleatoric_stds, fmt='o')\n",
    "axes[0].set_xlabel(var_names_flat[dname][var_N])\n",
    "axes[0].set_ylabel('Aleatoric')\n",
    "\n",
    "axes[1].errorbar(center_bins, epistemic_mean, yerr=epistemic_stds, fmt='o')\n",
    "axes[1].set_xlabel(var_names_flat[dname][var_N])\n",
    "axes[1].set_ylabel('Epistemic')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.7001734, 28.728306 ,  5.2988954, ...,  5.042375 ,  8.642808 ,\n",
       "        3.2509427], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_aleatoric_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
