{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['wine', 'default_credit', 'compas', 'lsat']\n",
    "bnn_widths = [200, 200, 200, 200]\n",
    "bnn_depths = [2, 2, 2, 2]\n",
    "\n",
    "vae_widths = [300, 300, 300, 300] # [200, 200, 200, 200]\n",
    "vae_depths = [3, 3, 3, 3] # We go deeper because we are using residual models\n",
    "vae_latent_dims = [6, 8, 4, 4]\n",
    "\n",
    "# For automatic explainer generation\n",
    "\n",
    "regression_bools = [True, False, False, True]\n",
    "gauss_cat_vae_bools = [False, True, True, True]\n",
    "flat_vae_bools = [False, False, False, False]\n",
    "\n",
    "var_names = {}\n",
    "var_names_flat = {}\n",
    "\n",
    "var_names['wine'] = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide',\n",
    "            'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n",
    "var_names_flat['wine'] = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide',\n",
    "            'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n",
    "\n",
    "var_names['default_credit'] = ['Given credit', 'Gender', 'Education', 'Marital status', 'Age', 'Payment delay 1', 'Payment delay 2',\n",
    "            'Payment delay 3', 'Payment delay 4', 'Payment delay 5', 'Payment delay 6', 'Bill statement 1', 'Bill statement 2',\n",
    "            'Bill statement 3', 'Bill statement 4', 'Bill statement 5', 'Bill statement 6', 'Previous payment 1', 'Previous payment 2',\n",
    "            'Previous payment 3', 'Previous payment 4', 'Previous payment 5', 'Previous payment 6']\n",
    "var_names_flat['default_credit'] = ['Given credit', 'Gender M', 'Gender F', 'Education grad', 'Education under', 'Education HS', 'Education Other',\n",
    "                 'Marital status M', 'Marital status S', 'Marital status Other', 'Age', 'Payment delay 1', 'Payment delay 2',\n",
    "            'Payment delay 3', 'Payment delay 4', 'Payment delay 5', 'Payment delay 6', 'Bill statement 1', 'Bill statement 2',\n",
    "            'Bill statement 3', 'Bill statement 4', 'Bill statement 5', 'Bill statement 6', 'Previous payment 1', 'Previous payment 2',\n",
    "            'Previous payment 3', 'Previous payment 4', 'Previous payment 5', 'Previous payment 6']\n",
    "\n",
    "var_names['compas'] = [\"age_cat\", \"race\", \"sex\", \"c_charge_degree\", 'is_recid', 'priors_count', 'time_served']\n",
    "var_names_flat['compas'] = ['25 - 45', 'Greater than 45', 'Less than 25', 'African-American', 'Asian', 'Caucasian', 'Hispanic', 'Native American', 'Other', 'Female', 'Male', 'Felony', 'misdemeanour', 'not_recid', 'is_recid', 'priors_count', 'time_served']\n",
    "\n",
    "var_names['lsat'] = ['LSAT', 'UGPA', 'race', 'sex']\n",
    "var_names_flat['lsat'] = ['LSAT', 'UGPA', 'amerind', 'mexican', 'other', 'black', 'asian', 'puerto', 'hisp', 'white', 'female', 'male']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dname = 'compas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_UCI(dset_name, splits=10, seed=0, separate_targets=True, save_dir='data/'):\n",
    "    mkdir(save_dir)\n",
    "\n",
    "    if dset_name == 'boston':\n",
    "        if not os.path.isfile(save_dir+'housing.data'):\n",
    "            urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\",\n",
    "                               filename=save_dir+'housing.data')\n",
    "        data = pd.read_csv(save_dir + 'housing.data', header=0, delimiter=\"\\s+\").values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'concrete':\n",
    "        if not os.path.isfile(save_dir+'Concrete_Data.xls'):\n",
    "            urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete_Data.xls\",\n",
    "                               filename=save_dir+'Concrete_Data.xls')\n",
    "        data = pd.read_excel(save_dir+ 'Concrete_Data.xls', header=0, delimiter=\"\\s+\").values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'energy':\n",
    "        if not os.path.isfile(save_dir+'ENB2012_data.xlsx'):\n",
    "            urllib.urlretrieve(\"http://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx\",\n",
    "                               filename=save_dir+'ENB2012_data.xlsx')\n",
    "        data = pd.read_excel(save_dir+'ENB2012_data.xlsx', header=0, delimiter=\"\\s+\").values\n",
    "        y_idx = [-2, -1]\n",
    "\n",
    "    elif dset_name == 'power':\n",
    "        if not os.path.isfile(save_dir+'CCPP.zip'):\n",
    "            urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00294/CCPP.zip\",\n",
    "                               filename=save_dir+'CCPP.zip')\n",
    "        zipped = zipfile.ZipFile(save_dir+\"CCPP.zip\")\n",
    "        data = pd.read_excel(zipped.open('CCPP/Folds5x2_pp.xlsx'), header=0, delimiter=\"\\t\").values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'wine':\n",
    "        if not os.path.isfile(save_dir+'winequality-red.csv'):\n",
    "            urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\",\n",
    "                               filename=save_dir+'winequality-red.csv')\n",
    "        data = pd.read_csv(save_dir+'winequality-red.csv', header=1, delimiter=';').values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'yatch':\n",
    "        if not os.path.isfile(save_dir+'yacht_hydrodynamics.data'):\n",
    "            urllib.urlretrieve(\"http://archive.ics.uci.edu/ml/machine-learning-databases/00243/yacht_hydrodynamics.data\",\n",
    "                               filename=save_dir+'yacht_hydrodynamics.data')\n",
    "        data = pd.read_csv(save_dir+'yacht_hydrodynamics.data', header=1, delimiter='\\s+').values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'kin8nm':\n",
    "        if not os.path.isfile(save_dir+'dataset_2175_kin8nm.csv'):\n",
    "            urllib.urlretrieve(\"https://www.openml.org/data/get_csv/3626/dataset_2175_kin8nm.csv\",\n",
    "                               filename=save_dir+'dataset_2175_kin8nm.csv')\n",
    "        data = pd.read_csv(save_dir+'dataset_2175_kin8nm.csv', header=1, delimiter=',').values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'kin8nm':\n",
    "        if not os.path.isfile(save_dir+'dataset_2175_kin8nm.csv'):\n",
    "            urllib.urlretrieve(\"https://www.openml.org/data/get_csv/3626/dataset_2175_kin8nm.csv\",\n",
    "                               filename=save_dir+'dataset_2175_kin8nm.csv')\n",
    "        data = pd.read_csv(save_dir+'dataset_2175_kin8nm.csv', header=1, delimiter=',').values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'naval':\n",
    "        if not os.path.isfile(save_dir + 'UCI%20CBM%20Dataset.zip'):\n",
    "            urllib.urlretrieve(\"http://archive.ics.uci.edu/ml/machine-learning-databases/00316/UCI%20CBM%20Dataset.zip\",\n",
    "                               filename=save_dir + 'UCI%20CBM%20Dataset.zip')\n",
    "        zipped = zipfile.ZipFile(save_dir + \"UCI%20CBM%20Dataset.zip\")\n",
    "        data = pd.read_csv(zipped.open('UCI CBM Dataset/data.txt'), header='infer', delimiter=\"\\s+\").values\n",
    "        y_idx = [-2, -1]\n",
    "\n",
    "    elif dset_name == 'protein':\n",
    "        if not os.path.isfile(save_dir+'CASP.csv'):\n",
    "            urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00265/CASP.csv\",\n",
    "                               filename=save_dir+'CASP.csv')\n",
    "        data = pd.read_csv(save_dir+'CASP.csv', header=1, delimiter=',').values\n",
    "        y_idx = [0]\n",
    "\n",
    "    elif dset_name == 'default_credit':\n",
    "        if not os.path.isfile(save_dir + 'default of credit card clients.xls'):\n",
    "            urllib.request.urlretrieve(\n",
    "                \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\",\n",
    "                filename=save_dir + 'default of credit card clients.xls')\n",
    "        data = pd.read_excel(save_dir + 'default of credit card clients.xls', header=[0, 1], index_col=0, # delimiter=\"\\s+\"\n",
    "                             ).values\n",
    "        y_idx = [-1]  # OK\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise Exception('Dataset name doesnt match any known datasets.')\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    data = data[np.random.permutation(np.arange(len(data)))]\n",
    "\n",
    "    kf = KFold(n_splits=splits)\n",
    "    for j, (train_index, test_index) in enumerate(kf.split(data)):\n",
    "\n",
    "        if separate_targets:\n",
    "            x_idx = list(range(data.shape[1]))\n",
    "            for e in y_idx:\n",
    "                x_idx.remove(x_idx[e])\n",
    "\n",
    "            x_idx = np.array(x_idx)\n",
    "            y_idx = np.array(y_idx)\n",
    "            x_train, y_train = data[train_index, :], data[train_index, :]\n",
    "            x_train, y_train = x_train[:, x_idx], y_train[:, y_idx]\n",
    "            x_test, y_test = data[test_index, :], data[test_index, :]\n",
    "            x_test, y_test = x_test[:, x_idx], y_test[:, y_idx]\n",
    "\n",
    "            x_means, x_stds = x_train.mean(axis=0), x_train.std(axis=0)\n",
    "            y_means, y_stds = y_train.mean(axis=0), y_train.std(axis=0)\n",
    "\n",
    "            y_stds[y_stds < 1e-10] = 1\n",
    "            x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "            x_train = ((x_train - x_means) / x_stds).astype(np.float32)\n",
    "            y_train = ((y_train - y_means) / y_stds).astype(np.float32)\n",
    "\n",
    "            x_test = ((x_test - x_means) / x_stds).astype(np.float32)\n",
    "            y_test = ((y_test - y_means) / y_stds).astype(np.float32)\n",
    "\n",
    "            return x_train, x_test, x_means, x_stds, y_train, y_test, y_means, y_stds\n",
    "\n",
    "        else:\n",
    "            x_train, x_test = data[train_index, :], data[test_index, :]\n",
    "            x_means, x_stds = x_train.mean(axis=0), x_train.std(axis=0)\n",
    "\n",
    "            x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "            x_train = ((x_train - x_means) / x_stds).astype(np.float32)\n",
    "            x_test = ((x_test - x_means) / x_stds).astype(np.float32)\n",
    "\n",
    "            return x_train, x_test, x_means, x_stds\n",
    "\n",
    "\n",
    "def get_my_COMPAS(rseed=0, separate_test=True, test_ratio=0.2, save_dir='../data/'):\n",
    "    \"\"\"\n",
    "        The adult dataset can be obtained from: https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\n",
    "        The code will look for the data file in the present directory, if it is not found, it will download them from GitHub.\n",
    "    \"\"\"\n",
    "\n",
    "    SEED = rseed\n",
    "    seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    their_FEATURES_CLASSIFICATION = [\"age_cat\", \"race\", \"sex\", \"priors_count\", \"c_charge_degree\"]\n",
    "    FEATURES_CLASSIFICATION = [\"age_cat\", \"race\", \"sex\", \"c_charge_degree\", \"is_recid\", \"priors_count\",\n",
    "                               \"time_served\"]  # features to be used for classification\n",
    "    CONT_VARIABLES = [\"priors_count\",\n",
    "                      \"time_served\"]  # continuous features, will need to be handled separately from categorical features, categorical features will be encoded using one-hot\n",
    "    CLASS_FEATURE = \"two_year_recid\"  # the decision variable\n",
    "\n",
    "\n",
    "    COMPAS_INPUT_FILE = save_dir + \"compas-scores-two-years.csv\"\n",
    "    check_data_file(COMPAS_INPUT_FILE)\n",
    "\n",
    "    # load the data and get some stats\n",
    "    df = pd.read_csv(COMPAS_INPUT_FILE)\n",
    "    df = df.dropna(subset=[\"days_b_screening_arrest\"])  # dropping missing vals\n",
    "\n",
    "    # convert to np array\n",
    "    data = df.to_dict('list')\n",
    "    for k in data.keys():\n",
    "        data[k] = np.array(data[k])\n",
    "\n",
    "    dates_in = data['c_jail_in']\n",
    "    dates_out = data['c_jail_out']\n",
    "    # this measures time in Jail\n",
    "    time_served = []\n",
    "    for i in range(len(dates_in)):\n",
    "        di = datetime.datetime.strptime(dates_in[i], '%Y-%m-%d %H:%M:%S')\n",
    "        do = datetime.datetime.strptime(dates_out[i], '%Y-%m-%d %H:%M:%S')\n",
    "        time_served.append((do - di).days)\n",
    "    time_served = np.array(time_served)\n",
    "    time_served[time_served < 0] = 0\n",
    "    data[\"time_served\"] = time_served\n",
    "\n",
    "    \"\"\" Filtering the data \"\"\"\n",
    "\n",
    "    # These filters are the same as propublica (refer to https://github.com/propublica/compas-analysis)\n",
    "    # If the charge date of a defendants Compas scored crime was not within 30 days from when the person was arrested, we assume that because of data quality reasons, that we do not have the right offense.\n",
    "    idx = np.logical_and(data[\"days_b_screening_arrest\"] <= 30, data[\"days_b_screening_arrest\"] >= -30)\n",
    "\n",
    "    # We coded the recidivist flag -- is_recid -- to be -1 if we could not find a compas case at all.\n",
    "    idx = np.logical_and(idx, data[\"is_recid\"] != -1)\n",
    "\n",
    "    # In a similar vein, ordinary traffic offenses -- those with a c_charge_degree of 'O' -- will not result in Jail time are removed (only two of them).\n",
    "    idx = np.logical_and(idx, data[\"c_charge_degree\"] != \"O\")  # F: felony, M: misconduct\n",
    "\n",
    "    # We filtered the underlying data from Broward county to include only those rows representing people who had either recidivated in two years, or had at least two years outside of a correctional facility.\n",
    "    idx = np.logical_and(idx, data[\"score_text\"] != \"NA\")\n",
    "\n",
    "    # select the examples that satisfy this criteria\n",
    "    for k in data.keys():\n",
    "        data[k] = data[k][idx]\n",
    "\n",
    "    y = data[CLASS_FEATURE]\n",
    "    \"\"\" Feature normalization and one hot encoding \"\"\"\n",
    "\n",
    "    print\n",
    "    \"\\nNumber of people recidivating within two years\"\n",
    "    print\n",
    "    pd.Series(y).value_counts()\n",
    "    print\n",
    "    \"\\n\"\n",
    "\n",
    "    X = []  # empty array with num rows same as num examples, will hstack the features to it\n",
    "    X_dims = []\n",
    "\n",
    "    feature_names = []\n",
    "    for attr in FEATURES_CLASSIFICATION:\n",
    "        vals = data[attr]\n",
    "        if attr in CONT_VARIABLES:\n",
    "            vals = [float(v) for v in vals]\n",
    "            # vals = preprocessing.scale(vals, axis=0, with_mean=True, with_std=True)  # 0 mean and 1 variance\n",
    "            vals = np.reshape(vals, (len(y), -1))  # convert from 1-d arr to a 2-d arr with one col\n",
    "            X_dims.append(1)\n",
    "\n",
    "        else:  # for binary categorical variables, the label binarizer uses just one var instead of two\n",
    "            enc = preprocessing.OneHotEncoder(categories='auto', handle_unknown='error')\n",
    "            enc.fit(vals.reshape(-1, 1))\n",
    "            vals = enc.transform(vals.reshape(-1, 1)).todense()\n",
    "            X_dims += [vals.shape[1]]*vals.shape[1]\n",
    "\n",
    "        # add to learnable features\n",
    "        X.append(vals)\n",
    "\n",
    "        if attr in CONT_VARIABLES:  # continuous feature, just append the name\n",
    "            feature_names.append(attr)\n",
    "        else:  # categorical features\n",
    "            if vals.shape[1] == 1:  # binary features that passed through lib binarizer\n",
    "                feature_names.append(attr)\n",
    "            else:\n",
    "                for k in enc.categories_:  # non-binary categorical features, need to add the names for each cat\n",
    "                    feature_names.append(attr + \"_\" + str(k))\n",
    "\n",
    "    X = np.array(np.concatenate(list(X), axis=1))\n",
    "    X_dims = np.array(X_dims)\n",
    "\n",
    "    if separate_test:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=rseed, shuffle=True)\n",
    "\n",
    "        x_means, x_stds = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "        x_means[X_dims>1] = 0\n",
    "        x_stds[X_dims>1] = 1\n",
    "        x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "        x_train = ((X_train - x_means) / x_stds).astype(np.float32)\n",
    "        x_test = ((X_test - x_means) / x_stds).astype(np.float32)\n",
    "\n",
    "        return x_train, x_test, x_means, x_stds, y_train, y_test, feature_names, X_dims\n",
    "    else:\n",
    "        x_means, x_stds = X.mean(axis=0), X.std(axis=0)\n",
    "        print(X_dims.shape, x_means.shape)\n",
    "        x_means[:,X_dims>1] = 0\n",
    "        x_stds[:,X_dims>1] = 1\n",
    "        x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "        x_train = ((X - x_means) / x_stds).astype(np.float32)\n",
    "\n",
    "        return x_train, x_means, x_stds, y, feature_names, X_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_file(fname):\n",
    "    files = os.listdir(\".\")  # get the current directory listing\n",
    "    print\n",
    "    \"Looking for file '%s' in the current directory...\" % fname\n",
    "\n",
    "    if fname not in files:\n",
    "        print\n",
    "        \"'%s' not found! Downloading from GitHub...\" % fname\n",
    "        addr = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
    "        \n",
    "        response = urllib.request.urlopen(addr)\n",
    "\n",
    "        data = response.read()\n",
    "        fileOut = open(fname, \"wb\")\n",
    "        fileOut.write(data)\n",
    "        fileOut.close()\n",
    "        print\n",
    "        \"'%s' download and saved locally..\" % fname\n",
    "    else:\n",
    "        print\n",
    "        \"File found in current directory..\"\n",
    "\n",
    "def mkdir(paths):\n",
    "    if not isinstance(paths, (list, tuple)):\n",
    "        paths = [paths]\n",
    "    for path in paths:\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path, mode=0o777)\n",
    "            \n",
    "def unnormalise_cat_vars(x, x_means, x_stds, input_dim_vec):\n",
    "    input_dim_vec = np.array(input_dim_vec)\n",
    "    unnorm_x = np.multiply(x, x_stds) + x_means\n",
    "\n",
    "    fixed_unnorm = unnorm_x.round()\n",
    "    # -=\n",
    "    fixed_unnorm = fixed_unnorm - fixed_unnorm.min(axis=0).reshape([1, fixed_unnorm.shape[1]])  # this sets all mins to 0\n",
    "    for idx, dims in enumerate(input_dim_vec):\n",
    "        if dims > 1:\n",
    "            vec = fixed_unnorm[:, idx]\n",
    "            vec[vec > dims - 1] = dims - 1\n",
    "            fixed_unnorm[:, idx] = vec\n",
    "\n",
    "    x[:, input_dim_vec > 1] = fixed_unnorm[:, input_dim_vec > 1]\n",
    "    return x\n",
    "\n",
    "def X_dims_to_input_dim_vec(X_dims):\n",
    "    \"\"\"This is for our cat_Gauss VAE model\"\"\"\n",
    "    input_dim_vec = []\n",
    "    i = 0\n",
    "    while i < len(X_dims):\n",
    "        input_dim_vec.append(X_dims[i])\n",
    "        i += X_dims[i]\n",
    "    return np.array(input_dim_vec)\n",
    "\n",
    "def gauss_cat_to_flat(x, input_dim_vec):\n",
    "    output = []\n",
    "    for idx, dim in enumerate(input_dim_vec):\n",
    "        if dim == 1:\n",
    "            output.append(tf.expand_dims(x[:, idx], axis=1))\n",
    "        elif dim > 1:            \n",
    "            oh_vec = tf.one_hot(x[:, idx], dim)\n",
    "            output.append(oh_vec)\n",
    "        else:\n",
    "            raise ValueError('Error, invalid dimension value')\n",
    "    return tf.concat(output, axis=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compas (5554, 17) (618, 17)\n",
      "[3 6 2 2 2 1 1]\n"
     ]
    }
   ],
   "source": [
    "if dname == 'wine':\n",
    "    x_train, x_test, x_means, x_stds, y_train, y_test, y_means, y_stds = \\\n",
    "    load_UCI(dset_name='wine', splits=10, seed=42, separate_targets=True, save_dir='../data/')\n",
    "    \n",
    "elif dname == 'default_credit':\n",
    "    # Note that this dataset is given without flattening\n",
    "    x_train, x_test, x_means, x_stds, y_train, y_test, y_means, y_stds = \\\n",
    "    load_UCI(dset_name='default_credit', splits=10, seed=42, separate_targets=True, save_dir='../data/')\n",
    "    print('Credit', x_train.shape, x_test.shape)\n",
    "    input_dim_vec = [1, 2, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ,1 ,1 ,1 ,1 ,1]\n",
    "    \n",
    "    x_train = unnormalise_cat_vars(x_train, x_means, x_stds, input_dim_vec)\n",
    "    x_test = unnormalise_cat_vars(x_test, x_means, x_stds, input_dim_vec)\n",
    "    # target unnormalisation\n",
    "    y_train = unnormalise_cat_vars(y_train, y_means, y_stds, [2])\n",
    "    y_test = unnormalise_cat_vars(y_test, y_means, y_stds, [2])\n",
    "    \n",
    "    x_train = gauss_cat_to_flat(torch.Tensor(x_train), input_dim_vec)\n",
    "    x_test = gauss_cat_to_flat(torch.Tensor(x_test), input_dim_vec)\n",
    "\n",
    "    print(input_dim_vec)\n",
    "\n",
    "elif dname == 'compas':\n",
    "    x_train, x_test, x_means, x_stds, y_train, y_test, feature_names, X_dims = \\\n",
    "    get_my_COMPAS(rseed=42, separate_test=True, test_ratio=0.1, save_dir='../data/')\n",
    "    input_dim_vec = X_dims_to_input_dim_vec(X_dims)\n",
    "    print('Compas', x_train.shape, x_test.shape)\n",
    "    print(input_dim_vec)\n",
    "    \n",
    "elif dname == 'lsat':\n",
    "    x_train, x_test, x_means, x_stds, y_train, y_test, y_means, y_stds, my_data_keys, input_dim_vec = \\\n",
    "    get_my_LSAT(save_dir='../data/')\n",
    "    print('LSAT', x_train.shape, x_test.shape)\n",
    "    print(input_dim_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.utils.data as data\n",
    "\n",
    "# class Datafeed(data.Dataset):\n",
    "\n",
    "#     def __init__(self, x_train, y_train=None, transform=None):\n",
    "#         self.data = x_train\n",
    "#         self.targets = y_train\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         img = self.data[index]\n",
    "#         if self.transform is not None:\n",
    "#             img = self.transform(img)\n",
    "#         if self.targets is not None:\n",
    "#             return img, self.targets[index]\n",
    "#         else:\n",
    "#             return img\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "# # from src.utils import Datafeed\n",
    "\n",
    "# trainset = Datafeed(x_train, y_train, transform=None)\n",
    "# valset = Datafeed(x_test, y_test, transform=None)\n",
    "\n",
    "# print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['wine', 'default_credit', 'compas', 'lsat']\n",
    "widths = [300, 300, 300, 300] # [200, 200, 200, 200]\n",
    "depths = [3, 3, 3, 3] # We go deeper because we are using residual models\n",
    "latent_dims = [6, 8, 4, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BNN Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\n",
      "Net:\u001b[0m\n",
      "\u001b[33mBNN categorical output\u001b[0m\n",
      "    Total params: 0.04M\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP_gauss(nn.Module):\n",
    "    def __init__(self, input_dim, width, depth, output_dim, flatten_image):\n",
    "        super(MLP_gauss, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.flatten_image = flatten_image\n",
    "\n",
    "#         layers = [nn.Linear(input_dim, width), nn.ReLU()]\n",
    "#         for i in range(depth - 1):\n",
    "#             layers.append(nn.Linear(width, width))\n",
    "#             layers.append(nn.ReLU())\n",
    "#         layers.append(nn.Linear(width, 2*output_dim))\n",
    "        \n",
    "        layers = [nn.Linear(input_dim,2*output_dim)]\n",
    "\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # if self.flatten_image:\n",
    "        #     x = x.view(-1, self.input_dim)\n",
    "        x = self.block(x)\n",
    "        mu = x[:, :self.output_dim]\n",
    "        sigma = F.softplus(x[:, self.output_dim:])\n",
    "        return mu, sigma\n",
    "\n",
    "class BaseNet(object):\n",
    "    def __init__(self):\n",
    "        cprint('c', '\\nNet:')\n",
    "\n",
    "    def get_nb_parameters(self):\n",
    "        return np.sum(np.fromiter((p.numel() for p in self.model.parameters()),int))\n",
    "#         return np.sum(np.fromiter(p.numel() for p in self.model.parameters(),dtype=int))\n",
    "#         k = np.sum()\n",
    "#         print(type(k))\n",
    "        return np.sum(p.numel() for p in self.model.parameters())\n",
    "\n",
    "    def set_mode_train(self, train=True):\n",
    "        if train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "    def update_lr(self, epoch, gamma=0.99):\n",
    "        self.epoch += 1\n",
    "        if self.schedule is not None:\n",
    "            if len(self.schedule) == 0 or epoch in self.schedule:\n",
    "                self.lr *= gamma\n",
    "                print('learning rate: %f  (%d)\\n' % (self.lr, epoch))\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = self.lr\n",
    "\n",
    "#     def save(self, filename):\n",
    "#         cprint('c', 'Writting %s\\n' % filename)\n",
    "#         torch.save({\n",
    "#             'epoch': self.epoch,\n",
    "#             'lr': self.lr,\n",
    "#             'model': self.model,\n",
    "#             'optimizer': self.optimizer}, filename)\n",
    "    def save(self,filename):\n",
    "        cprint('c', 'Writting %s\\n' % filename)\n",
    "        torch.save({\n",
    "            'epoch': self.epoch,\n",
    "            'lr': self.lr,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, filename)\n",
    "\n",
    "#     def load(self, filename):\n",
    "#         cprint('c', 'Reading %s\\n' % filename)\n",
    "#         state_dict = torch.load(filename)\n",
    "#         self.epoch = state_dict['epoch']\n",
    "#         self.lr = state_dict['lr']\n",
    "#         self.model = state_dict['model']\n",
    "#         self.optimizer = state_dict['optimizer']\n",
    "#         print('  restoring epoch: %d, lr: %f' % (self.epoch, self.lr))\n",
    "#         return self.epoch\n",
    "    def load(self,filename):\n",
    "        cprint('c', 'Reading %s\\n' % filename)\n",
    "        state_dict = torch.load(filename)\n",
    "        self.epoch = state_dict['epoch']\n",
    "        self.lr = state_dict['lr']\n",
    "        # self.model = state_dict['model_state_dict']\n",
    "        self.model.load_state_dict(state_dict['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "\n",
    "class H_SA_SGHMC(Optimizer):\n",
    "    \"\"\" Stochastic Gradient Hamiltonian Monte-Carlo Sampler that uses a burn-in\n",
    "        procedure to adapt its own hyperparameters during the initial stages\n",
    "        of sampling.\"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-2, base_C=0.05, gauss_sig=0.1, alpha0=10, beta0=10):\n",
    "        \"\"\" Set up a SGHMC Optimizer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : iterable\n",
    "            Parameters serving as optimization variable.\n",
    "        lr: float, optional\n",
    "            Base learning rate for this optimizer.\n",
    "            Must be tuned to the specific function being minimized.\n",
    "            Default: `1e-2`.\n",
    "        base_C:float, optional\n",
    "            (Constant) momentum decay per time-step.\n",
    "            Default: `0.05`.\n",
    "        \"\"\"\n",
    "\n",
    "        self.eps = 1e-6\n",
    "        self.alpha0 = alpha0\n",
    "        self.beta0 = beta0\n",
    "\n",
    "        if gauss_sig == 0:\n",
    "            self.weight_decay = 0\n",
    "        else:\n",
    "            self.weight_decay = 1 / (gauss_sig ** 2)\n",
    "\n",
    "        if self.weight_decay <= 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if base_C < 0:\n",
    "            raise ValueError(\"Invalid friction term: {}\".format(base_C))\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            base_C=base_C,\n",
    "        )\n",
    "        super(H_SA_SGHMC, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, burn_in=False, resample_momentum=False, resample_prior=False):\n",
    "        \"\"\"Simulate discretized Hamiltonian dynamics for one step\"\"\"\n",
    "        loss = None\n",
    "\n",
    "        for group in self.param_groups:  # iterate over blocks -> the ones defined in defaults. We dont use groups.\n",
    "            for p in group[\"params\"]:  # these are weight and bias matrices\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state = self.state[p]  # define dict for each individual param\n",
    "                if len(state) == 0:\n",
    "                    state[\"iteration\"] = 0\n",
    "                    state[\"tau\"] = torch.ones_like(p)\n",
    "                    state[\"g\"] = torch.ones_like(p)\n",
    "                    state[\"V_hat\"] = torch.ones_like(p)\n",
    "                    state[\"v_momentum\"] = torch.zeros_like(\n",
    "                        p)  # p.data.new(p.data.size()).normal_(mean=0, std=np.sqrt(group[\"lr\"])) #\n",
    "                    state['weight_decay'] = self.weight_decay\n",
    "\n",
    "                state[\"iteration\"] += 1  # this is kind of useless now but lets keep it provisionally\n",
    "\n",
    "                if resample_prior:\n",
    "                    alpha = self.alpha0 + p.data.nelement() / 2\n",
    "                    beta = self.beta0 + (p.data ** 2).sum().item() / 2\n",
    "                    gamma_sample = gamma(shape=alpha, scale=1 / (beta), size=None)\n",
    "                    #                     print('std', 1/np.sqrt(gamma_sample))\n",
    "                    state['weight_decay'] = gamma_sample\n",
    "\n",
    "                base_C, lr = group[\"base_C\"], group[\"lr\"]\n",
    "                weight_decay = state[\"weight_decay\"]\n",
    "                tau, g, V_hat = state[\"tau\"], state[\"g\"], state[\"V_hat\"]\n",
    "\n",
    "                d_p = p.grad.data\n",
    "                if weight_decay != 0:\n",
    "                    d_p.add_(weight_decay, p.data)\n",
    "\n",
    "                # update parameters during burn-in\n",
    "                if burn_in:  # We update g first as it makes most sense\n",
    "                    tau.add_(-tau * (g ** 2) / (\n",
    "                                V_hat + self.eps) + 1)  # specifies the moving average window, see Eq 9 in [1] left\n",
    "                    tau_inv = 1. / (tau + self.eps)\n",
    "                    g.add_(-tau_inv * g + tau_inv * d_p)  # average gradient see Eq 9 in [1] right\n",
    "                    V_hat.add_(-tau_inv * V_hat + tau_inv * (d_p ** 2))  # gradient variance see Eq 8 in [1]\n",
    "\n",
    "                V_sqrt = torch.sqrt(V_hat)\n",
    "                V_inv_sqrt = 1. / (V_sqrt + self.eps)  # preconditioner\n",
    "\n",
    "                if resample_momentum:  # equivalent to var = M under momentum reparametrisation\n",
    "                    state[\"v_momentum\"] = torch.normal(mean=torch.zeros_like(d_p),\n",
    "                                                       std=torch.sqrt((lr ** 2) * V_inv_sqrt))\n",
    "                v_momentum = state[\"v_momentum\"]\n",
    "\n",
    "                noise_var = (2. * (lr ** 2) * V_inv_sqrt * base_C - (lr ** 4))\n",
    "                noise_std = torch.sqrt(torch.clamp(noise_var, min=1e-16))\n",
    "                # sample random epsilon\n",
    "                noise_sample = torch.normal(mean=torch.zeros_like(d_p), std=torch.ones_like(d_p) * noise_std)\n",
    "\n",
    "                # update momentum (Eq 10 right in [1])\n",
    "                v_momentum.add_(- (lr ** 2) * V_inv_sqrt * d_p - base_C * v_momentum + noise_sample)\n",
    "\n",
    "                # update theta (Eq 10 left in [1])\n",
    "                p.data.add_(v_momentum)\n",
    "\n",
    "        return loss\n",
    "\n",
    "def load_object(filename):\n",
    "    with open(filename, 'rb') as input:\n",
    "        \n",
    "        from collections import OrderedDict\n",
    "        \n",
    "        samples = pickle.load(input, encoding='bytes')\n",
    "        new_samples = []\n",
    "        for i,weight_dict in enumerate(samples):\n",
    "            new_weight_dict = OrderedDict()\n",
    "            for key in weight_dict:\n",
    "                new_weight_dict[key.decode(\"utf-8\")] = weight_dict[key]\n",
    "            new_samples.append(new_weight_dict)\n",
    "        return new_samples\n",
    "#         try:\n",
    "#             return pickle.load(input)\n",
    "#         except Exception as e:\n",
    "#             try:\n",
    "#                 print(e)\n",
    "#                 return pickle.load(input, encoding=\"latin1\")\n",
    "#             except Exception as a:\n",
    "#                 print(a)\n",
    "#                 return pickle.load(input, encoding='bytes')\n",
    "\n",
    "import sys\n",
    "import torch.backends.cudnn as cudnn\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BNN_cat(BaseNet):  # for categorical distributions\n",
    "    def __init__(self, model, N_train, lr=1e-2, cuda=True, grad_std_mul=30, seed=None):\n",
    "        super(BNN_cat, self).__init__()\n",
    "\n",
    "        cprint('y', 'BNN categorical output')\n",
    "        self.lr = lr\n",
    "        self.model = model\n",
    "        self.cuda = cuda\n",
    "        self.seed = seed\n",
    "\n",
    "        self.N_train = N_train\n",
    "        self.create_net()\n",
    "        self.create_opt()\n",
    "        self.schedule = None  # [] #[50,200,400,600]\n",
    "        self.epoch = 0\n",
    "\n",
    "        self.grad_buff = []\n",
    "        self.max_grad = 1e20\n",
    "        self.grad_std_mul = grad_std_mul\n",
    "\n",
    "        self.weight_set_samples = []\n",
    "\n",
    "    def create_net(self):\n",
    "        if self.seed is None:\n",
    "            torch.manual_seed(42)\n",
    "        else:\n",
    "            torch.manual_seed(self.seed)\n",
    "        if self.cuda:\n",
    "            if self.seed is None:\n",
    "                torch.cuda.manual_seed(42)\n",
    "            else:\n",
    "                torch.cuda.manual_seed(self.seed)\n",
    "        if self.cuda:\n",
    "            self.model.cuda()\n",
    "            cudnn.benchmark = True\n",
    "\n",
    "        print('    Total params: %.2fM' % (self.get_nb_parameters() / 1000000.0))\n",
    "\n",
    "    def create_opt(self):\n",
    "        \"\"\"This optimiser incorporates the gaussian prior term automatically. The prior variance is gibbs sampled from\n",
    "        its posterior using a gamma hyper-prior.\"\"\"\n",
    "        self.optimizer = H_SA_SGHMC(params=self.model.parameters(), lr=self.lr, base_C=0.05, gauss_sig=0.1)  # this last parameter does nothing\n",
    "\n",
    "    def fit(self, x, y, burn_in=False, resample_momentum=False, resample_prior=False):\n",
    "        self.set_mode_train(train=True)\n",
    "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "        self.optimizer.zero_grad()\n",
    "        out = self.model(x)\n",
    "        loss = F.cross_entropy(out, y, reduction='mean')\n",
    "        loss = loss * self.N_train  # We use mean because we treat as an estimation of whole dataset\n",
    "        loss.backward()\n",
    "\n",
    "        if len(self.grad_buff) > 1000:\n",
    "            self.max_grad = np.mean(self.grad_buff) + self.grad_std_mul * np.std(self.grad_buff)\n",
    "            self.grad_buff.pop(0)\n",
    "\n",
    "        self.grad_buff.append(nn.utils.clip_grad_norm_(parameters=self.model.parameters(),\n",
    "                                                       max_norm=self.max_grad, norm_type=2))\n",
    "        if self.grad_buff[-1] >= self.max_grad:\n",
    "            print(self.max_grad, self.grad_buff[-1])\n",
    "            self.grad_buff.pop()\n",
    "        self.optimizer.step(burn_in=burn_in, resample_momentum=resample_momentum, resample_prior=resample_prior)\n",
    "\n",
    "        # out: (batch_size, out_channels, out_caps_dims)\n",
    "        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
    "        err = pred.ne(y.data).sum()\n",
    "\n",
    "        return loss.data * x.shape[0] / self.N_train, err\n",
    "\n",
    "    def eval(self, x, y, train=False):\n",
    "        self.set_mode_train(train=False)\n",
    "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "\n",
    "        out = self.model(x)\n",
    "        loss = F.cross_entropy(out, y, reduction='sum')\n",
    "        probs = F.softmax(out, dim=1).data.cpu()\n",
    "\n",
    "        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
    "        err = pred.ne(y.data).sum()\n",
    "\n",
    "        return loss.data, err, probs\n",
    "\n",
    "    def save_sampled_net(self, max_samples):\n",
    "\n",
    "        if len(self.weight_set_samples) >= max_samples:\n",
    "            self.weight_set_samples.pop(0)\n",
    "\n",
    "        self.weight_set_samples.append(copy.deepcopy(self.model.state_dict()))\n",
    "\n",
    "        cprint('c', ' saving weight samples %d/%d' % (len(self.weight_set_samples), max_samples))\n",
    "        return None\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.set_mode_train(train=False)\n",
    "        x, = to_variable(var=(x, ), cuda=self.cuda)\n",
    "        out = self.model(x)\n",
    "        probs = F.softmax(out, dim=1).data.cpu()\n",
    "        return probs.data\n",
    "\n",
    "    def sample_predict(self, x, Nsamples, grad=False):\n",
    "        \"\"\"return predictions using multiple samples from posterior\"\"\"\n",
    "        self.set_mode_train(train=False)\n",
    "        if Nsamples == 0:\n",
    "            Nsamples = len(self.weight_set_samples)\n",
    "        x, = to_variable(var=(x, ), cuda=self.cuda)\n",
    "\n",
    "        if grad:\n",
    "            self.optimizer.zero_grad()\n",
    "            if not x.requires_grad:\n",
    "                x.requires_grad = True\n",
    "\n",
    "        out = x.data.new(Nsamples, x.shape[0], self.model.output_dim)\n",
    "\n",
    "        # iterate over all saved weight configuration samples\n",
    "        for idx, weight_dict in enumerate(self.weight_set_samples):\n",
    "            if idx == Nsamples:\n",
    "                break\n",
    "            self.model.load_state_dict(weight_dict)\n",
    "            out[idx] = self.model(x)\n",
    "\n",
    "        out = out[:idx]\n",
    "        prob_out = F.softmax(out, dim=2)\n",
    "\n",
    "        if grad:\n",
    "            return prob_out\n",
    "        else:\n",
    "            return prob_out.data\n",
    "\n",
    "    def get_weight_samples(self, Nsamples=0):\n",
    "        \"\"\"return weight samples from posterior in a single-column array\"\"\"\n",
    "        weight_vec = []\n",
    "\n",
    "        if Nsamples == 0 or Nsamples > len(self.weight_set_samples):\n",
    "            Nsamples = len(self.weight_set_samples)\n",
    "\n",
    "        for idx, state_dict in enumerate(self.weight_set_samples):\n",
    "            if idx == Nsamples:\n",
    "                break\n",
    "\n",
    "            for key in state_dict.keys():\n",
    "                if 'weight' in key:\n",
    "                    weight_mtx = state_dict[key].cpu().data\n",
    "                    for weight in weight_mtx.view(-1):\n",
    "                        weight_vec.append(weight)\n",
    "\n",
    "        return np.array(weight_vec)\n",
    "\n",
    "    def save_weights(self, filename):\n",
    "        save_object(self.weight_set_samples, filename)\n",
    "\n",
    "    def load_weights(self, filename, subsample=1):\n",
    "        self.weight_set_samples = load_object(filename)\n",
    "        self.weight_set_samples = self.weight_set_samples[::subsample]\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, width, depth, output_dim, flatten_image):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.flatten_image = flatten_image\n",
    "\n",
    "        layers = [nn.Linear(input_dim, width), nn.ReLU(inplace=True)]\n",
    "        for i in range(depth - 1):\n",
    "            layers.append(nn.Linear(width, width))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        layers.append(nn.Linear(width, output_dim))\n",
    "\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # if self.flatten_image:\n",
    "        #    x = x.view(-1, self.input_dim)\n",
    "        print(x.shape)\n",
    "        return self.block(x)\n",
    "\n",
    "def cprint(color, text, **kwargs):\n",
    "    if color[0] == '*':\n",
    "        pre_code = '1;'\n",
    "        color = color[1:]\n",
    "    else:\n",
    "        pre_code = ''\n",
    "    code = {\n",
    "        'a': '30',\n",
    "        'r': '31',\n",
    "        'g': '32',\n",
    "        'y': '33',\n",
    "        'b': '34',\n",
    "        'p': '35',\n",
    "        'c': '36',\n",
    "        'w': '37'\n",
    "    }\n",
    "    print(\"\\x1b[%s%sm%s\\x1b[0m\" % (pre_code, code[color], text), **kwargs)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# from __future__ import division\n",
    "# %matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "# from BNN.models import MLP_gauss\n",
    "# from BNN.wrapper import BNN_gauss, BNN_cat, MLP\n",
    "import numpy as np\n",
    "\n",
    "if regression_bools[names.index(dname)]:\n",
    "    \n",
    "    input_dim = x_train.shape[1]\n",
    "    width = bnn_widths[names.index(dname)]\n",
    "    depth = bnn_depths[names.index(dname)]\n",
    "    output_dim = y_train.shape[1]\n",
    "    model = MLP_gauss(input_dim, width, depth, output_dim, flatten_image=False)\n",
    "\n",
    "    N_train = x_train.shape[0]\n",
    "    lr = 1e-2\n",
    "    cuda = torch.cuda.is_available()\n",
    "    BNN = BNN_gauss(model, N_train, lr=lr, cuda=cuda)\n",
    "    \n",
    "else:\n",
    "    N_train = x_train.shape[0]\n",
    "    input_dim = x_train.shape[1]\n",
    "    width = bnn_widths[names.index(dname)]\n",
    "    depth = bnn_depths[names.index(dname)]\n",
    "    output_dim = 2\n",
    "    model = MLP(input_dim, width, depth, output_dim, flatten_image=False)\n",
    "\n",
    "    lr = 1e-2\n",
    "    cuda = torch.cuda.is_available()\n",
    "    BNN = BNN_cat(model, N_train, lr=lr, cuda=cuda)\n",
    "\n",
    "\n",
    "\n",
    "save_dir = '../saves/fc_BNN_NEW_' + dname\n",
    "\n",
    "BNN.load_weights(save_dir + '_models/state_dicts.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def create_encoder(input_dim_vec,width,depth,latent_dim):\n",
    "    inputs = keras.Input(shape=(sum(input_dim_vec),))\n",
    "    input = layers.Dense(width)(inputs)\n",
    "\n",
    "    for i in range(depth-1):\n",
    "\n",
    "        x = layers.LeakyReLU()(input)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(width, kernel_initializer=tf.keras.initializers.Zeros())(x)\n",
    "\n",
    "        x = x + input\n",
    "\n",
    "        input = x\n",
    "\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    outputs = layers.Dense(latent_dim*2,kernel_initializer=tf.keras.initializers.Zeros())(x)\n",
    "\n",
    "    encoder = keras.Model(inputs=inputs, outputs=outputs, name=\"encoder_model\")\n",
    "\n",
    "    # encoder.summary()\n",
    "\n",
    "    # keras.utils.plot_model(encoder, \"encoder.png\")\n",
    "    keras.utils.plot_model(encoder, \"encoder.png\", show_shapes=True)\n",
    "    \n",
    "    return encoder\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def create_decoder(input_dim_vec,width,depth,latent_dim):\n",
    "\n",
    "    inputs = keras.Input(shape=(latent_dim,))\n",
    "    input = layers.Dense(width,kernel_initializer=tf.keras.initializers.Zeros())(inputs)\n",
    "    input = layers.LeakyReLU()(input)\n",
    "    input = layers.BatchNormalization()(input)\n",
    "\n",
    "    for i in range(depth-1):\n",
    "\n",
    "        x = layers.LeakyReLU()(input)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(width,kernel_initializer=tf.keras.initializers.Zeros())(x)\n",
    "\n",
    "        x = x + input\n",
    "\n",
    "        input = x\n",
    "\n",
    "    outputs = layers.Dense(sum(input_dim_vec),kernel_initializer=tf.keras.initializers.Zeros())(x)\n",
    "\n",
    "    decoder = keras.Model(inputs=inputs, outputs=outputs, name=\"decoder_model\")\n",
    "\n",
    "    # decoder.summary()\n",
    "\n",
    "    # keras.utils.plot_model(model, \"decoder_model.png\")\n",
    "    keras.utils.plot_model(decoder, \"decoder.png\", show_shapes=True)\n",
    "    \n",
    "    return decoder\n",
    "\n",
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, input_dim_vec,width,depth,latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = create_encoder(input_dim_vec,width,depth,latent_dim)\n",
    "        self.decoder = create_decoder(input_dim_vec,width,depth,latent_dim)\n",
    "\n",
    "    @tf.function\n",
    "    def sample(self, eps=None):\n",
    "        if eps is None:\n",
    "            eps = tf.random.normal(shape=(100, self.latent_dim))\n",
    "        return self.decode(eps, apply_sigmoid=True)\n",
    "\n",
    "    def encode(self, x):\n",
    "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "    \n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        logits = self.decoder(z)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits\n",
    "    \n",
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(-.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),axis=raxis)\n",
    "\n",
    "def compute_loss(model, x):\n",
    "    mean, logvar = model.encode(x)\n",
    "    z = model.reparameterize(mean, logvar)\n",
    "    x_logit = model.decode(z)\n",
    "    cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
    "    logpx_z = -tf.reduce_sum(cross_ent)#, axis=[1, 2, 3])\n",
    "    logpz = log_normal_pdf(z, 0., 0.)\n",
    "    logqz_x = log_normal_pdf(z, mean, logvar)\n",
    "    return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, x, optimizer):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(model, x)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "def train_VAE(model):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        start_time = time.time()\n",
    "        print(epoch,end=\"\\n\")\n",
    "\n",
    "        ## Training\n",
    "        for i,x in enumerate(x_train):\n",
    "            print(f\"Finished {i}/{len(x_train)}\",end=\"\\r\")\n",
    "\n",
    "            x = x.reshape(1,len(x)) # reshape(1,23)\n",
    "            x_flat = gauss_cat_to_flat(x, input_dim_vec)\n",
    "            train_step(model, x_flat, optimizer)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        ## Testing\n",
    "        loss = tf.keras.metrics.Mean()\n",
    "        for x in x_test:\n",
    "            x = x.reshape(1,len(x)) # reshape(1,23)\n",
    "            x_flat = gauss_cat_to_flat(x, input_dim_vec)\n",
    "            loss(compute_loss(model, x_flat))\n",
    "        elbo = -loss.result()\n",
    "\n",
    "    #     display.clear_output(wait=False)\n",
    "        print(f'Epoch: {epoch}, Test set ELBO: {elbo}, time elapse for current epoch: {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 21:26:55.682039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-08 21:26:55.698926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-08 21:26:55.699874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-08 21:26:55.701724: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-08 21:26:55.704052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-08 21:26:55.704693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-08 21:26:55.705258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-08 21:26:56.131361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-08 21:26:56.131567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-08 21:26:56.131732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-08 21:26:56.131895: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4368 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "width = widths[names.index(dname)]\n",
    "depth = depths[names.index(dname)] # number of hidden layers\n",
    "latent_dim = latent_dims[names.index(dname)]\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "vae = VAE(input_dim_vec,width,depth,latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fc177070370>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = \"../VAE/checkpoints/vae\"\n",
    "vae.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# map to latent space + get uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softplus\n",
    "\n",
    "def decompose_entropy_cat(probs, eps=1e-10):\n",
    "    # probs (Nsamples, batch_size, classes)\n",
    "    posterior_preds = probs.mean(dim=0, keepdim=False)\n",
    "    \n",
    "    # total_entropy = -(posterior_preds.cpu() * tf.math.log(posterior_preds.cpu() + eps))\n",
    "    total_entropy = -(posterior_preds * torch.log(posterior_preds + eps))\n",
    "    # total_entropy = tf.reduce_sum(total_entropy, 1)\n",
    "    # print(total_entropy.shape)\n",
    "    # print(type(total_entropy))\n",
    "    total_entropy = total_entropy.sum(dim=1, keepdim=False)\n",
    "    print(total_entropy.shape)\n",
    "    # total_entropy = -(posterior_preds * tf.math.log(posterior_preds + eps)).sum(dim=1, keepdim=False)\n",
    "\n",
    "    sample_preds_entropy = -(probs * torch.log(probs + eps)).sum(dim=2, keepdim=False)\n",
    "    aleatoric_entropy = sample_preds_entropy.mean(dim=0, keepdim=False)\n",
    "\n",
    "    epistemic_entropy = total_entropy - aleatoric_entropy\n",
    "\n",
    "    # returns (batch_size)\n",
    "    return total_entropy, aleatoric_entropy, epistemic_entropy\n",
    "\n",
    "\n",
    "def latent_project_cat(BNN, VAE, dset, batch_size=1024, cuda=True, prob_BNN=True):\n",
    "#     if cuda:\n",
    "#         loader = torch.utils.data.DataLoader(dset, batch_size=batch_size, shuffle=False, pin_memory=True,\n",
    "#                                              num_workers=3)\n",
    "#     else:\n",
    "#         loader = torch.utils.data.DataLoader(dset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "#                                              num_workers=3)\n",
    "    z_train = []\n",
    "    y_train = []\n",
    "    x_train = []\n",
    "    tr_aleatoric_vec = []\n",
    "    tr_epistemic_vec = []\n",
    "\n",
    "#     for j, (x, y_l) in enumerate(dset):\n",
    "    for j in range(len(dset[0])):\n",
    "        x = dset[0][j].reshape(1,len(dset[0][j]))\n",
    "        y_l = dset[1][j]# .reshape(dset[1][j],1)\n",
    "\n",
    "        # zz = VAE.recongnition(x).loc.data.cpu().numpy()\n",
    "        zz,__ = VAE.encode(x)\n",
    "        \n",
    "        x = torch.from_numpy(x)\n",
    "        # print(x.shape)\n",
    "        if prob_BNN:\n",
    "            probs = BNN.sample_predict(x, 0, False)\n",
    "            total_entropy, aleatoric_entropy, epistemic_entropy = decompose_entropy_cat(probs)\n",
    "        else:\n",
    "            probs = BNN.predict(x, grad=False)\n",
    "            total_entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=1, keepdim=False)\n",
    "            aleatoric_entropy = total_entropy\n",
    "            epistemic_entropy = total_entropy*0\n",
    "\n",
    "        tr_epistemic_vec.append(epistemic_entropy.data)\n",
    "        tr_aleatoric_vec.append(aleatoric_entropy.data)\n",
    "\n",
    "        z_train.append(zz)\n",
    "        y_train.append(y_l.numpy())\n",
    "        x_train.append(x.numpy())\n",
    "\n",
    "    tr_aleatoric_vec = torch.cat(tr_aleatoric_vec).cpu().numpy()\n",
    "    tr_epistemic_vec = torch.cat(tr_epistemic_vec).cpu().numpy()\n",
    "    z_train = np.concatenate(z_train)\n",
    "    x_train = np.concatenate(x_train)\n",
    "    y_train = np.concatenate(y_train)\n",
    "\n",
    "    return tr_aleatoric_vec, tr_epistemic_vec, z_train, x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THINGS NEEDED FROM BNN\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def to_variable(var=(), cuda=True, volatile=False):\n",
    "    out = []\n",
    "    for v in var:\n",
    "        if isinstance(v, np.ndarray):\n",
    "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
    "        if not v.is_cuda and cuda:\n",
    "            v = v.cuda()\n",
    "        if not isinstance(v, Variable):\n",
    "            v = Variable(v, volatile=volatile)\n",
    "        out.append(v)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 17])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_48130/3454730104.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtr_aleatoric_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_epistemic_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mlatent_project_cat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mtr_uncertainty_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtr_aleatoric_vec\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtr_epistemic_vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_48130/192229921.py\u001b[0m in \u001b[0;36mlatent_project_cat\u001b[0;34m(BNN, VAE, dset, batch_size, cuda, prob_BNN)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprob_BNN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mtotal_entropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maleatoric_entropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepistemic_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecompose_entropy_cat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_48130/2614182956.py\u001b[0m in \u001b[0;36msample_predict\u001b[0;34m(self, x, Nsamples, grad)\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_48130/2614182956.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;31m#    x = x.view(-1, self.input_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from interpret.visualization_tools import latent_map_2d_gauss, latent_project_gauss, latent_project_cat\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if regression_bools[names.index(dname)]:\n",
    "    \n",
    "    tr_aleatoric_vec, tr_epistemic_vec, z_train, x_train, y_train = \\\n",
    "        latent_project_gauss(BNN, VAE, dset=trainset, batch_size=2048, cuda=cuda)\n",
    "    \n",
    "    tr_uncertainty_vec = tr_aleatoric_vec + tr_epistemic_vec\n",
    "\n",
    "    te_aleatoric_vec, te_epistemic_vec, z_test, x_test, y_test = \\\n",
    "        latent_project_gauss(BNN, VAE, dset=valset, batch_size=2048, cuda=cuda)\n",
    "    \n",
    "    te_uncertainty_vec = (te_aleatoric_vec**2 + te_epistemic_vec**2)**(1.0/2)\n",
    "    \n",
    "else:\n",
    "    tr_aleatoric_vec, tr_epistemic_vec, z_train, x_train, y_train = \\\n",
    "        latent_project_cat(BNN, vae, dset=(x_train,y_train), batch_size=2048, cuda=cuda)\n",
    "    tr_uncertainty_vec = tr_aleatoric_vec + tr_epistemic_vec\n",
    "    \n",
    "    te_aleatoric_vec, te_epistemic_vec, z_test, x_test, y_test = \\\n",
    "        latent_project_cat(BNN, VAE, dset=valset, batch_size=2048, cuda=cuda)\n",
    "    \n",
    "    te_uncertainty_vec = te_aleatoric_vec + te_epistemic_vec\n",
    "    \n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "uncertainty_idxs_sorted = np.flipud(np.argsort(te_uncertainty_vec))\n",
    "aleatoric_idxs_sorted = np.flipud(np.argsort(te_aleatoric_vec))\n",
    "epistemic_idxs_sorted = np.flipud(np.argsort(te_epistemic_vec))\n",
    "\n",
    "plt.figure(dpi=80)\n",
    "plt.hist(te_uncertainty_vec, density=True, alpha=0.5)\n",
    "plt.hist(tr_uncertainty_vec, density=True, alpha=0.5)\n",
    "plt.legend(['test', 'train'])\n",
    "plt.ylabel('Uncertainty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
