{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_UCI(dset_name, splits=10, seed=0, separate_targets=True, save_dir='data/'):\n",
    "    mkdir(save_dir)\n",
    "\n",
    "    if dset_name == 'boston':\n",
    "        if not os.path.isfile(save_dir+'housing.data'):\n",
    "            urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\",\n",
    "                               filename=save_dir+'housing.data')\n",
    "        data = pd.read_csv(save_dir + 'housing.data', header=0, delimiter=\"\\s+\").values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'concrete':\n",
    "        if not os.path.isfile(save_dir+'Concrete_Data.xls'):\n",
    "            urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete_Data.xls\",\n",
    "                               filename=save_dir+'Concrete_Data.xls')\n",
    "        data = pd.read_excel(save_dir+ 'Concrete_Data.xls', header=0, delimiter=\"\\s+\").values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'energy':\n",
    "        if not os.path.isfile(save_dir+'ENB2012_data.xlsx'):\n",
    "            urllib.urlretrieve(\"http://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx\",\n",
    "                               filename=save_dir+'ENB2012_data.xlsx')\n",
    "        data = pd.read_excel(save_dir+'ENB2012_data.xlsx', header=0, delimiter=\"\\s+\").values\n",
    "        y_idx = [-2, -1]\n",
    "\n",
    "    elif dset_name == 'power':\n",
    "        if not os.path.isfile(save_dir+'CCPP.zip'):\n",
    "            urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00294/CCPP.zip\",\n",
    "                               filename=save_dir+'CCPP.zip')\n",
    "        zipped = zipfile.ZipFile(save_dir+\"CCPP.zip\")\n",
    "        data = pd.read_excel(zipped.open('CCPP/Folds5x2_pp.xlsx'), header=0, delimiter=\"\\t\").values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'wine':\n",
    "        if not os.path.isfile(save_dir+'winequality-red.csv'):\n",
    "            urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\",\n",
    "                               filename=save_dir+'winequality-red.csv')\n",
    "        data = pd.read_csv(save_dir+'winequality-red.csv', header=1, delimiter=';').values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'yatch':\n",
    "        if not os.path.isfile(save_dir+'yacht_hydrodynamics.data'):\n",
    "            urllib.urlretrieve(\"http://archive.ics.uci.edu/ml/machine-learning-databases/00243/yacht_hydrodynamics.data\",\n",
    "                               filename=save_dir+'yacht_hydrodynamics.data')\n",
    "        data = pd.read_csv(save_dir+'yacht_hydrodynamics.data', header=1, delimiter='\\s+').values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'kin8nm':\n",
    "        if not os.path.isfile(save_dir+'dataset_2175_kin8nm.csv'):\n",
    "            urllib.urlretrieve(\"https://www.openml.org/data/get_csv/3626/dataset_2175_kin8nm.csv\",\n",
    "                               filename=save_dir+'dataset_2175_kin8nm.csv')\n",
    "        data = pd.read_csv(save_dir+'dataset_2175_kin8nm.csv', header=1, delimiter=',').values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'kin8nm':\n",
    "        if not os.path.isfile(save_dir+'dataset_2175_kin8nm.csv'):\n",
    "            urllib.urlretrieve(\"https://www.openml.org/data/get_csv/3626/dataset_2175_kin8nm.csv\",\n",
    "                               filename=save_dir+'dataset_2175_kin8nm.csv')\n",
    "        data = pd.read_csv(save_dir+'dataset_2175_kin8nm.csv', header=1, delimiter=',').values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'naval':\n",
    "        if not os.path.isfile(save_dir + 'UCI%20CBM%20Dataset.zip'):\n",
    "            urllib.urlretrieve(\"http://archive.ics.uci.edu/ml/machine-learning-databases/00316/UCI%20CBM%20Dataset.zip\",\n",
    "                               filename=save_dir + 'UCI%20CBM%20Dataset.zip')\n",
    "        zipped = zipfile.ZipFile(save_dir + \"UCI%20CBM%20Dataset.zip\")\n",
    "        data = pd.read_csv(zipped.open('UCI CBM Dataset/data.txt'), header='infer', delimiter=\"\\s+\").values\n",
    "        y_idx = [-2, -1]\n",
    "\n",
    "    elif dset_name == 'protein':\n",
    "        if not os.path.isfile(save_dir+'CASP.csv'):\n",
    "            urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00265/CASP.csv\",\n",
    "                               filename=save_dir+'CASP.csv')\n",
    "        data = pd.read_csv(save_dir+'CASP.csv', header=1, delimiter=',').values\n",
    "        y_idx = [0]\n",
    "\n",
    "    elif dset_name == 'default_credit':\n",
    "        if not os.path.isfile(save_dir + 'default of credit card clients.xls'):\n",
    "            urllib.request.urlretrieve(\n",
    "                \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\",\n",
    "                filename=save_dir + 'default of credit card clients.xls')\n",
    "        data = pd.read_excel(save_dir + 'default of credit card clients.xls', header=[0, 1], index_col=0, # delimiter=\"\\s+\"\n",
    "                             ).values\n",
    "        y_idx = [-1]  # OK\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise Exception('Dataset name doesnt match any known datasets.')\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    data = data[np.random.permutation(np.arange(len(data)))]\n",
    "\n",
    "    kf = KFold(n_splits=splits)\n",
    "    for j, (train_index, test_index) in enumerate(kf.split(data)):\n",
    "\n",
    "        if separate_targets:\n",
    "            x_idx = list(range(data.shape[1]))\n",
    "            for e in y_idx:\n",
    "                x_idx.remove(x_idx[e])\n",
    "\n",
    "            x_idx = np.array(x_idx)\n",
    "            y_idx = np.array(y_idx)\n",
    "            x_train, y_train = data[train_index, :], data[train_index, :]\n",
    "            x_train, y_train = x_train[:, x_idx], y_train[:, y_idx]\n",
    "            x_test, y_test = data[test_index, :], data[test_index, :]\n",
    "            x_test, y_test = x_test[:, x_idx], y_test[:, y_idx]\n",
    "\n",
    "            x_means, x_stds = x_train.mean(axis=0), x_train.std(axis=0)\n",
    "            y_means, y_stds = y_train.mean(axis=0), y_train.std(axis=0)\n",
    "\n",
    "            y_stds[y_stds < 1e-10] = 1\n",
    "            x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "            x_train = ((x_train - x_means) / x_stds).astype(np.float32)\n",
    "            y_train = ((y_train - y_means) / y_stds).astype(np.float32)\n",
    "\n",
    "            x_test = ((x_test - x_means) / x_stds).astype(np.float32)\n",
    "            y_test = ((y_test - y_means) / y_stds).astype(np.float32)\n",
    "\n",
    "            return x_train, x_test, x_means, x_stds, y_train, y_test, y_means, y_stds\n",
    "\n",
    "        else:\n",
    "            x_train, x_test = data[train_index, :], data[test_index, :]\n",
    "            x_means, x_stds = x_train.mean(axis=0), x_train.std(axis=0)\n",
    "\n",
    "            x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "            x_train = ((x_train - x_means) / x_stds).astype(np.float32)\n",
    "            x_test = ((x_test - x_means) / x_stds).astype(np.float32)\n",
    "\n",
    "            return x_train, x_test, x_means, x_stds\n",
    "\n",
    "def mkdir(paths):\n",
    "    if not isinstance(paths, (list, tuple)):\n",
    "        paths = [paths]\n",
    "    for path in paths:\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path, mode=0o777)\n",
    "            \n",
    "def unnormalise_cat_vars(x, x_means, x_stds, input_dim_vec):\n",
    "    input_dim_vec = np.array(input_dim_vec)\n",
    "    unnorm_x = np.multiply(x, x_stds) + x_means\n",
    "\n",
    "    fixed_unnorm = unnorm_x.round()\n",
    "    fixed_unnorm -= fixed_unnorm.min(axis=0).reshape([1, fixed_unnorm.shape[1]])  # this sets all mins to 0\n",
    "    for idx, dims in enumerate(input_dim_vec):\n",
    "        if dims > 1:\n",
    "            vec = fixed_unnorm[:, idx]\n",
    "            vec[vec > dims - 1] = dims - 1\n",
    "            fixed_unnorm[:, idx] = vec\n",
    "\n",
    "    x[:, input_dim_vec > 1] = fixed_unnorm[:, input_dim_vec > 1]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['wine', 'default_credit', 'compas', 'lsat']\n",
    "widths = [300, 300, 300, 300] # [200, 200, 200, 200]\n",
    "depths = [3, 3, 3, 3] # We go deeper because we are using residual models\n",
    "latent_dims = [6, 8, 4, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wine (1438, 11) (160, 11)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, x_means, x_stds, y_train, y_test, y_means, y_stds = \\\n",
    "    load_UCI(dset_name='wine', splits=10, seed=42, separate_targets=True, save_dir='../data/')\n",
    "print('Wine', x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, x_means, x_stds, y_train, y_test, y_means, y_stds = \\\n",
    "load_UCI(dset_name='default_credit', splits=10, seed=42, separate_targets=True, save_dir='../data/')\n",
    "input_dim_vec = [1, 2, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ,1 ,1 ,1 ,1 ,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = unnormalise_cat_vars(x_train, x_means, x_stds, input_dim_vec)\n",
    "x_test = unnormalise_cat_vars(x_test, x_means, x_stds, input_dim_vec)\n",
    "# target unnormalisation\n",
    "y_train = unnormalise_cat_vars(y_train, y_means, y_stds, [2])\n",
    "y_test = unnormalise_cat_vars(y_test, y_means, y_stds, [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default_credit\n"
     ]
    }
   ],
   "source": [
    "dname = 'default_credit'\n",
    "print(dname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27000, 23)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27000, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim_vec = [1, 2, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ,1 ,1 ,1 ,1 ,1]\n",
    "len(input_dim_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(input_dim_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/cuda/__init__.py:80: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272128894/work/c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "cuda = torch.cuda.is_available()\n",
    "from torch.nn import MSELoss,CrossEntropyLoss\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.functional import softplus\n",
    "from torch.distributions import kl_divergence\n",
    "\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cprint(color, text, **kwargs):\n",
    "    if color[0] == '*':\n",
    "        pre_code = '1;'\n",
    "        color = color[1:]\n",
    "    else:\n",
    "        pre_code = ''\n",
    "    code = {\n",
    "        'a': '30',\n",
    "        'r': '31',\n",
    "        'g': '32',\n",
    "        'y': '33',\n",
    "        'b': '34',\n",
    "        'p': '35',\n",
    "        'c': '36',\n",
    "        'w': '37'\n",
    "    }\n",
    "    print(\"\\x1b[%s%sm%s\\x1b[0m\" % (pre_code, code[color], text), **kwargs)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# (used in sub network)\n",
    "def normal_parse_params(params, min_sigma=1e-3):\n",
    "    \"\"\"\n",
    "    Take a Tensor (e. g. neural network output) and return\n",
    "    torch.distributions.Normal distribution.\n",
    "    This Normal distribution is component-wise independent,\n",
    "    and its dimensionality depends on the input shape.\n",
    "    First half of channels is mean of the distribution,\n",
    "    the softplus of the second half is std (sigma), so there is\n",
    "    no restrictions on the input tensor.\n",
    "    min_sigma is the minimal value of sigma. I. e. if the above\n",
    "    softplus is less than min_sigma, then sigma is clipped\n",
    "    from below with value min_sigma. This regularization\n",
    "    is required for the numerical stability and may be considered\n",
    "    as a neural network architecture choice without any change\n",
    "    to the probabilistic model.\n",
    "    \"\"\"\n",
    "    n = params.shape[0]\n",
    "    d = params.shape[1]\n",
    "    mu = params[:, :d // 2]\n",
    "    sigma_params = params[:, d // 2:]\n",
    "    sigma = softplus(sigma_params)\n",
    "    sigma = sigma.clamp(min=min_sigma)\n",
    "    distr = Normal(mu, sigma)\n",
    "    return distr\n",
    "\n",
    "## (used in the next function)\n",
    "def torch_onehot(y, Nclass):\n",
    "    if y.is_cuda:\n",
    "        y = y.type(torch.cuda.LongTensor)\n",
    "    else:\n",
    "        y = y.type(torch.LongTensor)\n",
    "    y_onehot = torch.zeros((y.shape[0], Nclass)).type(y.type())\n",
    "    # In your for loop\n",
    "    y_onehot.scatter_(1, y.unsqueeze(1), 1)\n",
    "    return y_onehot\n",
    "\n",
    "## (used in the fit of the main network)\n",
    "def gauss_cat_to_flat(x, input_dim_vec):\n",
    "    output = []\n",
    "    for idx, dim in enumerate(input_dim_vec):\n",
    "        if dim == 1:\n",
    "            output.append(x[:, idx].unsqueeze(1))\n",
    "        elif dim > 1:\n",
    "            oh_vec = torch_onehot(x[:, idx], dim).type(x.type())\n",
    "            output.append(oh_vec)\n",
    "        else:\n",
    "            raise ValueError('Error, invalid dimension value')\n",
    "    return torch.cat(output, dim=1)\n",
    "\n",
    "## (also used in the fit of the main network)\n",
    "def flat_to_gauss_cat(x, input_dim_vec):\n",
    "    output = []\n",
    "    cum_dims = 0\n",
    "    for idx, dims in enumerate(input_dim_vec):\n",
    "        if dims == 1:\n",
    "            output.append(x[:, cum_dims].unsqueeze(1))\n",
    "            cum_dims += 1\n",
    "\n",
    "        elif dims > 1:\n",
    "            output.append(x[:, cum_dims:cum_dims + dims].max(dim=1)[1].type(x.type()).unsqueeze(1))\n",
    "            cum_dims += dims\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Error, invalid dimension value')\n",
    "\n",
    "    return torch.cat(output, dim=1)\n",
    "\n",
    "## (also used in the fit of the main network)\n",
    "def to_variable(var=(), cuda=True, volatile=False):\n",
    "    out = []\n",
    "    for v in var:\n",
    "        if isinstance(v, np.ndarray):\n",
    "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
    "        if not v.is_cuda and cuda:\n",
    "            v = v.cuda()\n",
    "        if not isinstance(v, Variable):\n",
    "            v = Variable(v, volatile=volatile)\n",
    "        out.append(v)\n",
    "    return out\n",
    "\n",
    "def humansize(nbytes):\n",
    "    suffixes = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']\n",
    "    i = 0\n",
    "    while nbytes >= 1024 and i < len(suffixes) - 1:\n",
    "        nbytes /= 1024.\n",
    "        i += 1\n",
    "    f = ('%.2f' % nbytes)\n",
    "    return '%s%s' % (f, suffixes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    Skip-connection over the sequence of layers in the constructor.\n",
    "    The module passes input data sequentially through these layers\n",
    "    and then adds original data to the result.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        super(SkipConnection, self).__init__()\n",
    "        self.inner_net = nn.Sequential(*args)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input + self.inner_net(input)\n",
    "\n",
    "def preact_leaky_MLPBlock(width):\n",
    "    return SkipConnection(\n",
    "        nn.LeakyReLU(),\n",
    "        nn.BatchNorm1d(num_features=width),\n",
    "        nn.Linear(width, width),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_preact_recognition_net(nn.Module):\n",
    "    def __init__(self, input_dim, width, depth, latent_dim):\n",
    "        super(MLP_preact_recognition_net, self).__init__()\n",
    "        # input layer\n",
    "        proposal_layers = [nn.Linear(input_dim, width)]\n",
    "        # body\n",
    "        for i in range(depth-1):\n",
    "            proposal_layers.append(\n",
    "                preact_leaky_MLPBlock(width)) ## *dependency\n",
    "        # output layer\n",
    "        proposal_layers.extend(\n",
    "            [nn.LeakyReLU(), nn.BatchNorm1d(num_features=width),\n",
    "            nn.Linear(width, latent_dim * 2)])\n",
    "\n",
    "        self.block = nn.Sequential(*proposal_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_preact_generator_net(nn.Module):\n",
    "    def __init__(self, input_dim, width, depth, latent_dim):\n",
    "        super(MLP_preact_generator_net, self).__init__()\n",
    "        # input layer\n",
    "        generative_layers = [nn.Linear(latent_dim, width), nn.LeakyReLU(), nn.BatchNorm1d(num_features=width)]\n",
    "        # body\n",
    "        for i in range(depth-1):\n",
    "            generative_layers.append(\n",
    "                    # skip-connection from prior network to generative network\n",
    "                    preact_leaky_MLPBlock(width))  ## *dependency\n",
    "        # output layer\n",
    "        generative_layers.extend([\n",
    "            nn.Linear(width,\n",
    "                      input_dim),\n",
    "        ])\n",
    "        self.block = nn.Sequential(*generative_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rms cat loglike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rms_cat_loglike(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim_vec, reduction='none'):\n",
    "        super(rms_cat_loglike, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        self.input_dim_vec = input_dim_vec\n",
    "        self.mse = MSELoss(reduction='none')  # takes(input, target)\n",
    "        self.ce = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def forward(self, x, y):\n",
    "\n",
    "        log_prob_vec = []\n",
    "        cum_dims = 0\n",
    "        for idx, dims in enumerate(self.input_dim_vec):\n",
    "            if dims == 1:\n",
    "                # Gaussian_case\n",
    "                log_prob_vec.append(-self.mse(x[:, cum_dims], y[:, idx]).unsqueeze(1))\n",
    "                cum_dims += 1\n",
    "\n",
    "            elif dims > 1:\n",
    "                if x.shape[1] == y.shape[1]:\n",
    "                    raise Exception('Input and target seem to be in flat format. Need integer cat targets.')\n",
    "                                \n",
    "                if y.is_cuda:\n",
    "                    tget = y[:, idx].type(torch.cuda.LongTensor)\n",
    "                else:\n",
    "                    tget = y[:, idx].type(torch.LongTensor)\n",
    "\n",
    "                log_prob_vec.append(-self.ce(x[:, cum_dims:cum_dims + dims], tget).unsqueeze(1))\n",
    "                cum_dims += dims\n",
    "\n",
    "            else:\n",
    "                raise ValueError('Error, invalid dimension value')\n",
    "\n",
    "        log_prob_vec = torch.cat(log_prob_vec, dim=1)\n",
    "\n",
    "        if self.reduction == 'none':\n",
    "            return log_prob_vec\n",
    "        elif self.reduction == 'sum':\n",
    "            return log_prob_vec.sum()\n",
    "        elif self.reduction == 'average':\n",
    "            return log_prob_vec.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sub network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_gauss_cat(nn.Module):\n",
    "    def __init__(self, input_dim_vec, width, depth, latent_dim, pred_sig=False):\n",
    "        super(VAE_gauss_cat, self).__init__()\n",
    "\n",
    "        input_dim = 0\n",
    "        self.input_dim_vec = input_dim_vec\n",
    "        for e in input_dim_vec:\n",
    "            input_dim += e\n",
    "        \n",
    "        self.encoder = MLP_preact_recognition_net(input_dim, width, depth, latent_dim) ## *dependency\n",
    "        if pred_sig:\n",
    "            raise NotImplementedError()\n",
    "            # self.decoder = generator_net(2*input_dim, width, depth, latent_dim)\n",
    "            # self.rec_loglike = GaussianLoglike(min_sigma=1e-2)\n",
    "        else:\n",
    "            self.decoder = MLP_preact_generator_net(input_dim, width, depth, latent_dim)\n",
    "            self.rec_loglike = rms_cat_loglike(self.input_dim_vec, reduction='none') ## *dependency\n",
    "        self.pred_sig = pred_sig\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Works with flattened representATION\"\"\"\n",
    "        approx_post_params = self.encoder(x)\n",
    "        approx_post = normal_parse_params(approx_post_params, 1e-3)\n",
    "        return approx_post\n",
    "\n",
    "    def decode(self, z_sample):\n",
    "        \"\"\"Works with flattened representATION\"\"\"\n",
    "        rec_params = self.decoder(z_sample)\n",
    "        return rec_params\n",
    "\n",
    "    def vlb(self, prior, approx_post, x, rec_params):\n",
    "        \"\"\"Works with flattened representATION\"\"\"\n",
    "        if self.pred_sig:\n",
    "            pass\n",
    "        else:\n",
    "            rec = self.rec_loglike(rec_params, x).view(x.shape[0], -1).sum(-1)\n",
    "        kl = kl_divergence(approx_post, prior).view(x.shape[0], -1).sum(-1)\n",
    "        return rec - kl\n",
    "\n",
    "    def iwlb(self, prior, approx_post, x, K=50):\n",
    "        estimates = []\n",
    "        for i in range(K):\n",
    "            latent = approx_post.rsample()\n",
    "            rec_params = self.decode(latent)\n",
    "            if self.pred_sig:\n",
    "                pass\n",
    "            else:\n",
    "                rec_loglike = self.rec_loglike(rec_params, x).view(x.shape[0], -1).sum(-1)\n",
    "\n",
    "            prior_log_prob = prior.log_prob(latent)\n",
    "            prior_log_prob = prior_log_prob.view(x.shape[0], -1)\n",
    "            prior_log_prob = prior_log_prob.sum(-1)\n",
    "\n",
    "            proposal_log_prob = approx_post.log_prob(latent)\n",
    "            proposal_log_prob = proposal_log_prob.view(x.shape[0], -1)\n",
    "            proposal_log_prob = proposal_log_prob.sum(-1)\n",
    "\n",
    "            estimate = rec_loglike + prior_log_prob - proposal_log_prob\n",
    "            estimates.append(estimate[:, None])\n",
    "\n",
    "        return torch.logsumexp(torch.cat(estimates, 1), 1) - np.log(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNet(object):\n",
    "    def __init__(self):\n",
    "        cprint('c', '\\nNet:')\n",
    "\n",
    "    def get_nb_parameters(self):\n",
    "        return np.sum(p.numel() for p in self.model.parameters())\n",
    "\n",
    "    def set_mode_train(self, train=True):\n",
    "        if train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "    def update_lr(self, epoch, gamma=0.99):\n",
    "        self.epoch += 1\n",
    "        if self.schedule is not None:\n",
    "            if len(self.schedule) == 0 or epoch in self.schedule:\n",
    "                self.lr *= gamma\n",
    "                print('learning rate: %f  (%d)\\n' % (self.lr, epoch))\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = self.lr\n",
    "\n",
    "#     def save(self, filename):\n",
    "#         cprint('c', 'Writting %s\\n' % filename)\n",
    "#         torch.save({\n",
    "#             'epoch': self.epoch,\n",
    "#             'lr': self.lr,\n",
    "#             'model': self.model,\n",
    "#             'optimizer': self.optimizer}, filename)\n",
    "    \n",
    "    def save(self,filename):\n",
    "        cprint('c', 'Writting %s\\n' % filename)\n",
    "        torch.save({\n",
    "            'epoch': self.epoch,\n",
    "            'lr': self.lr,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, filename)\n",
    "\n",
    "    \n",
    "    def load(self, filename):\n",
    "        cprint('c', 'Reading %s\\n' % filename)\n",
    "        state_dict = torch.load(filename)\n",
    "        self.epoch = state_dict['epoch']\n",
    "        self.lr = state_dict['lr']\n",
    "        self.model = state_dict['model']\n",
    "        self.optimizer = state_dict['optimizer']\n",
    "        print('  restoring epoch: %d, lr: %f' % (self.epoch, self.lr))\n",
    "        return self.epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reactified Adam (RAdam) Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = math.sqrt(\n",
    "                            (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (\n",
    "                                        N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_gauss_cat_net(BaseNet):\n",
    "    def __init__(self, input_dim_vec, width, depth, latent_dim, pred_sig=False, lr=1e-3, cuda=True, flatten=True):\n",
    "        super(VAE_gauss_cat_net, self).__init__()\n",
    "        cprint('y', 'VAE_gauss_net')\n",
    "\n",
    "        self.cuda = cuda\n",
    "        self.input_dim = 0\n",
    "        self.input_dim_vec = input_dim_vec\n",
    "        for e in self.input_dim_vec:\n",
    "            self.input_dim += e\n",
    "        self.flatten = flatten\n",
    "        if not self.flatten:\n",
    "            pass\n",
    "            # raise Exception('Error calculation not supported without flattening')\n",
    "\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.latent_dim = latent_dim\n",
    "        self.lr = lr\n",
    "        self.pred_sig = pred_sig\n",
    "        \n",
    "        # Here create the network\n",
    "        self.create_net()\n",
    "        \n",
    "        # Here create the optimizer\n",
    "        self.create_opt()\n",
    "        \n",
    "        self.epoch = 0\n",
    "        self.schedule = None\n",
    "\n",
    "        if self.cuda:\n",
    "            self.prior = self.prior = Normal(loc=torch.zeros(latent_dim).cuda(), scale=torch.ones(latent_dim).cuda())\n",
    "        else:\n",
    "            self.prior = Normal(loc=torch.zeros(latent_dim), scale=torch.ones(latent_dim))\n",
    "        self.vlb_scale = 1 / len(self.input_dim_vec)  # scale for dimensions of input so we can use same LR always\n",
    "\n",
    "    def create_net(self):\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed(42)\n",
    "        self.model = VAE_gauss_cat(self.input_dim_vec, self.width, self.depth, self.latent_dim, self.pred_sig)\n",
    "        if self.cuda:\n",
    "            self.model = self.model.cuda()\n",
    "            cudnn.benchmark = True\n",
    "        print('    Total params: %.2fM' % (self.get_nb_parameters() / 1000000.0))\n",
    "\n",
    "    def create_opt(self):\n",
    "        self.optimizer = RAdam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def fit(self, x):\n",
    "        self.set_mode_train(train=True)\n",
    "\n",
    "        if self.flatten:\n",
    "            x_flat = gauss_cat_to_flat(x, self.input_dim_vec)\n",
    "        else:\n",
    "            x_flat = x\n",
    "            x = flat_to_gauss_cat(x, self.input_dim_vec)\n",
    "\n",
    "        x, x_flat = to_variable(var=(x, x_flat), cuda=self.cuda)\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        approx_post = self.model.encode(x_flat)\n",
    "        z_sample = approx_post.rsample()\n",
    "        rec_params = self.model.decode(z_sample)\n",
    "\n",
    "        vlb = self.model.vlb(self.prior, approx_post, x, rec_params)\n",
    "        loss = (- vlb * self.vlb_scale).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return vlb.mean().item(), rec_params\n",
    "\n",
    "    def eval(self, x, sample=False):\n",
    "        self.set_mode_train(train=False)\n",
    "\n",
    "        if self.flatten:\n",
    "            x_flat = gauss_cat_to_flat(x, self.input_dim_vec)\n",
    "        else:\n",
    "            x_flat = x\n",
    "            x = flat_to_gauss_cat(x, self.input_dim_vec)\n",
    "\n",
    "        x, x_flat = to_variable(var=(x, x_flat), cuda=self.cuda)\n",
    "        approx_post = self.model.encode(x_flat)\n",
    "        if sample:\n",
    "            z_sample = approx_post.sample()\n",
    "        else:\n",
    "            z_sample = approx_post.loc\n",
    "        rec_params = self.model.decode(z_sample)\n",
    "\n",
    "        vlb = self.model.vlb(self.prior, approx_post, x, rec_params)\n",
    "\n",
    "        return vlb.mean().item(), rec_params\n",
    "\n",
    "    def eval_iw(self, x, k=50):\n",
    "        self.set_mode_train(train=False)\n",
    "        if self.flatten:\n",
    "            x_flat = gauss_cat_to_flat(x, self.input_dim_vec)\n",
    "        else:\n",
    "            x_flat = x\n",
    "            x = flat_to_gauss_cat(x, self.input_dim_vec)\n",
    "\n",
    "        x, x_flat = to_variable(var=(x, x_flat), cuda=self.cuda)\n",
    "\n",
    "        approx_post = self.model.recognition_encode(x)\n",
    "\n",
    "        iw_lb = self.model.iwlb(self.prior, approx_post, x, k)\n",
    "        return iw_lb.mean().item()\n",
    "\n",
    "    def recongnition(self, x, grad=False, flatten=None):\n",
    "        if flatten is None:\n",
    "            flatten = self.flatten\n",
    "        if flatten and grad:\n",
    "            raise Exception('flatten and grad options are not compatible')\n",
    "        self.set_mode_train(train=False)\n",
    "        if flatten:\n",
    "            x = gauss_cat_to_flat(x, self.input_dim_vec)\n",
    "        if grad:\n",
    "            if not x.requires_grad:\n",
    "                x.requires_grad = True\n",
    "        else:\n",
    "            x, = to_variable(var=(x,), volatile=True, cuda=self.cuda)\n",
    "        approx_post = self.model.encode(x)\n",
    "        return approx_post\n",
    "\n",
    "    def regenerate(self, z, grad=False, unflatten=False):\n",
    "        if unflatten and grad:\n",
    "            raise Exception('flatten and grad options are not compatible')\n",
    "        self.set_mode_train(train=False)\n",
    "        if grad:\n",
    "            if not z.requires_grad:\n",
    "                z.requires_grad = True\n",
    "        else:\n",
    "            z, = to_variable(var=(z,), volatile=True, cuda=self.cuda)\n",
    "        out = self.model.decode(z)\n",
    "\n",
    "        if unflatten:\n",
    "            out = flat_to_gauss_cat(out, self.input_dim_vec)\n",
    "        else:\n",
    "            out = selective_softmax(out, self.input_dim_vec, grad=grad)\n",
    "\n",
    "        if self.pred_sig:\n",
    "            raise Exception('Not implemented')\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = widths[names.index(dname)]\n",
    "depth = depths[names.index(dname)] # number of hidden layers\n",
    "latent_dim = latent_dims[names.index(dname)]\n",
    "\n",
    "batch_size = 128\n",
    "nb_epochs = 2500\n",
    "lr = 1e-4\n",
    "early_stop = 200\n",
    "\n",
    "# cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\n",
      "Net:\u001b[0m\n",
      "\u001b[33mVAE_gauss_net\u001b[0m\n",
      "    Total params: 0.39M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_76581/221739205.py:6: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return np.sum(p.numel() for p in self.model.parameters())\n"
     ]
    }
   ],
   "source": [
    "net = VAE_gauss_cat_net(input_dim_vec, width, depth, latent_dim, pred_sig=False, lr=lr, cuda=cuda, flatten=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "class Datafeed(data.Dataset):\n",
    "\n",
    "    def __init__(self, x_train, y_train=None, transform=None):\n",
    "        self.data = x_train\n",
    "        self.targets = y_train\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.data[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.targets is not None:\n",
    "            return img, self.targets[index]\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = Datafeed(x_train, x_train, transform=None)\n",
    "valset = Datafeed(x_test, x_test, transform=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_VAE(net, name, batch_size, nb_epochs, trainset, valset, cuda, flat_ims=False,\n",
    "              train_plot=False, Nclass=None, early_stop=None, script_mode=False):\n",
    "\n",
    "    models_dir = name + '_models'\n",
    "    results_dir = name + '_results'\n",
    "    mkdir(models_dir)\n",
    "    mkdir(results_dir)\n",
    "\n",
    "    if cuda:\n",
    "        trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True,\n",
    "                                                  num_workers=3)\n",
    "        valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True,\n",
    "                                                num_workers=3)\n",
    "\n",
    "    else:\n",
    "        trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n",
    "                                                  num_workers=3)\n",
    "        valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                                num_workers=3)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# net dims\n",
    "    cprint('c', '\\nNetwork:')\n",
    "\n",
    "    epoch = 0\n",
    "\n",
    "    ## ---------------------------------------------------------------------------------------------------------------------\n",
    "    # train\n",
    "    cprint('c', '\\nTrain:')\n",
    "\n",
    "    print('  init cost variables:')\n",
    "    vlb_train = np.zeros(nb_epochs)\n",
    "    vlb_dev = np.zeros(nb_epochs)\n",
    "    best_vlb = -np.inf\n",
    "    best_vlb_train = -np.inf\n",
    "    best_epoch = 0\n",
    "\n",
    "    nb_its_dev = 1\n",
    "\n",
    "    tic0 = time.time()\n",
    "    for i in range(epoch, nb_epochs):\n",
    "        net.set_mode_train(True)\n",
    "\n",
    "        tic = time.time()\n",
    "        nb_samples = 0\n",
    "        for x, y in trainloader:\n",
    "\n",
    "            if flat_ims:\n",
    "                x = x.view(x.shape[0], -1)\n",
    "            if Nclass is not None:\n",
    "                y_oh = torch_onehot(y, Nclass).type(x.type())\n",
    "                x = torch.cat([x, y_oh], 1)\n",
    "\n",
    "            cost, _ = net.fit(x)\n",
    "\n",
    "            vlb_train[i] += cost * len(x)\n",
    "            nb_samples += len(x)\n",
    "\n",
    "        vlb_train[i] /= nb_samples\n",
    "\n",
    "        toc = time.time()\n",
    "\n",
    "        # ---- print\n",
    "        print(\"it %d/%d, vlb %f, \" % (i, nb_epochs, vlb_train[i]), end=\"\")\n",
    "        cprint('r', '   time: %f seconds\\n' % (toc - tic))\n",
    "        net.update_lr(i)\n",
    "\n",
    "        if vlb_train[i] > best_vlb_train:\n",
    "            best_vlb_train = vlb_train[i]\n",
    "\n",
    "        # ---- dev\n",
    "        if i % nb_its_dev == 0:\n",
    "            nb_samples = 0\n",
    "            for j, (x, y) in enumerate(valloader):\n",
    "\n",
    "                if flat_ims:\n",
    "                    x = x.view(x.shape[0], -1)\n",
    "                if Nclass is not None:\n",
    "                    y_oh = torch_onehot(y, Nclass).type(x.type())\n",
    "                    x = torch.cat([x, y_oh], 1)\n",
    "\n",
    "                cost, _ = net.eval(x)\n",
    "\n",
    "                vlb_dev[i] += cost * len(x)\n",
    "                nb_samples += len(x)\n",
    "\n",
    "            vlb_dev[i] /= nb_samples\n",
    "\n",
    "            cprint('g', '    vlb %f (%f)\\n' % (vlb_dev[i], best_vlb))\n",
    "\n",
    "            if train_plot:\n",
    "                zz = net.recongnition(x).sample()\n",
    "                o = net.regenerate(zz)\n",
    "                try:\n",
    "                    o = o.cpu()\n",
    "                except:\n",
    "                    o = o.loc.cpu()\n",
    "                if len(x.shape) == 2:\n",
    "                    side = int(np.sqrt(x.shape[1]))\n",
    "                    x = x.view(-1, 1, side, side).data\n",
    "                    o = o.view(-1, 1, side, side).data\n",
    "\n",
    "                # save_image(torch.cat([x[:8], o[:8]]), results_dir + '/rec_%d.png' % i, nrow=8)\n",
    "                import matplotlib.pyplot as plt\n",
    "                plt.figure()\n",
    "                dd = make_grid(torch.cat([x[:10], o[:10]]), nrow=10).numpy()\n",
    "                plt.imshow(np.transpose(dd, (1, 2, 0)), interpolation='nearest')\n",
    "                if script_mode:\n",
    "                    plt.savefig(results_dir + '/rec%d.png' % i)\n",
    "                else:\n",
    "                    plt.show()\n",
    "\n",
    "                z_sample = normal(loc=0.0, scale=1.0, size=(36, net.latent_dim))\n",
    "                x_rec = net.regenerate(z_sample)\n",
    "                try:\n",
    "                    x_rec = x_rec.cpu()\n",
    "                except:\n",
    "                    x_rec = x_rec.loc.cpu()\n",
    "                if len(x_rec.shape) == 2:\n",
    "                    side = int(np.sqrt(x_rec.shape[1]))\n",
    "                    x_rec = x_rec.view(-1, 1, side, side)\n",
    "                plt.figure()\n",
    "                dd = make_grid(x_rec, nrow=6).numpy()\n",
    "                plt.imshow(np.transpose(dd, (1, 2, 0)), interpolation='nearest')\n",
    "                if script_mode:\n",
    "                    plt.savefig(results_dir + '/sample%d.png' % i)\n",
    "                else:\n",
    "                    plt.show()\n",
    "\n",
    "        if vlb_dev[i] > best_vlb:\n",
    "            best_vlb = vlb_dev[i]\n",
    "            best_epoch = i\n",
    "            # net.save(models_dir + '/theta_best.dat')\n",
    "            net.save(models_dir + '/theta_best.pt')\n",
    "\n",
    "        if early_stop is not None and (i - best_epoch) > early_stop:\n",
    "            break\n",
    "\n",
    "\n",
    "    # net.save(models_dir + '/theta_last.dat')\n",
    "    net.save(models_dir + '/theta_last.pt')\n",
    "    toc0 = time.time()\n",
    "    runtime_per_it = (toc0 - tic0) / float(nb_epochs)\n",
    "    cprint('r', '   average time: %f seconds\\n' % runtime_per_it)\n",
    "\n",
    "    ## ---------------------------------------------------------------------------------------------------------------------\n",
    "    # results\n",
    "    cprint('c', '\\nRESULTS:')\n",
    "    nb_parameters = net.get_nb_parameters()\n",
    "    best_cost_dev = best_vlb\n",
    "    best_cost_train = best_vlb_train\n",
    "\n",
    "    print('  best_vlb_dev: %f' % best_cost_dev)\n",
    "    print('  best_vlb_train: %f' % best_cost_train)\n",
    "    print('  nb_parameters: %d (%s)\\n' % (nb_parameters, humansize(nb_parameters)))\n",
    "\n",
    "    ## ---------------------------------------------------------------------------------------------------------------------\n",
    "    # fig cost vs its\n",
    "    if not train_plot:\n",
    "        import matplotlib\n",
    "        matplotlib.use('agg')\n",
    "    import matplotlib.pyplot as plt\n",
    "    if train_plot:\n",
    "        plt.figure()\n",
    "        plt.plot(np.clip(vlb_train, -1000, 1000), 'r')\n",
    "        plt.plot(np.clip(vlb_dev[::nb_its_dev], -1000, 1000), 'b')\n",
    "        plt.legend(['cost_train', 'cost_dev'])\n",
    "        plt.ylabel('vlb')\n",
    "        plt.xlabel('it')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(results_dir+'/train_cost.png')\n",
    "        if train_plot:\n",
    "            plt.show()\n",
    "    return vlb_train, vlb_dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dir = '../saves/fc_preact_VAE_NEW(300)_' + dname\n",
    "save_dir = 'saves/fc_preact_VAE_NEW(300)_' + dname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\n",
      "Network:\u001b[0m\n",
      "\u001b[36m\n",
      "Train:\u001b[0m\n",
      "  init cost variables:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_340521/550312925.py:50: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272128894/work/torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 0/2500, vlb -26.065230, \u001b[31m   time: 2.050342 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -14.462896 (-inf)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting saves/fc_preact_VAE_NEW(300)_default_credit_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 1/2500, vlb -17.109920, \u001b[31m   time: 1.807346 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -11.997658 (-14.462896)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting saves/fc_preact_VAE_NEW(300)_default_credit_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 2/2500, vlb -15.234471, \u001b[31m   time: 2.137175 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -10.817770 (-11.997658)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting saves/fc_preact_VAE_NEW(300)_default_credit_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 3/2500, vlb -14.294098, \u001b[31m   time: 1.868079 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -10.182988 (-10.817770)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting saves/fc_preact_VAE_NEW(300)_default_credit_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 4/2500, vlb -13.678397, \u001b[31m   time: 1.875153 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -9.874521 (-10.182988)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting saves/fc_preact_VAE_NEW(300)_default_credit_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 5/2500, vlb -13.223953, \u001b[31m   time: 1.955127 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -9.329113 (-9.874521)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting saves/fc_preact_VAE_NEW(300)_default_credit_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 6/2500, vlb -12.914721, \u001b[31m   time: 1.967848 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -9.183675 (-9.329113)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting saves/fc_preact_VAE_NEW(300)_default_credit_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 7/2500, vlb -12.612784, \u001b[31m   time: 1.854252 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -9.039571 (-9.183675)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting saves/fc_preact_VAE_NEW(300)_default_credit_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 8/2500, vlb -12.373571, \u001b[31m   time: 1.980202 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -8.886182 (-9.039571)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting saves/fc_preact_VAE_NEW(300)_default_credit_models/theta_best.pt\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_340521/2494851835.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m vlb_train, vlb_dev = train_VAE(net, save_dir, batch_size, nb_epochs, trainset, valset,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                cuda=cuda, flat_ims=False, train_plot=False, early_stop=early_stop)\n",
      "\u001b[0;32m/tmp/ipykernel_340521/785436706.py\u001b[0m in \u001b[0;36mtrain_VAE\u001b[0;34m(net, name, batch_size, nb_epochs, trainset, valset, cuda, flat_ims, train_plot, Nclass, early_stop, script_mode)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_oh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mvlb_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_340521/3228637120.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mvlb\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvlb_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vlb_train, vlb_dev = train_VAE(net, save_dir, batch_size, nb_epochs, trainset, valset,\n",
    "                               cuda=cuda, flat_ims=False, train_plot=False, early_stop=early_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPAS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def check_data_file(fname):\n",
    "    files = os.listdir(\".\")  # get the current directory listing\n",
    "    print\n",
    "    \"Looking for file '%s' in the current directory...\" % fname\n",
    "\n",
    "    if fname not in files:\n",
    "        print\n",
    "        \"'%s' not found! Downloading from GitHub...\" % fname\n",
    "        addr = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
    "        \n",
    "        response = urllib.request.urlopen(addr)\n",
    "\n",
    "        data = response.read()\n",
    "        fileOut = open(fname, \"wb\")\n",
    "        fileOut.write(data)\n",
    "        fileOut.close()\n",
    "        print\n",
    "        \"'%s' download and saved locally..\" % fname\n",
    "    else:\n",
    "        print\n",
    "        \"File found in current directory..\"\n",
    "\n",
    "\n",
    "def get_my_COMPAS(rseed=0, separate_test=True, test_ratio=0.2, save_dir='../data/'):\n",
    "    \"\"\"\n",
    "        The adult dataset can be obtained from: https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\n",
    "        The code will look for the data file in the present directory, if it is not found, it will download them from GitHub.\n",
    "    \"\"\"\n",
    "\n",
    "    SEED = rseed\n",
    "    seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    their_FEATURES_CLASSIFICATION = [\"age_cat\", \"race\", \"sex\", \"priors_count\", \"c_charge_degree\"]\n",
    "    FEATURES_CLASSIFICATION = [\"age_cat\", \"race\", \"sex\", \"c_charge_degree\", \"is_recid\", \"priors_count\",\n",
    "                               \"time_served\"]  # features to be used for classification\n",
    "    CONT_VARIABLES = [\"priors_count\",\n",
    "                      \"time_served\"]  # continuous features, will need to be handled separately from categorical features, categorical features will be encoded using one-hot\n",
    "    CLASS_FEATURE = \"two_year_recid\"  # the decision variable\n",
    "\n",
    "\n",
    "    COMPAS_INPUT_FILE = save_dir + \"compas-scores-two-years.csv\"\n",
    "    check_data_file(COMPAS_INPUT_FILE)\n",
    "\n",
    "    # load the data and get some stats\n",
    "    df = pd.read_csv(COMPAS_INPUT_FILE)\n",
    "    df = df.dropna(subset=[\"days_b_screening_arrest\"])  # dropping missing vals\n",
    "\n",
    "    # convert to np array\n",
    "    data = df.to_dict('list')\n",
    "    for k in data.keys():\n",
    "        data[k] = np.array(data[k])\n",
    "\n",
    "    dates_in = data['c_jail_in']\n",
    "    dates_out = data['c_jail_out']\n",
    "    # this measures time in Jail\n",
    "    time_served = []\n",
    "    for i in range(len(dates_in)):\n",
    "        di = datetime.datetime.strptime(dates_in[i], '%Y-%m-%d %H:%M:%S')\n",
    "        do = datetime.datetime.strptime(dates_out[i], '%Y-%m-%d %H:%M:%S')\n",
    "        time_served.append((do - di).days)\n",
    "    time_served = np.array(time_served)\n",
    "    time_served[time_served < 0] = 0\n",
    "    data[\"time_served\"] = time_served\n",
    "\n",
    "    \"\"\" Filtering the data \"\"\"\n",
    "\n",
    "    # These filters are the same as propublica (refer to https://github.com/propublica/compas-analysis)\n",
    "    # If the charge date of a defendants Compas scored crime was not within 30 days from when the person was arrested, we assume that because of data quality reasons, that we do not have the right offense.\n",
    "    idx = np.logical_and(data[\"days_b_screening_arrest\"] <= 30, data[\"days_b_screening_arrest\"] >= -30)\n",
    "\n",
    "    # We coded the recidivist flag -- is_recid -- to be -1 if we could not find a compas case at all.\n",
    "    idx = np.logical_and(idx, data[\"is_recid\"] != -1)\n",
    "\n",
    "    # In a similar vein, ordinary traffic offenses -- those with a c_charge_degree of 'O' -- will not result in Jail time are removed (only two of them).\n",
    "    idx = np.logical_and(idx, data[\"c_charge_degree\"] != \"O\")  # F: felony, M: misconduct\n",
    "\n",
    "    # We filtered the underlying data from Broward county to include only those rows representing people who had either recidivated in two years, or had at least two years outside of a correctional facility.\n",
    "    idx = np.logical_and(idx, data[\"score_text\"] != \"NA\")\n",
    "\n",
    "    # select the examples that satisfy this criteria\n",
    "    for k in data.keys():\n",
    "        data[k] = data[k][idx]\n",
    "\n",
    "    y = data[CLASS_FEATURE]\n",
    "    \"\"\" Feature normalization and one hot encoding \"\"\"\n",
    "\n",
    "    print\n",
    "    \"\\nNumber of people recidivating within two years\"\n",
    "    print\n",
    "    pd.Series(y).value_counts()\n",
    "    print\n",
    "    \"\\n\"\n",
    "\n",
    "    X = []  # empty array with num rows same as num examples, will hstack the features to it\n",
    "    X_dims = []\n",
    "\n",
    "    feature_names = []\n",
    "    for attr in FEATURES_CLASSIFICATION:\n",
    "        vals = data[attr]\n",
    "        if attr in CONT_VARIABLES:\n",
    "            vals = [float(v) for v in vals]\n",
    "            # vals = preprocessing.scale(vals, axis=0, with_mean=True, with_std=True)  # 0 mean and 1 variance\n",
    "            vals = np.reshape(vals, (len(y), -1))  # convert from 1-d arr to a 2-d arr with one col\n",
    "            X_dims.append(1)\n",
    "\n",
    "        else:  # for binary categorical variables, the label binarizer uses just one var instead of two\n",
    "            enc = preprocessing.OneHotEncoder(categories='auto', handle_unknown='error')\n",
    "            enc.fit(vals.reshape(-1, 1))\n",
    "            vals = enc.transform(vals.reshape(-1, 1)).todense()\n",
    "            X_dims += [vals.shape[1]]*vals.shape[1]\n",
    "\n",
    "        # add to learnable features\n",
    "        X.append(vals)\n",
    "\n",
    "        if attr in CONT_VARIABLES:  # continuous feature, just append the name\n",
    "            feature_names.append(attr)\n",
    "        else:  # categorical features\n",
    "            if vals.shape[1] == 1:  # binary features that passed through lib binarizer\n",
    "                feature_names.append(attr)\n",
    "            else:\n",
    "                for k in enc.categories_:  # non-binary categorical features, need to add the names for each cat\n",
    "                    feature_names.append(attr + \"_\" + str(k))\n",
    "\n",
    "    X = np.array(np.concatenate(list(X), axis=1))\n",
    "    X_dims = np.array(X_dims)\n",
    "\n",
    "    if separate_test:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=rseed, shuffle=True)\n",
    "\n",
    "        x_means, x_stds = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "        x_means[X_dims>1] = 0\n",
    "        x_stds[X_dims>1] = 1\n",
    "        x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "        x_train = ((X_train - x_means) / x_stds).astype(np.float32)\n",
    "        x_test = ((X_test - x_means) / x_stds).astype(np.float32)\n",
    "\n",
    "        return x_train, x_test, x_means, x_stds, y_train, y_test, feature_names, X_dims\n",
    "    else:\n",
    "        x_means, x_stds = X.mean(axis=0), X.std(axis=0)\n",
    "        print(X_dims.shape, x_means.shape)\n",
    "        x_means[:,X_dims>1] = 0\n",
    "        x_stds[:,X_dims>1] = 1\n",
    "        x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "        x_train = ((X - x_means) / x_stds).astype(np.float32)\n",
    "\n",
    "        return x_train, x_means, x_stds, y, feature_names, X_dims\n",
    "    \n",
    "def X_dims_to_input_dim_vec(X_dims):\n",
    "    \"\"\"This is for our cat_Gauss VAE model\"\"\"\n",
    "    input_dim_vec = []\n",
    "    i = 0\n",
    "    while i < len(X_dims):\n",
    "        input_dim_vec.append(X_dims[i])\n",
    "        i += X_dims[i]\n",
    "    return np.array(input_dim_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compas (5554, 17) (618, 17)\n",
      "[3 6 2 2 2 1 1]\n",
      "compas\n",
      "\u001b[36m\n",
      "Net:\u001b[0m\n",
      "\u001b[33mVAE_gauss_net\u001b[0m\n",
      "    Total params: 0.38M\n",
      "\u001b[36m\n",
      "Network:\u001b[0m\n",
      "\u001b[36m\n",
      "Train:\u001b[0m\n",
      "  init cost variables:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_76581/221739205.py:6: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return np.sum(p.numel() for p in self.model.parameters())\n",
      "/tmp/ipykernel_76581/550312925.py:50: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272128894/work/torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 0/2500, vlb -9.363647, \u001b[31m   time: 0.442762 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -7.991661 (-inf)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 1/2500, vlb -7.855183, \u001b[31m   time: 0.468221 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -7.181785 (-7.991661)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 2/2500, vlb -7.110823, \u001b[31m   time: 0.439539 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.490730 (-7.181785)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 3/2500, vlb -6.657334, \u001b[31m   time: 0.535310 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.013331 (-6.490730)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 4/2500, vlb -6.385300, \u001b[31m   time: 0.410872 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -5.662899 (-6.013331)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 5/2500, vlb -6.211547, \u001b[31m   time: 0.455290 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -5.456003 (-5.662899)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 6/2500, vlb -5.996038, \u001b[31m   time: 0.549377 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -5.154909 (-5.456003)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 7/2500, vlb -5.871952, \u001b[31m   time: 0.513042 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -5.051650 (-5.154909)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 8/2500, vlb -5.687235, \u001b[31m   time: 0.458290 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.800679 (-5.051650)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 9/2500, vlb -5.640815, \u001b[31m   time: 0.457315 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.737620 (-4.800679)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 10/2500, vlb -5.570623, \u001b[31m   time: 0.430623 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.624374 (-4.737620)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 11/2500, vlb -5.509835, \u001b[31m   time: 0.421152 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.509941 (-4.624374)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 12/2500, vlb -5.509683, \u001b[31m   time: 0.470492 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.488745 (-4.509941)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 13/2500, vlb -5.407712, \u001b[31m   time: 0.419948 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.461305 (-4.488745)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 14/2500, vlb -5.371319, \u001b[31m   time: 0.420182 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.400207 (-4.461305)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 15/2500, vlb -5.313659, \u001b[31m   time: 0.443300 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.317813 (-4.400207)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 16/2500, vlb -5.274110, \u001b[31m   time: 0.455154 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.292760 (-4.317813)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 17/2500, vlb -5.258071, \u001b[31m   time: 0.422896 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.327266 (-4.292760)\n",
      "\u001b[0m\n",
      "it 18/2500, vlb -5.222772, \u001b[31m   time: 0.451467 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.298559 (-4.292760)\n",
      "\u001b[0m\n",
      "it 19/2500, vlb -5.213345, \u001b[31m   time: 0.427847 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.199094 (-4.292760)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 20/2500, vlb -5.181499, \u001b[31m   time: 0.406690 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.230114 (-4.199094)\n",
      "\u001b[0m\n",
      "it 21/2500, vlb -5.145714, \u001b[31m   time: 0.417806 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.268492 (-4.199094)\n",
      "\u001b[0m\n",
      "it 22/2500, vlb -5.155572, \u001b[31m   time: 0.432899 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.213806 (-4.199094)\n",
      "\u001b[0m\n",
      "it 23/2500, vlb -5.151049, \u001b[31m   time: 0.407777 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.149559 (-4.199094)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 24/2500, vlb -5.156379, \u001b[31m   time: 0.413000 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.119809 (-4.149559)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 25/2500, vlb -5.148483, \u001b[31m   time: 0.416930 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.186621 (-4.119809)\n",
      "\u001b[0m\n",
      "it 26/2500, vlb -5.084880, \u001b[31m   time: 0.446321 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.154473 (-4.119809)\n",
      "\u001b[0m\n",
      "it 27/2500, vlb -5.103186, \u001b[31m   time: 0.427568 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.183422 (-4.119809)\n",
      "\u001b[0m\n",
      "it 28/2500, vlb -5.106659, \u001b[31m   time: 0.410100 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.175796 (-4.119809)\n",
      "\u001b[0m\n",
      "it 29/2500, vlb -5.080013, \u001b[31m   time: 0.446165 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.134993 (-4.119809)\n",
      "\u001b[0m\n",
      "it 30/2500, vlb -5.104992, \u001b[31m   time: 0.408806 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.138870 (-4.119809)\n",
      "\u001b[0m\n",
      "it 31/2500, vlb -5.061846, \u001b[31m   time: 0.465455 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.137542 (-4.119809)\n",
      "\u001b[0m\n",
      "it 32/2500, vlb -5.038992, \u001b[31m   time: 0.475172 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.131521 (-4.119809)\n",
      "\u001b[0m\n",
      "it 33/2500, vlb -5.050361, \u001b[31m   time: 0.435457 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.106247 (-4.119809)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 34/2500, vlb -5.053326, \u001b[31m   time: 0.423200 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.130567 (-4.106247)\n",
      "\u001b[0m\n",
      "it 35/2500, vlb -5.073531, \u001b[31m   time: 0.447791 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.123430 (-4.106247)\n",
      "\u001b[0m\n",
      "it 36/2500, vlb -5.047774, \u001b[31m   time: 0.467587 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.225395 (-4.106247)\n",
      "\u001b[0m\n",
      "it 37/2500, vlb -4.988760, \u001b[31m   time: 0.464625 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.040833 (-4.106247)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 38/2500, vlb -5.037895, \u001b[31m   time: 0.462152 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.117953 (-4.040833)\n",
      "\u001b[0m\n",
      "it 39/2500, vlb -5.014102, \u001b[31m   time: 0.481757 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.053563 (-4.040833)\n",
      "\u001b[0m\n",
      "it 40/2500, vlb -5.033003, \u001b[31m   time: 0.469051 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.080703 (-4.040833)\n",
      "\u001b[0m\n",
      "it 41/2500, vlb -5.048009, \u001b[31m   time: 0.619072 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.069890 (-4.040833)\n",
      "\u001b[0m\n",
      "it 42/2500, vlb -5.028993, \u001b[31m   time: 0.461826 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.240433 (-4.040833)\n",
      "\u001b[0m\n",
      "it 43/2500, vlb -5.000016, \u001b[31m   time: 0.491169 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.112259 (-4.040833)\n",
      "\u001b[0m\n",
      "it 44/2500, vlb -4.994445, \u001b[31m   time: 0.513133 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.023867 (-4.040833)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 45/2500, vlb -4.984028, \u001b[31m   time: 0.503598 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.004048 (-4.023867)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 46/2500, vlb -4.973745, \u001b[31m   time: 0.526913 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.046074 (-4.004048)\n",
      "\u001b[0m\n",
      "it 47/2500, vlb -5.001312, \u001b[31m   time: 0.487150 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.048434 (-4.004048)\n",
      "\u001b[0m\n",
      "it 48/2500, vlb -5.018472, \u001b[31m   time: 0.485193 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.076894 (-4.004048)\n",
      "\u001b[0m\n",
      "it 49/2500, vlb -4.988571, \u001b[31m   time: 0.431863 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.047270 (-4.004048)\n",
      "\u001b[0m\n",
      "it 50/2500, vlb -4.990287, \u001b[31m   time: 0.461963 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.964922 (-4.004048)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 51/2500, vlb -4.956168, \u001b[31m   time: 0.521473 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.018367 (-3.964922)\n",
      "\u001b[0m\n",
      "it 52/2500, vlb -4.968192, \u001b[31m   time: 0.448730 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.053312 (-3.964922)\n",
      "\u001b[0m\n",
      "it 53/2500, vlb -4.978900, \u001b[31m   time: 0.451957 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.057359 (-3.964922)\n",
      "\u001b[0m\n",
      "it 54/2500, vlb -4.997491, \u001b[31m   time: 0.460033 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.038628 (-3.964922)\n",
      "\u001b[0m\n",
      "it 55/2500, vlb -4.958280, \u001b[31m   time: 0.463533 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.072433 (-3.964922)\n",
      "\u001b[0m\n",
      "it 56/2500, vlb -4.957727, \u001b[31m   time: 0.477092 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.065631 (-3.964922)\n",
      "\u001b[0m\n",
      "it 57/2500, vlb -4.961570, \u001b[31m   time: 0.480440 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.042728 (-3.964922)\n",
      "\u001b[0m\n",
      "it 58/2500, vlb -4.969955, \u001b[31m   time: 0.462845 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.029290 (-3.964922)\n",
      "\u001b[0m\n",
      "it 59/2500, vlb -4.947271, \u001b[31m   time: 0.512543 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.076758 (-3.964922)\n",
      "\u001b[0m\n",
      "it 60/2500, vlb -4.981717, \u001b[31m   time: 0.462977 seconds\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m    vlb -4.089914 (-3.964922)\n",
      "\u001b[0m\n",
      "it 61/2500, vlb -4.973904, \u001b[31m   time: 0.612066 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.028319 (-3.964922)\n",
      "\u001b[0m\n",
      "it 62/2500, vlb -4.938012, \u001b[31m   time: 0.446390 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.984809 (-3.964922)\n",
      "\u001b[0m\n",
      "it 63/2500, vlb -4.959793, \u001b[31m   time: 0.485437 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.956442 (-3.964922)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 64/2500, vlb -4.945614, \u001b[31m   time: 0.521341 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.997900 (-3.956442)\n",
      "\u001b[0m\n",
      "it 65/2500, vlb -4.941867, \u001b[31m   time: 0.501841 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.988576 (-3.956442)\n",
      "\u001b[0m\n",
      "it 66/2500, vlb -4.951576, \u001b[31m   time: 0.491160 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.986135 (-3.956442)\n",
      "\u001b[0m\n",
      "it 67/2500, vlb -4.952270, \u001b[31m   time: 0.520615 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.019251 (-3.956442)\n",
      "\u001b[0m\n",
      "it 68/2500, vlb -4.936972, \u001b[31m   time: 0.461257 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.995089 (-3.956442)\n",
      "\u001b[0m\n",
      "it 69/2500, vlb -4.943334, \u001b[31m   time: 0.485653 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.015019 (-3.956442)\n",
      "\u001b[0m\n",
      "it 70/2500, vlb -4.939876, \u001b[31m   time: 0.523324 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.029826 (-3.956442)\n",
      "\u001b[0m\n",
      "it 71/2500, vlb -4.938886, \u001b[31m   time: 0.471669 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.072232 (-3.956442)\n",
      "\u001b[0m\n",
      "it 72/2500, vlb -4.958189, \u001b[31m   time: 0.486966 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.028841 (-3.956442)\n",
      "\u001b[0m\n",
      "it 73/2500, vlb -4.940566, \u001b[31m   time: 0.506179 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.046339 (-3.956442)\n",
      "\u001b[0m\n",
      "it 74/2500, vlb -4.944495, \u001b[31m   time: 0.449256 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.008118 (-3.956442)\n",
      "\u001b[0m\n",
      "it 75/2500, vlb -4.932842, \u001b[31m   time: 0.494694 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.994943 (-3.956442)\n",
      "\u001b[0m\n",
      "it 76/2500, vlb -4.904937, \u001b[31m   time: 0.438684 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.013362 (-3.956442)\n",
      "\u001b[0m\n",
      "it 77/2500, vlb -4.907111, \u001b[31m   time: 0.484664 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.960521 (-3.956442)\n",
      "\u001b[0m\n",
      "it 78/2500, vlb -4.917979, \u001b[31m   time: 0.489614 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.039012 (-3.956442)\n",
      "\u001b[0m\n",
      "it 79/2500, vlb -4.887315, \u001b[31m   time: 0.495541 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.017821 (-3.956442)\n",
      "\u001b[0m\n",
      "it 80/2500, vlb -4.917962, \u001b[31m   time: 0.454848 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.013318 (-3.956442)\n",
      "\u001b[0m\n",
      "it 81/2500, vlb -4.935418, \u001b[31m   time: 0.504418 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.985016 (-3.956442)\n",
      "\u001b[0m\n",
      "it 82/2500, vlb -4.915747, \u001b[31m   time: 0.435310 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.992255 (-3.956442)\n",
      "\u001b[0m\n",
      "it 83/2500, vlb -4.881832, \u001b[31m   time: 0.496011 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.013093 (-3.956442)\n",
      "\u001b[0m\n",
      "it 84/2500, vlb -4.904954, \u001b[31m   time: 0.421982 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.992067 (-3.956442)\n",
      "\u001b[0m\n",
      "it 85/2500, vlb -4.922845, \u001b[31m   time: 0.455069 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.087766 (-3.956442)\n",
      "\u001b[0m\n",
      "it 86/2500, vlb -4.932211, \u001b[31m   time: 0.453046 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.005137 (-3.956442)\n",
      "\u001b[0m\n",
      "it 87/2500, vlb -4.931690, \u001b[31m   time: 0.451996 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.962644 (-3.956442)\n",
      "\u001b[0m\n",
      "it 88/2500, vlb -4.906188, \u001b[31m   time: 0.459895 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.016226 (-3.956442)\n",
      "\u001b[0m\n",
      "it 89/2500, vlb -4.899275, \u001b[31m   time: 0.520802 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.030209 (-3.956442)\n",
      "\u001b[0m\n",
      "it 90/2500, vlb -4.918068, \u001b[31m   time: 0.455490 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.026929 (-3.956442)\n",
      "\u001b[0m\n",
      "it 91/2500, vlb -4.895583, \u001b[31m   time: 0.439848 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.010270 (-3.956442)\n",
      "\u001b[0m\n",
      "it 92/2500, vlb -4.885406, \u001b[31m   time: 0.444865 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.994636 (-3.956442)\n",
      "\u001b[0m\n",
      "it 93/2500, vlb -4.888325, \u001b[31m   time: 0.455240 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.000131 (-3.956442)\n",
      "\u001b[0m\n",
      "it 94/2500, vlb -4.938909, \u001b[31m   time: 0.454853 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.979610 (-3.956442)\n",
      "\u001b[0m\n",
      "it 95/2500, vlb -4.909238, \u001b[31m   time: 0.578282 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.056630 (-3.956442)\n",
      "\u001b[0m\n",
      "it 96/2500, vlb -4.937182, \u001b[31m   time: 0.499512 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.044178 (-3.956442)\n",
      "\u001b[0m\n",
      "it 97/2500, vlb -4.894396, \u001b[31m   time: 0.592448 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.990889 (-3.956442)\n",
      "\u001b[0m\n",
      "it 98/2500, vlb -4.913992, \u001b[31m   time: 0.450893 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.061130 (-3.956442)\n",
      "\u001b[0m\n",
      "it 99/2500, vlb -4.893633, \u001b[31m   time: 0.512556 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.036013 (-3.956442)\n",
      "\u001b[0m\n",
      "it 100/2500, vlb -4.920686, \u001b[31m   time: 0.495764 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.059428 (-3.956442)\n",
      "\u001b[0m\n",
      "it 101/2500, vlb -4.897391, \u001b[31m   time: 0.432616 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.969413 (-3.956442)\n",
      "\u001b[0m\n",
      "it 102/2500, vlb -4.920740, \u001b[31m   time: 0.508194 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.018564 (-3.956442)\n",
      "\u001b[0m\n",
      "it 103/2500, vlb -4.898244, \u001b[31m   time: 0.461952 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.941723 (-3.956442)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 104/2500, vlb -4.883472, \u001b[31m   time: 0.512764 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.963398 (-3.941723)\n",
      "\u001b[0m\n",
      "it 105/2500, vlb -4.912541, \u001b[31m   time: 0.509794 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.947126 (-3.941723)\n",
      "\u001b[0m\n",
      "it 106/2500, vlb -4.915112, \u001b[31m   time: 0.469124 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.991800 (-3.941723)\n",
      "\u001b[0m\n",
      "it 107/2500, vlb -4.893934, \u001b[31m   time: 0.424729 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.955498 (-3.941723)\n",
      "\u001b[0m\n",
      "it 108/2500, vlb -4.900553, \u001b[31m   time: 0.481089 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.017108 (-3.941723)\n",
      "\u001b[0m\n",
      "it 109/2500, vlb -4.876344, \u001b[31m   time: 0.445278 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.991316 (-3.941723)\n",
      "\u001b[0m\n",
      "it 110/2500, vlb -4.908031, \u001b[31m   time: 0.455039 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.995523 (-3.941723)\n",
      "\u001b[0m\n",
      "it 111/2500, vlb -4.909775, \u001b[31m   time: 0.472925 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.026706 (-3.941723)\n",
      "\u001b[0m\n",
      "it 112/2500, vlb -4.913233, \u001b[31m   time: 0.453803 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.971205 (-3.941723)\n",
      "\u001b[0m\n",
      "it 113/2500, vlb -4.883105, \u001b[31m   time: 0.505799 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.962500 (-3.941723)\n",
      "\u001b[0m\n",
      "it 114/2500, vlb -4.880519, \u001b[31m   time: 0.444189 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.988054 (-3.941723)\n",
      "\u001b[0m\n",
      "it 115/2500, vlb -4.882509, \u001b[31m   time: 0.465231 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.984809 (-3.941723)\n",
      "\u001b[0m\n",
      "it 116/2500, vlb -4.878394, \u001b[31m   time: 0.440033 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.022989 (-3.941723)\n",
      "\u001b[0m\n",
      "it 117/2500, vlb -4.859765, \u001b[31m   time: 0.446814 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.028372 (-3.941723)\n",
      "\u001b[0m\n",
      "it 118/2500, vlb -4.926308, \u001b[31m   time: 0.438845 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.989156 (-3.941723)\n",
      "\u001b[0m\n",
      "it 119/2500, vlb -4.870174, \u001b[31m   time: 0.479047 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.991675 (-3.941723)\n",
      "\u001b[0m\n",
      "it 120/2500, vlb -4.868364, \u001b[31m   time: 0.453959 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.044236 (-3.941723)\n",
      "\u001b[0m\n",
      "it 121/2500, vlb -4.898941, \u001b[31m   time: 0.591964 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.983424 (-3.941723)\n",
      "\u001b[0m\n",
      "it 122/2500, vlb -4.846698, \u001b[31m   time: 0.465236 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.995495 (-3.941723)\n",
      "\u001b[0m\n",
      "it 123/2500, vlb -4.878280, \u001b[31m   time: 0.437892 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.982327 (-3.941723)\n",
      "\u001b[0m\n",
      "it 124/2500, vlb -4.900157, \u001b[31m   time: 0.432122 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.970011 (-3.941723)\n",
      "\u001b[0m\n",
      "it 125/2500, vlb -4.874179, \u001b[31m   time: 0.511536 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.000817 (-3.941723)\n",
      "\u001b[0m\n",
      "it 126/2500, vlb -4.868779, \u001b[31m   time: 0.442276 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.969355 (-3.941723)\n",
      "\u001b[0m\n",
      "it 127/2500, vlb -4.896876, \u001b[31m   time: 0.472382 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.006438 (-3.941723)\n",
      "\u001b[0m\n",
      "it 128/2500, vlb -4.886309, \u001b[31m   time: 0.431108 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.960079 (-3.941723)\n",
      "\u001b[0m\n",
      "it 129/2500, vlb -4.863441, \u001b[31m   time: 0.449495 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.986562 (-3.941723)\n",
      "\u001b[0m\n",
      "it 130/2500, vlb -4.885542, \u001b[31m   time: 0.466313 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.025760 (-3.941723)\n",
      "\u001b[0m\n",
      "it 131/2500, vlb -4.896961, \u001b[31m   time: 0.507403 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.011917 (-3.941723)\n",
      "\u001b[0m\n",
      "it 132/2500, vlb -4.901379, \u001b[31m   time: 0.432875 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.988629 (-3.941723)\n",
      "\u001b[0m\n",
      "it 133/2500, vlb -4.867789, \u001b[31m   time: 0.438844 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.960462 (-3.941723)\n",
      "\u001b[0m\n",
      "it 134/2500, vlb -4.888707, \u001b[31m   time: 0.446405 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.005514 (-3.941723)\n",
      "\u001b[0m\n",
      "it 135/2500, vlb -4.857969, \u001b[31m   time: 0.440180 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.991671 (-3.941723)\n",
      "\u001b[0m\n",
      "it 136/2500, vlb -4.928339, \u001b[31m   time: 0.445000 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.030537 (-3.941723)\n",
      "\u001b[0m\n",
      "it 137/2500, vlb -4.887506, \u001b[31m   time: 0.476917 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.039149 (-3.941723)\n",
      "\u001b[0m\n",
      "it 138/2500, vlb -4.891415, \u001b[31m   time: 0.512696 seconds\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m    vlb -3.988769 (-3.941723)\n",
      "\u001b[0m\n",
      "it 139/2500, vlb -4.866027, \u001b[31m   time: 0.501762 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.970765 (-3.941723)\n",
      "\u001b[0m\n",
      "it 140/2500, vlb -4.874861, \u001b[31m   time: 0.461818 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.983204 (-3.941723)\n",
      "\u001b[0m\n",
      "it 141/2500, vlb -4.878360, \u001b[31m   time: 0.457032 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.946699 (-3.941723)\n",
      "\u001b[0m\n",
      "it 142/2500, vlb -4.853679, \u001b[31m   time: 0.494689 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.978713 (-3.941723)\n",
      "\u001b[0m\n",
      "it 143/2500, vlb -4.884981, \u001b[31m   time: 0.437635 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.007942 (-3.941723)\n",
      "\u001b[0m\n",
      "it 144/2500, vlb -4.866146, \u001b[31m   time: 0.443375 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.035341 (-3.941723)\n",
      "\u001b[0m\n",
      "it 145/2500, vlb -4.879997, \u001b[31m   time: 0.470198 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.990372 (-3.941723)\n",
      "\u001b[0m\n",
      "it 146/2500, vlb -4.893910, \u001b[31m   time: 0.443132 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.971783 (-3.941723)\n",
      "\u001b[0m\n",
      "it 147/2500, vlb -4.878151, \u001b[31m   time: 0.459334 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.943340 (-3.941723)\n",
      "\u001b[0m\n",
      "it 148/2500, vlb -4.883275, \u001b[31m   time: 0.434242 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.006128 (-3.941723)\n",
      "\u001b[0m\n",
      "it 149/2500, vlb -4.863866, \u001b[31m   time: 0.473187 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.954832 (-3.941723)\n",
      "\u001b[0m\n",
      "it 150/2500, vlb -4.877457, \u001b[31m   time: 0.454082 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.022789 (-3.941723)\n",
      "\u001b[0m\n",
      "it 151/2500, vlb -4.871658, \u001b[31m   time: 0.485876 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.973147 (-3.941723)\n",
      "\u001b[0m\n",
      "it 152/2500, vlb -4.878019, \u001b[31m   time: 0.446715 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.965419 (-3.941723)\n",
      "\u001b[0m\n",
      "it 153/2500, vlb -4.887165, \u001b[31m   time: 0.446280 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.970470 (-3.941723)\n",
      "\u001b[0m\n",
      "it 154/2500, vlb -4.887936, \u001b[31m   time: 0.450598 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.996213 (-3.941723)\n",
      "\u001b[0m\n",
      "it 155/2500, vlb -4.849063, \u001b[31m   time: 0.505066 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.943040 (-3.941723)\n",
      "\u001b[0m\n",
      "it 156/2500, vlb -4.876426, \u001b[31m   time: 0.490878 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.952231 (-3.941723)\n",
      "\u001b[0m\n",
      "it 157/2500, vlb -4.844931, \u001b[31m   time: 0.492573 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.960509 (-3.941723)\n",
      "\u001b[0m\n",
      "it 158/2500, vlb -4.886409, \u001b[31m   time: 0.460732 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.998058 (-3.941723)\n",
      "\u001b[0m\n",
      "it 159/2500, vlb -4.864543, \u001b[31m   time: 0.482257 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.994502 (-3.941723)\n",
      "\u001b[0m\n",
      "it 160/2500, vlb -4.863260, \u001b[31m   time: 0.473093 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.002700 (-3.941723)\n",
      "\u001b[0m\n",
      "it 161/2500, vlb -4.876890, \u001b[31m   time: 0.494988 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.984725 (-3.941723)\n",
      "\u001b[0m\n",
      "it 162/2500, vlb -4.839623, \u001b[31m   time: 0.461730 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.931604 (-3.941723)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 163/2500, vlb -4.854967, \u001b[31m   time: 0.476391 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.944733 (-3.931604)\n",
      "\u001b[0m\n",
      "it 164/2500, vlb -4.820298, \u001b[31m   time: 0.465935 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.953703 (-3.931604)\n",
      "\u001b[0m\n",
      "it 165/2500, vlb -4.868241, \u001b[31m   time: 0.486421 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.959733 (-3.931604)\n",
      "\u001b[0m\n",
      "it 166/2500, vlb -4.865569, \u001b[31m   time: 0.493279 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.919955 (-3.931604)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 167/2500, vlb -4.888312, \u001b[31m   time: 0.486171 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.970859 (-3.919955)\n",
      "\u001b[0m\n",
      "it 168/2500, vlb -4.869843, \u001b[31m   time: 0.480796 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.008160 (-3.919955)\n",
      "\u001b[0m\n",
      "it 169/2500, vlb -4.877344, \u001b[31m   time: 0.464081 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.040625 (-3.919955)\n",
      "\u001b[0m\n",
      "it 170/2500, vlb -4.863254, \u001b[31m   time: 0.486332 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.978613 (-3.919955)\n",
      "\u001b[0m\n",
      "it 171/2500, vlb -4.900942, \u001b[31m   time: 0.493155 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.972727 (-3.919955)\n",
      "\u001b[0m\n",
      "it 172/2500, vlb -4.868526, \u001b[31m   time: 0.520651 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.073657 (-3.919955)\n",
      "\u001b[0m\n",
      "it 173/2500, vlb -4.886151, \u001b[31m   time: 0.498576 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.033231 (-3.919955)\n",
      "\u001b[0m\n",
      "it 174/2500, vlb -4.858221, \u001b[31m   time: 0.508034 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.007856 (-3.919955)\n",
      "\u001b[0m\n",
      "it 175/2500, vlb -4.864168, \u001b[31m   time: 0.501178 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.032628 (-3.919955)\n",
      "\u001b[0m\n",
      "it 176/2500, vlb -4.870707, \u001b[31m   time: 0.465058 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.949004 (-3.919955)\n",
      "\u001b[0m\n",
      "it 177/2500, vlb -4.839658, \u001b[31m   time: 0.502088 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.946319 (-3.919955)\n",
      "\u001b[0m\n",
      "it 178/2500, vlb -4.863510, \u001b[31m   time: 0.457235 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.971249 (-3.919955)\n",
      "\u001b[0m\n",
      "it 179/2500, vlb -4.865141, \u001b[31m   time: 0.509226 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.963890 (-3.919955)\n",
      "\u001b[0m\n",
      "it 180/2500, vlb -4.888197, \u001b[31m   time: 0.492515 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.983635 (-3.919955)\n",
      "\u001b[0m\n",
      "it 181/2500, vlb -4.882877, \u001b[31m   time: 0.507954 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.975276 (-3.919955)\n",
      "\u001b[0m\n",
      "it 182/2500, vlb -4.873902, \u001b[31m   time: 0.484511 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.991230 (-3.919955)\n",
      "\u001b[0m\n",
      "it 183/2500, vlb -4.860006, \u001b[31m   time: 0.541288 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.995143 (-3.919955)\n",
      "\u001b[0m\n",
      "it 184/2500, vlb -4.850986, \u001b[31m   time: 0.480074 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.981662 (-3.919955)\n",
      "\u001b[0m\n",
      "it 185/2500, vlb -4.890752, \u001b[31m   time: 0.478120 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.983109 (-3.919955)\n",
      "\u001b[0m\n",
      "it 186/2500, vlb -4.899069, \u001b[31m   time: 0.527601 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.986975 (-3.919955)\n",
      "\u001b[0m\n",
      "it 187/2500, vlb -4.882724, \u001b[31m   time: 0.488940 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.968911 (-3.919955)\n",
      "\u001b[0m\n",
      "it 188/2500, vlb -4.845031, \u001b[31m   time: 0.452734 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.960197 (-3.919955)\n",
      "\u001b[0m\n",
      "it 189/2500, vlb -4.879203, \u001b[31m   time: 0.496420 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.983603 (-3.919955)\n",
      "\u001b[0m\n",
      "it 190/2500, vlb -4.837679, \u001b[31m   time: 0.508714 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.995432 (-3.919955)\n",
      "\u001b[0m\n",
      "it 191/2500, vlb -4.882571, \u001b[31m   time: 0.463272 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.978782 (-3.919955)\n",
      "\u001b[0m\n",
      "it 192/2500, vlb -4.851374, \u001b[31m   time: 0.476334 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.001288 (-3.919955)\n",
      "\u001b[0m\n",
      "it 193/2500, vlb -4.845728, \u001b[31m   time: 0.472864 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.936388 (-3.919955)\n",
      "\u001b[0m\n",
      "it 194/2500, vlb -4.857821, \u001b[31m   time: 0.474371 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.989283 (-3.919955)\n",
      "\u001b[0m\n",
      "it 195/2500, vlb -4.863362, \u001b[31m   time: 0.483766 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.030878 (-3.919955)\n",
      "\u001b[0m\n",
      "it 196/2500, vlb -4.907655, \u001b[31m   time: 0.445811 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.950705 (-3.919955)\n",
      "\u001b[0m\n",
      "it 197/2500, vlb -4.882627, \u001b[31m   time: 0.503167 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.986582 (-3.919955)\n",
      "\u001b[0m\n",
      "it 198/2500, vlb -4.873225, \u001b[31m   time: 0.441554 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.995533 (-3.919955)\n",
      "\u001b[0m\n",
      "it 199/2500, vlb -4.831888, \u001b[31m   time: 0.444468 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.987292 (-3.919955)\n",
      "\u001b[0m\n",
      "it 200/2500, vlb -4.838246, \u001b[31m   time: 0.455927 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.938077 (-3.919955)\n",
      "\u001b[0m\n",
      "it 201/2500, vlb -4.841554, \u001b[31m   time: 0.501774 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.972947 (-3.919955)\n",
      "\u001b[0m\n",
      "it 202/2500, vlb -4.808414, \u001b[31m   time: 0.442347 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.974040 (-3.919955)\n",
      "\u001b[0m\n",
      "it 203/2500, vlb -4.869206, \u001b[31m   time: 0.436750 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.958567 (-3.919955)\n",
      "\u001b[0m\n",
      "it 204/2500, vlb -4.850559, \u001b[31m   time: 0.466954 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.947244 (-3.919955)\n",
      "\u001b[0m\n",
      "it 205/2500, vlb -4.866245, \u001b[31m   time: 0.463826 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.021283 (-3.919955)\n",
      "\u001b[0m\n",
      "it 206/2500, vlb -4.866474, \u001b[31m   time: 0.440718 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.982316 (-3.919955)\n",
      "\u001b[0m\n",
      "it 207/2500, vlb -4.855536, \u001b[31m   time: 0.494038 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.964230 (-3.919955)\n",
      "\u001b[0m\n",
      "it 208/2500, vlb -4.860828, \u001b[31m   time: 0.479352 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.956474 (-3.919955)\n",
      "\u001b[0m\n",
      "it 209/2500, vlb -4.828756, \u001b[31m   time: 0.494220 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.934885 (-3.919955)\n",
      "\u001b[0m\n",
      "it 210/2500, vlb -4.865719, \u001b[31m   time: 0.468945 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.927437 (-3.919955)\n",
      "\u001b[0m\n",
      "it 211/2500, vlb -4.851802, \u001b[31m   time: 0.448053 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.985029 (-3.919955)\n",
      "\u001b[0m\n",
      "it 212/2500, vlb -4.855379, \u001b[31m   time: 0.461654 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.908598 (-3.919955)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 213/2500, vlb -4.851359, \u001b[31m   time: 0.430104 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.960088 (-3.908598)\n",
      "\u001b[0m\n",
      "it 214/2500, vlb -4.846305, \u001b[31m   time: 0.435778 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.953048 (-3.908598)\n",
      "\u001b[0m\n",
      "it 215/2500, vlb -4.849519, \u001b[31m   time: 0.459630 seconds\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m    vlb -3.926993 (-3.908598)\n",
      "\u001b[0m\n",
      "it 216/2500, vlb -4.874008, \u001b[31m   time: 0.445007 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.007206 (-3.908598)\n",
      "\u001b[0m\n",
      "it 217/2500, vlb -4.854072, \u001b[31m   time: 0.519642 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.964951 (-3.908598)\n",
      "\u001b[0m\n",
      "it 218/2500, vlb -4.844517, \u001b[31m   time: 0.444221 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.987813 (-3.908598)\n",
      "\u001b[0m\n",
      "it 219/2500, vlb -4.843956, \u001b[31m   time: 0.429592 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.015713 (-3.908598)\n",
      "\u001b[0m\n",
      "it 220/2500, vlb -4.851465, \u001b[31m   time: 0.440485 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.958597 (-3.908598)\n",
      "\u001b[0m\n",
      "it 221/2500, vlb -4.859725, \u001b[31m   time: 0.470357 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.931305 (-3.908598)\n",
      "\u001b[0m\n",
      "it 222/2500, vlb -4.861174, \u001b[31m   time: 0.462784 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.971828 (-3.908598)\n",
      "\u001b[0m\n",
      "it 223/2500, vlb -4.867642, \u001b[31m   time: 0.509588 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.950492 (-3.908598)\n",
      "\u001b[0m\n",
      "it 224/2500, vlb -4.862686, \u001b[31m   time: 0.473500 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.959377 (-3.908598)\n",
      "\u001b[0m\n",
      "it 225/2500, vlb -4.839013, \u001b[31m   time: 0.480033 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.926226 (-3.908598)\n",
      "\u001b[0m\n",
      "it 226/2500, vlb -4.863551, \u001b[31m   time: 0.450236 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.956805 (-3.908598)\n",
      "\u001b[0m\n",
      "it 227/2500, vlb -4.843881, \u001b[31m   time: 0.466912 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.936424 (-3.908598)\n",
      "\u001b[0m\n",
      "it 228/2500, vlb -4.867984, \u001b[31m   time: 0.443005 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.978413 (-3.908598)\n",
      "\u001b[0m\n",
      "it 229/2500, vlb -4.865493, \u001b[31m   time: 0.467443 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.979068 (-3.908598)\n",
      "\u001b[0m\n",
      "it 230/2500, vlb -4.853906, \u001b[31m   time: 0.436726 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.958868 (-3.908598)\n",
      "\u001b[0m\n",
      "it 231/2500, vlb -4.810275, \u001b[31m   time: 0.441072 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.947781 (-3.908598)\n",
      "\u001b[0m\n",
      "it 232/2500, vlb -4.876083, \u001b[31m   time: 0.464212 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.934392 (-3.908598)\n",
      "\u001b[0m\n",
      "it 233/2500, vlb -4.850779, \u001b[31m   time: 0.468659 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.944364 (-3.908598)\n",
      "\u001b[0m\n",
      "it 234/2500, vlb -4.864723, \u001b[31m   time: 0.513807 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.942341 (-3.908598)\n",
      "\u001b[0m\n",
      "it 235/2500, vlb -4.839603, \u001b[31m   time: 0.473938 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.932580 (-3.908598)\n",
      "\u001b[0m\n",
      "it 236/2500, vlb -4.851198, \u001b[31m   time: 0.498624 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.945135 (-3.908598)\n",
      "\u001b[0m\n",
      "it 237/2500, vlb -4.830475, \u001b[31m   time: 0.425262 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.987565 (-3.908598)\n",
      "\u001b[0m\n",
      "it 238/2500, vlb -4.865102, \u001b[31m   time: 0.443274 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.979899 (-3.908598)\n",
      "\u001b[0m\n",
      "it 239/2500, vlb -4.855204, \u001b[31m   time: 0.426631 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.952352 (-3.908598)\n",
      "\u001b[0m\n",
      "it 240/2500, vlb -4.830797, \u001b[31m   time: 0.448231 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.970857 (-3.908598)\n",
      "\u001b[0m\n",
      "it 241/2500, vlb -4.863189, \u001b[31m   time: 0.451776 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.949112 (-3.908598)\n",
      "\u001b[0m\n",
      "it 242/2500, vlb -4.856375, \u001b[31m   time: 0.461202 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.929169 (-3.908598)\n",
      "\u001b[0m\n",
      "it 243/2500, vlb -4.899865, \u001b[31m   time: 0.441794 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.999649 (-3.908598)\n",
      "\u001b[0m\n",
      "it 244/2500, vlb -4.844705, \u001b[31m   time: 0.635974 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.031336 (-3.908598)\n",
      "\u001b[0m\n",
      "it 245/2500, vlb -4.863244, \u001b[31m   time: 0.522994 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.997777 (-3.908598)\n",
      "\u001b[0m\n",
      "it 246/2500, vlb -4.845324, \u001b[31m   time: 0.461031 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.040314 (-3.908598)\n",
      "\u001b[0m\n",
      "it 247/2500, vlb -4.849427, \u001b[31m   time: 0.454621 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.959154 (-3.908598)\n",
      "\u001b[0m\n",
      "it 248/2500, vlb -4.858958, \u001b[31m   time: 0.467034 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.997267 (-3.908598)\n",
      "\u001b[0m\n",
      "it 249/2500, vlb -4.849563, \u001b[31m   time: 0.507485 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.986869 (-3.908598)\n",
      "\u001b[0m\n",
      "it 250/2500, vlb -4.825884, \u001b[31m   time: 0.487683 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.920826 (-3.908598)\n",
      "\u001b[0m\n",
      "it 251/2500, vlb -4.852023, \u001b[31m   time: 0.519505 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.003728 (-3.908598)\n",
      "\u001b[0m\n",
      "it 252/2500, vlb -4.837754, \u001b[31m   time: 0.474313 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.965289 (-3.908598)\n",
      "\u001b[0m\n",
      "it 253/2500, vlb -4.838918, \u001b[31m   time: 0.448121 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.964485 (-3.908598)\n",
      "\u001b[0m\n",
      "it 254/2500, vlb -4.823491, \u001b[31m   time: 0.433825 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.969769 (-3.908598)\n",
      "\u001b[0m\n",
      "it 255/2500, vlb -4.871362, \u001b[31m   time: 0.433273 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.923142 (-3.908598)\n",
      "\u001b[0m\n",
      "it 256/2500, vlb -4.844156, \u001b[31m   time: 0.437348 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.947076 (-3.908598)\n",
      "\u001b[0m\n",
      "it 257/2500, vlb -4.821365, \u001b[31m   time: 0.484448 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.965785 (-3.908598)\n",
      "\u001b[0m\n",
      "it 258/2500, vlb -4.840598, \u001b[31m   time: 0.493720 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.947105 (-3.908598)\n",
      "\u001b[0m\n",
      "it 259/2500, vlb -4.820517, \u001b[31m   time: 0.455247 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.923930 (-3.908598)\n",
      "\u001b[0m\n",
      "it 260/2500, vlb -4.822201, \u001b[31m   time: 0.521640 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.935659 (-3.908598)\n",
      "\u001b[0m\n",
      "it 261/2500, vlb -4.828143, \u001b[31m   time: 0.465893 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.964798 (-3.908598)\n",
      "\u001b[0m\n",
      "it 262/2500, vlb -4.838316, \u001b[31m   time: 0.566265 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.896620 (-3.908598)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAE_NEW(300)_compas_models/theta_best.pt\n",
      "\u001b[0m\n",
      "it 263/2500, vlb -4.816910, \u001b[31m   time: 0.481086 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.914609 (-3.896620)\n",
      "\u001b[0m\n",
      "it 264/2500, vlb -4.856339, \u001b[31m   time: 0.476810 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.956573 (-3.896620)\n",
      "\u001b[0m\n",
      "it 265/2500, vlb -4.868764, \u001b[31m   time: 0.492877 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.959517 (-3.896620)\n",
      "\u001b[0m\n",
      "it 266/2500, vlb -4.847199, \u001b[31m   time: 0.510171 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.984508 (-3.896620)\n",
      "\u001b[0m\n",
      "it 267/2500, vlb -4.827925, \u001b[31m   time: 0.441550 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.944065 (-3.896620)\n",
      "\u001b[0m\n",
      "it 268/2500, vlb -4.834561, \u001b[31m   time: 0.507035 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.918225 (-3.896620)\n",
      "\u001b[0m\n",
      "it 269/2500, vlb -4.865340, \u001b[31m   time: 0.484325 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.933677 (-3.896620)\n",
      "\u001b[0m\n",
      "it 270/2500, vlb -4.893125, \u001b[31m   time: 0.450617 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.937880 (-3.896620)\n",
      "\u001b[0m\n",
      "it 271/2500, vlb -4.856787, \u001b[31m   time: 0.511075 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.938780 (-3.896620)\n",
      "\u001b[0m\n",
      "it 272/2500, vlb -4.838110, \u001b[31m   time: 0.496035 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.982321 (-3.896620)\n",
      "\u001b[0m\n",
      "it 273/2500, vlb -4.837812, \u001b[31m   time: 0.608322 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.960526 (-3.896620)\n",
      "\u001b[0m\n",
      "it 274/2500, vlb -4.843984, \u001b[31m   time: 0.723664 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.915257 (-3.896620)\n",
      "\u001b[0m\n",
      "it 275/2500, vlb -4.840455, \u001b[31m   time: 0.653750 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.934421 (-3.896620)\n",
      "\u001b[0m\n",
      "it 276/2500, vlb -4.836261, \u001b[31m   time: 0.494988 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.925571 (-3.896620)\n",
      "\u001b[0m\n",
      "it 277/2500, vlb -4.847945, \u001b[31m   time: 0.486777 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.949985 (-3.896620)\n",
      "\u001b[0m\n",
      "it 278/2500, vlb -4.858394, \u001b[31m   time: 0.499685 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.934118 (-3.896620)\n",
      "\u001b[0m\n",
      "it 279/2500, vlb -4.826319, \u001b[31m   time: 0.487026 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.965357 (-3.896620)\n",
      "\u001b[0m\n",
      "it 280/2500, vlb -4.829515, \u001b[31m   time: 0.441784 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.903434 (-3.896620)\n",
      "\u001b[0m\n",
      "it 281/2500, vlb -4.823532, \u001b[31m   time: 0.489041 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.959522 (-3.896620)\n",
      "\u001b[0m\n",
      "it 282/2500, vlb -4.844238, \u001b[31m   time: 0.460664 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.947085 (-3.896620)\n",
      "\u001b[0m\n",
      "it 283/2500, vlb -4.847684, \u001b[31m   time: 0.469178 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.912310 (-3.896620)\n",
      "\u001b[0m\n",
      "it 284/2500, vlb -4.857152, \u001b[31m   time: 0.506473 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.913234 (-3.896620)\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_76581/3326291318.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# net.load(save_dir+'_models/theta_best.dat')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m vlb_train, vlb_dev = train_VAE(net, save_dir, batch_size, nb_epochs, trainset, valset,\n\u001b[0m\u001b[1;32m     32\u001b[0m                                cuda=cuda, flat_ims=False, train_plot=False, early_stop=early_stop)\n",
      "\u001b[0;32m/tmp/ipykernel_76581/785436706.py\u001b[0m in \u001b[0;36mtrain_VAE\u001b[0;34m(net, name, batch_size, nb_epochs, trainset, valset, cuda, flat_ims, train_plot, Nclass, early_stop, script_mode)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_oh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mvlb_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_76581/3228637120.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mapprox_post\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    215\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_train, x_test, x_means, x_stds, y_train, y_test, feature_names, X_dims = \\\n",
    "    get_my_COMPAS(rseed=42, separate_test=True, test_ratio=0.1, save_dir='../data/')\n",
    "input_dim_vec = X_dims_to_input_dim_vec(X_dims)\n",
    "print('Compas', x_train.shape, x_test.shape)\n",
    "print(input_dim_vec)\n",
    "\n",
    "\n",
    "dname = 'compas'\n",
    "print(dname)\n",
    "\n",
    "trainset = Datafeed(x_train, x_train, transform=None)\n",
    "valset = Datafeed(x_test, x_test, transform=None)\n",
    "\n",
    "save_dir = '../saves/fc_preact_VAE_NEW(300)_' + dname\n",
    "\n",
    "width = widths[names.index(dname)]\n",
    "depth = depths[names.index(dname)] # number of hidden layers\n",
    "latent_dim = latent_dims[names.index(dname)]\n",
    "\n",
    "batch_size = 128\n",
    "nb_epochs = 2500\n",
    "lr = 1e-4\n",
    "early_stop = 200\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "net = VAE_gauss_cat_net(input_dim_vec, width, depth, latent_dim, pred_sig=False, lr=lr, cuda=cuda, flatten=False)\n",
    "\n",
    "# net.load(save_dir+'_models/theta_best.dat')\n",
    "\n",
    "vlb_train, vlb_dev = train_VAE(net, save_dir, batch_size, nb_epochs, trainset, valset,\n",
    "                               cuda=cuda, flat_ims=False, train_plot=False, early_stop=early_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "282.986px",
    "width": "217.986px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
