{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8df6fa2c",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "500cba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "from numpy.random import uniform, binomial\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.functional import softplus\n",
    "from torch.distributions import kl_divergence\n",
    "import torch.nn as nn\n",
    "from torch.nn import MSELoss\n",
    "from torch.distributions import kl_divergence\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.distributions.normal import Normal\n",
    "import torch.utils.data\n",
    "from torchvision.utils import make_grid\n",
    "from torch.nn import Module\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2e311a",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62ea67c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cprint(color, text, **kwargs):\n",
    "    if color[0] == '*':\n",
    "        pre_code = '1;'\n",
    "        color = color[1:]\n",
    "    else:\n",
    "        pre_code = ''\n",
    "    code = {\n",
    "        'a': '30',\n",
    "        'r': '31',\n",
    "        'g': '32',\n",
    "        'y': '33',\n",
    "        'b': '34',\n",
    "        'p': '35',\n",
    "        'c': '36',\n",
    "        'w': '37'\n",
    "    }\n",
    "    print(\"\\x1b[%s%sm%s\\x1b[0m\" % (pre_code, code[color], text), **kwargs)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# (used in sub network)\n",
    "def normal_parse_params(params, min_sigma=1e-3):\n",
    "    \"\"\"\n",
    "    Take a Tensor (e. g. neural network output) and return\n",
    "    torch.distributions.Normal distribution.\n",
    "    This Normal distribution is component-wise independent,\n",
    "    and its dimensionality depends on the input shape.\n",
    "    First half of channels is mean of the distribution,\n",
    "    the softplus of the second half is std (sigma), so there is\n",
    "    no restrictions on the input tensor.\n",
    "    min_sigma is the minimal value of sigma. I. e. if the above\n",
    "    softplus is less than min_sigma, then sigma is clipped\n",
    "    from below with value min_sigma. This regularization\n",
    "    is required for the numerical stability and may be considered\n",
    "    as a neural network architecture choice without any change\n",
    "    to the probabilistic model.\n",
    "    \"\"\"\n",
    "    n = params.shape[0]\n",
    "    d = params.shape[1]\n",
    "    mu = params[:, :d // 2]\n",
    "    sigma_params = params[:, d // 2:]\n",
    "    sigma = softplus(sigma_params)\n",
    "    sigma = sigma.clamp(min=min_sigma)\n",
    "    distr = Normal(mu, sigma)\n",
    "    return distr\n",
    "\n",
    "## (used in the next function)\n",
    "def torch_onehot(y, Nclass):\n",
    "    if y.is_cuda:\n",
    "        y = y.type(torch.cuda.LongTensor)\n",
    "    else:\n",
    "        y = y.type(torch.LongTensor)\n",
    "    y_onehot = torch.zeros((y.shape[0], Nclass)).type(y.type())\n",
    "    # In your for loop\n",
    "    y_onehot.scatter_(1, y.unsqueeze(1), 1)\n",
    "    return y_onehot\n",
    "\n",
    "## (used in the fit of the main network)\n",
    "def gauss_cat_to_flat(x, input_dim_vec):\n",
    "    output = []\n",
    "    for idx, dim in enumerate(input_dim_vec):\n",
    "        if dim == 1:\n",
    "            output.append(x[:, idx].unsqueeze(1))\n",
    "        elif dim > 1:\n",
    "            oh_vec = torch_onehot(x[:, idx], dim).type(x.type())\n",
    "            output.append(oh_vec)\n",
    "        else:\n",
    "            raise ValueError('Error, invalid dimension value')\n",
    "    return torch.cat(output, dim=1)\n",
    "\n",
    "## (also used in the fit of the main network)\n",
    "def flat_to_gauss_cat(x, input_dim_vec):\n",
    "    output = []\n",
    "    cum_dims = 0\n",
    "    for idx, dims in enumerate(input_dim_vec):\n",
    "        if dims == 1:\n",
    "            output.append(x[:, cum_dims].unsqueeze(1))\n",
    "            cum_dims += 1\n",
    "\n",
    "        elif dims > 1:\n",
    "            output.append(x[:, cum_dims:cum_dims + dims].max(dim=1)[1].type(x.type()).unsqueeze(1))\n",
    "            cum_dims += dims\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Error, invalid dimension value')\n",
    "\n",
    "    return torch.cat(output, dim=1)\n",
    "\n",
    "## (also used in the fit of the main network)\n",
    "def to_variable(var=(), cuda=True, volatile=False):\n",
    "    out = []\n",
    "    for v in var:\n",
    "        if isinstance(v, np.ndarray):\n",
    "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
    "        if not v.is_cuda and cuda:\n",
    "            v = v.cuda()\n",
    "        if not isinstance(v, Variable):\n",
    "            v = Variable(v, volatile=volatile)\n",
    "        out.append(v)\n",
    "    return out\n",
    "\n",
    "\n",
    "# Datafeed function which does something, seems to just be a class for the data?\n",
    "class Datafeed(data.Dataset):\n",
    "\n",
    "    def __init__(self, x_train, y_train=None, transform=None):\n",
    "        self.data = x_train\n",
    "        self.targets = y_train\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.data[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.targets is not None:\n",
    "            return img, self.targets[index]\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009aebcb",
   "metadata": {},
   "source": [
    "## Skip Connection (Only for MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71c980a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\"\"\"\n",
    "class SkipConnection(nn.Module):\n",
    "\n",
    "    #Skip-connection over the sequence of layers in the constructor.\n",
    "    #The module passes input data sequentially through these layers\n",
    "    #and then adds original data to the result.\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        super(SkipConnection, self).__init__()\n",
    "        self.inner_net = nn.Sequential(*args)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input + self.inner_net(input)\n",
    "#\"\"\"\n",
    "def preact_leaky_MLPBlock(width):\n",
    "    return SkipConnection(\n",
    "        nn.LeakyReLU(),\n",
    "        nn.BatchNorm1d(num_features=width),\n",
    "        nn.Linear(width, width),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7b6cb5",
   "metadata": {},
   "source": [
    "## Fully connected neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52148503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC Networks\n",
    "\n",
    "#Non-leaky (not used)\n",
    "\"\"\"\n",
    "class MLP_prior_net(nn.Module):\n",
    "    def __init__(self, input_dim, width, depth, latent_dim):\n",
    "        super(MLP_prior_net, self).__init__()\n",
    "        # input layer\n",
    "        proposal_layers = [nn.Linear(input_dim*2, width), nn.ReLU(), nn.BatchNorm1d(num_features=width)]\n",
    "        # body\n",
    "        for i in range(depth-1):\n",
    "            proposal_layers.append(MLPBlock(width))\n",
    "        # output layer\n",
    "        proposal_layers.append(\n",
    "            nn.Linear(width, latent_dim * 2)\n",
    "        )\n",
    "\n",
    "        self.block = nn.Sequential(*proposal_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class MLP_recognition_net(nn.Module):\n",
    "    def __init__(self, input_dim, width, depth, latent_dim):\n",
    "        super(MLP_recognition_net, self).__init__()\n",
    "        # input layer\n",
    "        proposal_layers = [nn.Linear(input_dim, width), nn.ReLU(), nn.BatchNorm1d(num_features=width)]\n",
    "        # body\n",
    "        for i in range(depth-1):\n",
    "            proposal_layers.append(MLPBlock(width))\n",
    "        # output layer\n",
    "        proposal_layers.append(\n",
    "            nn.Linear(width, latent_dim * 2)\n",
    "        )\n",
    "\n",
    "        self.block = nn.Sequential(*proposal_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class MLP_generator_net(nn.Module):\n",
    "    def __init__(self, input_dim, width, depth, latent_dim):\n",
    "        super(MLP_generator_net, self).__init__()\n",
    "        # input layer\n",
    "        generative_layers = [nn.Linear(latent_dim, width), nn.LeakyReLU(), nn.BatchNorm1d(num_features=width)]\n",
    "        # body\n",
    "        for i in range(depth-1):\n",
    "            generative_layers.append(\n",
    "                # skip-connection from prior network to generative network\n",
    "                leaky_MLPBlock(width))\n",
    "        # output layer\n",
    "        generative_layers.extend([\n",
    "            nn.Linear(width,\n",
    "                      input_dim),\n",
    "        ])\n",
    "        self.block = nn.Sequential(*generative_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\"\"\"\n",
    "## Fully linear residual path preact models\n",
    "\n",
    "#Prior net\n",
    "class MLP_preact_prior_net(nn.Module):\n",
    "    def __init__(self, input_dim, width, depth, latent_dim):\n",
    "        super(MLP_preact_prior_net, self).__init__()\n",
    "        # input layer\n",
    "        proposal_layers = [nn.Linear(input_dim*2, width)]\n",
    "        # body\n",
    "        for i in range(depth-1):\n",
    "            proposal_layers.append(preact_leaky_MLPBlock(width))\n",
    "        # output layer\n",
    "        proposal_layers.extend([nn.LeakyReLU(), nn.BatchNorm1d(num_features=width), nn.Linear(width, latent_dim * 2)])\n",
    "\n",
    "\n",
    "        self.block = nn.Sequential(*proposal_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "#Encoder\n",
    "class MLP_preact_recognition_net(nn.Module):\n",
    "    def __init__(self, input_dim, width, depth, latent_dim):\n",
    "        super(MLP_preact_recognition_net, self).__init__()\n",
    "        # input layer\n",
    "        proposal_layers = [nn.Linear(input_dim, width)]\n",
    "        # body\n",
    "        for i in range(depth-1):\n",
    "            proposal_layers.append(preact_leaky_MLPBlock(width))\n",
    "        # output layer\n",
    "        proposal_layers.extend([nn.LeakyReLU(), nn.BatchNorm1d(num_features=width), nn.Linear(width, latent_dim * 2)])\n",
    "\n",
    "        self.block = nn.Sequential(*proposal_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "#Decoder\n",
    "class MLP_preact_generator_net(nn.Module):\n",
    "    def __init__(self, input_dim, width, depth, latent_dim):\n",
    "        super(MLP_preact_generator_net, self).__init__()\n",
    "        # input layer\n",
    "        generative_layers = [nn.Linear(latent_dim, width)]\n",
    "        # body\n",
    "        for i in range(depth-1):\n",
    "            generative_layers.append(\n",
    "                # skip-connection from prior network to generative network\n",
    "                preact_leaky_MLPBlock(width))\n",
    "        # output layer\n",
    "        generative_layers.extend([\n",
    "            nn.LeakyReLU(), nn.BatchNorm1d(num_features=width), nn.Linear(width, input_dim),\n",
    "        ])\n",
    "        self.block = nn.Sequential(*generative_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037739c1",
   "metadata": {},
   "source": [
    "## Functions for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c5a11d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the datasets in UCI\n",
    "def load_UCI(dset_name, splits=10, seed=0, separate_targets=True, save_dir='data/'):\n",
    "    mkdir(save_dir)\n",
    "\n",
    "    if dset_name == 'wine':\n",
    "        if not os.path.isfile(save_dir+'winequality-red.csv'):\n",
    "            urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\",\n",
    "                               filename=save_dir+'winequality-red.csv')\n",
    "        data = pd.read_csv(save_dir+'winequality-red.csv', header=1, delimiter=';').values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'default_credit':\n",
    "        if not os.path.isfile(save_dir + 'default of credit card clients.xls'):\n",
    "            urllib.request.urlretrieve(\n",
    "                \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\",\n",
    "                filename=save_dir + 'default of credit card clients.xls')\n",
    "        data = pd.read_excel(save_dir + 'default of credit card clients.xls', header=[0, 1], index_col=0, # delimiter=\"\\s+\"\n",
    "                             ).values\n",
    "        y_idx = [-1]  # OK\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise Exception('Dataset name doesnt match any known datasets.')\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    data = data[np.random.permutation(np.arange(len(data)))]\n",
    "\n",
    "    kf = KFold(n_splits=splits)\n",
    "    for j, (train_index, test_index) in enumerate(kf.split(data)):\n",
    "\n",
    "        # Not sure what separate targets is\n",
    "        if separate_targets:\n",
    "            x_idx = list(range(data.shape[1]))\n",
    "            for e in y_idx:\n",
    "                x_idx.remove(x_idx[e])\n",
    "\n",
    "            x_idx = np.array(x_idx)\n",
    "            y_idx = np.array(y_idx)\n",
    "            x_train, y_train = data[train_index, :], data[train_index, :]\n",
    "            x_train, y_train = x_train[:, x_idx], y_train[:, y_idx]\n",
    "            x_test, y_test = data[test_index, :], data[test_index, :]\n",
    "            x_test, y_test = x_test[:, x_idx], y_test[:, y_idx]\n",
    "\n",
    "            x_means, x_stds = x_train.mean(axis=0), x_train.std(axis=0)\n",
    "            y_means, y_stds = y_train.mean(axis=0), y_train.std(axis=0)\n",
    "\n",
    "            y_stds[y_stds < 1e-10] = 1\n",
    "            x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "            x_train = ((x_train - x_means) / x_stds).astype(np.float32)\n",
    "            y_train = ((y_train - y_means) / y_stds).astype(np.float32)\n",
    "\n",
    "            x_test = ((x_test - x_means) / x_stds).astype(np.float32)\n",
    "            y_test = ((y_test - y_means) / y_stds).astype(np.float32)\n",
    "\n",
    "            return x_train, x_test, x_means, x_stds, y_train, y_test, y_means, y_stds\n",
    "\n",
    "        else:\n",
    "            x_train, x_test = data[train_index, :], data[test_index, :]\n",
    "            x_means, x_stds = x_train.mean(axis=0), x_train.std(axis=0)\n",
    "\n",
    "            x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "            x_train = ((x_train - x_means) / x_stds).astype(np.float32)\n",
    "            x_test = ((x_test - x_means) / x_stds).astype(np.float32)\n",
    "\n",
    "            return x_train, x_test, x_means, x_stds\n",
    "\n",
    "def mkdir(paths):\n",
    "    if not isinstance(paths, (list, tuple)):\n",
    "        paths = [paths]\n",
    "    for path in paths:\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path, mode=0o777)\n",
    "\n",
    "# Not sure why this is needed\n",
    "def unnormalise_cat_vars(x, x_means, x_stds, input_dim_vec):\n",
    "    input_dim_vec = np.array(input_dim_vec)\n",
    "    unnorm_x = np.multiply(x, x_stds) + x_means\n",
    "\n",
    "    fixed_unnorm = unnorm_x.round()\n",
    "    fixed_unnorm -= fixed_unnorm.min(axis=0).reshape([1, fixed_unnorm.shape[1]])  # this sets all mins to 0\n",
    "    for idx, dims in enumerate(input_dim_vec):\n",
    "        if dims > 1:\n",
    "            vec = fixed_unnorm[:, idx]\n",
    "            vec[vec > dims - 1] = dims - 1\n",
    "            fixed_unnorm[:, idx] = vec\n",
    "\n",
    "    x[:, input_dim_vec > 1] = fixed_unnorm[:, input_dim_vec > 1]\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b56a00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wine\n"
     ]
    }
   ],
   "source": [
    "names = ['wine', 'default_credit', 'compas', 'lsat']\n",
    "widths = [350, 350, 350, 350] # Bigger than VAE because the task of modelling all conditionals is more complex\n",
    "depths = [3, 3, 3, 3] # We go deeper because we are using residual models\n",
    "latent_dims = [6, 8, 4, 4]\n",
    "under_latent_dims = [6, 8, 4, 4] # following the original paper we set dim(u) = dim(z) with d>r [r is true manifold dim]\n",
    "under_latent_dims2 = [4, 6, 3, 3] # following the original paper we set dim(u) = dim(z) with d>r [r is true manifold dim]\n",
    "\n",
    "dname = 'default_credit'\n",
    "print(dname)\n",
    "\n",
    "x_train, x_test, x_means, x_stds = \\\n",
    "load_UCI(dset_name=dname, splits=10, seed=42, separate_targets=False, save_dir='../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80fd341d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6024/4034711185.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'input_dim' is not defined"
     ]
    }
   ],
   "source": [
    "print(input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93410b7d",
   "metadata": {},
   "source": [
    "# Create datafeed for torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cfb4d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = Datafeed(x_train, x_train, transform=None) \n",
    "valset = Datafeed(x_test, x_train, transform=None)\n",
    "\n",
    "save_dir = '../saves/fc_preact_VAEAC_NEW_' + dname\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f024653",
   "metadata": {},
   "source": [
    "# Masker for the VAEAC training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68649573",
   "metadata": {},
   "outputs": [],
   "source": [
    "class top_masker:\n",
    "    \"\"\"\n",
    "    Returned mask is sampled from component-wise independent Bernoulli\n",
    "    distribution with probability of component to be unobserved p.\n",
    "    Such mask induces the type of missingness which is called\n",
    "    in literature \"missing completely at random\" (MCAR).\n",
    "    If some value in batch is missed, it automatically becomes unobserved.\n",
    "    \"\"\"\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        pp = uniform(low=0.0, high=self.p, size=batch.shape[0])\n",
    "        pp = np.expand_dims(pp, axis=1)\n",
    "        pp = np.repeat(pp, batch.shape[1], axis=1)\n",
    "        nan_mask = torch.isnan(batch).float()  # missed values\n",
    "#         bernoulli_mask_numpy = np.random.choice(2, size=batch.shape,\n",
    "#                                                 p=[1 - pp, pp])\n",
    "        bernoulli_mask_numpy = binomial(1, pp, size=None)\n",
    "#         print(bernoulli_mask_numpy.shape)\n",
    "        bernoulli_mask = torch.from_numpy(bernoulli_mask_numpy).float()\n",
    "        mask = torch.max(bernoulli_mask, nan_mask)  # logical or\n",
    "        return mask\n",
    "    \n",
    "masker = top_masker(p=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d145c7",
   "metadata": {},
   "source": [
    "# rms cat loglike (only for categorical data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15df99db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class rms_cat_loglike(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim_vec, reduction='none'):\n",
    "        super(rms_cat_loglike, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        self.input_dim_vec = input_dim_vec\n",
    "        self.mse = MSELoss(reduction='none')  # takes(input, target)\n",
    "        self.ce = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def forward(self, x, y):\n",
    "\n",
    "        log_prob_vec = []\n",
    "        cum_dims = 0\n",
    "        for idx, dims in enumerate(self.input_dim_vec):\n",
    "            if dims == 1:\n",
    "                # Gaussian_case\n",
    "                log_prob_vec.append(-self.mse(x[:, cum_dims], y[:, idx]).unsqueeze(1))\n",
    "                cum_dims += 1\n",
    "\n",
    "            elif dims > 1:\n",
    "                if x.shape[1] == y.shape[1]:\n",
    "                    raise Exception('Input and target seem to be in flat format. Need integer cat targets.')\n",
    "                                \n",
    "                if y.is_cuda:\n",
    "                    tget = y[:, idx].type(torch.cuda.LongTensor)\n",
    "                else:\n",
    "                    tget = y[:, idx].type(torch.LongTensor)\n",
    "\n",
    "                log_prob_vec.append(-self.ce(x[:, cum_dims:cum_dims + dims], tget).unsqueeze(1))\n",
    "                cum_dims += dims\n",
    "\n",
    "            else:\n",
    "                raise ValueError('Error, invalid dimension value')\n",
    "\n",
    "        log_prob_vec = torch.cat(log_prob_vec, dim=1)\n",
    "\n",
    "        if self.reduction == 'none':\n",
    "            return log_prob_vec\n",
    "        elif self.reduction == 'sum':\n",
    "            return log_prob_vec.sum()\n",
    "        elif self.reduction == 'average':\n",
    "            return log_prob_vec.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d94fdf",
   "metadata": {},
   "source": [
    "# Gaussian log likelihood (For datasets with continous with or without categorical data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0eb2f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GaussianLoglike(Module):\n",
    "    \"\"\"\n",
    "    Compute reconstruction log probability of groundtruth given\n",
    "    a tensor of Gaussian distribution parameters and a mask.\n",
    "    Gaussian distribution parameters are output of a neural network\n",
    "    without any restrictions, the minimal sigma value is clipped\n",
    "    from below to min_sigma (default: 1e-2) in order not to overfit\n",
    "    network on some exact pixels.\n",
    "    The first half of channels corresponds to mean, the second half\n",
    "    corresponds to std. See normal_parse_parameters for more info.\n",
    "    This layer doesn't work with NaNs in the data, it is used for\n",
    "    inpainting. Roughly speaking, this loss is similar to L2 loss.\n",
    "    Returns a vector of log probabilities for each object of the batch.\n",
    "    \"\"\"\n",
    "    def __init__(self, min_sigma=1e-2):\n",
    "        super(GaussianLoglike, self).__init__()\n",
    "        self.min_sigma = min_sigma\n",
    "\n",
    "    def forward(self, distr_params, groundtruth, mask=None):\n",
    "        distr = normal_parse_params(distr_params, self.min_sigma)\n",
    "        if mask is not None:\n",
    "            log_probs = distr.log_prob(groundtruth) * mask\n",
    "        else:\n",
    "            log_probs = distr.log_prob(groundtruth)\n",
    "        return log_probs.view(groundtruth.shape[0], -1).sum(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfcc971",
   "metadata": {},
   "source": [
    "# Rectified Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7401578",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = math.sqrt(\n",
    "                            (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (\n",
    "                                        N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e954f83",
   "metadata": {},
   "source": [
    "# Base Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46357ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNet(object):\n",
    "    def __init__(self):\n",
    "        cprint('c', '\\nNet:')\n",
    "\n",
    "    def get_nb_parameters(self):\n",
    "        return np.sum(p.numel() for p in self.model.parameters())\n",
    "\n",
    "    def set_mode_train(self, train=True):\n",
    "        if train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "    def update_lr(self, epoch, gamma=0.99):\n",
    "        self.epoch += 1\n",
    "        if self.schedule is not None:\n",
    "            if len(self.schedule) == 0 or epoch in self.schedule:\n",
    "                self.lr *= gamma\n",
    "                print('learning rate: %f  (%d)\\n' % (self.lr, epoch))\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = self.lr\n",
    "\n",
    "    def save(self, filename):\n",
    "        cprint('c', 'Writting %s\\n' % filename)\n",
    "        torch.save({\n",
    "            'epoch': self.epoch,\n",
    "            'lr': self.lr,\n",
    "            'model': self.model,\n",
    "            'optimizer': self.optimizer}, filename)\n",
    "\n",
    "    def load(self, filename):\n",
    "        cprint('c', 'Reading %s\\n' % filename)\n",
    "        state_dict = torch.load(filename)\n",
    "        self.epoch = state_dict['epoch']\n",
    "        self.lr = state_dict['lr']\n",
    "        self.model = state_dict['model']\n",
    "        self.optimizer = state_dict['optimizer']\n",
    "        print('  restoring epoch: %d, lr: %f' % (self.epoch, self.lr))\n",
    "        return self.epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfaacd1",
   "metadata": {},
   "source": [
    "# Sub network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc6654f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEAC_gauss(nn.Module):\n",
    "    def __init__(self, input_dim, width, depth, latent_dim, pred_sig=True):\n",
    "        super(VAEAC_gauss, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.recognition_net = MLP_preact_recognition_net(input_dim, width, depth, latent_dim)\n",
    "        self.prior_net = MLP_preact_prior_net(input_dim, width, depth, latent_dim)\n",
    "        if pred_sig:\n",
    "            self.generator_net = MLP_preact_generator_net(2*input_dim, width, depth, latent_dim)\n",
    "            self.rec_loglike = GaussianLoglike(min_sigma=1e-2)\n",
    "        else:\n",
    "            self.generator_net = MLP_preact_generator_net(input_dim, width, depth, latent_dim)\n",
    "            self.m_rec_loglike = MSELoss(reduction='none')\n",
    "        self.pred_sig = pred_sig\n",
    "        self.sigma_mu = 1e4\n",
    "        self.sigma_sigma = 1e-4\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_mask(x, mask):\n",
    "        \"\"\"Positive bits in mask are set to 0 in x (observed)\"\"\"\n",
    "        observed = x.clone()  # torch.tensor(x)\n",
    "        observed[mask.bool()] = 0\n",
    "        return observed\n",
    "\n",
    "    def recognition_encode(self, x):\n",
    "        approx_post_params = self.recognition_net(x)\n",
    "        approx_post = normal_parse_params(approx_post_params, 1e-3)\n",
    "        return approx_post\n",
    "\n",
    "    def prior_encode(self, x, mask):\n",
    "        x = self.apply_mask(x, mask)\n",
    "        x = torch.cat([x, mask], 1)\n",
    "        prior_params = self.prior_net(x)\n",
    "        prior = normal_parse_params(prior_params, 1e-3)\n",
    "        return prior\n",
    "\n",
    "    def decode(self, z_sample):\n",
    "        rec_params = self.generator_net(z_sample)\n",
    "        return rec_params\n",
    "\n",
    "    def reg_cost(self, prior):\n",
    "        num_objects = prior.mean.shape[0]\n",
    "        mu = prior.mean.view(num_objects, -1)\n",
    "        sigma = prior.scale.view(num_objects, -1)\n",
    "        mu_regularizer = -(mu ** 2).sum(-1) / 2 / (self.sigma_mu ** 2)\n",
    "        sigma_regularizer = (sigma.log() - sigma).sum(-1) * self.sigma_sigma\n",
    "        return mu_regularizer + sigma_regularizer\n",
    "\n",
    "    def vlb(self, prior, approx_post, x, rec_params):\n",
    "        if self.pred_sig:\n",
    "            rec = self.rec_loglike(rec_params, x).view(x.shape[0], -1).sum(-1)\n",
    "        else:\n",
    "            rec = -self.m_rec_loglike(rec_params, x).view(x.shape[0], -1).sum(-1)\n",
    "        prior_regularization = self.reg_cost(prior).view(x.shape[0], -1).sum(-1)\n",
    "        kl = kl_divergence(approx_post, prior).view(x.shape[0], -1).sum(-1)\n",
    "        return rec - kl + prior_regularization\n",
    "\n",
    "    def iwlb(self, prior, approx_post, x, K=50):\n",
    "        estimates = []\n",
    "        for i in range(K):\n",
    "            latent = approx_post.rsample()\n",
    "            rec_params = self.decode(latent)\n",
    "            if self.pred_sig:\n",
    "                rec_loglike = self.rec_loglike(rec_params, x).view(x.shape[0], -1).sum(-1)\n",
    "            else:\n",
    "                rec_loglike = -self.m_rec_loglike(rec_params, x).view(x.shape[0], -1).sum(-1)\n",
    "\n",
    "            prior_log_prob = prior.log_prob(latent)\n",
    "            prior_log_prob = prior_log_prob.view(x.shape[0], -1)\n",
    "            prior_log_prob = prior_log_prob.sum(-1)\n",
    "\n",
    "            proposal_log_prob = approx_post.log_prob(latent)\n",
    "            proposal_log_prob = proposal_log_prob.view(x.shape[0], -1)\n",
    "            proposal_log_prob = proposal_log_prob.sum(-1)\n",
    "\n",
    "            estimate = rec_loglike + prior_log_prob - proposal_log_prob\n",
    "            estimates.append(estimate[:, None])\n",
    "\n",
    "        return torch.logsumexp(torch.cat(estimates, 1), 1) - np.log(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f89180",
   "metadata": {},
   "source": [
    "# Main Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bb03688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Network\n",
    "class VAEAC_gauss_net(BaseNet):\n",
    "    def __init__(self, input_dim, width, depth, latent_dim, pred_sig=True, lr=1e-3, cuda=True):\n",
    "        super(VAEAC_gauss_net, self).__init__()\n",
    "        cprint('y', 'VAE_gauss_net')\n",
    "\n",
    "        self.cuda = cuda\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.latent_dim = latent_dim\n",
    "        self.lr = lr\n",
    "        self.pred_sig = pred_sig\n",
    "\n",
    "        self.create_net()\n",
    "        self.create_opt()\n",
    "        self.epoch = 0\n",
    "        self.schedule = None\n",
    "\n",
    "        self.vlb_scale = 1 / input_dim  # scale for dimensions of input so we can use same LR always\n",
    "\n",
    "    def create_net(self):\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed(42)\n",
    "        self.model = VAEAC_gauss(self.input_dim, self.width, self.depth, self.latent_dim, self.pred_sig)\n",
    "        if self.cuda:\n",
    "            self.model = self.model.cuda()\n",
    "            cudnn.benchmark = True\n",
    "        print('    Total params: %.2fM' % (self.get_nb_parameters() / 1000000.0))\n",
    "\n",
    "    def create_opt(self):\n",
    "        self.optimizer = RAdam(self.model.parameters(), lr=self.lr) # torch.optim.Adam\n",
    "\n",
    "    def fit(self, x, mask):\n",
    "        self.set_mode_train(train=True)\n",
    "\n",
    "        x, mask = to_variable(var=(x, mask), cuda=self.cuda)\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        prior = self.model.prior_encode(x, mask)\n",
    "        approx_post = self.model.recognition_encode(x)\n",
    "        z_sample = approx_post.rsample()\n",
    "        rec_params = self.model.decode(z_sample)\n",
    "\n",
    "        vlb = self.model.vlb(prior, approx_post, x, rec_params)\n",
    "        loss = (- vlb * self.vlb_scale).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.pred_sig:\n",
    "            rec_return = normal_parse_params(rec_params, 1e-3)\n",
    "        else:\n",
    "            rec_return = rec_params\n",
    "        return vlb.mean().item(), rec_return\n",
    "\n",
    "    def eval(self, x, mask, sample=False):\n",
    "        self.set_mode_train(train=False)\n",
    "\n",
    "        x, mask = to_variable(var=(x, mask), cuda=self.cuda)\n",
    "        prior = self.model.prior_encode(x, mask)\n",
    "\n",
    "        approx_post = self.model.recognition_encode(x)\n",
    "        if sample:\n",
    "            z_sample = approx_post.sample()\n",
    "        else:\n",
    "            z_sample = approx_post.loc\n",
    "        rec_params = self.model.decode(z_sample)\n",
    "\n",
    "        vlb = self.model.vlb(prior, approx_post, x, rec_params)\n",
    "\n",
    "        if self.pred_sig:\n",
    "            rec_return = normal_parse_params(rec_params, 1e-3)\n",
    "        else:\n",
    "            rec_return = rec_params\n",
    "        return vlb.mean().item(), rec_return\n",
    "\n",
    "    def eval_iw(self, x, mask, k=50):\n",
    "        self.set_mode_train(train=False)\n",
    "        x, mask = to_variable(var=(x, mask), cuda=self.cuda)\n",
    "\n",
    "        prior = self.model.prior_encode(x, mask)\n",
    "        approx_post = self.model.recognition_encode(x)\n",
    "\n",
    "        iw_lb = self.model.iwlb(prior, approx_post, x, k)\n",
    "        return iw_lb.mean().item()\n",
    "\n",
    "    def get_prior(self, x, mask):\n",
    "        self.set_mode_train(train=False)\n",
    "        x, mask = to_variable(var=(x, mask), cuda=self.cuda)\n",
    "        prior = self.model.prior_encode(x, mask)\n",
    "        return prior\n",
    "\n",
    "    def get_post(self, x):\n",
    "        self.set_mode_train(train=False)\n",
    "        x, = to_variable(var=(x,), cuda=self.cuda)\n",
    "        approx_post = self.model.recognition_encode(x)\n",
    "        return approx_post\n",
    "\n",
    "    def inpaint(self, x, mask, Nsample=1, z_mean=False):\n",
    "        self.set_mode_train(train=False)\n",
    "        x, mask = to_variable(var=(x, mask), cuda=self.cuda)\n",
    "        prior = self.model.prior_encode(x, mask)\n",
    "        out = []\n",
    "        for i in range(Nsample):\n",
    "            if z_mean:\n",
    "                z_sample = prior.loc.data\n",
    "            else:\n",
    "                z_sample = prior.sample()\n",
    "            rec_params = self.model.decode(z_sample)\n",
    "            out.append(rec_params.data)\n",
    "        out = torch.stack(out, dim=0)\n",
    "\n",
    "        if self.pred_sig:\n",
    "            return [normal_parse_params(out[i], 1e-2) for i in range(Nsample)]\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    def regenerate(self, z, grad=False):\n",
    "        self.set_mode_train(train=False)\n",
    "        if grad:\n",
    "            if not z.requires_grad:\n",
    "                z.requires_grad = True\n",
    "        else:\n",
    "            z, = to_variable(var=(z,), volatile=True, cuda=self.cuda)\n",
    "        out = self.model.decode(z)\n",
    "        if self.pred_sig:\n",
    "            return normal_parse_params(out, 1e-2)\n",
    "        else:\n",
    "            return out.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b166b7",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0642f319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_VAEAC(net, masker, name, batch_size, nb_epochs, trainset, valset, cuda,\n",
    "                flat_ims=False, train_plot=False, Nclass=None, early_stop=None, script_mode=False):\n",
    "\n",
    "    models_dir = name + '_models'\n",
    "    results_dir = name + '_results'\n",
    "    mkdir(models_dir)\n",
    "    mkdir(results_dir)\n",
    "\n",
    "    if cuda:\n",
    "        trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True,\n",
    "                                                  num_workers=0)\n",
    "        valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True,\n",
    "                                                num_workers=0)\n",
    "\n",
    "    else:\n",
    "        trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n",
    "                                                  num_workers=3)\n",
    "        valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                                num_workers=3)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# net dims\n",
    "    cprint('c', '\\nNetwork:')\n",
    "\n",
    "    epoch = 0\n",
    "\n",
    "    ## ---------------------------------------------------------------------------------------------------------------------\n",
    "    # train\n",
    "    cprint('c', '\\nTrain:')\n",
    "\n",
    "    print('  init cost variables:')\n",
    "    vlb_train = np.zeros(nb_epochs)\n",
    "    vlb_dev = np.zeros(nb_epochs)\n",
    "    iwlb_dev = np.zeros(nb_epochs)\n",
    "    best_vlb = -np.inf\n",
    "    best_epoch = 0\n",
    "\n",
    "    nb_its_dev = 1\n",
    "\n",
    "    tic0 = time.time()\n",
    "    for i in range(epoch, nb_epochs):\n",
    "        net.set_mode_train(True)\n",
    "\n",
    "        tic = time.time()\n",
    "        nb_samples = 0\n",
    "        for x, y in trainloader:\n",
    "\n",
    "            if flat_ims:\n",
    "                x = x.view(x.shape[0], -1)\n",
    "            if Nclass is not None:\n",
    "                y_oh = torch_onehot(y, Nclass).type(x.type())\n",
    "                x = torch.cat([x, y_oh], 1)\n",
    "\n",
    "            mask = masker(x)\n",
    "            cost, _ = net.fit(x, mask)\n",
    "\n",
    "            vlb_train[i] += cost * len(x)\n",
    "            nb_samples += len(x)\n",
    "\n",
    "        vlb_train[i] /= nb_samples\n",
    "\n",
    "        toc = time.time()\n",
    "\n",
    "        # ---- print\n",
    "        print(\"it %d/%d, vlb %f, \" % (i, nb_epochs, vlb_train[i]), end=\"\")\n",
    "        cprint('r', '   time: %f seconds\\n' % (toc - tic))\n",
    "        net.update_lr(i)\n",
    "\n",
    "        # ---- dev\n",
    "        if i % nb_its_dev == 0:\n",
    "            nb_samples = 0\n",
    "            for j, (x, y) in enumerate(valloader):\n",
    "\n",
    "                if flat_ims:\n",
    "                    x = x.view(x.shape[0], -1)\n",
    "                if Nclass is not None:\n",
    "                    y_oh = torch_onehot(y, Nclass).type(x.type())\n",
    "                    x = torch.cat([x, y_oh], 1)\n",
    "\n",
    "                mask = masker(x)\n",
    "                cost, rec_mean = net.eval(x, mask)\n",
    "                # iwlb = net.eval_iw(x, mask, 25)\n",
    "\n",
    "                vlb_dev[i] += cost * len(x)\n",
    "                # iwlb_dev[i] += iwlb\n",
    "                nb_samples += len(x)\n",
    "\n",
    "            vlb_dev[i] /= nb_samples\n",
    "            # iwlb_dev[i] /= nb_samples\n",
    "\n",
    "            cprint('g', '    vlb %f (%f)\\n' % (vlb_dev[i], best_vlb))\n",
    "\n",
    "            if train_plot:\n",
    "                xm = net.model.apply_mask(x, mask)\n",
    "                \n",
    "                xr = x.cpu()\n",
    "                rec_inpaint = net.inpaint(xm, mask)\n",
    "                try:\n",
    "                    o = rec_mean.cpu()\n",
    "                    rec_inpaint = rec_inpaint[0].cpu()\n",
    "                except:\n",
    "                    o = rec_mean.loc.cpu()\n",
    "                    rec_inpaint = rec_inpaint[0].loc.cpu()\n",
    "\n",
    "                if Nclass is not None:\n",
    "                    xm = xm[:, :-Nclass]\n",
    "                    rec_inpaint = rec_inpaint[:, :-Nclass]\n",
    "                    xr = xr[:, :-Nclass]\n",
    "                    o = o[:, :-Nclass]\n",
    "\n",
    "                if len(x.shape) == 2:\n",
    "                    side = int(np.sqrt(xm.shape[1]))\n",
    "                    xm = xm.view(-1, 1, side, side).data\n",
    "                    rec_inpaint = rec_inpaint.view(-1, 1, side, side).data\n",
    "                    xr = xr.view(-1, 1, side, side).data\n",
    "                    o = o.view(-1, 1, side, side).data\n",
    "\n",
    "                import matplotlib.pyplot as plt\n",
    "                plt.figure()\n",
    "                dd = make_grid(torch.cat([xr[:10], o[:10]]), nrow=10).numpy()\n",
    "                plt.imshow(np.transpose(dd, (1, 2, 0)), interpolation='nearest')\n",
    "                plt.title('reconstruct')\n",
    "                if script_mode:\n",
    "                    plt.savefig(results_dir + '/rec%d.png' % i)\n",
    "                else:\n",
    "                    plt.show()\n",
    "\n",
    "                import matplotlib.pyplot as plt\n",
    "                plt.figure()\n",
    "                dd = make_grid(torch.cat([xm[:10], rec_inpaint[:10]]), nrow=10).numpy()\n",
    "                plt.imshow(np.transpose(dd, (1, 2, 0)), interpolation='nearest')\n",
    "                plt.title('inpaint')\n",
    "                if script_mode:\n",
    "                    plt.savefig(results_dir + '/inp%d.png' % i)\n",
    "                else:\n",
    "                    plt.show()\n",
    "\n",
    "        if vlb_dev[i] > best_vlb:\n",
    "            best_vlb = vlb_dev[i]\n",
    "            best_epoch = i\n",
    "            net.save(models_dir + '/theta_best.dat')\n",
    "\n",
    "        if early_stop is not None and (i - best_epoch) > early_stop:\n",
    "            break\n",
    "\n",
    "\n",
    "    net.save(models_dir + '/theta_last.dat')\n",
    "    toc0 = time.time()\n",
    "    runtime_per_it = (toc0 - tic0) / float(nb_epochs)\n",
    "    cprint('r', '   average time: %f seconds\\n' % runtime_per_it)\n",
    "\n",
    "    ## ---------------------------------------------------------------------------------------------------------------------\n",
    "    # results\n",
    "    cprint('c', '\\nRESULTS:')\n",
    "    nb_parameters = net.get_nb_parameters()\n",
    "    best_cost_dev = np.max(vlb_dev)\n",
    "    # best_iw_dev = np.max(iwlb_dev)\n",
    "    best_cost_train = np.max(vlb_train)\n",
    "\n",
    "    print('  best_vlb_dev: %f' % best_cost_dev)\n",
    "    # print('  best_iwlb_dev: %f' % best_iw_dev)\n",
    "    print('  best_vlb_train: %f' % best_cost_train)\n",
    "    print('  nb_parameters: %d (%s)\\n' % (nb_parameters, humansize(nb_parameters)))\n",
    "\n",
    "    ## ---------------------------------------------------------------------------------------------------------------------\n",
    "    # fig cost vs its\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(np.clip(vlb_train, -1000, 1000), 'r')\n",
    "    plt.plot(np.clip(vlb_dev[::nb_its_dev], -1000, 1000), 'b')\n",
    "    plt.legend(['cost_train', 'cost_dev'])\n",
    "    plt.ylabel('vlb')\n",
    "    plt.xlabel('it')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(results_dir+'/train_cost.png')\n",
    "\n",
    "    # plt.figure()\n",
    "    # plt.plot(np.clip(iwlb_dev[::nb_its_dev], -1000, 1000), 'b')\n",
    "    # plt.ylabel('dev iwlb')\n",
    "    # plt.xlabel('it')\n",
    "    # plt.grid(True)\n",
    "    # plt.savefig(results_dir + '/train_iwlb.png')\n",
    "    if train_plot:\n",
    "        plt.show()\n",
    "    return vlb_train, vlb_dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8d719bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default_credit\n",
      "24\n",
      "350\n",
      "3\n",
      "8\n",
      "True\n",
      "\u001b[36m\n",
      "Net:\u001b[0m\n",
      "\u001b[33mVAE_gauss_net\u001b[0m\n",
      "    Total params: 0.80M\n",
      "\u001b[36m\n",
      "Network:\u001b[0m\n",
      "\u001b[36m\n",
      "Train:\u001b[0m\n",
      "  init cost variables:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mortimer\\AppData\\Local\\Temp/ipykernel_6024/1070947545.py:6: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return np.sum(p.numel() for p in self.model.parameters())\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6024/610988655.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m vlb_train, vlb_dev = train_VAEAC(net, masker, save_dir, batch_size, nb_epochs, trainset, valset, cuda,\n\u001b[0m\u001b[0;32m     48\u001b[0m             flat_ims=False, train_plot=False, Nclass=None, early_stop=early_stop)\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6024/869475507.py\u001b[0m in \u001b[0;36mtrain_VAEAC\u001b[1;34m(net, masker, name, batch_size, nb_epochs, trainset, valset, cuda, flat_ims, train_plot, Nclass, early_stop, script_mode)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmasker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0mvlb_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcost\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6024/778532943.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mprior\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprior_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mapprox_post\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecognition_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mz_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapprox_post\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrsample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6024/593254102.py\u001b[0m in \u001b[0;36mprior_encode\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mprior_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprior_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mprior\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormal_parse_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprior_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mprior\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6024/2972363544.py\u001b[0m in \u001b[0;36mnormal_parse_params\u001b[1;34m(params, min_sigma)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0msigma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftplus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigma_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0msigma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_sigma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0mdistr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdistr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\distributions\\normal.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mbatch_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNormal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\distributions\\distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[0mvalid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m                     raise ValueError(\n\u001b[0;32m     56\u001b[0m                         \u001b[1;34mf\"Expected parameter {param} \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "names = ['wine', 'default_credit', 'compas', 'lsat']\n",
    "widths = [350, 350, 350, 350] # Bigger than VAE because the task of modelling all conditionals is more complex\n",
    "depths = [3, 3, 3, 3] # We go deeper because we are using residual models\n",
    "latent_dims = [6, 8, 4, 4]\n",
    "under_latent_dims = [6, 8, 4, 4] # following the original paper we set dim(u) = dim(z) with d>r [r is true manifold dim]\n",
    "under_latent_dims2 = [4, 6, 3, 3] # following the original paper we set dim(u) = dim(z) with d>r [r is true manifold dim]\n",
    "\n",
    "dname = 'default_credit'\n",
    "print(dname)\n",
    "\n",
    "x_train, x_test, x_means, x_stds = \\\n",
    "load_UCI(dset_name=dname, splits=10, seed=42, separate_targets=False, save_dir='../data/')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainset = Datafeed(x_train, x_train, transform=None) \n",
    "valset = Datafeed(x_test, x_train, transform=None)\n",
    "\n",
    "save_dir = '../saves/fc_preact_VAEAC_NEW_' + dname\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_dim = x_train.shape[1]\n",
    "print(input_dim)\n",
    "width = widths[names.index(dname)]\n",
    "print(width)\n",
    "depth = depths[names.index(dname)] # number of hidden layers\n",
    "print(depth)\n",
    "latent_dim = latent_dims[names.index(dname)]\n",
    "print(latent_dim)\n",
    "\n",
    "batch_size = 128\n",
    "nb_epochs = 2500\n",
    "lr = 1e-4\n",
    "early_stop = 200\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "print(cuda)\n",
    "\n",
    "input_dim = sum([1, 2, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2])\n",
    "\n",
    "net = VAEAC_gauss_net(input_dim, width, depth, latent_dim, pred_sig=True, lr=lr, cuda=cuda)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vlb_train, vlb_dev = train_VAEAC(net, masker, save_dir, batch_size, nb_epochs, trainset, valset, cuda,\n",
    "            flat_ims=False, train_plot=False, Nclass=None, early_stop=early_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "885fae6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAEAC_gauss(\n",
      "  (recognition_net): MLP_preact_recognition_net(\n",
      "    (block): Sequential(\n",
      "      (0): Linear(in_features=24, out_features=350, bias=True)\n",
      "      (1): SkipConnection(\n",
      "        (inner_net): Sequential(\n",
      "          (0): LeakyReLU(negative_slope=0.01)\n",
      "          (1): BatchNorm1d(350, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Linear(in_features=350, out_features=350, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (2): SkipConnection(\n",
      "        (inner_net): Sequential(\n",
      "          (0): LeakyReLU(negative_slope=0.01)\n",
      "          (1): BatchNorm1d(350, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Linear(in_features=350, out_features=350, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "      (4): BatchNorm1d(350, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Linear(in_features=350, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (prior_net): MLP_preact_prior_net(\n",
      "    (block): Sequential(\n",
      "      (0): Linear(in_features=48, out_features=350, bias=True)\n",
      "      (1): SkipConnection(\n",
      "        (inner_net): Sequential(\n",
      "          (0): LeakyReLU(negative_slope=0.01)\n",
      "          (1): BatchNorm1d(350, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Linear(in_features=350, out_features=350, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (2): SkipConnection(\n",
      "        (inner_net): Sequential(\n",
      "          (0): LeakyReLU(negative_slope=0.01)\n",
      "          (1): BatchNorm1d(350, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Linear(in_features=350, out_features=350, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "      (4): BatchNorm1d(350, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Linear(in_features=350, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (generator_net): MLP_preact_generator_net(\n",
      "    (block): Sequential(\n",
      "      (0): Linear(in_features=8, out_features=350, bias=True)\n",
      "      (1): SkipConnection(\n",
      "        (inner_net): Sequential(\n",
      "          (0): LeakyReLU(negative_slope=0.01)\n",
      "          (1): BatchNorm1d(350, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Linear(in_features=350, out_features=350, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (2): SkipConnection(\n",
      "        (inner_net): Sequential(\n",
      "          (0): LeakyReLU(negative_slope=0.01)\n",
      "          (1): BatchNorm1d(350, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Linear(in_features=350, out_features=350, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "      (4): BatchNorm1d(350, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Linear(in_features=350, out_features=48, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (rec_loglike): GaussianLoglike()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91578089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
