{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8df6fa2c",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "500cba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "from numpy.random import uniform, binomial, normal\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.utils.data as data\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.functional import softplus\n",
    "from torch.distributions import kl_divergence\n",
    "import torch.nn as nn\n",
    "from torch.nn import MSELoss, CrossEntropyLoss\n",
    "from torch.distributions import kl_divergence\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.distributions.normal import Normal\n",
    "import torch.utils.data\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from torch.nn import Module\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2e311a",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62ea67c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cprint(color, text, **kwargs):\n",
    "    if color[0] == '*':\n",
    "        pre_code = '1;'\n",
    "        color = color[1:]\n",
    "    else:\n",
    "        pre_code = ''\n",
    "    code = {\n",
    "        'a': '30',\n",
    "        'r': '31',\n",
    "        'g': '32',\n",
    "        'y': '33',\n",
    "        'b': '34',\n",
    "        'p': '35',\n",
    "        'c': '36',\n",
    "        'w': '37'\n",
    "    }\n",
    "    print(\"\\x1b[%s%sm%s\\x1b[0m\" % (pre_code, code[color], text), **kwargs)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# (used in sub network)\n",
    "def normal_parse_params(params, min_sigma=1e-3):\n",
    "    \"\"\"\n",
    "    Take a Tensor (e. g. neural network output) and return\n",
    "    torch.distributions.Normal distribution.\n",
    "    This Normal distribution is component-wise independent,\n",
    "    and its dimensionality depends on the input shape.\n",
    "    First half of channels is mean of the distribution,\n",
    "    the softplus of the second half is std (sigma), so there is\n",
    "    no restrictions on the input tensor.\n",
    "    min_sigma is the minimal value of sigma. I. e. if the above\n",
    "    softplus is less than min_sigma, then sigma is clipped\n",
    "    from below with value min_sigma. This regularization\n",
    "    is required for the numerical stability and may be considered\n",
    "    as a neural network architecture choice without any change\n",
    "    to the probabilistic model.\n",
    "    \"\"\"\n",
    "    n = params.shape[0]\n",
    "    d = params.shape[1]\n",
    "    mu = params[:, :d // 2]\n",
    "    sigma_params = params[:, d // 2:]\n",
    "    sigma = softplus(sigma_params)\n",
    "    sigma = sigma.clamp(min=min_sigma)\n",
    "    distr = Normal(mu, sigma)\n",
    "    return distr\n",
    "\n",
    "## (used in the next function)\n",
    "def torch_onehot(y, Nclass):\n",
    "    if y.is_cuda:\n",
    "        y = y.type(torch.cuda.LongTensor)\n",
    "    else:\n",
    "        y = y.type(torch.LongTensor)\n",
    "    y_onehot = torch.zeros((y.shape[0], Nclass)).type(y.type())\n",
    "    # In your for loop\n",
    "    y_onehot.scatter_(1, y.unsqueeze(1), 1)\n",
    "    return y_onehot\n",
    "\n",
    "## (used in the fit of the main network)\n",
    "def gauss_cat_to_flat(x, input_dim_vec):\n",
    "    output = []\n",
    "    for idx, dim in enumerate(input_dim_vec):\n",
    "        if dim == 1:\n",
    "            output.append(x[:, idx].unsqueeze(1))\n",
    "        elif dim > 1:\n",
    "            oh_vec = torch_onehot(x[:, idx], dim).type(x.type())\n",
    "            output.append(oh_vec)\n",
    "        else:\n",
    "            raise ValueError('Error, invalid dimension value')\n",
    "    return torch.cat(output, dim=1)\n",
    "\n",
    "## (also used in the fit of the main network)\n",
    "def flat_to_gauss_cat(x, input_dim_vec):\n",
    "    output = []\n",
    "    cum_dims = 0\n",
    "    for idx, dims in enumerate(input_dim_vec):\n",
    "        if dims == 1:\n",
    "            output.append(x[:, cum_dims].unsqueeze(1))\n",
    "            cum_dims += 1\n",
    "\n",
    "        elif dims > 1:\n",
    "            output.append(x[:, cum_dims:cum_dims + dims].max(dim=1)[1].type(x.type()).unsqueeze(1))\n",
    "            cum_dims += dims\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Error, invalid dimension value')\n",
    "\n",
    "    return torch.cat(output, dim=1)\n",
    "\n",
    "def gauss_cat_to_flat_mask(x, input_dim_vec):\n",
    "    output = []\n",
    "    for idx, dim in enumerate(input_dim_vec):\n",
    "        if dim == 1:\n",
    "            output.append(x[:, idx].unsqueeze(1))\n",
    "        elif dim > 1:\n",
    "            oh_vec = x.new_ones(x.shape[0], dim) * x[:, idx].unsqueeze(1)\n",
    "            output.append(oh_vec)\n",
    "        else:\n",
    "            raise ValueError('Error, invalid dimension value')\n",
    "    return torch.cat(output, dim=1)\n",
    "\n",
    "## (also used in the fit of the main network)\n",
    "def to_variable(var=(), cuda=True, volatile=False):\n",
    "    out = []\n",
    "    for v in var:\n",
    "        if isinstance(v, np.ndarray):\n",
    "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
    "        if not v.is_cuda and cuda:\n",
    "            v = v.cuda()\n",
    "        if not isinstance(v, Variable):\n",
    "            v = Variable(v, volatile=volatile)\n",
    "        out.append(v)\n",
    "    return out\n",
    "\n",
    "\n",
    "# Datafeed function which does something, seems to just be a class for the data?\n",
    "class Datafeed(data.Dataset):\n",
    "\n",
    "    def __init__(self, x_train, y_train=None, transform=None):\n",
    "        self.data = x_train\n",
    "        self.targets = y_train\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.data[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.targets is not None:\n",
    "            return img, self.targets[index]\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009aebcb",
   "metadata": {},
   "source": [
    "## Skip Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71c980a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\"\"\"\n",
    "class SkipConnection(nn.Module):\n",
    "\n",
    "    #Skip-connection over the sequence of layers in the constructor.\n",
    "    #The module passes input data sequentially through these layers\n",
    "    #and then adds original data to the result.\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        super(SkipConnection, self).__init__()\n",
    "        self.inner_net = nn.Sequential(*args)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input + self.inner_net(input)\n",
    "#\"\"\"\n",
    "def preact_leaky_MLPBlock(width):\n",
    "    return SkipConnection(\n",
    "        nn.LeakyReLU(),\n",
    "        nn.BatchNorm1d(num_features=width),\n",
    "        nn.Linear(width, width),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7b6cb5",
   "metadata": {},
   "source": [
    "## Fully connected neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52148503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC Networks\n",
    "\n",
    "#Non-leaky (not used)\n",
    "\"\"\"\n",
    "class MLP_prior_net(nn.Module):\n",
    "    def __init__(self, input_dim, width, depth, latent_dim):\n",
    "        super(MLP_prior_net, self).__init__()\n",
    "        # input layer\n",
    "        proposal_layers = [nn.Linear(input_dim*2, width), nn.ReLU(), nn.BatchNorm1d(num_features=width)]\n",
    "        # body\n",
    "        for i in range(depth-1):\n",
    "            proposal_layers.append(MLPBlock(width))\n",
    "        # output layer\n",
    "        proposal_layers.append(\n",
    "            nn.Linear(width, latent_dim * 2)\n",
    "        )\n",
    "\n",
    "        self.block = nn.Sequential(*proposal_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class MLP_recognition_net(nn.Module):\n",
    "    def __init__(self, input_dim, width, depth, latent_dim):\n",
    "        super(MLP_recognition_net, self).__init__()\n",
    "        # input layer\n",
    "        proposal_layers = [nn.Linear(input_dim, width), nn.ReLU(), nn.BatchNorm1d(num_features=width)]\n",
    "        # body\n",
    "        for i in range(depth-1):\n",
    "            proposal_layers.append(MLPBlock(width))\n",
    "        # output layer\n",
    "        proposal_layers.append(\n",
    "            nn.Linear(width, latent_dim * 2)\n",
    "        )\n",
    "\n",
    "        self.block = nn.Sequential(*proposal_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class MLP_generator_net(nn.Module):\n",
    "    def __init__(self, input_dim, width, depth, latent_dim):\n",
    "        super(MLP_generator_net, self).__init__()\n",
    "        # input layer\n",
    "        generative_layers = [nn.Linear(latent_dim, width), nn.LeakyReLU(), nn.BatchNorm1d(num_features=width)]\n",
    "        # body\n",
    "        for i in range(depth-1):\n",
    "            generative_layers.append(\n",
    "                # skip-connection from prior network to generative network\n",
    "                leaky_MLPBlock(width))\n",
    "        # output layer\n",
    "        generative_layers.extend([\n",
    "            nn.Linear(width,\n",
    "                      input_dim),\n",
    "        ])\n",
    "        self.block = nn.Sequential(*generative_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\"\"\"\n",
    "## Fully linear residual path preact models\n",
    "\n",
    "#Prior net\n",
    "class MLP_preact_prior_net(nn.Module):\n",
    "    def __init__(self, input_dim, width, depth, latent_dim):\n",
    "        super(MLP_preact_prior_net, self).__init__()\n",
    "        # input layer\n",
    "        proposal_layers = [nn.Linear(input_dim*2, width)]\n",
    "        # body\n",
    "        for i in range(depth-1):\n",
    "            proposal_layers.append(preact_leaky_MLPBlock(width))\n",
    "        # output layer\n",
    "        proposal_layers.extend([nn.LeakyReLU(), nn.BatchNorm1d(num_features=width), nn.Linear(width, latent_dim * 2)])\n",
    "\n",
    "\n",
    "        self.block = nn.Sequential(*proposal_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "#Encoder\n",
    "class MLP_preact_recognition_net(nn.Module):\n",
    "    def __init__(self, input_dim, width, depth, latent_dim):\n",
    "        super(MLP_preact_recognition_net, self).__init__()\n",
    "        # input layer\n",
    "        proposal_layers = [nn.Linear(input_dim, width)]\n",
    "        # body\n",
    "        for i in range(depth-1):\n",
    "            proposal_layers.append(preact_leaky_MLPBlock(width))\n",
    "        # output layer\n",
    "        proposal_layers.extend([nn.LeakyReLU(), nn.BatchNorm1d(num_features=width), nn.Linear(width, latent_dim * 2)])\n",
    "\n",
    "        self.block = nn.Sequential(*proposal_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "#Decoder\n",
    "class MLP_preact_generator_net(nn.Module):\n",
    "    def __init__(self, input_dim, width, depth, latent_dim):\n",
    "        super(MLP_preact_generator_net, self).__init__()\n",
    "        # input layer\n",
    "        generative_layers = [nn.Linear(latent_dim, width)]\n",
    "        # body\n",
    "        for i in range(depth-1):\n",
    "            generative_layers.append(\n",
    "                # skip-connection from prior network to generative network\n",
    "                preact_leaky_MLPBlock(width))\n",
    "        # output layer\n",
    "        generative_layers.extend([\n",
    "            nn.LeakyReLU(), nn.BatchNorm1d(num_features=width), nn.Linear(width, input_dim),\n",
    "        ])\n",
    "        self.block = nn.Sequential(*generative_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037739c1",
   "metadata": {},
   "source": [
    "## Functions for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c5a11d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the datasets in UCI\n",
    "def load_UCI(dset_name, splits=10, seed=0, separate_targets=True, save_dir='data/'):\n",
    "    mkdir(save_dir)\n",
    "\n",
    "    if dset_name == 'wine':\n",
    "        if not os.path.isfile(save_dir+'winequality-red.csv'):\n",
    "            urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\",\n",
    "                               filename=save_dir+'winequality-red.csv')\n",
    "        data = pd.read_csv(save_dir+'winequality-red.csv', header=1, delimiter=';').values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'default_credit':\n",
    "        if not os.path.isfile(save_dir + 'default of credit card clients.xls'):\n",
    "            urllib.request.urlretrieve(\n",
    "                \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\",\n",
    "                filename=save_dir + 'default of credit card clients.xls')\n",
    "        data = pd.read_excel(save_dir + 'default of credit card clients.xls', header=[0, 1], index_col=0, # delimiter=\"\\s+\"\n",
    "                             ).values\n",
    "        y_idx = [-1]  # OK\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise Exception('Dataset name doesnt match any known datasets.')\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    data = data[np.random.permutation(np.arange(len(data)))]\n",
    "\n",
    "    kf = KFold(n_splits=splits)\n",
    "    for j, (train_index, test_index) in enumerate(kf.split(data)):\n",
    "\n",
    "        # Not sure what separate targets is\n",
    "        if separate_targets:\n",
    "            x_idx = list(range(data.shape[1]))\n",
    "            for e in y_idx:\n",
    "                x_idx.remove(x_idx[e])\n",
    "\n",
    "            x_idx = np.array(x_idx)\n",
    "            y_idx = np.array(y_idx)\n",
    "            x_train, y_train = data[train_index, :], data[train_index, :]\n",
    "            x_train, y_train = x_train[:, x_idx], y_train[:, y_idx]\n",
    "            x_test, y_test = data[test_index, :], data[test_index, :]\n",
    "            x_test, y_test = x_test[:, x_idx], y_test[:, y_idx]\n",
    "\n",
    "            x_means, x_stds = x_train.mean(axis=0), x_train.std(axis=0)\n",
    "            y_means, y_stds = y_train.mean(axis=0), y_train.std(axis=0)\n",
    "\n",
    "            y_stds[y_stds < 1e-10] = 1\n",
    "            x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "            x_train = ((x_train - x_means) / x_stds).astype(np.float32)\n",
    "            y_train = ((y_train - y_means) / y_stds).astype(np.float32)\n",
    "\n",
    "            x_test = ((x_test - x_means) / x_stds).astype(np.float32)\n",
    "            y_test = ((y_test - y_means) / y_stds).astype(np.float32)\n",
    "\n",
    "            return x_train, x_test, x_means, x_stds, y_train, y_test, y_means, y_stds\n",
    "\n",
    "        else:\n",
    "            x_train, x_test = data[train_index, :], data[test_index, :]\n",
    "            x_means, x_stds = x_train.mean(axis=0), x_train.std(axis=0)\n",
    "\n",
    "            x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "            x_train = ((x_train - x_means) / x_stds).astype(np.float32)\n",
    "            x_test = ((x_test - x_means) / x_stds).astype(np.float32)\n",
    "\n",
    "            return x_train, x_test, x_means, x_stds\n",
    "\n",
    "def mkdir(paths):\n",
    "    if not isinstance(paths, (list, tuple)):\n",
    "        paths = [paths]\n",
    "    for path in paths:\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path, mode=0o777)\n",
    "\n",
    "# Not sure why this is needed\n",
    "def unnormalise_cat_vars(x, x_means, x_stds, input_dim_vec):\n",
    "    input_dim_vec = np.array(input_dim_vec)\n",
    "    unnorm_x = np.multiply(x, x_stds) + x_means\n",
    "\n",
    "    fixed_unnorm = unnorm_x.round()\n",
    "    fixed_unnorm -= fixed_unnorm.min(axis=0).reshape([1, fixed_unnorm.shape[1]])  # this sets all mins to 0\n",
    "    for idx, dims in enumerate(input_dim_vec):\n",
    "        if dims > 1:\n",
    "            vec = fixed_unnorm[:, idx]\n",
    "            vec[vec > dims - 1] = dims - 1\n",
    "            fixed_unnorm[:, idx] = vec\n",
    "\n",
    "    x[:, input_dim_vec > 1] = fixed_unnorm[:, input_dim_vec > 1]\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b56a00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default_credit\n"
     ]
    }
   ],
   "source": [
    "names = ['wine', 'default_credit', 'compas', 'lsat']\n",
    "widths = [350, 350, 350, 350] # Bigger than VAE because the task of modelling all conditionals is more complex\n",
    "depths = [3, 3, 3, 3] # We go deeper because we are using residual models\n",
    "latent_dims = [6, 8, 4, 4]\n",
    "under_latent_dims = [6, 8, 4, 4] # following the original paper we set dim(u) = dim(z) with d>r [r is true manifold dim]\n",
    "under_latent_dims2 = [4, 6, 3, 3] # following the original paper we set dim(u) = dim(z) with d>r [r is true manifold dim]\n",
    "\n",
    "dname = 'default_credit'\n",
    "print(dname)\n",
    "\n",
    "x_train, x_test, x_means, x_stds = \\\n",
    "load_UCI(dset_name=dname, splits=10, seed=42, separate_targets=False, save_dir='../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80fd341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import seed\n",
    "#from .UCI_loader import unnormalise_cat_vars\n",
    "\n",
    "\n",
    "# TODO return mean and std for variables + train test split\n",
    "\n",
    "\"\"\"\n",
    "def check_data_file(fname):\n",
    "    files = os.listdir(\".\")  # get the current directory listing\n",
    "    print\n",
    "    \"Looking for file '%s' in the current directory...\" % fname\n",
    "\n",
    "    if fname not in files:\n",
    "        print\n",
    "        \"'%s' not found! Downloading from GitHub...\" % fname\n",
    "        addr = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
    "        try:\n",
    "            response = urllib2.urlopen(addr)\n",
    "        except:\n",
    "            response = urllib3.urlopen(addr)\n",
    "        data = response.read()\n",
    "        fileOut = open(fname, \"w\")\n",
    "        fileOut.write(data)\n",
    "        fileOut.close()\n",
    "        print\n",
    "        \"'%s' download and saved locally..\" % fname\n",
    "    else:\n",
    "        print\n",
    "        \"File found in current directory..\"\n",
    "\"\"\"\n",
    "\n",
    "def check_data_file(fname):\n",
    "    files = os.listdir(\".\")  # get the current directory listing\n",
    "    print\n",
    "    \"Looking for file '%s' in the current directory...\" % fname\n",
    "\n",
    "    if fname not in files:\n",
    "        print\n",
    "        \"'%s' not found! Downloading from GitHub...\" % fname\n",
    "        addr = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
    "        \n",
    "        response = urllib.request.urlopen(addr)\n",
    "\n",
    "        data = response.read()\n",
    "        fileOut = open(fname, \"wb\")\n",
    "        fileOut.write(data)\n",
    "        fileOut.close()\n",
    "        print\n",
    "        \"'%s' download and saved locally..\" % fname\n",
    "    else:\n",
    "        print\n",
    "        \"File found in current directory..\"\n",
    "\n",
    "def get_my_COMPAS(rseed=0, separate_test=True, test_ratio=0.2, save_dir='../data/'):\n",
    "    \"\"\"\n",
    "        The adult dataset can be obtained from: https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\n",
    "        The code will look for the data file in the present directory, if it is not found, it will download them from GitHub.\n",
    "    \"\"\"\n",
    "\n",
    "    SEED = rseed\n",
    "    seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    their_FEATURES_CLASSIFICATION = [\"age_cat\", \"race\", \"sex\", \"priors_count\", \"c_charge_degree\"]\n",
    "    FEATURES_CLASSIFICATION = [\"age_cat\", \"race\", \"sex\", \"c_charge_degree\", \"is_recid\", \"priors_count\",\n",
    "                               \"time_served\"]  # features to be used for classification\n",
    "    CONT_VARIABLES = [\"priors_count\",\n",
    "                      \"time_served\"]  # continuous features, will need to be handled separately from categorical features, categorical features will be encoded using one-hot\n",
    "    CLASS_FEATURE = \"two_year_recid\"  # the decision variable\n",
    "\n",
    "\n",
    "    COMPAS_INPUT_FILE = save_dir + \"compas-scores-two-years.csv\"\n",
    "    check_data_file(COMPAS_INPUT_FILE)\n",
    "\n",
    "    # load the data and get some stats\n",
    "    df = pd.read_csv(COMPAS_INPUT_FILE)\n",
    "    df = df.dropna(subset=[\"days_b_screening_arrest\"])  # dropping missing vals\n",
    "\n",
    "    # convert to np array\n",
    "    data = df.to_dict('list')\n",
    "    for k in data.keys():\n",
    "        data[k] = np.array(data[k])\n",
    "\n",
    "    dates_in = data['c_jail_in']\n",
    "    dates_out = data['c_jail_out']\n",
    "    # this measures time in Jail\n",
    "    time_served = []\n",
    "    for i in range(len(dates_in)):\n",
    "        di = datetime.datetime.strptime(dates_in[i], '%Y-%m-%d %H:%M:%S')\n",
    "        do = datetime.datetime.strptime(dates_out[i], '%Y-%m-%d %H:%M:%S')\n",
    "        time_served.append((do - di).days)\n",
    "    time_served = np.array(time_served)\n",
    "    time_served[time_served < 0] = 0\n",
    "    data[\"time_served\"] = time_served\n",
    "\n",
    "    \"\"\" Filtering the data \"\"\"\n",
    "\n",
    "    # These filters are the same as propublica (refer to https://github.com/propublica/compas-analysis)\n",
    "    # If the charge date of a defendants Compas scored crime was not within 30 days from when the person was arrested, we assume that because of data quality reasons, that we do not have the right offense.\n",
    "    idx = np.logical_and(data[\"days_b_screening_arrest\"] <= 30, data[\"days_b_screening_arrest\"] >= -30)\n",
    "\n",
    "    # We coded the recidivist flag -- is_recid -- to be -1 if we could not find a compas case at all.\n",
    "    idx = np.logical_and(idx, data[\"is_recid\"] != -1)\n",
    "\n",
    "    # In a similar vein, ordinary traffic offenses -- those with a c_charge_degree of 'O' -- will not result in Jail time are removed (only two of them).\n",
    "    idx = np.logical_and(idx, data[\"c_charge_degree\"] != \"O\")  # F: felony, M: misconduct\n",
    "\n",
    "    # We filtered the underlying data from Broward county to include only those rows representing people who had either recidivated in two years, or had at least two years outside of a correctional facility.\n",
    "    idx = np.logical_and(idx, data[\"score_text\"] != \"NA\")\n",
    "\n",
    "    # select the examples that satisfy this criteria\n",
    "    for k in data.keys():\n",
    "        data[k] = data[k][idx]\n",
    "\n",
    "    y = data[CLASS_FEATURE]\n",
    "    \"\"\" Feature normalization and one hot encoding \"\"\"\n",
    "\n",
    "    print\n",
    "    \"\\nNumber of people recidivating within two years\"\n",
    "    print\n",
    "    pd.Series(y).value_counts()\n",
    "    print\n",
    "    \"\\n\"\n",
    "\n",
    "    X = []  # empty array with num rows same as num examples, will hstack the features to it\n",
    "    X_dims = []\n",
    "\n",
    "    feature_names = []\n",
    "    for attr in FEATURES_CLASSIFICATION:\n",
    "        vals = data[attr]\n",
    "        if attr in CONT_VARIABLES:\n",
    "            vals = [float(v) for v in vals]\n",
    "            # vals = preprocessing.scale(vals, axis=0, with_mean=True, with_std=True)  # 0 mean and 1 variance\n",
    "            vals = np.reshape(vals, (len(y), -1))  # convert from 1-d arr to a 2-d arr with one col\n",
    "            X_dims.append(1)\n",
    "\n",
    "        else:  # for binary categorical variables, the label binarizer uses just one var instead of two\n",
    "            enc = preprocessing.OneHotEncoder(categories='auto', handle_unknown='error')\n",
    "            enc.fit(vals.reshape(-1, 1))\n",
    "            vals = enc.transform(vals.reshape(-1, 1)).todense()\n",
    "            X_dims += [vals.shape[1]]*vals.shape[1]\n",
    "\n",
    "        # add to learnable features\n",
    "        X.append(vals)\n",
    "\n",
    "        if attr in CONT_VARIABLES:  # continuous feature, just append the name\n",
    "            feature_names.append(attr)\n",
    "        else:  # categorical features\n",
    "            if vals.shape[1] == 1:  # binary features that passed through lib binarizer\n",
    "                feature_names.append(attr)\n",
    "            else:\n",
    "                for k in enc.categories_:  # non-binary categorical features, need to add the names for each cat\n",
    "                    feature_names.append(attr + \"_\" + str(k))\n",
    "\n",
    "    X = np.array(np.concatenate(list(X), axis=1))\n",
    "    X_dims = np.array(X_dims)\n",
    "\n",
    "    if separate_test:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=rseed, shuffle=True)\n",
    "\n",
    "        x_means, x_stds = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "        x_means[X_dims>1] = 0\n",
    "        x_stds[X_dims>1] = 1\n",
    "        x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "        x_train = ((X_train - x_means) / x_stds).astype(np.float32)\n",
    "        x_test = ((X_test - x_means) / x_stds).astype(np.float32)\n",
    "\n",
    "        return x_train, x_test, x_means, x_stds, y_train, y_test, feature_names, X_dims\n",
    "    else:\n",
    "        x_means, x_stds = X.mean(axis=0), X.std(axis=0)\n",
    "        print(X_dims.shape, x_means.shape)\n",
    "        x_means[:,X_dims>1] = 0\n",
    "        x_stds[:,X_dims>1] = 1\n",
    "        x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "        x_train = ((X - x_means) / x_stds).astype(np.float32)\n",
    "\n",
    "        return x_train, x_means, x_stds, y, feature_names, X_dims\n",
    "\n",
    "\n",
    "def join_compas_targets(x_train, x_test, y_train, y_test, X_dims):\n",
    "    # output from get method is onehot so we need to flatten and append 2\n",
    "    input_dim_vec = X_dims_to_input_dim_vec(X_dims)\n",
    "    input_dim_vec = np.append(input_dim_vec, 2)\n",
    "    enc = preprocessing.OneHotEncoder(categories='auto', handle_unknown='error')\n",
    "    enc.fit(y_train.reshape(-1, 1))\n",
    "\n",
    "    vals_train = np.array(enc.transform(y_train.reshape(-1, 1)).todense()).astype(np.float32)\n",
    "    vals_test = np.array(enc.transform(y_test.reshape(-1, 1)).todense()).astype(np.float32)\n",
    "\n",
    "    x_train = np.concatenate([x_train, vals_train], axis=1)\n",
    "    x_test = np.concatenate([x_test, vals_test], axis=1)\n",
    "    return x_train, x_test, input_dim_vec\n",
    "\n",
    "\n",
    "def X_dims_to_input_dim_vec(X_dims):\n",
    "    \"\"\"This is for our cat_Gauss VAE model\"\"\"\n",
    "    input_dim_vec = []\n",
    "    i = 0\n",
    "    while i < len(X_dims):\n",
    "        input_dim_vec.append(X_dims[i])\n",
    "        i += X_dims[i]\n",
    "    return np.array(input_dim_vec)\n",
    "\n",
    "#\"\"\"\n",
    "def input_dim_vec_to_X_dims(input_dim_vec):\n",
    "    # This is for our cat_Gauss VAE model\n",
    "    X_dims = []\n",
    "    for i in input_dim_vec:\n",
    "        for ii in range(i):\n",
    "            X_dims.append(i)\n",
    "    return np.array(X_dims)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93410b7d",
   "metadata": {},
   "source": [
    "# Create datafeed for torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cfb4d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = Datafeed(x_train, x_train, transform=None) \n",
    "valset = Datafeed(x_test, x_train, transform=None)\n",
    "\n",
    "save_dir = '../saves/fc_preact_VAEAC_NEW_' + dname\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f024653",
   "metadata": {},
   "source": [
    "# Masker for the VAEAC training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68649573",
   "metadata": {},
   "outputs": [],
   "source": [
    "class top_masker:\n",
    "    \"\"\"\n",
    "    Returned mask is sampled from component-wise independent Bernoulli\n",
    "    distribution with probability of component to be unobserved p.\n",
    "    Such mask induces the type of missingness which is called\n",
    "    in literature \"missing completely at random\" (MCAR).\n",
    "    If some value in batch is missed, it automatically becomes unobserved.\n",
    "    \"\"\"\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        pp = uniform(low=0.0, high=self.p, size=batch.shape[0])\n",
    "        pp = np.expand_dims(pp, axis=1)\n",
    "        pp = np.repeat(pp, batch.shape[1], axis=1)\n",
    "        nan_mask = torch.isnan(batch).float()  # missed values\n",
    "#         bernoulli_mask_numpy = np.random.choice(2, size=batch.shape,\n",
    "#                                                 p=[1 - pp, pp])\n",
    "        bernoulli_mask_numpy = binomial(1, pp, size=None)\n",
    "#         print(bernoulli_mask_numpy.shape)\n",
    "        bernoulli_mask = torch.from_numpy(bernoulli_mask_numpy).float()\n",
    "        mask = torch.max(bernoulli_mask, nan_mask)  # logical or\n",
    "        return mask\n",
    "    \n",
    "masker = top_masker(p=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d145c7",
   "metadata": {},
   "source": [
    "# rms cat loglike (only for categorical data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15df99db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class rms_cat_loglike(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim_vec, reduction='none'):\n",
    "        super(rms_cat_loglike, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        self.input_dim_vec = input_dim_vec\n",
    "        self.mse = MSELoss(reduction='none')  # takes(input, target)\n",
    "        self.ce = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def forward(self, x, y):\n",
    "\n",
    "        log_prob_vec = []\n",
    "        cum_dims = 0\n",
    "        for idx, dims in enumerate(self.input_dim_vec):\n",
    "            if dims == 1:\n",
    "                # Gaussian_case\n",
    "                log_prob_vec.append(-self.mse(x[:, cum_dims], y[:, idx]).unsqueeze(1))\n",
    "                cum_dims += 1\n",
    "\n",
    "            elif dims > 1:\n",
    "                if x.shape[1] == y.shape[1]:\n",
    "                    raise Exception('Input and target seem to be in flat format. Need integer cat targets.')\n",
    "                                \n",
    "                if y.is_cuda:\n",
    "                    tget = y[:, idx].type(torch.cuda.LongTensor)\n",
    "                else:\n",
    "                    tget = y[:, idx].type(torch.LongTensor)\n",
    "\n",
    "                log_prob_vec.append(-self.ce(x[:, cum_dims:cum_dims + dims], tget).unsqueeze(1))\n",
    "                cum_dims += dims\n",
    "\n",
    "            else:\n",
    "                raise ValueError('Error, invalid dimension value')\n",
    "\n",
    "        log_prob_vec = torch.cat(log_prob_vec, dim=1)\n",
    "\n",
    "        if self.reduction == 'none':\n",
    "            return log_prob_vec\n",
    "        elif self.reduction == 'sum':\n",
    "            return log_prob_vec.sum()\n",
    "        elif self.reduction == 'average':\n",
    "            return log_prob_vec.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d94fdf",
   "metadata": {},
   "source": [
    "# Gaussian log likelihood (For datasets with continous with or without categorical data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0eb2f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GaussianLoglike(Module):\n",
    "    \"\"\"\n",
    "    Compute reconstruction log probability of groundtruth given\n",
    "    a tensor of Gaussian distribution parameters and a mask.\n",
    "    Gaussian distribution parameters are output of a neural network\n",
    "    without any restrictions, the minimal sigma value is clipped\n",
    "    from below to min_sigma (default: 1e-2) in order not to overfit\n",
    "    network on some exact pixels.\n",
    "    The first half of channels corresponds to mean, the second half\n",
    "    corresponds to std. See normal_parse_parameters for more info.\n",
    "    This layer doesn't work with NaNs in the data, it is used for\n",
    "    inpainting. Roughly speaking, this loss is similar to L2 loss.\n",
    "    Returns a vector of log probabilities for each object of the batch.\n",
    "    \"\"\"\n",
    "    def __init__(self, min_sigma=1e-2):\n",
    "        super(GaussianLoglike, self).__init__()\n",
    "        self.min_sigma = min_sigma\n",
    "\n",
    "    def forward(self, distr_params, groundtruth, mask=None):\n",
    "        distr = normal_parse_params(distr_params, self.min_sigma)\n",
    "        if mask is not None:\n",
    "            log_probs = distr.log_prob(groundtruth) * mask\n",
    "        else:\n",
    "            log_probs = distr.log_prob(groundtruth)\n",
    "        return log_probs.view(groundtruth.shape[0], -1).sum(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfcc971",
   "metadata": {},
   "source": [
    "# Rectified Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7401578",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = math.sqrt(\n",
    "                            (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (\n",
    "                                        N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e954f83",
   "metadata": {},
   "source": [
    "# Base Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46357ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNet(object):\n",
    "    def __init__(self):\n",
    "        cprint('c', '\\nNet:')\n",
    "\n",
    "    def get_nb_parameters(self):\n",
    "        return np.sum(p.numel() for p in self.model.parameters())\n",
    "\n",
    "    def set_mode_train(self, train=True):\n",
    "        if train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "    def update_lr(self, epoch, gamma=0.99):\n",
    "        self.epoch += 1\n",
    "        if self.schedule is not None:\n",
    "            if len(self.schedule) == 0 or epoch in self.schedule:\n",
    "                self.lr *= gamma\n",
    "                print('learning rate: %f  (%d)\\n' % (self.lr, epoch))\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = self.lr\n",
    "\n",
    "    def save(self, filename):\n",
    "        cprint('c', 'Writting %s\\n' % filename)\n",
    "        torch.save({\n",
    "            'epoch': self.epoch,\n",
    "            'lr': self.lr,\n",
    "            'model': self.model,\n",
    "            'optimizer': self.optimizer}, filename)\n",
    "\n",
    "    def load(self, filename):\n",
    "        cprint('c', 'Reading %s\\n' % filename)\n",
    "        state_dict = torch.load(filename)\n",
    "        self.epoch = state_dict['epoch']\n",
    "        self.lr = state_dict['lr']\n",
    "        self.model = state_dict['model']\n",
    "        self.optimizer = state_dict['optimizer']\n",
    "        print('  restoring epoch: %d, lr: %f' % (self.epoch, self.lr))\n",
    "        return self.epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfaacd1",
   "metadata": {},
   "source": [
    "# Sub network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc6654f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEAC_gauss(nn.Module):\n",
    "    def __init__(self, input_dim, width, depth, latent_dim, pred_sig=True):\n",
    "        super(VAEAC_gauss, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.recognition_net = MLP_preact_recognition_net(input_dim, width, depth, latent_dim)\n",
    "        self.prior_net = MLP_preact_prior_net(input_dim, width, depth, latent_dim)\n",
    "        if pred_sig:\n",
    "            self.generator_net = MLP_preact_generator_net(2*input_dim, width, depth, latent_dim)\n",
    "            self.rec_loglike = GaussianLoglike(min_sigma=1e-2)\n",
    "        else:\n",
    "            self.generator_net = MLP_preact_generator_net(input_dim, width, depth, latent_dim)\n",
    "            self.m_rec_loglike = MSELoss(reduction='none')\n",
    "        self.pred_sig = pred_sig\n",
    "        self.sigma_mu = 1e4\n",
    "        self.sigma_sigma = 1e-4\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_mask(x, mask):\n",
    "        \"\"\"Positive bits in mask are set to 0 in x (observed)\"\"\"\n",
    "        observed = x.clone()  # torch.tensor(x)\n",
    "        observed[mask.bool()] = 0\n",
    "        return observed\n",
    "\n",
    "    def recognition_encode(self, x):\n",
    "        approx_post_params = self.recognition_net(x)\n",
    "        approx_post = normal_parse_params(approx_post_params, 1e-3)\n",
    "        return approx_post\n",
    "\n",
    "    def prior_encode(self, x, mask):\n",
    "        x = self.apply_mask(x, mask)\n",
    "        x = torch.cat([x, mask], 1)\n",
    "        prior_params = self.prior_net(x)\n",
    "        prior = normal_parse_params(prior_params, 1e-3)\n",
    "        return prior\n",
    "\n",
    "    def decode(self, z_sample):\n",
    "        rec_params = self.generator_net(z_sample)\n",
    "        return rec_params\n",
    "\n",
    "    def reg_cost(self, prior):\n",
    "        num_objects = prior.mean.shape[0]\n",
    "        mu = prior.mean.view(num_objects, -1)\n",
    "        sigma = prior.scale.view(num_objects, -1)\n",
    "        mu_regularizer = -(mu ** 2).sum(-1) / 2 / (self.sigma_mu ** 2)\n",
    "        sigma_regularizer = (sigma.log() - sigma).sum(-1) * self.sigma_sigma\n",
    "        return mu_regularizer + sigma_regularizer\n",
    "\n",
    "    def vlb(self, prior, approx_post, x, rec_params):\n",
    "        if self.pred_sig:\n",
    "            rec = self.rec_loglike(rec_params, x).view(x.shape[0], -1).sum(-1)\n",
    "        else:\n",
    "            rec = -self.m_rec_loglike(rec_params, x).view(x.shape[0], -1).sum(-1)\n",
    "        prior_regularization = self.reg_cost(prior).view(x.shape[0], -1).sum(-1)\n",
    "        kl = kl_divergence(approx_post, prior).view(x.shape[0], -1).sum(-1)\n",
    "        return rec - kl + prior_regularization\n",
    "\n",
    "    def iwlb(self, prior, approx_post, x, K=50):\n",
    "        estimates = []\n",
    "        for i in range(K):\n",
    "            latent = approx_post.rsample()\n",
    "            rec_params = self.decode(latent)\n",
    "            if self.pred_sig:\n",
    "                rec_loglike = self.rec_loglike(rec_params, x).view(x.shape[0], -1).sum(-1)\n",
    "            else:\n",
    "                rec_loglike = -self.m_rec_loglike(rec_params, x).view(x.shape[0], -1).sum(-1)\n",
    "\n",
    "            prior_log_prob = prior.log_prob(latent)\n",
    "            prior_log_prob = prior_log_prob.view(x.shape[0], -1)\n",
    "            prior_log_prob = prior_log_prob.sum(-1)\n",
    "\n",
    "            proposal_log_prob = approx_post.log_prob(latent)\n",
    "            proposal_log_prob = proposal_log_prob.view(x.shape[0], -1)\n",
    "            proposal_log_prob = proposal_log_prob.sum(-1)\n",
    "\n",
    "            estimate = rec_loglike + prior_log_prob - proposal_log_prob\n",
    "            estimates.append(estimate[:, None])\n",
    "\n",
    "        return torch.logsumexp(torch.cat(estimates, 1), 1) - np.log(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f89180",
   "metadata": {},
   "source": [
    "# Main Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bb03688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Network\n",
    "class VAEAC_gauss_net(BaseNet):\n",
    "    def __init__(self, input_dim, width, depth, latent_dim, pred_sig=True, lr=1e-3, cuda=True):\n",
    "        super(VAEAC_gauss_net, self).__init__()\n",
    "        cprint('y', 'VAE_gauss_net')\n",
    "\n",
    "        self.cuda = cuda\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.latent_dim = latent_dim\n",
    "        self.lr = lr\n",
    "        self.pred_sig = pred_sig\n",
    "\n",
    "        self.create_net()\n",
    "        self.create_opt()\n",
    "        self.epoch = 0\n",
    "        self.schedule = None\n",
    "\n",
    "        self.vlb_scale = 1 / input_dim  # scale for dimensions of input so we can use same LR always\n",
    "\n",
    "    def create_net(self):\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed(42)\n",
    "        self.model = VAEAC_gauss(self.input_dim, self.width, self.depth, self.latent_dim, self.pred_sig)\n",
    "        if self.cuda:\n",
    "            self.model = self.model.cuda()\n",
    "            cudnn.benchmark = True\n",
    "        print('    Total params: %.2fM' % (self.get_nb_parameters() / 1000000.0))\n",
    "\n",
    "    def create_opt(self):\n",
    "        self.optimizer = RAdam(self.model.parameters(), lr=self.lr) # torch.optim.Adam\n",
    "\n",
    "    def fit(self, x, mask):\n",
    "        self.set_mode_train(train=True)\n",
    "\n",
    "        x, mask = to_variable(var=(x, mask), cuda=self.cuda)\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        prior = self.model.prior_encode(x, mask)\n",
    "        approx_post = self.model.recognition_encode(x)\n",
    "        z_sample = approx_post.rsample()\n",
    "        rec_params = self.model.decode(z_sample)\n",
    "\n",
    "        vlb = self.model.vlb(prior, approx_post, x, rec_params)\n",
    "        loss = (- vlb * self.vlb_scale).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.pred_sig:\n",
    "            rec_return = normal_parse_params(rec_params, 1e-3)\n",
    "        else:\n",
    "            rec_return = rec_params\n",
    "        return vlb.mean().item(), rec_return\n",
    "\n",
    "    def eval(self, x, mask, sample=False):\n",
    "        self.set_mode_train(train=False)\n",
    "\n",
    "        x, mask = to_variable(var=(x, mask), cuda=self.cuda)\n",
    "        prior = self.model.prior_encode(x, mask)\n",
    "\n",
    "        approx_post = self.model.recognition_encode(x)\n",
    "        if sample:\n",
    "            z_sample = approx_post.sample()\n",
    "        else:\n",
    "            z_sample = approx_post.loc\n",
    "        rec_params = self.model.decode(z_sample)\n",
    "\n",
    "        vlb = self.model.vlb(prior, approx_post, x, rec_params)\n",
    "\n",
    "        if self.pred_sig:\n",
    "            rec_return = normal_parse_params(rec_params, 1e-3)\n",
    "        else:\n",
    "            rec_return = rec_params\n",
    "        return vlb.mean().item(), rec_return\n",
    "\n",
    "    def eval_iw(self, x, mask, k=50):\n",
    "        self.set_mode_train(train=False)\n",
    "        x, mask = to_variable(var=(x, mask), cuda=self.cuda)\n",
    "\n",
    "        prior = self.model.prior_encode(x, mask)\n",
    "        approx_post = self.model.recognition_encode(x)\n",
    "\n",
    "        iw_lb = self.model.iwlb(prior, approx_post, x, k)\n",
    "        return iw_lb.mean().item()\n",
    "\n",
    "    def get_prior(self, x, mask):\n",
    "        self.set_mode_train(train=False)\n",
    "        x, mask = to_variable(var=(x, mask), cuda=self.cuda)\n",
    "        prior = self.model.prior_encode(x, mask)\n",
    "        return prior\n",
    "\n",
    "    def get_post(self, x):\n",
    "        self.set_mode_train(train=False)\n",
    "        x, = to_variable(var=(x,), cuda=self.cuda)\n",
    "        approx_post = self.model.recognition_encode(x)\n",
    "        return approx_post\n",
    "\n",
    "    def inpaint(self, x, mask, Nsample=1, z_mean=False):\n",
    "        self.set_mode_train(train=False)\n",
    "        x, mask = to_variable(var=(x, mask), cuda=self.cuda)\n",
    "        prior = self.model.prior_encode(x, mask)\n",
    "        out = []\n",
    "        for i in range(Nsample):\n",
    "            if z_mean:\n",
    "                z_sample = prior.loc.data\n",
    "            else:\n",
    "                z_sample = prior.sample()\n",
    "            rec_params = self.model.decode(z_sample)\n",
    "            out.append(rec_params.data)\n",
    "        out = torch.stack(out, dim=0)\n",
    "\n",
    "        if self.pred_sig:\n",
    "            return [normal_parse_params(out[i], 1e-2) for i in range(Nsample)]\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    def regenerate(self, z, grad=False):\n",
    "        self.set_mode_train(train=False)\n",
    "        if grad:\n",
    "            if not z.requires_grad:\n",
    "                z.requires_grad = True\n",
    "        else:\n",
    "            z, = to_variable(var=(z,), volatile=True, cuda=self.cuda)\n",
    "        out = self.model.decode(z)\n",
    "        if self.pred_sig:\n",
    "            return normal_parse_params(out, 1e-2)\n",
    "        else:\n",
    "            return out.data\n",
    "class VAEAC_gauss_cat(nn.Module):\n",
    "    def __init__(self, input_dim_vec, width, depth, latent_dim, pred_sig=True):\n",
    "        super(VAEAC_gauss_cat, self).__init__()\n",
    "\n",
    "        self.input_dim = 0\n",
    "        self.input_dim_vec = input_dim_vec\n",
    "        for e in self.input_dim_vec:\n",
    "            self.input_dim += e\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.recognition_net = MLP_preact_recognition_net(self.input_dim, width, depth, latent_dim)\n",
    "        self.prior_net = MLP_preact_prior_net(self.input_dim, width, depth, latent_dim)\n",
    "        if pred_sig:\n",
    "            raise Exception('Not yet implemented')\n",
    "        else:\n",
    "            self.generator_net = MLP_preact_generator_net(self.input_dim, width, depth, latent_dim)\n",
    "            self.rec_loglike = rms_cat_loglike(self.input_dim_vec, reduction='none')\n",
    "        self.pred_sig = pred_sig\n",
    "        self.sigma_mu = 1e4\n",
    "        self.sigma_sigma = 1e-4\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_mask(x, mask):\n",
    "        \"\"\"Positive bits in mask are set to 0 in x (observed)\"\"\"\n",
    "        observed = x.clone()  # torch.tensor(x)\n",
    "        observed[mask.bool()] = 0\n",
    "        return observed\n",
    "\n",
    "    def recognition_encode(self, x):\n",
    "        \"\"\"Works with flattened representATION\"\"\"\n",
    "        approx_post_params = self.recognition_net(x)\n",
    "        approx_post = normal_parse_params(approx_post_params, 1e-3)\n",
    "        return approx_post\n",
    "\n",
    "    def prior_encode(self, x, mask):\n",
    "        \"\"\"Works with flattened representATION\"\"\"\n",
    "        x = self.apply_mask(x, mask)\n",
    "        x = torch.cat([x, mask], 1)\n",
    "        prior_params = self.prior_net(x)\n",
    "        prior = normal_parse_params(prior_params, 1e-3)\n",
    "        return prior\n",
    "\n",
    "    def decode(self, z_sample):\n",
    "        rec_params = self.generator_net(z_sample)\n",
    "        return rec_params\n",
    "\n",
    "    def reg_cost(self, prior):\n",
    "        num_objects = prior.mean.shape[0]\n",
    "        mu = prior.mean.view(num_objects, -1)\n",
    "        sigma = prior.scale.view(num_objects, -1)\n",
    "        mu_regularizer = -(mu ** 2).sum(-1) / 2 / (self.sigma_mu ** 2)\n",
    "        sigma_regularizer = (sigma.log() - sigma).sum(-1) * self.sigma_sigma\n",
    "        return mu_regularizer + sigma_regularizer\n",
    "\n",
    "    def vlb(self, prior, approx_post, x, rec_params):\n",
    "        if self.pred_sig:\n",
    "            raise Exception('Not yet imeplemented')\n",
    "        else:\n",
    "            rec = self.rec_loglike(rec_params, x).view(x.shape[0], -1).sum(-1)\n",
    "        prior_regularization = self.reg_cost(prior).view(x.shape[0], -1).sum(-1)\n",
    "        kl = kl_divergence(approx_post, prior).view(x.shape[0], -1).sum(-1)\n",
    "        return rec - kl + prior_regularization\n",
    "\n",
    "    def iwlb(self, prior, approx_post, x, K=50):\n",
    "        estimates = []\n",
    "        for i in range(K):\n",
    "            latent = approx_post.rsample()\n",
    "            rec_params = self.decode(latent)\n",
    "            if self.pred_sig:\n",
    "                raise Exception('Not yet imeplemented')\n",
    "            else:\n",
    "                rec_loglike = self.rec_loglike(rec_params, x).view(x.shape[0], -1).sum(-1)\n",
    "\n",
    "            prior_log_prob = prior.log_prob(latent)\n",
    "            prior_log_prob = prior_log_prob.view(x.shape[0], -1)\n",
    "            prior_log_prob = prior_log_prob.sum(-1)\n",
    "\n",
    "            proposal_log_prob = approx_post.log_prob(latent)\n",
    "            proposal_log_prob = proposal_log_prob.view(x.shape[0], -1)\n",
    "            proposal_log_prob = proposal_log_prob.sum(-1)\n",
    "\n",
    "            estimate = rec_loglike + prior_log_prob - proposal_log_prob\n",
    "            estimates.append(estimate[:, None])\n",
    "\n",
    "        return torch.logsumexp(torch.cat(estimates, 1), 1) - np.log(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b75a7732",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class VAEAC_gauss_cat_net(BaseNet):\n",
    "    def __init__(self, input_dim_vec, width, depth, latent_dim, pred_sig=False, lr=1e-3, cuda=True, flatten=True):\n",
    "        super(VAEAC_gauss_cat_net, self).__init__()\n",
    "        cprint('y', 'VAE_gauss_net')\n",
    "\n",
    "        self.cuda = cuda\n",
    "\n",
    "        self.input_dim = 0\n",
    "        self.input_dim_vec = input_dim_vec\n",
    "        for e in self.input_dim_vec:\n",
    "            self.input_dim += e\n",
    "        self.flatten = flatten\n",
    "        if not self.flatten:\n",
    "            pass\n",
    "            # raise Exception('Error calculation not supported without flattening')\n",
    "\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.latent_dim = latent_dim\n",
    "        self.lr = lr\n",
    "        self.pred_sig = pred_sig\n",
    "\n",
    "        self.create_net()\n",
    "        self.create_opt()\n",
    "        self.epoch = 0\n",
    "        self.schedule = None\n",
    "\n",
    "        self.vlb_scale = 1 / len(self.input_dim_vec)  # scale for dimensions of input so we can use same LR always\n",
    "\n",
    "    def create_net(self):\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed(42)\n",
    "        self.model = VAEAC_gauss_cat(self.input_dim_vec, self.width, self.depth, self.latent_dim, self.pred_sig)\n",
    "        if self.cuda:\n",
    "            self.model = self.model.cuda()\n",
    "            cudnn.benchmark = True\n",
    "        print('    Total params: %.2fM' % (self.get_nb_parameters() / 1000000.0))\n",
    "\n",
    "    def create_opt(self):\n",
    "        self.optimizer = RAdam(self.model.parameters(), lr=self.lr)  # torch.optim.Adam\n",
    "\n",
    "    def fit(self, x, mask):\n",
    "        self.set_mode_train(train=True)\n",
    "        \n",
    "        #print(\"bef x: \", x.shape)\n",
    "        if self.flatten:\n",
    "            mask = gauss_cat_to_flat_mask(mask, self.input_dim_vec)\n",
    "            x_flat = gauss_cat_to_flat(x, self.input_dim_vec)\n",
    "        \n",
    "        else:\n",
    "            x_flat = x # X already flattened\n",
    "            x = flat_to_gauss_cat(x, self.input_dim_vec) # unflattened x (used for loss computation)\n",
    "        #print(\"aft x: \", x.shape)\n",
    "        x_flat, x, mask = to_variable(var=(x_flat, x, mask), cuda=self.cuda)\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        prior = self.model.prior_encode(x_flat, mask)\n",
    "        approx_post = self.model.recognition_encode(x_flat)\n",
    "        z_sample = approx_post.rsample()\n",
    "        rec_params = self.model.decode(z_sample)\n",
    "\n",
    "        vlb = self.model.vlb(prior, approx_post, x, rec_params)\n",
    "        loss = (- vlb * self.vlb_scale).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.pred_sig:\n",
    "            rec_return = normal_parse_params(rec_params, 1e-3)\n",
    "        else:\n",
    "            rec_return = rec_params\n",
    "        return vlb.mean().item(), rec_return\n",
    "\n",
    "    def eval(self, x, mask, sample=False):\n",
    "        self.set_mode_train(train=False)\n",
    "\n",
    "        if self.flatten:\n",
    "            mask = gauss_cat_to_flat_mask(mask, self.input_dim_vec)\n",
    "            x_flat = gauss_cat_to_flat(x, self.input_dim_vec)\n",
    "        else:\n",
    "            x_flat = x\n",
    "            x = flat_to_gauss_cat(x, self.input_dim_vec)\n",
    "\n",
    "        x_flat, x, mask = to_variable(var=(x_flat, x, mask), cuda=self.cuda)\n",
    "        prior = self.model.prior_encode(x_flat, mask)\n",
    "\n",
    "        approx_post = self.model.recognition_encode(x_flat)\n",
    "        if sample:\n",
    "            z_sample = approx_post.sample()\n",
    "        else:\n",
    "            z_sample = approx_post.loc\n",
    "        rec_params = self.model.decode(z_sample)\n",
    "\n",
    "        vlb = self.model.vlb(prior, approx_post, x, rec_params)\n",
    "\n",
    "        if self.pred_sig:\n",
    "            rec_return = normal_parse_params(rec_params, 1e-3)\n",
    "        else:\n",
    "            rec_return = rec_params\n",
    "        return vlb.mean().item(), rec_return\n",
    "\n",
    "    def eval_iw(self, x, mask, k=50):\n",
    "        self.set_mode_train(train=False)\n",
    "\n",
    "        if self.flatten:\n",
    "            mask = gauss_cat_to_flat_mask(mask, self.input_dim_vec)\n",
    "            x_flat = gauss_cat_to_flat(x, self.input_dim_vec)\n",
    "        else:\n",
    "            x_flat = x\n",
    "            x = flat_to_gauss_cat(x, self.input_dim_vec)\n",
    "\n",
    "\n",
    "        x_flat, x, mask = to_variable(var=(x_flat, x, mask), cuda=self.cuda)\n",
    "\n",
    "        prior = self.model.prior_encode(x, mask)\n",
    "        approx_post = self.model.recognition_encode(x_flat)\n",
    "\n",
    "        iw_lb = self.model.iwlb(prior, approx_post, x, k)\n",
    "        return iw_lb.mean().item()\n",
    "\n",
    "    def get_prior(self, x, mask, flatten=None):\n",
    "        self.set_mode_train(train=False)\n",
    "        if flatten is None:\n",
    "            flatten = self.flatten\n",
    "        if flatten:\n",
    "            mask = gauss_cat_to_flat_mask(mask, self.input_dim_vec)\n",
    "            x = gauss_cat_to_flat(x, self.input_dim_vec)\n",
    "\n",
    "        x, mask = to_variable(var=(x, mask), cuda=self.cuda)\n",
    "        prior = self.model.prior_encode(x, mask)\n",
    "        return prior\n",
    "\n",
    "    def get_post(self, x, flatten=None):\n",
    "        self.set_mode_train(train=False)\n",
    "        if flatten is None:\n",
    "            flatten = self.flatten\n",
    "        if flatten:\n",
    "            x = gauss_cat_to_flat(x, self.input_dim_vec)\n",
    "\n",
    "        x, = to_variable(var=(x,), cuda=self.cuda)\n",
    "        approx_post = self.model.recognition_encode(x)\n",
    "        return approx_post\n",
    "\n",
    "    def inpaint(self, x, mask, Nsample=1, z_mean=False, flatten=False, unflatten=False, cat_probs=False):\n",
    "        self.set_mode_train(train=False)\n",
    "\n",
    "        if flatten:\n",
    "            mask = gauss_cat_to_flat_mask(mask, self.input_dim_vec)\n",
    "            x = gauss_cat_to_flat(x, self.input_dim_vec)\n",
    "\n",
    "        x, mask = to_variable(var=(x, mask), cuda=self.cuda)\n",
    "        prior = self.model.prior_encode(x, mask)\n",
    "        out = []\n",
    "        for i in range(Nsample):\n",
    "            if z_mean:\n",
    "                z_sample = prior.loc.data\n",
    "            else:\n",
    "                z_sample = prior.sample()\n",
    "            rec_params = self.model.decode(z_sample)\n",
    "            out.append(rec_params.data)\n",
    "        out = torch.stack(out, dim=0)\n",
    "\n",
    "        if self.pred_sig:\n",
    "            raise Exception('Not yet implemented')\n",
    "        else:\n",
    "            dim0 = out.shape[0]\n",
    "            dim1 = out.shape[1]\n",
    "            out = out.view(dim0*dim1, -1)\n",
    "            if unflatten:\n",
    "                out = flat_to_gauss_cat(out, self.input_dim_vec)\n",
    "            else:\n",
    "                out = selective_softmax(out, self.input_dim_vec, grad=False, cat_probs=cat_probs)\n",
    "            out = out.view(dim0, dim1, -1)\n",
    "            return out.data\n",
    "\n",
    "    def regenerate(self, z, grad=False, unflatten=False):\n",
    "        self.set_mode_train(train=False)\n",
    "        if unflatten and grad:\n",
    "            raise Exception('flatten and grad options are not compatible')\n",
    "        if grad:\n",
    "            if not z.requires_grad:\n",
    "                z.requires_grad = True\n",
    "        else:\n",
    "            z, = to_variable(var=(z,), volatile=True, cuda=self.cuda)\n",
    "        out = self.model.decode(z)\n",
    "        if self.pred_sig:\n",
    "            raise Exception('Not yet implemented')\n",
    "        else:\n",
    "            if unflatten:\n",
    "                out = flat_to_gauss_cat(out, self.input_dim_vec)\n",
    "            else:\n",
    "                out = selective_softmax(out, self.input_dim_vec, grad=grad)\n",
    "            return out.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b166b7",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0642f319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_VAEAC(net, masker, name, batch_size, nb_epochs, trainset, valset, cuda,\n",
    "                flat_ims=False, train_plot=False, Nclass=None, early_stop=None, script_mode=False):\n",
    "\n",
    "    models_dir = name + '_models'\n",
    "    results_dir = name + '_results'\n",
    "    mkdir(models_dir)\n",
    "    mkdir(results_dir)\n",
    "\n",
    "    if cuda:\n",
    "        trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True,\n",
    "                                                  num_workers=0)\n",
    "        valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True,\n",
    "                                                num_workers=0)\n",
    "\n",
    "    else:\n",
    "        trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n",
    "                                                  num_workers=0)\n",
    "        valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                                num_workers=0)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# net dims\n",
    "    cprint('c', '\\nNetwork:')\n",
    "\n",
    "    epoch = 0\n",
    "\n",
    "    ## ---------------------------------------------------------------------------------------------------------------------\n",
    "    # train\n",
    "    cprint('c', '\\nTrain:')\n",
    "\n",
    "    print('  init cost variables:')\n",
    "    vlb_train = np.zeros(nb_epochs)\n",
    "    vlb_dev = np.zeros(nb_epochs)\n",
    "    iwlb_dev = np.zeros(nb_epochs)\n",
    "    best_vlb = -np.inf\n",
    "    best_epoch = 0\n",
    "\n",
    "    nb_its_dev = 1\n",
    "    \n",
    "    tic0 = time.time()\n",
    "    for i in range(epoch, nb_epochs):\n",
    "        net.set_mode_train(True)\n",
    "\n",
    "        tic = time.time()\n",
    "        nb_samples = 0\n",
    "        for x, y in trainloader:\n",
    "\n",
    "            if flat_ims:\n",
    "                x = x.view(x.shape[0], -1)\n",
    "            if Nclass is not None:\n",
    "                y_oh = torch_onehot(y, Nclass).type(x.type())\n",
    "                x = torch.cat([x, y_oh], 1)\n",
    "\n",
    "            mask = masker(x)\n",
    "            cost, _ = net.fit(x, mask)\n",
    "\n",
    "            vlb_train[i] += cost * len(x)\n",
    "            nb_samples += len(x)\n",
    "\n",
    "        vlb_train[i] /= nb_samples\n",
    "\n",
    "        toc = time.time()\n",
    "\n",
    "        # ---- print\n",
    "        print(\"it %d/%d, vlb %f, \" % (i, nb_epochs, vlb_train[i]), end=\"\")\n",
    "        cprint('r', '   time: %f seconds\\n' % (toc - tic))\n",
    "        net.update_lr(i)\n",
    "\n",
    "        # ---- dev\n",
    "        if i % nb_its_dev == 0:\n",
    "            nb_samples = 0\n",
    "            for j, (x, y) in enumerate(valloader):\n",
    "\n",
    "                if flat_ims:\n",
    "                    x = x.view(x.shape[0], -1)\n",
    "                if Nclass is not None:\n",
    "                    y_oh = torch_onehot(y, Nclass).type(x.type())\n",
    "                    x = torch.cat([x, y_oh], 1)\n",
    "\n",
    "                mask = masker(x)\n",
    "                cost, rec_mean = net.eval(x, mask)\n",
    "                # iwlb = net.eval_iw(x, mask, 25)\n",
    "\n",
    "                vlb_dev[i] += cost * len(x)\n",
    "                # iwlb_dev[i] += iwlb\n",
    "                nb_samples += len(x)\n",
    "\n",
    "            vlb_dev[i] /= nb_samples\n",
    "            # iwlb_dev[i] /= nb_samples\n",
    "\n",
    "            cprint('g', '    vlb %f (%f)\\n' % (vlb_dev[i], best_vlb))\n",
    "\n",
    "            if train_plot:\n",
    "                xm = net.model.apply_mask(x, mask)\n",
    "                \n",
    "                xr = x.cpu()\n",
    "                rec_inpaint = net.inpaint(xm, mask)\n",
    "                try:\n",
    "                    o = rec_mean.cpu()\n",
    "                    rec_inpaint = rec_inpaint[0].cpu()\n",
    "                except:\n",
    "                    o = rec_mean.loc.cpu()\n",
    "                    rec_inpaint = rec_inpaint[0].loc.cpu()\n",
    "\n",
    "                if Nclass is not None:\n",
    "                    xm = xm[:, :-Nclass]\n",
    "                    rec_inpaint = rec_inpaint[:, :-Nclass]\n",
    "                    xr = xr[:, :-Nclass]\n",
    "                    o = o[:, :-Nclass]\n",
    "\n",
    "                if len(x.shape) == 2:\n",
    "                    side = int(np.sqrt(xm.shape[1]))\n",
    "                    xm = xm.view(-1, 1, side, side).data\n",
    "                    rec_inpaint = rec_inpaint.view(-1, 1, side, side).data\n",
    "                    xr = xr.view(-1, 1, side, side).data\n",
    "                    o = o.view(-1, 1, side, side).data\n",
    "\n",
    "                import matplotlib.pyplot as plt\n",
    "                plt.figure()\n",
    "                dd = make_grid(torch.cat([xr[:10], o[:10]]), nrow=10).numpy()\n",
    "                plt.imshow(np.transpose(dd, (1, 2, 0)), interpolation='nearest')\n",
    "                plt.title('reconstruct')\n",
    "                if script_mode:\n",
    "                    plt.savefig(results_dir + '/rec%d.png' % i)\n",
    "                else:\n",
    "                    plt.show()\n",
    "\n",
    "                import matplotlib.pyplot as plt\n",
    "                plt.figure()\n",
    "                dd = make_grid(torch.cat([xm[:10], rec_inpaint[:10]]), nrow=10).numpy()\n",
    "                plt.imshow(np.transpose(dd, (1, 2, 0)), interpolation='nearest')\n",
    "                plt.title('inpaint')\n",
    "                if script_mode:\n",
    "                    plt.savefig(results_dir + '/inp%d.png' % i)\n",
    "                else:\n",
    "                    plt.show()\n",
    "\n",
    "        if vlb_dev[i] > best_vlb:\n",
    "            best_vlb = vlb_dev[i]\n",
    "            best_epoch = i\n",
    "            net.save(models_dir + '/theta_best.dat')\n",
    "\n",
    "        if early_stop is not None and (i - best_epoch) > early_stop:\n",
    "            break\n",
    "\n",
    "   \n",
    "    net.save(models_dir + '/theta_last.dat')\n",
    "    toc0 = time.time()\n",
    "    runtime_per_it = (toc0 - tic0) / float(nb_epochs)\n",
    "    cprint('r', '   average time: %f seconds\\n' % runtime_per_it)\n",
    "\n",
    "    ## ---------------------------------------------------------------------------------------------------------------------\n",
    "    # results\n",
    "    \"\"\"\n",
    "    cprint('c', '\\nRESULTS:')\n",
    "    nb_parameters = net.get_nb_parameters()\n",
    "    best_cost_dev = np.max(vlb_dev)\n",
    "    # best_iw_dev = np.max(iwlb_dev)\n",
    "    best_cost_train = np.max(vlb_train)\n",
    "\n",
    "    print('  best_vlb_dev: %f' % best_cost_dev)\n",
    "    # print('  best_iwlb_dev: %f' % best_iw_dev)\n",
    "    print('  best_vlb_train: %f' % best_cost_train)\n",
    "    print('  nb_parameters: %d (%s)\\n' % (nb_parameters, humansize(nb_parameters)))\n",
    "\n",
    "    ## ---------------------------------------------------------------------------------------------------------------------\n",
    "    # fig cost vs its\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(np.clip(vlb_train, -1000, 1000), 'r')\n",
    "    plt.plot(np.clip(vlb_dev[::nb_its_dev], -1000, 1000), 'b')\n",
    "    plt.legend(['cost_train', 'cost_dev'])\n",
    "    plt.ylabel('vlb')\n",
    "    plt.xlabel('it')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(results_dir+'/train_cost.png')\n",
    "\n",
    "    # plt.figure()\n",
    "    # plt.plot(np.clip(iwlb_dev[::nb_its_dev], -1000, 1000), 'b')\n",
    "    # plt.ylabel('dev iwlb')\n",
    "    # plt.xlabel('it')\n",
    "    # plt.grid(True)\n",
    "    # plt.savefig(results_dir + '/train_iwlb.png')\n",
    "    if train_plot:\n",
    "        plt.show()\n",
    "    \"\"\"\n",
    "    return vlb_train, vlb_dev, best_epoch, best_vlb, i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2046f0e6",
   "metadata": {},
   "source": [
    "## COMPAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b188c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compas (5554, 19) (618, 19)\n",
      "[3 6 2 2 2 1 1 2]\n",
      "compas\n",
      "\u001b[36m\n",
      "Net:\u001b[0m\n",
      "\u001b[33mVAE_gauss_net\u001b[0m\n",
      "    Total params: 0.78M\n",
      "\u001b[36m\n",
      "Network:\u001b[0m\n",
      "\u001b[36m\n",
      "Train:\u001b[0m\n",
      "  init cost variables:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mortimer\\AppData\\Local\\Temp/ipykernel_22248/1070947545.py:6: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return np.sum(p.numel() for p in self.model.parameters())\n",
      "C:\\Users\\mortimer\\AppData\\Local\\Temp/ipykernel_22248/550312925.py:50: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1050.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 0/2000, vlb -18.386189, \u001b[31m   time: 2.919683 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -10.611259 (-inf)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 1/2000, vlb -12.882605, \u001b[31m   time: 0.745697 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -11.213885 (-10.611259)\n",
      "\u001b[0m\n",
      "it 2/2000, vlb -10.996987, \u001b[31m   time: 0.699239 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -10.212140 (-10.611259)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 3/2000, vlb -10.062862, \u001b[31m   time: 0.744037 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -9.367107 (-10.212140)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 4/2000, vlb -9.372795, \u001b[31m   time: 0.708894 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -8.290073 (-9.367107)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 5/2000, vlb -8.790879, \u001b[31m   time: 0.698157 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -7.669571 (-8.290073)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 6/2000, vlb -8.312908, \u001b[31m   time: 0.890160 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -7.368272 (-7.669571)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 7/2000, vlb -7.961177, \u001b[31m   time: 0.800033 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -7.078975 (-7.368272)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 8/2000, vlb -7.713377, \u001b[31m   time: 0.761950 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.316738 (-7.078975)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 9/2000, vlb -7.369177, \u001b[31m   time: 0.753014 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.005692 (-6.316738)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 10/2000, vlb -7.125621, \u001b[31m   time: 0.736397 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -5.809776 (-6.005692)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 11/2000, vlb -6.880792, \u001b[31m   time: 0.724877 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -5.704949 (-5.809776)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 12/2000, vlb -6.665609, \u001b[31m   time: 0.815363 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -5.278133 (-5.704949)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 13/2000, vlb -6.497762, \u001b[31m   time: 0.695011 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -5.111536 (-5.278133)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 14/2000, vlb -6.371811, \u001b[31m   time: 0.702404 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.962671 (-5.111536)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 15/2000, vlb -6.193625, \u001b[31m   time: 0.732785 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.940750 (-4.962671)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 16/2000, vlb -5.992591, \u001b[31m   time: 0.784428 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.766333 (-4.940750)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 17/2000, vlb -5.897525, \u001b[31m   time: 0.703357 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.656284 (-4.766333)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 18/2000, vlb -5.805537, \u001b[31m   time: 0.696161 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.499444 (-4.656284)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 19/2000, vlb -5.698241, \u001b[31m   time: 0.696007 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.453269 (-4.499444)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 20/2000, vlb -5.623831, \u001b[31m   time: 0.726495 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.336713 (-4.453269)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 21/2000, vlb -5.508428, \u001b[31m   time: 0.763048 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.249264 (-4.336713)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 22/2000, vlb -5.397835, \u001b[31m   time: 0.830326 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.297232 (-4.249264)\n",
      "\u001b[0m\n",
      "it 23/2000, vlb -5.308727, \u001b[31m   time: 0.724762 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.122668 (-4.249264)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 24/2000, vlb -5.178244, \u001b[31m   time: 0.738052 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.138687 (-4.122668)\n",
      "\u001b[0m\n",
      "it 25/2000, vlb -5.079787, \u001b[31m   time: 0.778451 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.857971 (-4.122668)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 26/2000, vlb -5.037640, \u001b[31m   time: 0.698158 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -4.054466 (-3.857971)\n",
      "\u001b[0m\n",
      "it 27/2000, vlb -5.013553, \u001b[31m   time: 0.744921 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.860186 (-3.857971)\n",
      "\u001b[0m\n",
      "it 28/2000, vlb -4.936213, \u001b[31m   time: 0.685209 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.844999 (-3.857971)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 29/2000, vlb -4.899140, \u001b[31m   time: 0.686290 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.736672 (-3.844999)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 30/2000, vlb -4.842682, \u001b[31m   time: 0.695431 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.684936 (-3.736672)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 31/2000, vlb -4.780688, \u001b[31m   time: 0.683202 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.670363 (-3.684936)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 32/2000, vlb -4.678962, \u001b[31m   time: 0.684687 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.622099 (-3.670363)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 33/2000, vlb -4.692232, \u001b[31m   time: 0.706630 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.615932 (-3.622099)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 34/2000, vlb -4.595277, \u001b[31m   time: 0.718776 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.610565 (-3.615932)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 35/2000, vlb -4.588123, \u001b[31m   time: 0.737285 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.516787 (-3.610565)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 36/2000, vlb -4.570967, \u001b[31m   time: 0.744505 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.652461 (-3.516787)\n",
      "\u001b[0m\n",
      "it 37/2000, vlb -4.492027, \u001b[31m   time: 0.691673 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.410728 (-3.516787)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 38/2000, vlb -4.471122, \u001b[31m   time: 0.699142 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.375048 (-3.410728)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 39/2000, vlb -4.410701, \u001b[31m   time: 0.710092 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.344375 (-3.375048)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 40/2000, vlb -4.378808, \u001b[31m   time: 0.696335 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.326165 (-3.344375)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 41/2000, vlb -4.362676, \u001b[31m   time: 0.685428 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.286866 (-3.326165)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 42/2000, vlb -4.303691, \u001b[31m   time: 0.684678 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.287227 (-3.286866)\n",
      "\u001b[0m\n",
      "it 43/2000, vlb -4.295746, \u001b[31m   time: 0.689304 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.208240 (-3.286866)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 44/2000, vlb -4.300197, \u001b[31m   time: 0.681917 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.175062 (-3.208240)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 45/2000, vlb -4.207724, \u001b[31m   time: 0.678133 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.199848 (-3.175062)\n",
      "\u001b[0m\n",
      "it 46/2000, vlb -4.179704, \u001b[31m   time: 0.686687 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.129444 (-3.175062)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 47/2000, vlb -4.217487, \u001b[31m   time: 0.692065 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.040395 (-3.129444)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 48/2000, vlb -4.125743, \u001b[31m   time: 0.716112 seconds\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m    vlb -3.106750 (-3.040395)\n",
      "\u001b[0m\n",
      "it 49/2000, vlb -4.171590, \u001b[31m   time: 0.678968 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.166676 (-3.040395)\n",
      "\u001b[0m\n",
      "it 50/2000, vlb -4.073566, \u001b[31m   time: 0.689266 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.007398 (-3.040395)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 51/2000, vlb -4.092963, \u001b[31m   time: 0.741138 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.000216 (-3.007398)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 52/2000, vlb -4.100424, \u001b[31m   time: 0.702148 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.114018 (-3.000216)\n",
      "\u001b[0m\n",
      "it 53/2000, vlb -4.014147, \u001b[31m   time: 0.700155 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.038027 (-3.000216)\n",
      "\u001b[0m\n",
      "it 54/2000, vlb -3.990794, \u001b[31m   time: 0.691779 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.871552 (-3.000216)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 55/2000, vlb -4.041033, \u001b[31m   time: 0.744155 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.084946 (-2.871552)\n",
      "\u001b[0m\n",
      "it 56/2000, vlb -3.991604, \u001b[31m   time: 0.696799 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.775376 (-2.871552)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 57/2000, vlb -3.974439, \u001b[31m   time: 0.696702 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.949463 (-2.775376)\n",
      "\u001b[0m\n",
      "it 58/2000, vlb -3.962918, \u001b[31m   time: 0.698232 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -3.059560 (-2.775376)\n",
      "\u001b[0m\n",
      "it 59/2000, vlb -3.938503, \u001b[31m   time: 0.696037 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.966804 (-2.775376)\n",
      "\u001b[0m\n",
      "it 60/2000, vlb -3.888219, \u001b[31m   time: 0.700364 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.831682 (-2.775376)\n",
      "\u001b[0m\n",
      "it 61/2000, vlb -3.865113, \u001b[31m   time: 0.711051 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.785720 (-2.775376)\n",
      "\u001b[0m\n",
      "it 62/2000, vlb -3.883492, \u001b[31m   time: 0.712838 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.797597 (-2.775376)\n",
      "\u001b[0m\n",
      "it 63/2000, vlb -3.910164, \u001b[31m   time: 0.749028 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.858030 (-2.775376)\n",
      "\u001b[0m\n",
      "it 64/2000, vlb -3.804244, \u001b[31m   time: 0.769514 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.721553 (-2.775376)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 65/2000, vlb -3.836740, \u001b[31m   time: 0.712309 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.745134 (-2.721553)\n",
      "\u001b[0m\n",
      "it 66/2000, vlb -3.830578, \u001b[31m   time: 0.710352 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.885604 (-2.721553)\n",
      "\u001b[0m\n",
      "it 67/2000, vlb -3.812892, \u001b[31m   time: 0.709626 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.866673 (-2.721553)\n",
      "\u001b[0m\n",
      "it 68/2000, vlb -3.783762, \u001b[31m   time: 0.711133 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.819752 (-2.721553)\n",
      "\u001b[0m\n",
      "it 69/2000, vlb -3.813649, \u001b[31m   time: 0.701009 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.787646 (-2.721553)\n",
      "\u001b[0m\n",
      "it 70/2000, vlb -3.722793, \u001b[31m   time: 0.730079 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.696993 (-2.721553)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 71/2000, vlb -3.739492, \u001b[31m   time: 0.692962 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.862550 (-2.696993)\n",
      "\u001b[0m\n",
      "it 72/2000, vlb -3.730213, \u001b[31m   time: 0.685190 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.720135 (-2.696993)\n",
      "\u001b[0m\n",
      "it 73/2000, vlb -3.747218, \u001b[31m   time: 0.685750 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.534539 (-2.696993)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 74/2000, vlb -3.712130, \u001b[31m   time: 0.692485 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.786182 (-2.534539)\n",
      "\u001b[0m\n",
      "it 75/2000, vlb -3.678096, \u001b[31m   time: 0.711221 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.696300 (-2.534539)\n",
      "\u001b[0m\n",
      "it 76/2000, vlb -3.707709, \u001b[31m   time: 0.699157 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.616877 (-2.534539)\n",
      "\u001b[0m\n",
      "it 77/2000, vlb -3.706352, \u001b[31m   time: 0.682167 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.677322 (-2.534539)\n",
      "\u001b[0m\n",
      "it 78/2000, vlb -3.691995, \u001b[31m   time: 0.698679 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.786619 (-2.534539)\n",
      "\u001b[0m\n",
      "it 79/2000, vlb -3.689209, \u001b[31m   time: 0.698992 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.726159 (-2.534539)\n",
      "\u001b[0m\n",
      "it 80/2000, vlb -3.618232, \u001b[31m   time: 0.697176 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.649192 (-2.534539)\n",
      "\u001b[0m\n",
      "it 81/2000, vlb -3.601482, \u001b[31m   time: 0.686682 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.678708 (-2.534539)\n",
      "\u001b[0m\n",
      "it 82/2000, vlb -3.660556, \u001b[31m   time: 0.694956 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.598956 (-2.534539)\n",
      "\u001b[0m\n",
      "it 83/2000, vlb -3.621960, \u001b[31m   time: 0.712614 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.651375 (-2.534539)\n",
      "\u001b[0m\n",
      "it 84/2000, vlb -3.518759, \u001b[31m   time: 0.739932 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.560134 (-2.534539)\n",
      "\u001b[0m\n",
      "it 85/2000, vlb -3.597633, \u001b[31m   time: 0.703969 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.757093 (-2.534539)\n",
      "\u001b[0m\n",
      "it 86/2000, vlb -3.607848, \u001b[31m   time: 0.841671 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.786545 (-2.534539)\n",
      "\u001b[0m\n",
      "it 87/2000, vlb -3.565724, \u001b[31m   time: 0.695870 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.623851 (-2.534539)\n",
      "\u001b[0m\n",
      "it 88/2000, vlb -3.562561, \u001b[31m   time: 0.726763 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.619375 (-2.534539)\n",
      "\u001b[0m\n",
      "it 89/2000, vlb -3.555107, \u001b[31m   time: 0.722751 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.571130 (-2.534539)\n",
      "\u001b[0m\n",
      "it 90/2000, vlb -3.520586, \u001b[31m   time: 0.682846 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.599100 (-2.534539)\n",
      "\u001b[0m\n",
      "it 91/2000, vlb -3.521489, \u001b[31m   time: 0.677289 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.611980 (-2.534539)\n",
      "\u001b[0m\n",
      "it 92/2000, vlb -3.463492, \u001b[31m   time: 0.689674 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.613153 (-2.534539)\n",
      "\u001b[0m\n",
      "it 93/2000, vlb -3.503594, \u001b[31m   time: 0.701565 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.562561 (-2.534539)\n",
      "\u001b[0m\n",
      "it 94/2000, vlb -3.458165, \u001b[31m   time: 0.707471 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.674007 (-2.534539)\n",
      "\u001b[0m\n",
      "it 95/2000, vlb -3.453394, \u001b[31m   time: 0.693664 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.646948 (-2.534539)\n",
      "\u001b[0m\n",
      "it 96/2000, vlb -3.463362, \u001b[31m   time: 0.707089 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.605928 (-2.534539)\n",
      "\u001b[0m\n",
      "it 97/2000, vlb -3.516120, \u001b[31m   time: 0.730072 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.470436 (-2.534539)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 98/2000, vlb -3.408804, \u001b[31m   time: 0.708623 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.641191 (-2.470436)\n",
      "\u001b[0m\n",
      "it 99/2000, vlb -3.483872, \u001b[31m   time: 0.701151 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.451914 (-2.470436)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 100/2000, vlb -3.455257, \u001b[31m   time: 0.686683 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.534658 (-2.451914)\n",
      "\u001b[0m\n",
      "it 101/2000, vlb -3.456838, \u001b[31m   time: 0.691989 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.499417 (-2.451914)\n",
      "\u001b[0m\n",
      "it 102/2000, vlb -3.478433, \u001b[31m   time: 0.695657 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.587495 (-2.451914)\n",
      "\u001b[0m\n",
      "it 103/2000, vlb -3.429413, \u001b[31m   time: 0.758548 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.519199 (-2.451914)\n",
      "\u001b[0m\n",
      "it 104/2000, vlb -3.433497, \u001b[31m   time: 0.690898 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.641336 (-2.451914)\n",
      "\u001b[0m\n",
      "it 105/2000, vlb -3.514968, \u001b[31m   time: 0.691250 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.643816 (-2.451914)\n",
      "\u001b[0m\n",
      "it 106/2000, vlb -3.415078, \u001b[31m   time: 0.699236 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.591522 (-2.451914)\n",
      "\u001b[0m\n",
      "it 107/2000, vlb -3.476779, \u001b[31m   time: 0.695036 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.580076 (-2.451914)\n",
      "\u001b[0m\n",
      "it 108/2000, vlb -3.401961, \u001b[31m   time: 0.719003 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.519040 (-2.451914)\n",
      "\u001b[0m\n",
      "it 109/2000, vlb -3.424440, \u001b[31m   time: 0.708139 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.570803 (-2.451914)\n",
      "\u001b[0m\n",
      "it 110/2000, vlb -3.364931, \u001b[31m   time: 0.822787 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.548483 (-2.451914)\n",
      "\u001b[0m\n",
      "it 111/2000, vlb -3.430250, \u001b[31m   time: 0.685259 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.502320 (-2.451914)\n",
      "\u001b[0m\n",
      "it 112/2000, vlb -3.398353, \u001b[31m   time: 0.692045 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.514548 (-2.451914)\n",
      "\u001b[0m\n",
      "it 113/2000, vlb -3.338888, \u001b[31m   time: 0.692174 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.543195 (-2.451914)\n",
      "\u001b[0m\n",
      "it 114/2000, vlb -3.289201, \u001b[31m   time: 0.698558 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.609114 (-2.451914)\n",
      "\u001b[0m\n",
      "it 115/2000, vlb -3.388646, \u001b[31m   time: 0.692871 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.452309 (-2.451914)\n",
      "\u001b[0m\n",
      "it 116/2000, vlb -3.337090, \u001b[31m   time: 0.683017 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.479579 (-2.451914)\n",
      "\u001b[0m\n",
      "it 117/2000, vlb -3.348916, \u001b[31m   time: 0.696200 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.464896 (-2.451914)\n",
      "\u001b[0m\n",
      "it 118/2000, vlb -3.335123, \u001b[31m   time: 0.682188 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.389194 (-2.451914)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 119/2000, vlb -3.335748, \u001b[31m   time: 0.677623 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.498423 (-2.389194)\n",
      "\u001b[0m\n",
      "it 120/2000, vlb -3.356945, \u001b[31m   time: 0.692065 seconds\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m    vlb -2.472693 (-2.389194)\n",
      "\u001b[0m\n",
      "it 121/2000, vlb -3.336367, \u001b[31m   time: 0.696216 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.496231 (-2.389194)\n",
      "\u001b[0m\n",
      "it 122/2000, vlb -3.324256, \u001b[31m   time: 0.681899 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.513239 (-2.389194)\n",
      "\u001b[0m\n",
      "it 123/2000, vlb -3.319148, \u001b[31m   time: 0.686744 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.543611 (-2.389194)\n",
      "\u001b[0m\n",
      "it 124/2000, vlb -3.268432, \u001b[31m   time: 0.692968 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.544091 (-2.389194)\n",
      "\u001b[0m\n",
      "it 125/2000, vlb -3.279088, \u001b[31m   time: 0.684222 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.492832 (-2.389194)\n",
      "\u001b[0m\n",
      "it 126/2000, vlb -3.276651, \u001b[31m   time: 0.697798 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.425328 (-2.389194)\n",
      "\u001b[0m\n",
      "it 127/2000, vlb -3.283475, \u001b[31m   time: 0.698063 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.442968 (-2.389194)\n",
      "\u001b[0m\n",
      "it 128/2000, vlb -3.276904, \u001b[31m   time: 0.696809 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.386448 (-2.389194)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 129/2000, vlb -3.296070, \u001b[31m   time: 0.683679 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.481448 (-2.386448)\n",
      "\u001b[0m\n",
      "it 130/2000, vlb -3.295955, \u001b[31m   time: 0.715146 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.409304 (-2.386448)\n",
      "\u001b[0m\n",
      "it 131/2000, vlb -3.219111, \u001b[31m   time: 0.713120 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.382624 (-2.386448)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 132/2000, vlb -3.296515, \u001b[31m   time: 0.697164 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.605482 (-2.382624)\n",
      "\u001b[0m\n",
      "it 133/2000, vlb -3.248000, \u001b[31m   time: 0.695657 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.397853 (-2.382624)\n",
      "\u001b[0m\n",
      "it 134/2000, vlb -3.270854, \u001b[31m   time: 0.684196 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.407330 (-2.382624)\n",
      "\u001b[0m\n",
      "it 135/2000, vlb -3.322722, \u001b[31m   time: 0.697471 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.382317 (-2.382624)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 136/2000, vlb -3.243346, \u001b[31m   time: 0.679009 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.340804 (-2.382317)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 137/2000, vlb -3.248988, \u001b[31m   time: 0.690193 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.451604 (-2.340804)\n",
      "\u001b[0m\n",
      "it 138/2000, vlb -3.248142, \u001b[31m   time: 0.687186 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.439644 (-2.340804)\n",
      "\u001b[0m\n",
      "it 139/2000, vlb -3.197877, \u001b[31m   time: 0.698693 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.388404 (-2.340804)\n",
      "\u001b[0m\n",
      "it 140/2000, vlb -3.239397, \u001b[31m   time: 0.694397 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.409456 (-2.340804)\n",
      "\u001b[0m\n",
      "it 141/2000, vlb -3.212378, \u001b[31m   time: 0.688166 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.390752 (-2.340804)\n",
      "\u001b[0m\n",
      "it 142/2000, vlb -3.225934, \u001b[31m   time: 0.697811 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.476599 (-2.340804)\n",
      "\u001b[0m\n",
      "it 143/2000, vlb -3.212602, \u001b[31m   time: 0.690275 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.356308 (-2.340804)\n",
      "\u001b[0m\n",
      "it 144/2000, vlb -3.184585, \u001b[31m   time: 0.684956 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.510939 (-2.340804)\n",
      "\u001b[0m\n",
      "it 145/2000, vlb -3.213584, \u001b[31m   time: 0.699641 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.433324 (-2.340804)\n",
      "\u001b[0m\n",
      "it 146/2000, vlb -3.215649, \u001b[31m   time: 0.680560 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.354865 (-2.340804)\n",
      "\u001b[0m\n",
      "it 147/2000, vlb -3.237000, \u001b[31m   time: 0.698409 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.385459 (-2.340804)\n",
      "\u001b[0m\n",
      "it 148/2000, vlb -3.191654, \u001b[31m   time: 0.680053 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.278108 (-2.340804)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 149/2000, vlb -3.202738, \u001b[31m   time: 0.696089 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.418020 (-2.278108)\n",
      "\u001b[0m\n",
      "it 150/2000, vlb -3.144161, \u001b[31m   time: 0.698248 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.493737 (-2.278108)\n",
      "\u001b[0m\n",
      "it 151/2000, vlb -3.176134, \u001b[31m   time: 0.688962 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.423895 (-2.278108)\n",
      "\u001b[0m\n",
      "it 152/2000, vlb -3.222806, \u001b[31m   time: 0.701302 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.389456 (-2.278108)\n",
      "\u001b[0m\n",
      "it 153/2000, vlb -3.130701, \u001b[31m   time: 0.682792 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.470754 (-2.278108)\n",
      "\u001b[0m\n",
      "it 154/2000, vlb -3.186787, \u001b[31m   time: 0.692627 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.487949 (-2.278108)\n",
      "\u001b[0m\n",
      "it 155/2000, vlb -3.221180, \u001b[31m   time: 0.681205 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.340311 (-2.278108)\n",
      "\u001b[0m\n",
      "it 156/2000, vlb -3.158770, \u001b[31m   time: 0.683043 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.429671 (-2.278108)\n",
      "\u001b[0m\n",
      "it 157/2000, vlb -3.160985, \u001b[31m   time: 0.700917 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.281283 (-2.278108)\n",
      "\u001b[0m\n",
      "it 158/2000, vlb -3.083100, \u001b[31m   time: 0.696066 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.517433 (-2.278108)\n",
      "\u001b[0m\n",
      "it 159/2000, vlb -3.179753, \u001b[31m   time: 0.756016 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.308659 (-2.278108)\n",
      "\u001b[0m\n",
      "it 160/2000, vlb -3.187616, \u001b[31m   time: 0.714119 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.437445 (-2.278108)\n",
      "\u001b[0m\n",
      "it 161/2000, vlb -3.098550, \u001b[31m   time: 0.740102 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.448228 (-2.278108)\n",
      "\u001b[0m\n",
      "it 162/2000, vlb -3.127061, \u001b[31m   time: 0.696327 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.291877 (-2.278108)\n",
      "\u001b[0m\n",
      "it 163/2000, vlb -3.146279, \u001b[31m   time: 0.701871 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.379398 (-2.278108)\n",
      "\u001b[0m\n",
      "it 164/2000, vlb -3.126552, \u001b[31m   time: 0.714058 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.319061 (-2.278108)\n",
      "\u001b[0m\n",
      "it 165/2000, vlb -3.094608, \u001b[31m   time: 0.685006 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.385372 (-2.278108)\n",
      "\u001b[0m\n",
      "it 166/2000, vlb -3.130034, \u001b[31m   time: 0.697143 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.321026 (-2.278108)\n",
      "\u001b[0m\n",
      "it 167/2000, vlb -3.100221, \u001b[31m   time: 0.761995 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.466900 (-2.278108)\n",
      "\u001b[0m\n",
      "it 168/2000, vlb -3.126579, \u001b[31m   time: 0.697164 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.428947 (-2.278108)\n",
      "\u001b[0m\n",
      "it 169/2000, vlb -3.147303, \u001b[31m   time: 0.699036 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.364555 (-2.278108)\n",
      "\u001b[0m\n",
      "it 170/2000, vlb -3.111623, \u001b[31m   time: 0.724093 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.354353 (-2.278108)\n",
      "\u001b[0m\n",
      "it 171/2000, vlb -3.107836, \u001b[31m   time: 0.724047 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.445489 (-2.278108)\n",
      "\u001b[0m\n",
      "it 172/2000, vlb -3.120277, \u001b[31m   time: 0.745328 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.405740 (-2.278108)\n",
      "\u001b[0m\n",
      "it 173/2000, vlb -3.091649, \u001b[31m   time: 0.695161 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.267270 (-2.278108)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 174/2000, vlb -3.111685, \u001b[31m   time: 0.692177 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.257876 (-2.267270)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 175/2000, vlb -3.096980, \u001b[31m   time: 0.694177 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.363581 (-2.257876)\n",
      "\u001b[0m\n",
      "it 176/2000, vlb -3.048321, \u001b[31m   time: 0.693172 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.298329 (-2.257876)\n",
      "\u001b[0m\n",
      "it 177/2000, vlb -3.092251, \u001b[31m   time: 0.698392 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.362255 (-2.257876)\n",
      "\u001b[0m\n",
      "it 178/2000, vlb -3.117787, \u001b[31m   time: 0.694028 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.495524 (-2.257876)\n",
      "\u001b[0m\n",
      "it 179/2000, vlb -3.129722, \u001b[31m   time: 0.698544 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.227813 (-2.257876)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 180/2000, vlb -3.045820, \u001b[31m   time: 0.692553 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.400267 (-2.227813)\n",
      "\u001b[0m\n",
      "it 181/2000, vlb -3.041934, \u001b[31m   time: 0.696258 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.307196 (-2.227813)\n",
      "\u001b[0m\n",
      "it 182/2000, vlb -3.071494, \u001b[31m   time: 0.697127 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.405936 (-2.227813)\n",
      "\u001b[0m\n",
      "it 183/2000, vlb -3.061893, \u001b[31m   time: 0.682702 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.333433 (-2.227813)\n",
      "\u001b[0m\n",
      "it 184/2000, vlb -3.077793, \u001b[31m   time: 0.697159 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.310133 (-2.227813)\n",
      "\u001b[0m\n",
      "it 185/2000, vlb -3.036956, \u001b[31m   time: 0.697934 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.322360 (-2.227813)\n",
      "\u001b[0m\n",
      "it 186/2000, vlb -3.039683, \u001b[31m   time: 0.681040 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.398244 (-2.227813)\n",
      "\u001b[0m\n",
      "it 187/2000, vlb -3.048560, \u001b[31m   time: 0.684120 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.420351 (-2.227813)\n",
      "\u001b[0m\n",
      "it 188/2000, vlb -3.079334, \u001b[31m   time: 0.696825 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.463806 (-2.227813)\n",
      "\u001b[0m\n",
      "it 189/2000, vlb -3.120975, \u001b[31m   time: 0.694110 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.337977 (-2.227813)\n",
      "\u001b[0m\n",
      "it 190/2000, vlb -3.049734, \u001b[31m   time: 0.683362 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.410160 (-2.227813)\n",
      "\u001b[0m\n",
      "it 191/2000, vlb -3.005980, \u001b[31m   time: 0.688393 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.218680 (-2.227813)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 192/2000, vlb -3.078658, \u001b[31m   time: 0.688453 seconds\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m    vlb -2.334578 (-2.218680)\n",
      "\u001b[0m\n",
      "it 193/2000, vlb -3.096401, \u001b[31m   time: 0.696067 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.381440 (-2.218680)\n",
      "\u001b[0m\n",
      "it 194/2000, vlb -3.115523, \u001b[31m   time: 0.685194 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.277959 (-2.218680)\n",
      "\u001b[0m\n",
      "it 195/2000, vlb -2.979169, \u001b[31m   time: 0.682854 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.339864 (-2.218680)\n",
      "\u001b[0m\n",
      "it 196/2000, vlb -3.053114, \u001b[31m   time: 0.697067 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.384498 (-2.218680)\n",
      "\u001b[0m\n",
      "it 197/2000, vlb -3.041884, \u001b[31m   time: 0.696657 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.408216 (-2.218680)\n",
      "\u001b[0m\n",
      "it 198/2000, vlb -3.024978, \u001b[31m   time: 0.683790 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.320795 (-2.218680)\n",
      "\u001b[0m\n",
      "it 199/2000, vlb -3.076251, \u001b[31m   time: 0.695162 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.227119 (-2.218680)\n",
      "\u001b[0m\n",
      "it 200/2000, vlb -3.005144, \u001b[31m   time: 0.695918 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.247794 (-2.218680)\n",
      "\u001b[0m\n",
      "it 201/2000, vlb -3.003044, \u001b[31m   time: 0.701937 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.211763 (-2.218680)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 202/2000, vlb -3.042872, \u001b[31m   time: 0.697391 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.381695 (-2.211763)\n",
      "\u001b[0m\n",
      "it 203/2000, vlb -2.995418, \u001b[31m   time: 0.751512 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.191425 (-2.211763)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 204/2000, vlb -3.050738, \u001b[31m   time: 0.698941 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.363728 (-2.191425)\n",
      "\u001b[0m\n",
      "it 205/2000, vlb -3.030187, \u001b[31m   time: 0.735160 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.250151 (-2.191425)\n",
      "\u001b[0m\n",
      "it 206/2000, vlb -3.008002, \u001b[31m   time: 0.724878 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.323914 (-2.191425)\n",
      "\u001b[0m\n",
      "it 207/2000, vlb -3.042062, \u001b[31m   time: 0.696438 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.217192 (-2.191425)\n",
      "\u001b[0m\n",
      "it 208/2000, vlb -3.042316, \u001b[31m   time: 0.746093 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.467630 (-2.191425)\n",
      "\u001b[0m\n",
      "it 209/2000, vlb -2.986709, \u001b[31m   time: 0.774992 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.284165 (-2.191425)\n",
      "\u001b[0m\n",
      "it 210/2000, vlb -3.037593, \u001b[31m   time: 0.860478 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.364884 (-2.191425)\n",
      "\u001b[0m\n",
      "it 211/2000, vlb -3.017605, \u001b[31m   time: 0.795058 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.447170 (-2.191425)\n",
      "\u001b[0m\n",
      "it 212/2000, vlb -3.027709, \u001b[31m   time: 0.707025 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.325996 (-2.191425)\n",
      "\u001b[0m\n",
      "it 213/2000, vlb -3.007460, \u001b[31m   time: 0.694662 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.316254 (-2.191425)\n",
      "\u001b[0m\n",
      "it 214/2000, vlb -3.039515, \u001b[31m   time: 0.758002 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.306768 (-2.191425)\n",
      "\u001b[0m\n",
      "it 215/2000, vlb -3.031788, \u001b[31m   time: 0.740397 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.367637 (-2.191425)\n",
      "\u001b[0m\n",
      "it 216/2000, vlb -3.003602, \u001b[31m   time: 0.741938 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.270637 (-2.191425)\n",
      "\u001b[0m\n",
      "it 217/2000, vlb -3.015003, \u001b[31m   time: 0.740054 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.274640 (-2.191425)\n",
      "\u001b[0m\n",
      "it 218/2000, vlb -3.009376, \u001b[31m   time: 0.746022 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.309613 (-2.191425)\n",
      "\u001b[0m\n",
      "it 219/2000, vlb -3.031034, \u001b[31m   time: 0.700011 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.214165 (-2.191425)\n",
      "\u001b[0m\n",
      "it 220/2000, vlb -2.965892, \u001b[31m   time: 0.699058 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.404611 (-2.191425)\n",
      "\u001b[0m\n",
      "it 221/2000, vlb -2.995132, \u001b[31m   time: 0.698160 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.231092 (-2.191425)\n",
      "\u001b[0m\n",
      "it 222/2000, vlb -2.945247, \u001b[31m   time: 0.695125 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.221341 (-2.191425)\n",
      "\u001b[0m\n",
      "it 223/2000, vlb -2.963459, \u001b[31m   time: 0.691670 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.390652 (-2.191425)\n",
      "\u001b[0m\n",
      "it 224/2000, vlb -3.054459, \u001b[31m   time: 0.685367 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.281459 (-2.191425)\n",
      "\u001b[0m\n",
      "it 225/2000, vlb -2.995245, \u001b[31m   time: 0.686155 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.270514 (-2.191425)\n",
      "\u001b[0m\n",
      "it 226/2000, vlb -2.977872, \u001b[31m   time: 0.684115 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.230757 (-2.191425)\n",
      "\u001b[0m\n",
      "it 227/2000, vlb -2.994770, \u001b[31m   time: 0.684999 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.410161 (-2.191425)\n",
      "\u001b[0m\n",
      "it 228/2000, vlb -2.991275, \u001b[31m   time: 0.688606 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.290763 (-2.191425)\n",
      "\u001b[0m\n",
      "it 229/2000, vlb -2.931577, \u001b[31m   time: 0.701313 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.361467 (-2.191425)\n",
      "\u001b[0m\n",
      "it 230/2000, vlb -2.962680, \u001b[31m   time: 0.746155 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.111904 (-2.191425)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 231/2000, vlb -2.906918, \u001b[31m   time: 0.860921 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.206914 (-2.111904)\n",
      "\u001b[0m\n",
      "it 232/2000, vlb -2.941715, \u001b[31m   time: 0.749029 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.227946 (-2.111904)\n",
      "\u001b[0m\n",
      "it 233/2000, vlb -2.987417, \u001b[31m   time: 0.699907 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.109325 (-2.111904)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 234/2000, vlb -2.915830, \u001b[31m   time: 0.700663 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.229293 (-2.109325)\n",
      "\u001b[0m\n",
      "it 235/2000, vlb -2.915792, \u001b[31m   time: 0.698654 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.225832 (-2.109325)\n",
      "\u001b[0m\n",
      "it 236/2000, vlb -2.986247, \u001b[31m   time: 0.723117 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.305655 (-2.109325)\n",
      "\u001b[0m\n",
      "it 237/2000, vlb -2.983895, \u001b[31m   time: 0.697278 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.263440 (-2.109325)\n",
      "\u001b[0m\n",
      "it 238/2000, vlb -2.998195, \u001b[31m   time: 0.683442 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.372103 (-2.109325)\n",
      "\u001b[0m\n",
      "it 239/2000, vlb -2.962948, \u001b[31m   time: 0.736551 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.335827 (-2.109325)\n",
      "\u001b[0m\n",
      "it 240/2000, vlb -2.969950, \u001b[31m   time: 0.691178 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.218926 (-2.109325)\n",
      "\u001b[0m\n",
      "it 241/2000, vlb -2.954399, \u001b[31m   time: 0.695984 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.270566 (-2.109325)\n",
      "\u001b[0m\n",
      "it 242/2000, vlb -2.921851, \u001b[31m   time: 0.698666 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.382055 (-2.109325)\n",
      "\u001b[0m\n",
      "it 243/2000, vlb -2.917049, \u001b[31m   time: 0.693172 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.263773 (-2.109325)\n",
      "\u001b[0m\n",
      "it 244/2000, vlb -2.930275, \u001b[31m   time: 0.685192 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.261347 (-2.109325)\n",
      "\u001b[0m\n",
      "it 245/2000, vlb -2.933927, \u001b[31m   time: 0.692240 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.381695 (-2.109325)\n",
      "\u001b[0m\n",
      "it 246/2000, vlb -2.949690, \u001b[31m   time: 0.716026 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.164408 (-2.109325)\n",
      "\u001b[0m\n",
      "it 247/2000, vlb -2.968953, \u001b[31m   time: 0.708044 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.382374 (-2.109325)\n",
      "\u001b[0m\n",
      "it 248/2000, vlb -2.924842, \u001b[31m   time: 0.731074 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.304837 (-2.109325)\n",
      "\u001b[0m\n",
      "it 249/2000, vlb -2.933627, \u001b[31m   time: 0.753073 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.132434 (-2.109325)\n",
      "\u001b[0m\n",
      "it 250/2000, vlb -2.959403, \u001b[31m   time: 0.687852 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.269086 (-2.109325)\n",
      "\u001b[0m\n",
      "it 251/2000, vlb -2.883117, \u001b[31m   time: 0.700177 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.245100 (-2.109325)\n",
      "\u001b[0m\n",
      "it 252/2000, vlb -2.905735, \u001b[31m   time: 0.694674 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.364290 (-2.109325)\n",
      "\u001b[0m\n",
      "it 253/2000, vlb -2.916731, \u001b[31m   time: 0.695761 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.237615 (-2.109325)\n",
      "\u001b[0m\n",
      "it 254/2000, vlb -2.914444, \u001b[31m   time: 0.727157 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.167344 (-2.109325)\n",
      "\u001b[0m\n",
      "it 255/2000, vlb -2.910258, \u001b[31m   time: 0.685930 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.321786 (-2.109325)\n",
      "\u001b[0m\n",
      "it 256/2000, vlb -2.907008, \u001b[31m   time: 0.691177 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.296821 (-2.109325)\n",
      "\u001b[0m\n",
      "it 257/2000, vlb -2.943627, \u001b[31m   time: 0.700220 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.271471 (-2.109325)\n",
      "\u001b[0m\n",
      "it 258/2000, vlb -2.959099, \u001b[31m   time: 0.705816 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.286673 (-2.109325)\n",
      "\u001b[0m\n",
      "it 259/2000, vlb -2.886142, \u001b[31m   time: 0.691085 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.337081 (-2.109325)\n",
      "\u001b[0m\n",
      "it 260/2000, vlb -2.955296, \u001b[31m   time: 0.769008 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.121723 (-2.109325)\n",
      "\u001b[0m\n",
      "it 261/2000, vlb -2.954246, \u001b[31m   time: 0.727781 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.155764 (-2.109325)\n",
      "\u001b[0m\n",
      "it 262/2000, vlb -2.934619, \u001b[31m   time: 0.727379 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.252136 (-2.109325)\n",
      "\u001b[0m\n",
      "it 263/2000, vlb -2.901162, \u001b[31m   time: 0.696164 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.169229 (-2.109325)\n",
      "\u001b[0m\n",
      "it 264/2000, vlb -2.933507, \u001b[31m   time: 0.730062 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.198039 (-2.109325)\n",
      "\u001b[0m\n",
      "it 265/2000, vlb -2.888775, \u001b[31m   time: 0.693038 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.191147 (-2.109325)\n",
      "\u001b[0m\n",
      "it 266/2000, vlb -2.887873, \u001b[31m   time: 0.697086 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.408917 (-2.109325)\n",
      "\u001b[0m\n",
      "it 267/2000, vlb -2.944383, \u001b[31m   time: 0.684720 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.200958 (-2.109325)\n",
      "\u001b[0m\n",
      "it 268/2000, vlb -2.879460, \u001b[31m   time: 0.685686 seconds\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m    vlb -2.316104 (-2.109325)\n",
      "\u001b[0m\n",
      "it 269/2000, vlb -2.907237, \u001b[31m   time: 0.693359 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.337363 (-2.109325)\n",
      "\u001b[0m\n",
      "it 270/2000, vlb -2.921379, \u001b[31m   time: 0.687799 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.266761 (-2.109325)\n",
      "\u001b[0m\n",
      "it 271/2000, vlb -2.865087, \u001b[31m   time: 0.681591 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.181430 (-2.109325)\n",
      "\u001b[0m\n",
      "it 272/2000, vlb -2.889970, \u001b[31m   time: 0.698650 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.160904 (-2.109325)\n",
      "\u001b[0m\n",
      "it 273/2000, vlb -2.952025, \u001b[31m   time: 0.688086 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.185376 (-2.109325)\n",
      "\u001b[0m\n",
      "it 274/2000, vlb -2.901129, \u001b[31m   time: 0.720309 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.188873 (-2.109325)\n",
      "\u001b[0m\n",
      "it 275/2000, vlb -2.884046, \u001b[31m   time: 0.698159 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.317495 (-2.109325)\n",
      "\u001b[0m\n",
      "it 276/2000, vlb -2.891608, \u001b[31m   time: 0.685686 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.361709 (-2.109325)\n",
      "\u001b[0m\n",
      "it 277/2000, vlb -2.918211, \u001b[31m   time: 0.698639 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.259400 (-2.109325)\n",
      "\u001b[0m\n",
      "it 278/2000, vlb -2.888765, \u001b[31m   time: 0.694111 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.241178 (-2.109325)\n",
      "\u001b[0m\n",
      "it 279/2000, vlb -2.888428, \u001b[31m   time: 0.698891 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.236273 (-2.109325)\n",
      "\u001b[0m\n",
      "it 280/2000, vlb -2.856043, \u001b[31m   time: 0.682861 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.143636 (-2.109325)\n",
      "\u001b[0m\n",
      "it 281/2000, vlb -2.881690, \u001b[31m   time: 0.700088 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.292683 (-2.109325)\n",
      "\u001b[0m\n",
      "it 282/2000, vlb -2.924006, \u001b[31m   time: 0.694173 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.223734 (-2.109325)\n",
      "\u001b[0m\n",
      "it 283/2000, vlb -2.891857, \u001b[31m   time: 0.693051 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.233830 (-2.109325)\n",
      "\u001b[0m\n",
      "it 284/2000, vlb -2.919004, \u001b[31m   time: 0.686321 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.273234 (-2.109325)\n",
      "\u001b[0m\n",
      "it 285/2000, vlb -2.884756, \u001b[31m   time: 0.684035 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.141199 (-2.109325)\n",
      "\u001b[0m\n",
      "it 286/2000, vlb -2.917236, \u001b[31m   time: 0.695685 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.150919 (-2.109325)\n",
      "\u001b[0m\n",
      "it 287/2000, vlb -2.862706, \u001b[31m   time: 0.681914 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.291478 (-2.109325)\n",
      "\u001b[0m\n",
      "it 288/2000, vlb -2.884653, \u001b[31m   time: 0.686389 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.199272 (-2.109325)\n",
      "\u001b[0m\n",
      "it 289/2000, vlb -2.864001, \u001b[31m   time: 0.693766 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.065660 (-2.109325)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 290/2000, vlb -2.944922, \u001b[31m   time: 0.681798 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.127023 (-2.065660)\n",
      "\u001b[0m\n",
      "it 291/2000, vlb -2.847731, \u001b[31m   time: 0.696145 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.305454 (-2.065660)\n",
      "\u001b[0m\n",
      "it 292/2000, vlb -2.880352, \u001b[31m   time: 0.687261 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.191474 (-2.065660)\n",
      "\u001b[0m\n",
      "it 293/2000, vlb -2.862627, \u001b[31m   time: 0.698651 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.367073 (-2.065660)\n",
      "\u001b[0m\n",
      "it 294/2000, vlb -2.874000, \u001b[31m   time: 0.804409 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.295345 (-2.065660)\n",
      "\u001b[0m\n",
      "it 295/2000, vlb -2.899461, \u001b[31m   time: 0.697105 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.141010 (-2.065660)\n",
      "\u001b[0m\n",
      "it 296/2000, vlb -2.897190, \u001b[31m   time: 0.716601 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.304191 (-2.065660)\n",
      "\u001b[0m\n",
      "it 297/2000, vlb -2.870177, \u001b[31m   time: 0.701102 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.249905 (-2.065660)\n",
      "\u001b[0m\n",
      "it 298/2000, vlb -2.863251, \u001b[31m   time: 0.687975 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.218685 (-2.065660)\n",
      "\u001b[0m\n",
      "it 299/2000, vlb -2.881726, \u001b[31m   time: 0.708134 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.260970 (-2.065660)\n",
      "\u001b[0m\n",
      "it 300/2000, vlb -2.840692, \u001b[31m   time: 0.776677 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.305228 (-2.065660)\n",
      "\u001b[0m\n",
      "it 301/2000, vlb -2.852805, \u001b[31m   time: 0.789410 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.247443 (-2.065660)\n",
      "\u001b[0m\n",
      "it 302/2000, vlb -2.835415, \u001b[31m   time: 0.745037 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.258767 (-2.065660)\n",
      "\u001b[0m\n",
      "it 303/2000, vlb -2.864322, \u001b[31m   time: 0.921079 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.203889 (-2.065660)\n",
      "\u001b[0m\n",
      "it 304/2000, vlb -2.852235, \u001b[31m   time: 0.712612 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.259793 (-2.065660)\n",
      "\u001b[0m\n",
      "it 305/2000, vlb -2.851288, \u001b[31m   time: 0.685955 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.310327 (-2.065660)\n",
      "\u001b[0m\n",
      "it 306/2000, vlb -2.908327, \u001b[31m   time: 0.694661 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.236602 (-2.065660)\n",
      "\u001b[0m\n",
      "it 307/2000, vlb -2.842275, \u001b[31m   time: 0.696735 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.253624 (-2.065660)\n",
      "\u001b[0m\n",
      "it 308/2000, vlb -2.812819, \u001b[31m   time: 0.687168 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.111938 (-2.065660)\n",
      "\u001b[0m\n",
      "it 309/2000, vlb -2.854517, \u001b[31m   time: 0.685988 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.266971 (-2.065660)\n",
      "\u001b[0m\n",
      "it 310/2000, vlb -2.848086, \u001b[31m   time: 0.697164 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.452899 (-2.065660)\n",
      "\u001b[0m\n",
      "it 311/2000, vlb -2.841430, \u001b[31m   time: 0.715117 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.293580 (-2.065660)\n",
      "\u001b[0m\n",
      "it 312/2000, vlb -2.865421, \u001b[31m   time: 0.702145 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.298900 (-2.065660)\n",
      "\u001b[0m\n",
      "it 313/2000, vlb -2.864629, \u001b[31m   time: 0.726058 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.256938 (-2.065660)\n",
      "\u001b[0m\n",
      "it 314/2000, vlb -2.834224, \u001b[31m   time: 0.693932 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.154607 (-2.065660)\n",
      "\u001b[0m\n",
      "it 315/2000, vlb -2.824908, \u001b[31m   time: 0.694440 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.340129 (-2.065660)\n",
      "\u001b[0m\n",
      "it 316/2000, vlb -2.861745, \u001b[31m   time: 0.699813 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.147756 (-2.065660)\n",
      "\u001b[0m\n",
      "it 317/2000, vlb -2.864896, \u001b[31m   time: 0.731861 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.169032 (-2.065660)\n",
      "\u001b[0m\n",
      "it 318/2000, vlb -2.877579, \u001b[31m   time: 0.711337 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.289010 (-2.065660)\n",
      "\u001b[0m\n",
      "it 319/2000, vlb -2.844824, \u001b[31m   time: 0.684800 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.262548 (-2.065660)\n",
      "\u001b[0m\n",
      "it 320/2000, vlb -2.859917, \u001b[31m   time: 0.690996 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.253909 (-2.065660)\n",
      "\u001b[0m\n",
      "it 321/2000, vlb -2.847920, \u001b[31m   time: 0.690019 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.299832 (-2.065660)\n",
      "\u001b[0m\n",
      "it 322/2000, vlb -2.816638, \u001b[31m   time: 0.695953 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.311733 (-2.065660)\n",
      "\u001b[0m\n",
      "it 323/2000, vlb -2.903569, \u001b[31m   time: 0.680945 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.066535 (-2.065660)\n",
      "\u001b[0m\n",
      "it 324/2000, vlb -2.733931, \u001b[31m   time: 0.685475 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.261379 (-2.065660)\n",
      "\u001b[0m\n",
      "it 325/2000, vlb -2.855147, \u001b[31m   time: 0.722099 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.302795 (-2.065660)\n",
      "\u001b[0m\n",
      "it 326/2000, vlb -2.869001, \u001b[31m   time: 0.688280 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.213270 (-2.065660)\n",
      "\u001b[0m\n",
      "it 327/2000, vlb -2.812223, \u001b[31m   time: 0.697806 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.126788 (-2.065660)\n",
      "\u001b[0m\n",
      "it 328/2000, vlb -2.796885, \u001b[31m   time: 0.700458 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.369323 (-2.065660)\n",
      "\u001b[0m\n",
      "it 329/2000, vlb -2.853854, \u001b[31m   time: 0.696767 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.122774 (-2.065660)\n",
      "\u001b[0m\n",
      "it 330/2000, vlb -2.824252, \u001b[31m   time: 0.701644 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.137052 (-2.065660)\n",
      "\u001b[0m\n",
      "it 331/2000, vlb -2.849739, \u001b[31m   time: 0.694030 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.111796 (-2.065660)\n",
      "\u001b[0m\n",
      "it 332/2000, vlb -2.851515, \u001b[31m   time: 0.693664 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.187922 (-2.065660)\n",
      "\u001b[0m\n",
      "it 333/2000, vlb -2.879780, \u001b[31m   time: 0.698362 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.259670 (-2.065660)\n",
      "\u001b[0m\n",
      "it 334/2000, vlb -2.838274, \u001b[31m   time: 0.696338 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.122736 (-2.065660)\n",
      "\u001b[0m\n",
      "it 335/2000, vlb -2.834771, \u001b[31m   time: 0.684766 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.057936 (-2.065660)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 336/2000, vlb -2.877942, \u001b[31m   time: 0.693895 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.252135 (-2.057936)\n",
      "\u001b[0m\n",
      "it 337/2000, vlb -2.810771, \u001b[31m   time: 0.695091 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.283182 (-2.057936)\n",
      "\u001b[0m\n",
      "it 338/2000, vlb -2.844550, \u001b[31m   time: 0.693408 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.192018 (-2.057936)\n",
      "\u001b[0m\n",
      "it 339/2000, vlb -2.744817, \u001b[31m   time: 0.734244 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.163479 (-2.057936)\n",
      "\u001b[0m\n",
      "it 340/2000, vlb -2.832674, \u001b[31m   time: 0.705050 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.140975 (-2.057936)\n",
      "\u001b[0m\n",
      "it 341/2000, vlb -2.802451, \u001b[31m   time: 0.719770 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.154258 (-2.057936)\n",
      "\u001b[0m\n",
      "it 342/2000, vlb -2.751436, \u001b[31m   time: 0.685348 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.097779 (-2.057936)\n",
      "\u001b[0m\n",
      "it 343/2000, vlb -2.820837, \u001b[31m   time: 0.701345 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.380548 (-2.057936)\n",
      "\u001b[0m\n",
      "it 344/2000, vlb -2.779460, \u001b[31m   time: 0.694730 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.166675 (-2.057936)\n",
      "\u001b[0m\n",
      "it 345/2000, vlb -2.808176, \u001b[31m   time: 0.685004 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.187574 (-2.057936)\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 346/2000, vlb -2.828385, \u001b[31m   time: 0.720204 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.111417 (-2.057936)\n",
      "\u001b[0m\n",
      "it 347/2000, vlb -2.807245, \u001b[31m   time: 0.730203 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.230100 (-2.057936)\n",
      "\u001b[0m\n",
      "it 348/2000, vlb -2.816463, \u001b[31m   time: 0.691174 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.284725 (-2.057936)\n",
      "\u001b[0m\n",
      "it 349/2000, vlb -2.788471, \u001b[31m   time: 0.702287 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.311867 (-2.057936)\n",
      "\u001b[0m\n",
      "it 350/2000, vlb -2.852033, \u001b[31m   time: 0.696545 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.281101 (-2.057936)\n",
      "\u001b[0m\n",
      "it 351/2000, vlb -2.823932, \u001b[31m   time: 0.697653 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.264631 (-2.057936)\n",
      "\u001b[0m\n",
      "it 352/2000, vlb -2.789295, \u001b[31m   time: 0.699525 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.090330 (-2.057936)\n",
      "\u001b[0m\n",
      "it 353/2000, vlb -2.874635, \u001b[31m   time: 0.690906 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.171448 (-2.057936)\n",
      "\u001b[0m\n",
      "it 354/2000, vlb -2.790507, \u001b[31m   time: 0.684420 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.296802 (-2.057936)\n",
      "\u001b[0m\n",
      "it 355/2000, vlb -2.798415, \u001b[31m   time: 0.699526 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.211168 (-2.057936)\n",
      "\u001b[0m\n",
      "it 356/2000, vlb -2.839779, \u001b[31m   time: 0.694438 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.170195 (-2.057936)\n",
      "\u001b[0m\n",
      "it 357/2000, vlb -2.846270, \u001b[31m   time: 0.699160 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.278514 (-2.057936)\n",
      "\u001b[0m\n",
      "it 358/2000, vlb -2.789750, \u001b[31m   time: 0.685931 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.255480 (-2.057936)\n",
      "\u001b[0m\n",
      "it 359/2000, vlb -2.896021, \u001b[31m   time: 0.752020 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.186889 (-2.057936)\n",
      "\u001b[0m\n",
      "it 360/2000, vlb -2.835259, \u001b[31m   time: 0.714714 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.185975 (-2.057936)\n",
      "\u001b[0m\n",
      "it 361/2000, vlb -2.810228, \u001b[31m   time: 0.687823 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.295773 (-2.057936)\n",
      "\u001b[0m\n",
      "it 362/2000, vlb -2.789543, \u001b[31m   time: 0.715186 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.163514 (-2.057936)\n",
      "\u001b[0m\n",
      "it 363/2000, vlb -2.820597, \u001b[31m   time: 0.742460 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.162952 (-2.057936)\n",
      "\u001b[0m\n",
      "it 364/2000, vlb -2.807265, \u001b[31m   time: 0.685952 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.212102 (-2.057936)\n",
      "\u001b[0m\n",
      "it 365/2000, vlb -2.805907, \u001b[31m   time: 0.688678 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.043894 (-2.057936)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 366/2000, vlb -2.775415, \u001b[31m   time: 0.697946 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.147191 (-2.043894)\n",
      "\u001b[0m\n",
      "it 367/2000, vlb -2.851249, \u001b[31m   time: 0.682631 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.177385 (-2.043894)\n",
      "\u001b[0m\n",
      "it 368/2000, vlb -2.828647, \u001b[31m   time: 0.735554 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.152969 (-2.043894)\n",
      "\u001b[0m\n",
      "it 369/2000, vlb -2.806391, \u001b[31m   time: 0.693114 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.235525 (-2.043894)\n",
      "\u001b[0m\n",
      "it 370/2000, vlb -2.761632, \u001b[31m   time: 0.701643 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.163073 (-2.043894)\n",
      "\u001b[0m\n",
      "it 371/2000, vlb -2.791879, \u001b[31m   time: 0.693755 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.182424 (-2.043894)\n",
      "\u001b[0m\n",
      "it 372/2000, vlb -2.758498, \u001b[31m   time: 0.701360 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.170845 (-2.043894)\n",
      "\u001b[0m\n",
      "it 373/2000, vlb -2.741057, \u001b[31m   time: 0.698658 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.132655 (-2.043894)\n",
      "\u001b[0m\n",
      "it 374/2000, vlb -2.774749, \u001b[31m   time: 0.681883 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.310914 (-2.043894)\n",
      "\u001b[0m\n",
      "it 375/2000, vlb -2.787770, \u001b[31m   time: 0.681999 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.141591 (-2.043894)\n",
      "\u001b[0m\n",
      "it 376/2000, vlb -2.825864, \u001b[31m   time: 0.683997 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.338080 (-2.043894)\n",
      "\u001b[0m\n",
      "it 377/2000, vlb -2.796846, \u001b[31m   time: 0.773605 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.225596 (-2.043894)\n",
      "\u001b[0m\n",
      "it 378/2000, vlb -2.808040, \u001b[31m   time: 0.713734 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.123673 (-2.043894)\n",
      "\u001b[0m\n",
      "it 379/2000, vlb -2.774615, \u001b[31m   time: 0.684834 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.199166 (-2.043894)\n",
      "\u001b[0m\n",
      "it 380/2000, vlb -2.775625, \u001b[31m   time: 0.684827 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.309327 (-2.043894)\n",
      "\u001b[0m\n",
      "it 381/2000, vlb -2.777847, \u001b[31m   time: 0.699119 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.177257 (-2.043894)\n",
      "\u001b[0m\n",
      "it 382/2000, vlb -2.736243, \u001b[31m   time: 0.694004 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.069164 (-2.043894)\n",
      "\u001b[0m\n",
      "it 383/2000, vlb -2.739200, \u001b[31m   time: 0.696540 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.191500 (-2.043894)\n",
      "\u001b[0m\n",
      "it 384/2000, vlb -2.785635, \u001b[31m   time: 0.686340 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.130351 (-2.043894)\n",
      "\u001b[0m\n",
      "it 385/2000, vlb -2.764466, \u001b[31m   time: 0.713898 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.046915 (-2.043894)\n",
      "\u001b[0m\n",
      "it 386/2000, vlb -2.842043, \u001b[31m   time: 0.697162 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.180068 (-2.043894)\n",
      "\u001b[0m\n",
      "it 387/2000, vlb -2.778302, \u001b[31m   time: 0.698872 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.334853 (-2.043894)\n",
      "\u001b[0m\n",
      "it 388/2000, vlb -2.808215, \u001b[31m   time: 0.685928 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.215704 (-2.043894)\n",
      "\u001b[0m\n",
      "it 389/2000, vlb -2.827546, \u001b[31m   time: 0.717090 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.208730 (-2.043894)\n",
      "\u001b[0m\n",
      "it 390/2000, vlb -2.807586, \u001b[31m   time: 0.702181 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.263889 (-2.043894)\n",
      "\u001b[0m\n",
      "it 391/2000, vlb -2.732313, \u001b[31m   time: 0.697161 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.239028 (-2.043894)\n",
      "\u001b[0m\n",
      "it 392/2000, vlb -2.779078, \u001b[31m   time: 0.693846 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.218996 (-2.043894)\n",
      "\u001b[0m\n",
      "it 393/2000, vlb -2.732532, \u001b[31m   time: 0.684358 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.234182 (-2.043894)\n",
      "\u001b[0m\n",
      "it 394/2000, vlb -2.792754, \u001b[31m   time: 0.707891 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.182581 (-2.043894)\n",
      "\u001b[0m\n",
      "it 395/2000, vlb -2.756375, \u001b[31m   time: 0.689081 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.186493 (-2.043894)\n",
      "\u001b[0m\n",
      "it 396/2000, vlb -2.758683, \u001b[31m   time: 0.681327 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.206933 (-2.043894)\n",
      "\u001b[0m\n",
      "it 397/2000, vlb -2.743214, \u001b[31m   time: 0.701093 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.170325 (-2.043894)\n",
      "\u001b[0m\n",
      "it 398/2000, vlb -2.782893, \u001b[31m   time: 0.714880 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.137901 (-2.043894)\n",
      "\u001b[0m\n",
      "it 399/2000, vlb -2.790305, \u001b[31m   time: 0.703145 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.201628 (-2.043894)\n",
      "\u001b[0m\n",
      "it 400/2000, vlb -2.783289, \u001b[31m   time: 0.690371 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.140014 (-2.043894)\n",
      "\u001b[0m\n",
      "it 401/2000, vlb -2.781742, \u001b[31m   time: 0.679778 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.216932 (-2.043894)\n",
      "\u001b[0m\n",
      "it 402/2000, vlb -2.790474, \u001b[31m   time: 0.684848 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.206616 (-2.043894)\n",
      "\u001b[0m\n",
      "it 403/2000, vlb -2.728526, \u001b[31m   time: 0.701143 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.201982 (-2.043894)\n",
      "\u001b[0m\n",
      "it 404/2000, vlb -2.770531, \u001b[31m   time: 0.713122 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.170592 (-2.043894)\n",
      "\u001b[0m\n",
      "it 405/2000, vlb -2.770586, \u001b[31m   time: 0.690850 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.320578 (-2.043894)\n",
      "\u001b[0m\n",
      "it 406/2000, vlb -2.769490, \u001b[31m   time: 0.684182 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.185835 (-2.043894)\n",
      "\u001b[0m\n",
      "it 407/2000, vlb -2.744462, \u001b[31m   time: 0.695009 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.141704 (-2.043894)\n",
      "\u001b[0m\n",
      "it 408/2000, vlb -2.793889, \u001b[31m   time: 0.733106 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.211936 (-2.043894)\n",
      "\u001b[0m\n",
      "it 409/2000, vlb -2.780710, \u001b[31m   time: 0.689113 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.181174 (-2.043894)\n",
      "\u001b[0m\n",
      "it 410/2000, vlb -2.792354, \u001b[31m   time: 0.693907 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.142916 (-2.043894)\n",
      "\u001b[0m\n",
      "it 411/2000, vlb -2.790650, \u001b[31m   time: 0.698985 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.183675 (-2.043894)\n",
      "\u001b[0m\n",
      "it 412/2000, vlb -2.767256, \u001b[31m   time: 0.700646 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.277348 (-2.043894)\n",
      "\u001b[0m\n",
      "it 413/2000, vlb -2.785454, \u001b[31m   time: 0.738561 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.271687 (-2.043894)\n",
      "\u001b[0m\n",
      "it 414/2000, vlb -2.797549, \u001b[31m   time: 0.733375 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.082600 (-2.043894)\n",
      "\u001b[0m\n",
      "it 415/2000, vlb -2.750489, \u001b[31m   time: 0.694906 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.108676 (-2.043894)\n",
      "\u001b[0m\n",
      "it 416/2000, vlb -2.788201, \u001b[31m   time: 0.734622 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.103538 (-2.043894)\n",
      "\u001b[0m\n",
      "it 417/2000, vlb -2.750183, \u001b[31m   time: 0.705141 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.155931 (-2.043894)\n",
      "\u001b[0m\n",
      "it 418/2000, vlb -2.740536, \u001b[31m   time: 0.692603 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.154975 (-2.043894)\n",
      "\u001b[0m\n",
      "it 419/2000, vlb -2.777522, \u001b[31m   time: 0.703542 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.195649 (-2.043894)\n",
      "\u001b[0m\n",
      "it 420/2000, vlb -2.777125, \u001b[31m   time: 0.691105 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.176723 (-2.043894)\n",
      "\u001b[0m\n",
      "it 421/2000, vlb -2.827548, \u001b[31m   time: 0.730625 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.179794 (-2.043894)\n",
      "\u001b[0m\n",
      "it 422/2000, vlb -2.783346, \u001b[31m   time: 0.729078 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.130113 (-2.043894)\n",
      "\u001b[0m\n",
      "it 423/2000, vlb -2.722304, \u001b[31m   time: 0.762599 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.147803 (-2.043894)\n",
      "\u001b[0m\n",
      "it 424/2000, vlb -2.744847, \u001b[31m   time: 0.686683 seconds\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m    vlb -2.207596 (-2.043894)\n",
      "\u001b[0m\n",
      "it 425/2000, vlb -2.708654, \u001b[31m   time: 0.714118 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.210960 (-2.043894)\n",
      "\u001b[0m\n",
      "it 426/2000, vlb -2.778585, \u001b[31m   time: 0.683151 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.260309 (-2.043894)\n",
      "\u001b[0m\n",
      "it 427/2000, vlb -2.743289, \u001b[31m   time: 0.700035 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.093877 (-2.043894)\n",
      "\u001b[0m\n",
      "it 428/2000, vlb -2.769493, \u001b[31m   time: 0.698097 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.259317 (-2.043894)\n",
      "\u001b[0m\n",
      "it 429/2000, vlb -2.747008, \u001b[31m   time: 0.684161 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.202420 (-2.043894)\n",
      "\u001b[0m\n",
      "it 430/2000, vlb -2.776110, \u001b[31m   time: 0.698328 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.278796 (-2.043894)\n",
      "\u001b[0m\n",
      "it 431/2000, vlb -2.778138, \u001b[31m   time: 0.704997 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.087617 (-2.043894)\n",
      "\u001b[0m\n",
      "it 432/2000, vlb -2.713357, \u001b[31m   time: 0.685193 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.281676 (-2.043894)\n",
      "\u001b[0m\n",
      "it 433/2000, vlb -2.768932, \u001b[31m   time: 0.734071 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.287826 (-2.043894)\n",
      "\u001b[0m\n",
      "it 434/2000, vlb -2.735997, \u001b[31m   time: 0.752248 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.187082 (-2.043894)\n",
      "\u001b[0m\n",
      "it 435/2000, vlb -2.768670, \u001b[31m   time: 0.733778 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.220824 (-2.043894)\n",
      "\u001b[0m\n",
      "it 436/2000, vlb -2.736110, \u001b[31m   time: 0.726085 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.206737 (-2.043894)\n",
      "\u001b[0m\n",
      "it 437/2000, vlb -2.800667, \u001b[31m   time: 0.717186 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.333652 (-2.043894)\n",
      "\u001b[0m\n",
      "it 438/2000, vlb -2.742554, \u001b[31m   time: 0.702283 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.168495 (-2.043894)\n",
      "\u001b[0m\n",
      "it 439/2000, vlb -2.755618, \u001b[31m   time: 0.724092 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.056727 (-2.043894)\n",
      "\u001b[0m\n",
      "it 440/2000, vlb -2.742563, \u001b[31m   time: 0.767749 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.171561 (-2.043894)\n",
      "\u001b[0m\n",
      "it 441/2000, vlb -2.759586, \u001b[31m   time: 0.814780 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.275591 (-2.043894)\n",
      "\u001b[0m\n",
      "it 442/2000, vlb -2.743859, \u001b[31m   time: 0.730321 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.310626 (-2.043894)\n",
      "\u001b[0m\n",
      "it 443/2000, vlb -2.748075, \u001b[31m   time: 0.716271 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.211030 (-2.043894)\n",
      "\u001b[0m\n",
      "it 444/2000, vlb -2.742017, \u001b[31m   time: 0.758491 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.185937 (-2.043894)\n",
      "\u001b[0m\n",
      "it 445/2000, vlb -2.764992, \u001b[31m   time: 0.704957 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.229957 (-2.043894)\n",
      "\u001b[0m\n",
      "it 446/2000, vlb -2.713066, \u001b[31m   time: 0.705830 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.170993 (-2.043894)\n",
      "\u001b[0m\n",
      "it 447/2000, vlb -2.777960, \u001b[31m   time: 0.704296 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.264235 (-2.043894)\n",
      "\u001b[0m\n",
      "it 448/2000, vlb -2.706996, \u001b[31m   time: 0.696166 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.161532 (-2.043894)\n",
      "\u001b[0m\n",
      "it 449/2000, vlb -2.799682, \u001b[31m   time: 0.699651 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.218413 (-2.043894)\n",
      "\u001b[0m\n",
      "it 450/2000, vlb -2.787657, \u001b[31m   time: 0.747804 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.234519 (-2.043894)\n",
      "\u001b[0m\n",
      "it 451/2000, vlb -2.733118, \u001b[31m   time: 0.762827 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.252941 (-2.043894)\n",
      "\u001b[0m\n",
      "it 452/2000, vlb -2.737801, \u001b[31m   time: 0.768713 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.168366 (-2.043894)\n",
      "\u001b[0m\n",
      "it 453/2000, vlb -2.756562, \u001b[31m   time: 0.717262 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.138451 (-2.043894)\n",
      "\u001b[0m\n",
      "it 454/2000, vlb -2.786824, \u001b[31m   time: 0.714609 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.229494 (-2.043894)\n",
      "\u001b[0m\n",
      "it 455/2000, vlb -2.781122, \u001b[31m   time: 0.725086 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.075901 (-2.043894)\n",
      "\u001b[0m\n",
      "it 456/2000, vlb -2.768324, \u001b[31m   time: 0.687186 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.142825 (-2.043894)\n",
      "\u001b[0m\n",
      "it 457/2000, vlb -2.735405, \u001b[31m   time: 0.698305 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.193390 (-2.043894)\n",
      "\u001b[0m\n",
      "it 458/2000, vlb -2.736835, \u001b[31m   time: 0.682100 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.257380 (-2.043894)\n",
      "\u001b[0m\n",
      "it 459/2000, vlb -2.706224, \u001b[31m   time: 0.719595 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.267548 (-2.043894)\n",
      "\u001b[0m\n",
      "it 460/2000, vlb -2.745599, \u001b[31m   time: 0.713367 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.211194 (-2.043894)\n",
      "\u001b[0m\n",
      "it 461/2000, vlb -2.773314, \u001b[31m   time: 0.719052 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.193347 (-2.043894)\n",
      "\u001b[0m\n",
      "it 462/2000, vlb -2.799241, \u001b[31m   time: 0.710948 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.258835 (-2.043894)\n",
      "\u001b[0m\n",
      "it 463/2000, vlb -2.774735, \u001b[31m   time: 0.701233 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.123402 (-2.043894)\n",
      "\u001b[0m\n",
      "it 464/2000, vlb -2.731312, \u001b[31m   time: 0.723319 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.207364 (-2.043894)\n",
      "\u001b[0m\n",
      "it 465/2000, vlb -2.758085, \u001b[31m   time: 0.712641 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.197215 (-2.043894)\n",
      "\u001b[0m\n",
      "it 466/2000, vlb -2.800504, \u001b[31m   time: 0.730169 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.128615 (-2.043894)\n",
      "\u001b[0m\n",
      "it 467/2000, vlb -2.760026, \u001b[31m   time: 0.771562 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.262131 (-2.043894)\n",
      "\u001b[0m\n",
      "it 468/2000, vlb -2.694537, \u001b[31m   time: 0.686106 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.229281 (-2.043894)\n",
      "\u001b[0m\n",
      "it 469/2000, vlb -2.776320, \u001b[31m   time: 0.686706 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.131562 (-2.043894)\n",
      "\u001b[0m\n",
      "it 470/2000, vlb -2.712101, \u001b[31m   time: 0.702148 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.177889 (-2.043894)\n",
      "\u001b[0m\n",
      "it 471/2000, vlb -2.772629, \u001b[31m   time: 0.739053 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.036573 (-2.043894)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 472/2000, vlb -2.756973, \u001b[31m   time: 0.691677 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.077035 (-2.036573)\n",
      "\u001b[0m\n",
      "it 473/2000, vlb -2.767446, \u001b[31m   time: 0.714046 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.263905 (-2.036573)\n",
      "\u001b[0m\n",
      "it 474/2000, vlb -2.699329, \u001b[31m   time: 0.700245 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.310790 (-2.036573)\n",
      "\u001b[0m\n",
      "it 475/2000, vlb -2.736603, \u001b[31m   time: 0.711126 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.084969 (-2.036573)\n",
      "\u001b[0m\n",
      "it 476/2000, vlb -2.789919, \u001b[31m   time: 0.766001 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.131605 (-2.036573)\n",
      "\u001b[0m\n",
      "it 477/2000, vlb -2.777159, \u001b[31m   time: 0.758317 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.232087 (-2.036573)\n",
      "\u001b[0m\n",
      "it 478/2000, vlb -2.757908, \u001b[31m   time: 0.774642 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.192619 (-2.036573)\n",
      "\u001b[0m\n",
      "it 479/2000, vlb -2.769235, \u001b[31m   time: 0.800893 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.115018 (-2.036573)\n",
      "\u001b[0m\n",
      "it 480/2000, vlb -2.761717, \u001b[31m   time: 0.756009 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.207735 (-2.036573)\n",
      "\u001b[0m\n",
      "it 481/2000, vlb -2.774032, \u001b[31m   time: 0.739942 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.237147 (-2.036573)\n",
      "\u001b[0m\n",
      "it 482/2000, vlb -2.716389, \u001b[31m   time: 0.703148 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.243229 (-2.036573)\n",
      "\u001b[0m\n",
      "it 483/2000, vlb -2.711830, \u001b[31m   time: 0.683147 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.205362 (-2.036573)\n",
      "\u001b[0m\n",
      "it 484/2000, vlb -2.699943, \u001b[31m   time: 0.715883 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.197684 (-2.036573)\n",
      "\u001b[0m\n",
      "it 485/2000, vlb -2.767066, \u001b[31m   time: 0.718110 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.353302 (-2.036573)\n",
      "\u001b[0m\n",
      "it 486/2000, vlb -2.743010, \u001b[31m   time: 0.738077 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.088479 (-2.036573)\n",
      "\u001b[0m\n",
      "it 487/2000, vlb -2.781025, \u001b[31m   time: 0.725089 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.266903 (-2.036573)\n",
      "\u001b[0m\n",
      "it 488/2000, vlb -2.700399, \u001b[31m   time: 0.719106 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.144429 (-2.036573)\n",
      "\u001b[0m\n",
      "it 489/2000, vlb -2.774219, \u001b[31m   time: 0.807284 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.133505 (-2.036573)\n",
      "\u001b[0m\n",
      "it 490/2000, vlb -2.810291, \u001b[31m   time: 0.686077 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.291685 (-2.036573)\n",
      "\u001b[0m\n",
      "it 491/2000, vlb -2.742051, \u001b[31m   time: 0.714592 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.200863 (-2.036573)\n",
      "\u001b[0m\n",
      "it 492/2000, vlb -2.693591, \u001b[31m   time: 0.694416 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.183369 (-2.036573)\n",
      "\u001b[0m\n",
      "it 493/2000, vlb -2.774890, \u001b[31m   time: 0.689934 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.354095 (-2.036573)\n",
      "\u001b[0m\n",
      "it 494/2000, vlb -2.703550, \u001b[31m   time: 0.718221 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.240417 (-2.036573)\n",
      "\u001b[0m\n",
      "it 495/2000, vlb -2.735144, \u001b[31m   time: 0.693854 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.255652 (-2.036573)\n",
      "\u001b[0m\n",
      "it 496/2000, vlb -2.714535, \u001b[31m   time: 0.771105 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.090703 (-2.036573)\n",
      "\u001b[0m\n",
      "it 497/2000, vlb -2.745147, \u001b[31m   time: 0.696874 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.096172 (-2.036573)\n",
      "\u001b[0m\n",
      "it 498/2000, vlb -2.689770, \u001b[31m   time: 0.685156 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.191353 (-2.036573)\n",
      "\u001b[0m\n",
      "it 499/2000, vlb -2.730770, \u001b[31m   time: 0.698483 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.203472 (-2.036573)\n",
      "\u001b[0m\n",
      "it 500/2000, vlb -2.744371, \u001b[31m   time: 0.683916 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.163388 (-2.036573)\n",
      "\u001b[0m\n",
      "it 501/2000, vlb -2.741262, \u001b[31m   time: 0.681598 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.184932 (-2.036573)\n",
      "\u001b[0m\n",
      "it 502/2000, vlb -2.714629, \u001b[31m   time: 0.681036 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.098112 (-2.036573)\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 503/2000, vlb -2.734505, \u001b[31m   time: 0.701599 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.226565 (-2.036573)\n",
      "\u001b[0m\n",
      "it 504/2000, vlb -2.747137, \u001b[31m   time: 0.694412 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.241108 (-2.036573)\n",
      "\u001b[0m\n",
      "it 505/2000, vlb -2.726878, \u001b[31m   time: 0.720916 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.196774 (-2.036573)\n",
      "\u001b[0m\n",
      "it 506/2000, vlb -2.691871, \u001b[31m   time: 0.694359 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.204791 (-2.036573)\n",
      "\u001b[0m\n",
      "it 507/2000, vlb -2.662539, \u001b[31m   time: 0.745527 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.311679 (-2.036573)\n",
      "\u001b[0m\n",
      "it 508/2000, vlb -2.698276, \u001b[31m   time: 0.715113 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.162030 (-2.036573)\n",
      "\u001b[0m\n",
      "it 509/2000, vlb -2.760982, \u001b[31m   time: 0.716092 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.184859 (-2.036573)\n",
      "\u001b[0m\n",
      "it 510/2000, vlb -2.689157, \u001b[31m   time: 0.695831 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.184807 (-2.036573)\n",
      "\u001b[0m\n",
      "it 511/2000, vlb -2.722954, \u001b[31m   time: 0.754309 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.319658 (-2.036573)\n",
      "\u001b[0m\n",
      "it 512/2000, vlb -2.684918, \u001b[31m   time: 0.786698 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.301517 (-2.036573)\n",
      "\u001b[0m\n",
      "it 513/2000, vlb -2.741891, \u001b[31m   time: 0.697893 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.207156 (-2.036573)\n",
      "\u001b[0m\n",
      "it 514/2000, vlb -2.775144, \u001b[31m   time: 0.688173 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.139011 (-2.036573)\n",
      "\u001b[0m\n",
      "it 515/2000, vlb -2.713758, \u001b[31m   time: 0.693805 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.090626 (-2.036573)\n",
      "\u001b[0m\n",
      "it 516/2000, vlb -2.756239, \u001b[31m   time: 0.726978 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.297772 (-2.036573)\n",
      "\u001b[0m\n",
      "it 517/2000, vlb -2.705046, \u001b[31m   time: 0.685588 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.258505 (-2.036573)\n",
      "\u001b[0m\n",
      "it 518/2000, vlb -2.689278, \u001b[31m   time: 0.699156 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.241571 (-2.036573)\n",
      "\u001b[0m\n",
      "it 519/2000, vlb -2.791406, \u001b[31m   time: 0.696847 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.147746 (-2.036573)\n",
      "\u001b[0m\n",
      "it 520/2000, vlb -2.697475, \u001b[31m   time: 0.736061 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.215059 (-2.036573)\n",
      "\u001b[0m\n",
      "it 521/2000, vlb -2.673381, \u001b[31m   time: 0.766980 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.152076 (-2.036573)\n",
      "\u001b[0m\n",
      "it 522/2000, vlb -2.741101, \u001b[31m   time: 0.787844 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.044749 (-2.036573)\n",
      "\u001b[0m\n",
      "it 523/2000, vlb -2.703803, \u001b[31m   time: 0.693172 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.283927 (-2.036573)\n",
      "\u001b[0m\n",
      "it 524/2000, vlb -2.755357, \u001b[31m   time: 0.688130 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.216686 (-2.036573)\n",
      "\u001b[0m\n",
      "it 525/2000, vlb -2.696157, \u001b[31m   time: 0.681984 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.137493 (-2.036573)\n",
      "\u001b[0m\n",
      "it 526/2000, vlb -2.739162, \u001b[31m   time: 0.731146 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.207980 (-2.036573)\n",
      "\u001b[0m\n",
      "it 527/2000, vlb -2.693229, \u001b[31m   time: 0.844770 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.242084 (-2.036573)\n",
      "\u001b[0m\n",
      "it 528/2000, vlb -2.739132, \u001b[31m   time: 0.891203 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.234284 (-2.036573)\n",
      "\u001b[0m\n",
      "it 529/2000, vlb -2.741275, \u001b[31m   time: 0.790915 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.291808 (-2.036573)\n",
      "\u001b[0m\n",
      "it 530/2000, vlb -2.680514, \u001b[31m   time: 0.738810 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.203754 (-2.036573)\n",
      "\u001b[0m\n",
      "it 531/2000, vlb -2.763924, \u001b[31m   time: 0.718365 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.170595 (-2.036573)\n",
      "\u001b[0m\n",
      "it 532/2000, vlb -2.726446, \u001b[31m   time: 0.711131 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.176173 (-2.036573)\n",
      "\u001b[0m\n",
      "it 533/2000, vlb -2.748903, \u001b[31m   time: 0.809362 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.126646 (-2.036573)\n",
      "\u001b[0m\n",
      "it 534/2000, vlb -2.688532, \u001b[31m   time: 0.868109 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.285049 (-2.036573)\n",
      "\u001b[0m\n",
      "it 535/2000, vlb -2.776838, \u001b[31m   time: 0.794852 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.286359 (-2.036573)\n",
      "\u001b[0m\n",
      "it 536/2000, vlb -2.764794, \u001b[31m   time: 0.720103 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.133530 (-2.036573)\n",
      "\u001b[0m\n",
      "it 537/2000, vlb -2.722405, \u001b[31m   time: 0.728866 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.187036 (-2.036573)\n",
      "\u001b[0m\n",
      "it 538/2000, vlb -2.724625, \u001b[31m   time: 0.736060 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.315435 (-2.036573)\n",
      "\u001b[0m\n",
      "it 539/2000, vlb -2.705267, \u001b[31m   time: 0.732615 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.124929 (-2.036573)\n",
      "\u001b[0m\n",
      "it 540/2000, vlb -2.749610, \u001b[31m   time: 0.721590 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.139882 (-2.036573)\n",
      "\u001b[0m\n",
      "it 541/2000, vlb -2.761292, \u001b[31m   time: 0.701616 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.241847 (-2.036573)\n",
      "\u001b[0m\n",
      "it 542/2000, vlb -2.691111, \u001b[31m   time: 0.787038 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.148927 (-2.036573)\n",
      "\u001b[0m\n",
      "it 543/2000, vlb -2.731251, \u001b[31m   time: 0.798195 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.080578 (-2.036573)\n",
      "\u001b[0m\n",
      "it 544/2000, vlb -2.696786, \u001b[31m   time: 0.739025 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.147969 (-2.036573)\n",
      "\u001b[0m\n",
      "it 545/2000, vlb -2.686983, \u001b[31m   time: 0.684715 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.153745 (-2.036573)\n",
      "\u001b[0m\n",
      "it 546/2000, vlb -2.719747, \u001b[31m   time: 0.696055 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.132069 (-2.036573)\n",
      "\u001b[0m\n",
      "it 547/2000, vlb -2.767144, \u001b[31m   time: 0.697148 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.344101 (-2.036573)\n",
      "\u001b[0m\n",
      "it 548/2000, vlb -2.731426, \u001b[31m   time: 0.695156 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.101665 (-2.036573)\n",
      "\u001b[0m\n",
      "it 549/2000, vlb -2.713506, \u001b[31m   time: 0.695168 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.231204 (-2.036573)\n",
      "\u001b[0m\n",
      "it 550/2000, vlb -2.711511, \u001b[31m   time: 0.711332 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.186518 (-2.036573)\n",
      "\u001b[0m\n",
      "it 551/2000, vlb -2.729136, \u001b[31m   time: 0.697651 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.244544 (-2.036573)\n",
      "\u001b[0m\n",
      "it 552/2000, vlb -2.711595, \u001b[31m   time: 0.701495 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.087925 (-2.036573)\n",
      "\u001b[0m\n",
      "it 553/2000, vlb -2.754845, \u001b[31m   time: 0.746909 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.172715 (-2.036573)\n",
      "\u001b[0m\n",
      "it 554/2000, vlb -2.695308, \u001b[31m   time: 0.711128 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.172138 (-2.036573)\n",
      "\u001b[0m\n",
      "it 555/2000, vlb -2.730271, \u001b[31m   time: 0.709234 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.142332 (-2.036573)\n",
      "\u001b[0m\n",
      "it 556/2000, vlb -2.721197, \u001b[31m   time: 0.685195 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.287277 (-2.036573)\n",
      "\u001b[0m\n",
      "it 557/2000, vlb -2.670600, \u001b[31m   time: 0.697163 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.164874 (-2.036573)\n",
      "\u001b[0m\n",
      "it 558/2000, vlb -2.730351, \u001b[31m   time: 0.703556 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.287276 (-2.036573)\n",
      "\u001b[0m\n",
      "it 559/2000, vlb -2.730941, \u001b[31m   time: 0.792851 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.332687 (-2.036573)\n",
      "\u001b[0m\n",
      "it 560/2000, vlb -2.786506, \u001b[31m   time: 0.683202 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.086027 (-2.036573)\n",
      "\u001b[0m\n",
      "it 561/2000, vlb -2.686045, \u001b[31m   time: 0.798916 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.106324 (-2.036573)\n",
      "\u001b[0m\n",
      "it 562/2000, vlb -2.727805, \u001b[31m   time: 0.796412 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.109466 (-2.036573)\n",
      "\u001b[0m\n",
      "it 563/2000, vlb -2.713259, \u001b[31m   time: 0.748041 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.272809 (-2.036573)\n",
      "\u001b[0m\n",
      "it 564/2000, vlb -2.729067, \u001b[31m   time: 1.006845 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.215360 (-2.036573)\n",
      "\u001b[0m\n",
      "it 565/2000, vlb -2.677899, \u001b[31m   time: 0.714121 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.133889 (-2.036573)\n",
      "\u001b[0m\n",
      "it 566/2000, vlb -2.778652, \u001b[31m   time: 0.784164 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.191261 (-2.036573)\n",
      "\u001b[0m\n",
      "it 567/2000, vlb -2.695696, \u001b[31m   time: 0.718893 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.210438 (-2.036573)\n",
      "\u001b[0m\n",
      "it 568/2000, vlb -2.734762, \u001b[31m   time: 0.717017 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.170627 (-2.036573)\n",
      "\u001b[0m\n",
      "it 569/2000, vlb -2.692186, \u001b[31m   time: 0.745167 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.272243 (-2.036573)\n",
      "\u001b[0m\n",
      "it 570/2000, vlb -2.710615, \u001b[31m   time: 0.724105 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.250269 (-2.036573)\n",
      "\u001b[0m\n",
      "it 571/2000, vlb -2.709682, \u001b[31m   time: 0.775009 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.135287 (-2.036573)\n",
      "\u001b[0m\n",
      "it 572/2000, vlb -2.684951, \u001b[31m   time: 0.742070 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.164649 (-2.036573)\n",
      "\u001b[0m\n",
      "it 573/2000, vlb -2.731806, \u001b[31m   time: 0.698158 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.182501 (-2.036573)\n",
      "\u001b[0m\n",
      "it 574/2000, vlb -2.722862, \u001b[31m   time: 0.695283 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.164580 (-2.036573)\n",
      "\u001b[0m\n",
      "it 575/2000, vlb -2.736867, \u001b[31m   time: 0.748896 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.200428 (-2.036573)\n",
      "\u001b[0m\n",
      "it 576/2000, vlb -2.669083, \u001b[31m   time: 0.826328 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.212063 (-2.036573)\n",
      "\u001b[0m\n",
      "it 577/2000, vlb -2.706586, \u001b[31m   time: 0.725569 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.218121 (-2.036573)\n",
      "\u001b[0m\n",
      "it 578/2000, vlb -2.716646, \u001b[31m   time: 0.765463 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.209943 (-2.036573)\n",
      "\u001b[0m\n",
      "it 579/2000, vlb -2.712876, \u001b[31m   time: 0.794408 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.266863 (-2.036573)\n",
      "\u001b[0m\n",
      "it 580/2000, vlb -2.724373, \u001b[31m   time: 0.749995 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.292791 (-2.036573)\n",
      "\u001b[0m\n",
      "it 581/2000, vlb -2.723260, \u001b[31m   time: 0.705114 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.196752 (-2.036573)\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 582/2000, vlb -2.732912, \u001b[31m   time: 0.716084 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.178544 (-2.036573)\n",
      "\u001b[0m\n",
      "it 583/2000, vlb -2.736286, \u001b[31m   time: 0.744534 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.146886 (-2.036573)\n",
      "\u001b[0m\n",
      "it 584/2000, vlb -2.711902, \u001b[31m   time: 0.696139 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.109070 (-2.036573)\n",
      "\u001b[0m\n",
      "it 585/2000, vlb -2.667521, \u001b[31m   time: 0.935500 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.122671 (-2.036573)\n",
      "\u001b[0m\n",
      "it 586/2000, vlb -2.723993, \u001b[31m   time: 0.862747 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.234972 (-2.036573)\n",
      "\u001b[0m\n",
      "it 587/2000, vlb -2.732872, \u001b[31m   time: 0.823799 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.315156 (-2.036573)\n",
      "\u001b[0m\n",
      "it 588/2000, vlb -2.775515, \u001b[31m   time: 0.825871 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.110216 (-2.036573)\n",
      "\u001b[0m\n",
      "it 589/2000, vlb -2.713912, \u001b[31m   time: 0.820887 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.227614 (-2.036573)\n",
      "\u001b[0m\n",
      "it 590/2000, vlb -2.719133, \u001b[31m   time: 0.692083 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.133394 (-2.036573)\n",
      "\u001b[0m\n",
      "it 591/2000, vlb -2.700829, \u001b[31m   time: 0.729591 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.182223 (-2.036573)\n",
      "\u001b[0m\n",
      "it 592/2000, vlb -2.696868, \u001b[31m   time: 0.723998 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.232917 (-2.036573)\n",
      "\u001b[0m\n",
      "it 593/2000, vlb -2.699206, \u001b[31m   time: 0.702642 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.268172 (-2.036573)\n",
      "\u001b[0m\n",
      "it 594/2000, vlb -2.713372, \u001b[31m   time: 0.710791 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.214713 (-2.036573)\n",
      "\u001b[0m\n",
      "it 595/2000, vlb -2.652847, \u001b[31m   time: 0.876685 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.150181 (-2.036573)\n",
      "\u001b[0m\n",
      "it 596/2000, vlb -2.684209, \u001b[31m   time: 0.770478 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.226771 (-2.036573)\n",
      "\u001b[0m\n",
      "it 597/2000, vlb -2.691393, \u001b[31m   time: 0.742048 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.186623 (-2.036573)\n",
      "\u001b[0m\n",
      "it 598/2000, vlb -2.734327, \u001b[31m   time: 0.990899 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.222663 (-2.036573)\n",
      "\u001b[0m\n",
      "it 599/2000, vlb -2.664770, \u001b[31m   time: 0.904489 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.158855 (-2.036573)\n",
      "\u001b[0m\n",
      "it 600/2000, vlb -2.717242, \u001b[31m   time: 0.834805 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.094117 (-2.036573)\n",
      "\u001b[0m\n",
      "it 601/2000, vlb -2.771076, \u001b[31m   time: 0.803880 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.361476 (-2.036573)\n",
      "\u001b[0m\n",
      "it 602/2000, vlb -2.716443, \u001b[31m   time: 0.798900 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.181282 (-2.036573)\n",
      "\u001b[0m\n",
      "it 603/2000, vlb -2.679692, \u001b[31m   time: 0.755041 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.205429 (-2.036573)\n",
      "\u001b[0m\n",
      "it 604/2000, vlb -2.742535, \u001b[31m   time: 0.695169 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.116207 (-2.036573)\n",
      "\u001b[0m\n",
      "it 605/2000, vlb -2.733819, \u001b[31m   time: 0.700155 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.064721 (-2.036573)\n",
      "\u001b[0m\n",
      "it 606/2000, vlb -2.672282, \u001b[31m   time: 0.715889 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.409281 (-2.036573)\n",
      "\u001b[0m\n",
      "it 607/2000, vlb -2.698498, \u001b[31m   time: 0.765389 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.038547 (-2.036573)\n",
      "\u001b[0m\n",
      "it 608/2000, vlb -2.724620, \u001b[31m   time: 0.735943 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.150050 (-2.036573)\n",
      "\u001b[0m\n",
      "it 609/2000, vlb -2.694302, \u001b[31m   time: 0.763219 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.335625 (-2.036573)\n",
      "\u001b[0m\n",
      "it 610/2000, vlb -2.706561, \u001b[31m   time: 0.716807 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.294153 (-2.036573)\n",
      "\u001b[0m\n",
      "it 611/2000, vlb -2.648442, \u001b[31m   time: 0.737280 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.110389 (-2.036573)\n",
      "\u001b[0m\n",
      "it 612/2000, vlb -2.698479, \u001b[31m   time: 0.815357 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.174665 (-2.036573)\n",
      "\u001b[0m\n",
      "it 613/2000, vlb -2.706644, \u001b[31m   time: 0.747034 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.149417 (-2.036573)\n",
      "\u001b[0m\n",
      "it 614/2000, vlb -2.691724, \u001b[31m   time: 0.700540 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.155648 (-2.036573)\n",
      "\u001b[0m\n",
      "it 615/2000, vlb -2.724399, \u001b[31m   time: 0.708625 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.245198 (-2.036573)\n",
      "\u001b[0m\n",
      "it 616/2000, vlb -2.698596, \u001b[31m   time: 0.699628 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.137443 (-2.036573)\n",
      "\u001b[0m\n",
      "it 617/2000, vlb -2.644904, \u001b[31m   time: 0.751394 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.235745 (-2.036573)\n",
      "\u001b[0m\n",
      "it 618/2000, vlb -2.715858, \u001b[31m   time: 0.912135 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.228426 (-2.036573)\n",
      "\u001b[0m\n",
      "it 619/2000, vlb -2.723119, \u001b[31m   time: 0.866278 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.137290 (-2.036573)\n",
      "\u001b[0m\n",
      "it 620/2000, vlb -2.700179, \u001b[31m   time: 0.946021 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.112137 (-2.036573)\n",
      "\u001b[0m\n",
      "it 621/2000, vlb -2.685352, \u001b[31m   time: 0.814365 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.217330 (-2.036573)\n",
      "\u001b[0m\n",
      "it 622/2000, vlb -2.710225, \u001b[31m   time: 0.699616 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.047966 (-2.036573)\n",
      "\u001b[0m\n",
      "it 623/2000, vlb -2.689793, \u001b[31m   time: 0.701084 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.130337 (-2.036573)\n",
      "\u001b[0m\n",
      "it 624/2000, vlb -2.705380, \u001b[31m   time: 0.696918 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.138707 (-2.036573)\n",
      "\u001b[0m\n",
      "it 625/2000, vlb -2.701123, \u001b[31m   time: 0.712060 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.078432 (-2.036573)\n",
      "\u001b[0m\n",
      "it 626/2000, vlb -2.698514, \u001b[31m   time: 0.723344 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.267060 (-2.036573)\n",
      "\u001b[0m\n",
      "it 627/2000, vlb -2.731767, \u001b[31m   time: 0.884662 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.220410 (-2.036573)\n",
      "\u001b[0m\n",
      "it 628/2000, vlb -2.713600, \u001b[31m   time: 0.793416 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.144088 (-2.036573)\n",
      "\u001b[0m\n",
      "it 629/2000, vlb -2.649501, \u001b[31m   time: 0.813863 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.094324 (-2.036573)\n",
      "\u001b[0m\n",
      "it 630/2000, vlb -2.677532, \u001b[31m   time: 0.717083 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.110198 (-2.036573)\n",
      "\u001b[0m\n",
      "it 631/2000, vlb -2.698109, \u001b[31m   time: 0.705620 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.276845 (-2.036573)\n",
      "\u001b[0m\n",
      "it 632/2000, vlb -2.712931, \u001b[31m   time: 0.782908 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.132342 (-2.036573)\n",
      "\u001b[0m\n",
      "it 633/2000, vlb -2.672762, \u001b[31m   time: 0.723067 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.164785 (-2.036573)\n",
      "\u001b[0m\n",
      "it 634/2000, vlb -2.706347, \u001b[31m   time: 0.726059 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.034814 (-2.036573)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 635/2000, vlb -2.719074, \u001b[31m   time: 0.728054 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.284873 (-2.034814)\n",
      "\u001b[0m\n",
      "it 636/2000, vlb -2.705517, \u001b[31m   time: 0.741019 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.267415 (-2.034814)\n",
      "\u001b[0m\n",
      "it 637/2000, vlb -2.748690, \u001b[31m   time: 0.684171 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.055407 (-2.034814)\n",
      "\u001b[0m\n",
      "it 638/2000, vlb -2.721162, \u001b[31m   time: 0.747004 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.305054 (-2.034814)\n",
      "\u001b[0m\n",
      "it 639/2000, vlb -2.735918, \u001b[31m   time: 0.749996 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.253033 (-2.034814)\n",
      "\u001b[0m\n",
      "it 640/2000, vlb -2.745817, \u001b[31m   time: 0.693146 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.319927 (-2.034814)\n",
      "\u001b[0m\n",
      "it 641/2000, vlb -2.691354, \u001b[31m   time: 0.856710 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.225494 (-2.034814)\n",
      "\u001b[0m\n",
      "it 642/2000, vlb -2.715847, \u001b[31m   time: 0.927521 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.191245 (-2.034814)\n",
      "\u001b[0m\n",
      "it 643/2000, vlb -2.667840, \u001b[31m   time: 0.878651 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.210924 (-2.034814)\n",
      "\u001b[0m\n",
      "it 644/2000, vlb -2.703427, \u001b[31m   time: 0.849196 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.231750 (-2.034814)\n",
      "\u001b[0m\n",
      "it 645/2000, vlb -2.686976, \u001b[31m   time: 0.890923 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.071774 (-2.034814)\n",
      "\u001b[0m\n",
      "it 646/2000, vlb -2.687831, \u001b[31m   time: 0.888989 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.199260 (-2.034814)\n",
      "\u001b[0m\n",
      "it 647/2000, vlb -2.698553, \u001b[31m   time: 0.833939 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.307041 (-2.034814)\n",
      "\u001b[0m\n",
      "it 648/2000, vlb -2.686710, \u001b[31m   time: 1.049668 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.051949 (-2.034814)\n",
      "\u001b[0m\n",
      "it 649/2000, vlb -2.743003, \u001b[31m   time: 1.029053 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.142850 (-2.034814)\n",
      "\u001b[0m\n",
      "it 650/2000, vlb -2.723463, \u001b[31m   time: 0.815784 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.334710 (-2.034814)\n",
      "\u001b[0m\n",
      "it 651/2000, vlb -2.689589, \u001b[31m   time: 0.840850 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.234839 (-2.034814)\n",
      "\u001b[0m\n",
      "it 652/2000, vlb -2.688215, \u001b[31m   time: 0.818827 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.221969 (-2.034814)\n",
      "\u001b[0m\n",
      "it 653/2000, vlb -2.698635, \u001b[31m   time: 0.711804 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.085902 (-2.034814)\n",
      "\u001b[0m\n",
      "it 654/2000, vlb -2.680691, \u001b[31m   time: 0.681202 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.210036 (-2.034814)\n",
      "\u001b[0m\n",
      "it 655/2000, vlb -2.719904, \u001b[31m   time: 0.699530 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -1.995376 (-2.034814)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 656/2000, vlb -2.700922, \u001b[31m   time: 0.730905 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.236371 (-1.995376)\n",
      "\u001b[0m\n",
      "it 657/2000, vlb -2.735956, \u001b[31m   time: 0.702460 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.201028 (-1.995376)\n",
      "\u001b[0m\n",
      "it 658/2000, vlb -2.708168, \u001b[31m   time: 0.696428 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.143314 (-1.995376)\n",
      "\u001b[0m\n",
      "it 659/2000, vlb -2.667573, \u001b[31m   time: 0.770940 seconds\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m    vlb -2.236074 (-1.995376)\n",
      "\u001b[0m\n",
      "it 660/2000, vlb -2.733121, \u001b[31m   time: 0.760966 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.307176 (-1.995376)\n",
      "\u001b[0m\n",
      "it 661/2000, vlb -2.718051, \u001b[31m   time: 0.766950 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.165196 (-1.995376)\n",
      "\u001b[0m\n",
      "it 662/2000, vlb -2.679208, \u001b[31m   time: 0.704117 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.213124 (-1.995376)\n",
      "\u001b[0m\n",
      "it 663/2000, vlb -2.656048, \u001b[31m   time: 0.710102 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.132722 (-1.995376)\n",
      "\u001b[0m\n",
      "it 664/2000, vlb -2.706052, \u001b[31m   time: 0.699131 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.215022 (-1.995376)\n",
      "\u001b[0m\n",
      "it 665/2000, vlb -2.659546, \u001b[31m   time: 0.773931 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.184735 (-1.995376)\n",
      "\u001b[0m\n",
      "it 666/2000, vlb -2.688236, \u001b[31m   time: 0.786896 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.142908 (-1.995376)\n",
      "\u001b[0m\n",
      "it 667/2000, vlb -2.656050, \u001b[31m   time: 0.773931 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.228830 (-1.995376)\n",
      "\u001b[0m\n",
      "it 668/2000, vlb -2.639480, \u001b[31m   time: 0.738533 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.216284 (-1.995376)\n",
      "\u001b[0m\n",
      "it 669/2000, vlb -2.662332, \u001b[31m   time: 0.741018 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.078763 (-1.995376)\n",
      "\u001b[0m\n",
      "it 670/2000, vlb -2.664374, \u001b[31m   time: 0.737030 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.120441 (-1.995376)\n",
      "\u001b[0m\n",
      "it 671/2000, vlb -2.689722, \u001b[31m   time: 0.685169 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.093674 (-1.995376)\n",
      "\u001b[0m\n",
      "it 672/2000, vlb -2.671872, \u001b[31m   time: 0.729051 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.195672 (-1.995376)\n",
      "\u001b[0m\n",
      "it 673/2000, vlb -2.679224, \u001b[31m   time: 0.700129 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.229226 (-1.995376)\n",
      "\u001b[0m\n",
      "it 674/2000, vlb -2.717275, \u001b[31m   time: 0.709105 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.247429 (-1.995376)\n",
      "\u001b[0m\n",
      "it 675/2000, vlb -2.645776, \u001b[31m   time: 0.769450 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.144840 (-1.995376)\n",
      "\u001b[0m\n",
      "it 676/2000, vlb -2.724768, \u001b[31m   time: 0.904581 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.106954 (-1.995376)\n",
      "\u001b[0m\n",
      "it 677/2000, vlb -2.707273, \u001b[31m   time: 0.995338 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.113377 (-1.995376)\n",
      "\u001b[0m\n",
      "it 678/2000, vlb -2.687150, \u001b[31m   time: 0.723067 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.170171 (-1.995376)\n",
      "\u001b[0m\n",
      "it 679/2000, vlb -2.640350, \u001b[31m   time: 0.696140 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.160564 (-1.995376)\n",
      "\u001b[0m\n",
      "it 680/2000, vlb -2.649394, \u001b[31m   time: 0.726059 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.221718 (-1.995376)\n",
      "\u001b[0m\n",
      "it 681/2000, vlb -2.733499, \u001b[31m   time: 0.697136 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.150727 (-1.995376)\n",
      "\u001b[0m\n",
      "it 682/2000, vlb -2.651906, \u001b[31m   time: 0.712113 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.250763 (-1.995376)\n",
      "\u001b[0m\n",
      "it 683/2000, vlb -2.676211, \u001b[31m   time: 0.744517 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.063895 (-1.995376)\n",
      "\u001b[0m\n",
      "it 684/2000, vlb -2.686968, \u001b[31m   time: 0.704624 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.179927 (-1.995376)\n",
      "\u001b[0m\n",
      "it 685/2000, vlb -2.718924, \u001b[31m   time: 0.921538 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.177310 (-1.995376)\n",
      "\u001b[0m\n",
      "it 686/2000, vlb -2.730095, \u001b[31m   time: 0.860699 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.278331 (-1.995376)\n",
      "\u001b[0m\n",
      "it 687/2000, vlb -2.670147, \u001b[31m   time: 0.889622 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.309896 (-1.995376)\n",
      "\u001b[0m\n",
      "it 688/2000, vlb -2.627052, \u001b[31m   time: 0.852135 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.137567 (-1.995376)\n",
      "\u001b[0m\n",
      "it 689/2000, vlb -2.694832, \u001b[31m   time: 0.822801 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.147507 (-1.995376)\n",
      "\u001b[0m\n",
      "it 690/2000, vlb -2.701676, \u001b[31m   time: 0.845740 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.247656 (-1.995376)\n",
      "\u001b[0m\n",
      "it 691/2000, vlb -2.726222, \u001b[31m   time: 0.917081 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.242565 (-1.995376)\n",
      "\u001b[0m\n",
      "it 692/2000, vlb -2.676101, \u001b[31m   time: 0.710102 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.175610 (-1.995376)\n",
      "\u001b[0m\n",
      "it 693/2000, vlb -2.689460, \u001b[31m   time: 0.685169 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.131486 (-1.995376)\n",
      "\u001b[0m\n",
      "it 694/2000, vlb -2.672120, \u001b[31m   time: 0.824306 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.138638 (-1.995376)\n",
      "\u001b[0m\n",
      "it 695/2000, vlb -2.653051, \u001b[31m   time: 0.728055 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.338659 (-1.995376)\n",
      "\u001b[0m\n",
      "it 696/2000, vlb -2.674446, \u001b[31m   time: 0.790885 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.295405 (-1.995376)\n",
      "\u001b[0m\n",
      "it 697/2000, vlb -2.673180, \u001b[31m   time: 0.682176 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.136619 (-1.995376)\n",
      "\u001b[0m\n",
      "it 698/2000, vlb -2.674434, \u001b[31m   time: 0.806845 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.169364 (-1.995376)\n",
      "\u001b[0m\n",
      "it 699/2000, vlb -2.660677, \u001b[31m   time: 0.931510 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.187064 (-1.995376)\n",
      "\u001b[0m\n",
      "it 700/2000, vlb -2.692399, \u001b[31m   time: 0.751990 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.229991 (-1.995376)\n",
      "\u001b[0m\n",
      "it 701/2000, vlb -2.676256, \u001b[31m   time: 0.749995 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.231215 (-1.995376)\n",
      "\u001b[0m\n",
      "it 702/2000, vlb -2.662345, \u001b[31m   time: 0.916549 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.163399 (-1.995376)\n",
      "\u001b[0m\n",
      "it 703/2000, vlb -2.723554, \u001b[31m   time: 0.775436 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.166144 (-1.995376)\n",
      "\u001b[0m\n",
      "it 704/2000, vlb -2.656869, \u001b[31m   time: 0.729052 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.354058 (-1.995376)\n",
      "\u001b[0m\n",
      "it 705/2000, vlb -2.683836, \u001b[31m   time: 0.900593 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.280221 (-1.995376)\n",
      "\u001b[0m\n",
      "it 706/2000, vlb -2.623891, \u001b[31m   time: 0.906577 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.091782 (-1.995376)\n",
      "\u001b[0m\n",
      "it 707/2000, vlb -2.659673, \u001b[31m   time: 0.799861 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.024967 (-1.995376)\n",
      "\u001b[0m\n",
      "it 708/2000, vlb -2.698103, \u001b[31m   time: 0.796870 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.091788 (-1.995376)\n",
      "\u001b[0m\n",
      "it 709/2000, vlb -2.670314, \u001b[31m   time: 0.775926 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.191908 (-1.995376)\n",
      "\u001b[0m\n",
      "it 710/2000, vlb -2.692421, \u001b[31m   time: 0.734038 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.260762 (-1.995376)\n",
      "\u001b[0m\n",
      "it 711/2000, vlb -2.702557, \u001b[31m   time: 0.711099 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.114165 (-1.995376)\n",
      "\u001b[0m\n",
      "it 712/2000, vlb -2.719611, \u001b[31m   time: 0.825792 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.133538 (-1.995376)\n",
      "\u001b[0m\n",
      "it 713/2000, vlb -2.636856, \u001b[31m   time: 0.742016 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.200696 (-1.995376)\n",
      "\u001b[0m\n",
      "it 714/2000, vlb -2.669996, \u001b[31m   time: 0.802854 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.252003 (-1.995376)\n",
      "\u001b[0m\n",
      "it 715/2000, vlb -2.735581, \u001b[31m   time: 0.869676 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.208389 (-1.995376)\n",
      "\u001b[0m\n",
      "it 716/2000, vlb -2.681952, \u001b[31m   time: 0.723066 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.210081 (-1.995376)\n",
      "\u001b[0m\n",
      "it 717/2000, vlb -2.665588, \u001b[31m   time: 0.695143 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.103404 (-1.995376)\n",
      "\u001b[0m\n",
      "it 718/2000, vlb -2.726592, \u001b[31m   time: 0.687164 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.247543 (-1.995376)\n",
      "\u001b[0m\n",
      "it 719/2000, vlb -2.712065, \u001b[31m   time: 0.937495 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.215242 (-1.995376)\n",
      "\u001b[0m\n",
      "it 720/2000, vlb -2.704701, \u001b[31m   time: 0.939940 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.168113 (-1.995376)\n",
      "\u001b[0m\n",
      "it 721/2000, vlb -2.656892, \u001b[31m   time: 0.756899 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.234060 (-1.995376)\n",
      "\u001b[0m\n",
      "it 722/2000, vlb -2.606669, \u001b[31m   time: 0.686166 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.193618 (-1.995376)\n",
      "\u001b[0m\n",
      "it 723/2000, vlb -2.675171, \u001b[31m   time: 0.689180 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.156437 (-1.995376)\n",
      "\u001b[0m\n",
      "it 724/2000, vlb -2.681972, \u001b[31m   time: 0.882165 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.135173 (-1.995376)\n",
      "\u001b[0m\n",
      "it 725/2000, vlb -2.685879, \u001b[31m   time: 0.897600 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.144549 (-1.995376)\n",
      "\u001b[0m\n",
      "it 726/2000, vlb -2.591559, \u001b[31m   time: 0.777160 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.170145 (-1.995376)\n",
      "\u001b[0m\n",
      "it 727/2000, vlb -2.687709, \u001b[31m   time: 0.752076 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.273519 (-1.995376)\n",
      "\u001b[0m\n",
      "it 728/2000, vlb -2.669713, \u001b[31m   time: 0.738472 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.238788 (-1.995376)\n",
      "\u001b[0m\n",
      "it 729/2000, vlb -2.707559, \u001b[31m   time: 0.698272 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.250081 (-1.995376)\n",
      "\u001b[0m\n",
      "it 730/2000, vlb -2.658416, \u001b[31m   time: 0.777447 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.231719 (-1.995376)\n",
      "\u001b[0m\n",
      "it 731/2000, vlb -2.656204, \u001b[31m   time: 0.738542 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.072825 (-1.995376)\n",
      "\u001b[0m\n",
      "it 732/2000, vlb -2.673447, \u001b[31m   time: 0.736709 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.230030 (-1.995376)\n",
      "\u001b[0m\n",
      "it 733/2000, vlb -2.656777, \u001b[31m   time: 0.721171 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.192088 (-1.995376)\n",
      "\u001b[0m\n",
      "it 734/2000, vlb -2.709630, \u001b[31m   time: 0.784265 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.132731 (-1.995376)\n",
      "\u001b[0m\n",
      "it 735/2000, vlb -2.742180, \u001b[31m   time: 0.736061 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.184939 (-1.995376)\n",
      "\u001b[0m\n",
      "it 736/2000, vlb -2.717976, \u001b[31m   time: 0.694168 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.185855 (-1.995376)\n",
      "\u001b[0m\n",
      "it 737/2000, vlb -2.699198, \u001b[31m   time: 0.731728 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.109644 (-1.995376)\n",
      "\u001b[0m\n",
      "it 738/2000, vlb -2.657812, \u001b[31m   time: 0.698652 seconds\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m    vlb -2.065816 (-1.995376)\n",
      "\u001b[0m\n",
      "it 739/2000, vlb -2.643009, \u001b[31m   time: 0.698120 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.225010 (-1.995376)\n",
      "\u001b[0m\n",
      "it 740/2000, vlb -2.677890, \u001b[31m   time: 0.706140 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.090471 (-1.995376)\n",
      "\u001b[0m\n",
      "it 741/2000, vlb -2.683334, \u001b[31m   time: 0.792133 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.184475 (-1.995376)\n",
      "\u001b[0m\n",
      "it 742/2000, vlb -2.674784, \u001b[31m   time: 0.730078 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.244048 (-1.995376)\n",
      "\u001b[0m\n",
      "it 743/2000, vlb -2.684073, \u001b[31m   time: 0.712141 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.246861 (-1.995376)\n",
      "\u001b[0m\n",
      "it 744/2000, vlb -2.627785, \u001b[31m   time: 0.697909 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.280935 (-1.995376)\n",
      "\u001b[0m\n",
      "it 745/2000, vlb -2.682022, \u001b[31m   time: 0.693170 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.205950 (-1.995376)\n",
      "\u001b[0m\n",
      "it 746/2000, vlb -2.624478, \u001b[31m   time: 0.695963 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.384511 (-1.995376)\n",
      "\u001b[0m\n",
      "it 747/2000, vlb -2.672402, \u001b[31m   time: 0.686010 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.199208 (-1.995376)\n",
      "\u001b[0m\n",
      "it 748/2000, vlb -2.628230, \u001b[31m   time: 0.684876 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.131675 (-1.995376)\n",
      "\u001b[0m\n",
      "it 749/2000, vlb -2.736576, \u001b[31m   time: 0.699156 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.127789 (-1.995376)\n",
      "\u001b[0m\n",
      "it 750/2000, vlb -2.719100, \u001b[31m   time: 0.697657 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.225947 (-1.995376)\n",
      "\u001b[0m\n",
      "it 751/2000, vlb -2.676647, \u001b[31m   time: 0.704020 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.092795 (-1.995376)\n",
      "\u001b[0m\n",
      "it 752/2000, vlb -2.695887, \u001b[31m   time: 0.684002 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.228019 (-1.995376)\n",
      "\u001b[0m\n",
      "it 753/2000, vlb -2.669855, \u001b[31m   time: 0.698953 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.209762 (-1.995376)\n",
      "\u001b[0m\n",
      "it 754/2000, vlb -2.701614, \u001b[31m   time: 0.719089 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.290104 (-1.995376)\n",
      "\u001b[0m\n",
      "it 755/2000, vlb -2.676992, \u001b[31m   time: 0.745038 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.180172 (-1.995376)\n",
      "\u001b[0m\n",
      "it 756/2000, vlb -2.683024, \u001b[31m   time: 0.717099 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.221952 (-1.995376)\n",
      "\u001b[0m\n",
      "it 757/2000, vlb -2.644439, \u001b[31m   time: 0.722096 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.184517 (-1.995376)\n",
      "\u001b[0m\n",
      "it 758/2000, vlb -2.709678, \u001b[31m   time: 0.748027 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.154343 (-1.995376)\n",
      "\u001b[0m\n",
      "it 759/2000, vlb -2.697041, \u001b[31m   time: 0.787951 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.220956 (-1.995376)\n",
      "\u001b[0m\n",
      "it 760/2000, vlb -2.629993, \u001b[31m   time: 0.964452 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.141172 (-1.995376)\n",
      "\u001b[0m\n",
      "it 761/2000, vlb -2.700858, \u001b[31m   time: 0.849271 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.337202 (-1.995376)\n",
      "\u001b[0m\n",
      "it 762/2000, vlb -2.653224, \u001b[31m   time: 0.699888 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.162142 (-1.995376)\n",
      "\u001b[0m\n",
      "it 763/2000, vlb -2.601296, \u001b[31m   time: 0.716112 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.309728 (-1.995376)\n",
      "\u001b[0m\n",
      "it 764/2000, vlb -2.659660, \u001b[31m   time: 0.730076 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.219546 (-1.995376)\n",
      "\u001b[0m\n",
      "it 765/2000, vlb -2.637667, \u001b[31m   time: 0.704144 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.079090 (-1.995376)\n",
      "\u001b[0m\n",
      "it 766/2000, vlb -2.723403, \u001b[31m   time: 0.692225 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.075482 (-1.995376)\n",
      "\u001b[0m\n",
      "it 767/2000, vlb -2.641449, \u001b[31m   time: 0.705976 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.153374 (-1.995376)\n",
      "\u001b[0m\n",
      "it 768/2000, vlb -2.668773, \u001b[31m   time: 0.698279 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.248077 (-1.995376)\n",
      "\u001b[0m\n",
      "it 769/2000, vlb -2.697957, \u001b[31m   time: 0.692689 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.286471 (-1.995376)\n",
      "\u001b[0m\n",
      "it 770/2000, vlb -2.640907, \u001b[31m   time: 0.698426 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.180160 (-1.995376)\n",
      "\u001b[0m\n",
      "it 771/2000, vlb -2.655776, \u001b[31m   time: 0.697205 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.235143 (-1.995376)\n",
      "\u001b[0m\n",
      "it 772/2000, vlb -2.660828, \u001b[31m   time: 0.699977 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.297164 (-1.995376)\n",
      "\u001b[0m\n",
      "it 773/2000, vlb -2.678326, \u001b[31m   time: 0.715113 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.144745 (-1.995376)\n",
      "\u001b[0m\n",
      "it 774/2000, vlb -2.626595, \u001b[31m   time: 0.728079 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.128818 (-1.995376)\n",
      "\u001b[0m\n",
      "it 775/2000, vlb -2.683004, \u001b[31m   time: 0.702146 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.311884 (-1.995376)\n",
      "\u001b[0m\n",
      "it 776/2000, vlb -2.702028, \u001b[31m   time: 0.730081 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.121674 (-1.995376)\n",
      "\u001b[0m\n",
      "it 777/2000, vlb -2.623677, \u001b[31m   time: 0.710770 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.073083 (-1.995376)\n",
      "\u001b[0m\n",
      "it 778/2000, vlb -2.662954, \u001b[31m   time: 0.715100 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.269938 (-1.995376)\n",
      "\u001b[0m\n",
      "it 779/2000, vlb -2.652959, \u001b[31m   time: 0.844344 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.248137 (-1.995376)\n",
      "\u001b[0m\n",
      "it 780/2000, vlb -2.627088, \u001b[31m   time: 0.792912 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.120227 (-1.995376)\n",
      "\u001b[0m\n",
      "it 781/2000, vlb -2.685265, \u001b[31m   time: 0.875216 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.218355 (-1.995376)\n",
      "\u001b[0m\n",
      "it 782/2000, vlb -2.658064, \u001b[31m   time: 0.713113 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.269865 (-1.995376)\n",
      "\u001b[0m\n",
      "it 783/2000, vlb -2.671164, \u001b[31m   time: 0.699156 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.154037 (-1.995376)\n",
      "\u001b[0m\n",
      "it 784/2000, vlb -2.679108, \u001b[31m   time: 1.138031 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.180358 (-1.995376)\n",
      "\u001b[0m\n",
      "it 785/2000, vlb -2.695801, \u001b[31m   time: 0.768375 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.190698 (-1.995376)\n",
      "\u001b[0m\n",
      "it 786/2000, vlb -2.651230, \u001b[31m   time: 0.697658 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.269302 (-1.995376)\n",
      "\u001b[0m\n",
      "it 787/2000, vlb -2.669257, \u001b[31m   time: 0.698322 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.215690 (-1.995376)\n",
      "\u001b[0m\n",
      "it 788/2000, vlb -2.654492, \u001b[31m   time: 0.780618 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.227125 (-1.995376)\n",
      "\u001b[0m\n",
      "it 789/2000, vlb -2.676104, \u001b[31m   time: 0.702613 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.195255 (-1.995376)\n",
      "\u001b[0m\n",
      "it 790/2000, vlb -2.696581, \u001b[31m   time: 0.726583 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.211703 (-1.995376)\n",
      "\u001b[0m\n",
      "it 791/2000, vlb -2.695838, \u001b[31m   time: 0.703147 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.256864 (-1.995376)\n",
      "\u001b[0m\n",
      "it 792/2000, vlb -2.681197, \u001b[31m   time: 0.699156 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.176590 (-1.995376)\n",
      "\u001b[0m\n",
      "it 793/2000, vlb -2.675231, \u001b[31m   time: 0.703474 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.135448 (-1.995376)\n",
      "\u001b[0m\n",
      "it 794/2000, vlb -2.670026, \u001b[31m   time: 0.733469 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.279490 (-1.995376)\n",
      "\u001b[0m\n",
      "it 795/2000, vlb -2.692539, \u001b[31m   time: 0.787924 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.207676 (-1.995376)\n",
      "\u001b[0m\n",
      "it 796/2000, vlb -2.681063, \u001b[31m   time: 0.835796 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.106129 (-1.995376)\n",
      "\u001b[0m\n",
      "it 797/2000, vlb -2.646888, \u001b[31m   time: 0.839787 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.044771 (-1.995376)\n",
      "\u001b[0m\n",
      "it 798/2000, vlb -2.679916, \u001b[31m   time: 0.740141 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.263546 (-1.995376)\n",
      "\u001b[0m\n",
      "it 799/2000, vlb -2.629528, \u001b[31m   time: 0.866825 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.087671 (-1.995376)\n",
      "\u001b[0m\n",
      "it 800/2000, vlb -2.690138, \u001b[31m   time: 0.820806 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.148657 (-1.995376)\n",
      "\u001b[0m\n",
      "it 801/2000, vlb -2.676389, \u001b[31m   time: 0.987867 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.195614 (-1.995376)\n",
      "\u001b[0m\n",
      "it 802/2000, vlb -2.637998, \u001b[31m   time: 0.944490 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.105599 (-1.995376)\n",
      "\u001b[0m\n",
      "it 803/2000, vlb -2.677987, \u001b[31m   time: 0.928540 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.060632 (-1.995376)\n",
      "\u001b[0m\n",
      "it 804/2000, vlb -2.708307, \u001b[31m   time: 0.994435 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.308760 (-1.995376)\n",
      "\u001b[0m\n",
      "it 805/2000, vlb -2.611785, \u001b[31m   time: 0.960861 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.173874 (-1.995376)\n",
      "\u001b[0m\n",
      "it 806/2000, vlb -2.672914, \u001b[31m   time: 0.759798 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.243665 (-1.995376)\n",
      "\u001b[0m\n",
      "it 807/2000, vlb -2.695491, \u001b[31m   time: 0.744286 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.131239 (-1.995376)\n",
      "\u001b[0m\n",
      "it 808/2000, vlb -2.661083, \u001b[31m   time: 0.745528 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.097945 (-1.995376)\n",
      "\u001b[0m\n",
      "it 809/2000, vlb -2.644735, \u001b[31m   time: 0.919079 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.060907 (-1.995376)\n",
      "\u001b[0m\n",
      "it 810/2000, vlb -2.599294, \u001b[31m   time: 0.757099 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.097932 (-1.995376)\n",
      "\u001b[0m\n",
      "it 811/2000, vlb -2.637720, \u001b[31m   time: 0.796744 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.126105 (-1.995376)\n",
      "\u001b[0m\n",
      "it 812/2000, vlb -2.677523, \u001b[31m   time: 0.750887 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.235685 (-1.995376)\n",
      "\u001b[0m\n",
      "it 813/2000, vlb -2.710712, \u001b[31m   time: 0.741875 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.021689 (-1.995376)\n",
      "\u001b[0m\n",
      "it 814/2000, vlb -2.633925, \u001b[31m   time: 0.787435 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.245963 (-1.995376)\n",
      "\u001b[0m\n",
      "it 815/2000, vlb -2.673660, \u001b[31m   time: 0.770749 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.109973 (-1.995376)\n",
      "\u001b[0m\n",
      "it 816/2000, vlb -2.717463, \u001b[31m   time: 0.744759 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.262907 (-1.995376)\n",
      "\u001b[0m\n",
      "it 817/2000, vlb -2.669458, \u001b[31m   time: 0.748671 seconds\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m    vlb -2.133677 (-1.995376)\n",
      "\u001b[0m\n",
      "it 818/2000, vlb -2.698073, \u001b[31m   time: 0.753018 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.111809 (-1.995376)\n",
      "\u001b[0m\n",
      "it 819/2000, vlb -2.701164, \u001b[31m   time: 0.758792 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.327444 (-1.995376)\n",
      "\u001b[0m\n",
      "it 820/2000, vlb -2.707260, \u001b[31m   time: 0.747380 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.029817 (-1.995376)\n",
      "\u001b[0m\n",
      "it 821/2000, vlb -2.632108, \u001b[31m   time: 0.759321 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.195480 (-1.995376)\n",
      "\u001b[0m\n",
      "it 822/2000, vlb -2.668401, \u001b[31m   time: 0.741049 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.155878 (-1.995376)\n",
      "\u001b[0m\n",
      "it 823/2000, vlb -2.692500, \u001b[31m   time: 0.768974 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.250560 (-1.995376)\n",
      "\u001b[0m\n",
      "it 824/2000, vlb -2.637047, \u001b[31m   time: 0.746390 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.149534 (-1.995376)\n",
      "\u001b[0m\n",
      "it 825/2000, vlb -2.685492, \u001b[31m   time: 0.747057 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.190716 (-1.995376)\n",
      "\u001b[0m\n",
      "it 826/2000, vlb -2.673742, \u001b[31m   time: 0.779578 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.251671 (-1.995376)\n",
      "\u001b[0m\n",
      "it 827/2000, vlb -2.666700, \u001b[31m   time: 0.784423 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.092890 (-1.995376)\n",
      "\u001b[0m\n",
      "it 828/2000, vlb -2.687901, \u001b[31m   time: 0.748210 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.381459 (-1.995376)\n",
      "\u001b[0m\n",
      "it 829/2000, vlb -2.600323, \u001b[31m   time: 0.750025 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.112925 (-1.995376)\n",
      "\u001b[0m\n",
      "it 830/2000, vlb -2.688508, \u001b[31m   time: 0.743043 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.125334 (-1.995376)\n",
      "\u001b[0m\n",
      "it 831/2000, vlb -2.673603, \u001b[31m   time: 0.743321 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.100316 (-1.995376)\n",
      "\u001b[0m\n",
      "it 832/2000, vlb -2.661024, \u001b[31m   time: 0.737875 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.149498 (-1.995376)\n",
      "\u001b[0m\n",
      "it 833/2000, vlb -2.653297, \u001b[31m   time: 0.742029 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.157037 (-1.995376)\n",
      "\u001b[0m\n",
      "it 834/2000, vlb -2.704860, \u001b[31m   time: 0.803752 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.053803 (-1.995376)\n",
      "\u001b[0m\n",
      "it 835/2000, vlb -2.678557, \u001b[31m   time: 0.792420 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.016468 (-1.995376)\n",
      "\u001b[0m\n",
      "it 836/2000, vlb -2.668113, \u001b[31m   time: 0.764979 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.115319 (-1.995376)\n",
      "\u001b[0m\n",
      "it 837/2000, vlb -2.675214, \u001b[31m   time: 0.747031 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.241012 (-1.995376)\n",
      "\u001b[0m\n",
      "it 838/2000, vlb -2.689767, \u001b[31m   time: 0.775953 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.204336 (-1.995376)\n",
      "\u001b[0m\n",
      "it 839/2000, vlb -2.627542, \u001b[31m   time: 0.932049 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.262314 (-1.995376)\n",
      "\u001b[0m\n",
      "it 840/2000, vlb -2.709262, \u001b[31m   time: 0.744135 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.252685 (-1.995376)\n",
      "\u001b[0m\n",
      "it 841/2000, vlb -2.667608, \u001b[31m   time: 0.757151 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.249409 (-1.995376)\n",
      "\u001b[0m\n",
      "it 842/2000, vlb -2.681247, \u001b[31m   time: 0.740396 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.133130 (-1.995376)\n",
      "\u001b[0m\n",
      "it 843/2000, vlb -2.621620, \u001b[31m   time: 0.732852 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.168506 (-1.995376)\n",
      "\u001b[0m\n",
      "it 844/2000, vlb -2.677188, \u001b[31m   time: 0.726265 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.163097 (-1.995376)\n",
      "\u001b[0m\n",
      "it 845/2000, vlb -2.588202, \u001b[31m   time: 0.741248 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.070511 (-1.995376)\n",
      "\u001b[0m\n",
      "it 846/2000, vlb -2.642905, \u001b[31m   time: 0.738059 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.272035 (-1.995376)\n",
      "\u001b[0m\n",
      "it 847/2000, vlb -2.672096, \u001b[31m   time: 0.750027 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.197540 (-1.995376)\n",
      "\u001b[0m\n",
      "it 848/2000, vlb -2.727752, \u001b[31m   time: 0.954794 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.137240 (-1.995376)\n",
      "\u001b[0m\n",
      "it 849/2000, vlb -2.665103, \u001b[31m   time: 0.797901 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.331444 (-1.995376)\n",
      "\u001b[0m\n",
      "it 850/2000, vlb -2.673039, \u001b[31m   time: 0.892237 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.126811 (-1.995376)\n",
      "\u001b[0m\n",
      "it 851/2000, vlb -2.603204, \u001b[31m   time: 0.815120 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.233109 (-1.995376)\n",
      "\u001b[0m\n",
      "it 852/2000, vlb -2.620964, \u001b[31m   time: 0.747693 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.123055 (-1.995376)\n",
      "\u001b[0m\n",
      "it 853/2000, vlb -2.699991, \u001b[31m   time: 0.765020 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.288273 (-1.995376)\n",
      "\u001b[0m\n",
      "it 854/2000, vlb -2.627818, \u001b[31m   time: 0.793417 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.216731 (-1.995376)\n",
      "\u001b[0m\n",
      "it 855/2000, vlb -2.671151, \u001b[31m   time: 0.736834 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.241980 (-1.995376)\n",
      "\u001b[0m\n",
      "it 856/2000, vlb -2.692619, \u001b[31m   time: 0.748419 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -2.194565 (-1.995376)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ./torch_vlb/fc_preact_VAEAC_NEW_compas_models/theta_last.dat\n",
      "\u001b[0m\n",
      "\u001b[31m   average time: 0.332717 seconds\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "masker = top_masker(p=1)\n",
    "\n",
    "x_train, x_test, x_means, x_stds, y_train, y_test, feature_names, X_dims = \\\n",
    "    get_my_COMPAS(rseed=42, separate_test=True, test_ratio=0.1, save_dir='../data/')\n",
    "x_train, x_test, input_dim_vec = join_compas_targets(x_train, x_test, y_train, y_test, X_dims)\n",
    "print('Compas', x_train.shape, x_test.shape)\n",
    "print(input_dim_vec)\n",
    "\n",
    "dname = 'compas'\n",
    "print(dname)\n",
    "\n",
    "trainset = Datafeed(x_train, x_train, transform=None)\n",
    "valset = Datafeed(x_test, x_test, transform=None)\n",
    "\n",
    "save_dir = './torch_vlb/fc_preact_VAEAC_NEW_' + dname\n",
    "\n",
    "width = widths[names.index(dname)]\n",
    "depth = depths[names.index(dname)] # number of hidden layers\n",
    "latent_dim = latent_dims[names.index(dname)]\n",
    "\n",
    "batch_size = 128\n",
    "nb_epochs = 2000\n",
    "early_stop = 200\n",
    "lr = 1e-4\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "net = VAEAC_gauss_cat_net(input_dim_vec, width, depth, latent_dim, pred_sig=False, lr=lr,\n",
    "                          cuda=cuda, flatten=False)\n",
    "\n",
    "vlb_train, vlb_val, best_epoch, best_vlb, curr_epoch = train_VAEAC(net, masker, save_dir, batch_size, nb_epochs, trainset, valset, cuda,\n",
    "                     flat_ims=False, train_plot=False, Nclass=None, early_stop=early_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b052964d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "600a5b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best vlb  -1.995376444557338\n",
      "best epoch  655\n",
      "curr epoch 856\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "np.save(\"./torch_vlb/\" + str(dname) + \"_vlb_train_lr_\" + str(lr), vlb_train)\n",
    "np.save(\"./torch_vlb/\" + str(dname) + \"_vlb_val_lr_\" + str(lr), vlb_val)\n",
    "\n",
    "# best_epoch = 321 (pos 320 in train_val)\n",
    "#curr_epoch = 522\n",
    "\n",
    "print(\"best vlb \", best_vlb)\n",
    "print(\"best epoch \", best_epoch)\n",
    "print(\"curr epoch\", curr_epoch)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.clip(vlb_train[:curr_epoch], -1000, 1000), 'r')\n",
    "plt.plot(np.clip(vlb_val[:curr_epoch], -1000, 1000), 'b')\n",
    "plt.legend(['train', 'validation'], fontsize =15)\n",
    "plt.ylabel('Variational Lower Bound', fontweight='bold', fontsize =15)\n",
    "plt.xlabel('Epoch', fontweight='bold', fontsize =15)\n",
    "plt.yticks(fontsize = 13)\n",
    "plt.xticks(fontsize=13)\n",
    "plt.title('VAEAC Original', fontweight='bold', fontsize= 15)\n",
    "plt.grid(True)\n",
    "plt.savefig('./torch_vlb/' +  'torch_compas_vaeac' + '.png', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e335395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best vlb  -1.995376444557338\n",
      "best epoch  655\n",
      "curr epoch 856\n"
     ]
    }
   ],
   "source": [
    "print(\"best vlb \", best_vlb)\n",
    "print(\"best epoch \", best_epoch)\n",
    "print(\"curr epoch\", curr_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa2f4957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "plt.figure()\n",
    "plt.plot(np.clip(vlb_train[:curr_epoch], -1000, 1000), 'r')\n",
    "plt.plot(np.clip(vlb_dev[:curr_epoch], -1000, 1000), 'b')\n",
    "plt.legend(['cost_train', 'cost_dev'])\n",
    "plt.ylabel('vlb')\n",
    "plt.xlabel('it')\n",
    "plt.grid(True)\n",
    "plt.savefig( str(dname) + '_vlb_lr_' + str(lr) + '.png')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0318b9",
   "metadata": {},
   "source": [
    "## Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8d719bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]\n",
      "(27000, 24)\n",
      "(3000, 24)\n",
      "default_credit\n",
      "\u001b[36m\n",
      "Net:\u001b[0m\n",
      "\u001b[33mVAE_gauss_net\u001b[0m\n",
      "    Total params: 0.80M\n",
      "\u001b[36m\n",
      "Network:\u001b[0m\n",
      "\u001b[36m\n",
      "Train:\u001b[0m\n",
      "  init cost variables:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_31096/1070947545.py:6: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return np.sum(p.numel() for p in self.model.parameters())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 0/2000, vlb -19.182486, \u001b[31m   time: 16.533756 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -9.254449 (-inf)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAEAC_NEW2_default_credit_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 1/2000, vlb -11.698959, \u001b[31m   time: 18.969596 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -7.840795 (-9.254449)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAEAC_NEW2_default_credit_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 2/2000, vlb -10.394569, \u001b[31m   time: 15.788894 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -7.176111 (-7.840795)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAEAC_NEW2_default_credit_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 3/2000, vlb -9.829430, \u001b[31m   time: 17.466390 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.881644 (-7.176111)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAEAC_NEW2_default_credit_models/theta_best.dat\n",
      "\u001b[0m\n",
      "it 4/2000, vlb -9.581752, \u001b[31m   time: 16.366702 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.702829 (-6.881644)\n",
      "\u001b[0m\n",
      "\u001b[36mWritting ../saves/fc_preact_VAEAC_NEW2_default_credit_models/theta_best.dat\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_31096/1230764105.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m vlb_train, vlb_dev = train_VAEAC(net, masker, save_dir, batch_size, nb_epochs, trainset, valset, cuda,\n\u001b[0m\u001b[1;32m     38\u001b[0m                      flat_ims=False, train_plot=False, Nclass=None, early_stop=early_stop)\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_31096/4196670429.py\u001b[0m in \u001b[0;36mtrain_VAEAC\u001b[0;34m(net, masker, name, batch_size, nb_epochs, trainset, valset, cuda, flat_ims, train_plot, Nclass, early_stop, script_mode)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mvlb_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_31096/1958406935.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mapprox_post\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecognition_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mz_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapprox_post\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_31096/2369653937.py\u001b[0m in \u001b[0;36mprior_encode\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0mprior_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormal_parse_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_31096/3151266841.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m#Encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_31096/3581392151.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m#\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreact_leaky_MLPBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "masker = top_masker(p=1)\n",
    "\n",
    "x_train, x_test, x_means, x_stds = \\\n",
    "load_UCI(dset_name='default_credit', splits=10, seed=42, separate_targets=False, save_dir='../data/')\n",
    "\n",
    "input_dim_vec = [1, 2, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ,1 ,1 ,1 ,1 ,1, 2] #has this form for targets\n",
    "print(input_dim_vec)\n",
    "\n",
    "x_train = unnormalise_cat_vars(x_train, x_means, x_stds, input_dim_vec)\n",
    "x_test = unnormalise_cat_vars(x_test, x_means, x_stds, input_dim_vec)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "dname = 'default_credit'\n",
    "print(dname)\n",
    "\n",
    "trainset = Datafeed(x_train, x_train, transform=None)\n",
    "valset = Datafeed(x_test, x_test, transform=None)\n",
    "\n",
    "save_dir = '../saves/fc_preact_VAEAC_NEW2_' + dname\n",
    "\n",
    "width = widths[names.index(dname)]\n",
    "depth = depths[names.index(dname)] # number of hidden layers\n",
    "latent_dim = latent_dims[names.index(dname)]\n",
    "\n",
    "batch_size = 64\n",
    "nb_epochs = 2000\n",
    "early_stop = 200\n",
    "lr = 7e-4\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "net = VAEAC_gauss_cat_net(input_dim_vec, width, depth, latent_dim, pred_sig=False, lr=lr, cuda=cuda, flatten=True)\n",
    "\n",
    "\n",
    "vlb_train, vlb_dev = train_VAEAC(net, masker, save_dir, batch_size, nb_epochs, trainset, valset, cuda,\n",
    "                     flat_ims=False, train_plot=False, Nclass=None, early_stop=early_stop)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885fae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91578089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72bdadb1",
   "metadata": {},
   "source": [
    "## Under VAEAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd078b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_gauss(nn.Module):\n",
    "    def __init__(self, input_dim, width, depth, latent_dim, pred_sig=True):\n",
    "        super(VAE_gauss, self).__init__()\n",
    "\n",
    "        self.encoder = MLP_preact_recognition_net(input_dim, width, depth, latent_dim)\n",
    "        if pred_sig:\n",
    "            self.decoder = MLP_preact_generator_net(2*input_dim, width, depth, latent_dim)\n",
    "            self.rec_loglike = GaussianLoglike(min_sigma=1e-2)\n",
    "        else:\n",
    "            self.decoder = MLP_preact_generator_net(input_dim, width, depth, latent_dim)\n",
    "            self.m_rec_loglike = MSELoss(reduction='none')\n",
    "        self.pred_sig = pred_sig\n",
    "\n",
    "    def encode(self, x):\n",
    "        approx_post_params = self.encoder(x)\n",
    "        approx_post = normal_parse_params(approx_post_params, 1e-3)\n",
    "        return approx_post\n",
    "\n",
    "    def decode(self, z_sample):\n",
    "        rec_params = self.decoder(z_sample)\n",
    "        return rec_params\n",
    "\n",
    "    def vlb(self, prior, approx_post, x, rec_params):\n",
    "        if self.pred_sig:\n",
    "            rec = self.rec_loglike(rec_params, x).view(x.shape[0], -1).sum(-1)\n",
    "        else:\n",
    "            rec = -self.m_rec_loglike(rec_params, x).view(x.shape[0], -1).sum(-1)\n",
    "        kl = kl_divergence(approx_post, prior).view(x.shape[0], -1).sum(-1)\n",
    "        return rec - kl\n",
    "\n",
    "    def iwlb(self, prior, approx_post, x, K=50):\n",
    "        estimates = []\n",
    "        for i in range(K):\n",
    "            latent = approx_post.rsample()\n",
    "            rec_params = self.decode(latent)\n",
    "            if self.pred_sig:\n",
    "                rec_loglike = self.rec_loglike(rec_params, x).view(x.shape[0], -1).sum(-1)\n",
    "            else:\n",
    "                rec_loglike = -self.m_rec_loglike(rec_params, x).view(x.shape[0], -1).sum(-1)\n",
    "\n",
    "            prior_log_prob = prior.log_prob(latent)\n",
    "            prior_log_prob = prior_log_prob.view(x.shape[0], -1)\n",
    "            prior_log_prob = prior_log_prob.sum(-1)\n",
    "\n",
    "            proposal_log_prob = approx_post.log_prob(latent)\n",
    "            proposal_log_prob = proposal_log_prob.view(x.shape[0], -1)\n",
    "            proposal_log_prob = proposal_log_prob.sum(-1)\n",
    "\n",
    "            estimate = rec_loglike + prior_log_prob - proposal_log_prob\n",
    "            estimates.append(estimate[:, None])\n",
    "\n",
    "        return torch.logsumexp(torch.cat(estimates, 1), 1) - np.log(K)\n",
    "\n",
    "\n",
    "class VAE_gauss_net(BaseNet):\n",
    "    def __init__(self, input_dim, width, depth, latent_dim, pred_sig=True, lr=1e-3, cuda=True):\n",
    "        super(VAE_gauss_net, self).__init__()\n",
    "        cprint('y', 'VAE_gauss_net')\n",
    "\n",
    "        self.cuda = cuda\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.latent_dim = latent_dim\n",
    "        self.lr = lr\n",
    "        self.pred_sig = pred_sig\n",
    "\n",
    "        self.create_net()\n",
    "        self.create_opt()\n",
    "        self.epoch = 0\n",
    "        self.schedule = None\n",
    "\n",
    "        if self.cuda:\n",
    "            self.prior = self.prior = Normal(loc=torch.zeros(latent_dim).cuda(), scale=torch.ones(latent_dim).cuda())\n",
    "        else:\n",
    "            self.prior = Normal(loc=torch.zeros(latent_dim), scale=torch.ones(latent_dim))\n",
    "        self.vlb_scale = 1 / input_dim  # scale for dimensions of input so we can use same LR always\n",
    "\n",
    "    def create_net(self):\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed(42)\n",
    "        self.model = VAE_gauss(self.input_dim, self.width, self.depth, self.latent_dim, self.pred_sig)\n",
    "        if self.cuda:\n",
    "            self.model = self.model.cuda()\n",
    "            cudnn.benchmark = True\n",
    "        print('    Total params: %.2fM' % (self.get_nb_parameters() / 1000000.0))\n",
    "\n",
    "    def create_opt(self):\n",
    "        self.optimizer = RAdam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def fit(self, x):\n",
    "        self.set_mode_train(train=True)\n",
    "\n",
    "        x, = to_variable(var=(x, ), cuda=self.cuda)\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        approx_post = self.model.encode(x)\n",
    "        z_sample = approx_post.rsample()\n",
    "        rec_params = self.model.decode(z_sample)\n",
    "\n",
    "        vlb = self.model.vlb(self.prior, approx_post, x, rec_params)\n",
    "        loss = (- vlb * self.vlb_scale).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return vlb.mean().item(), rec_params\n",
    "\n",
    "    def eval(self, x, sample=False):\n",
    "        self.set_mode_train(train=False)\n",
    "\n",
    "        x, = to_variable(var=(x, ), cuda=self.cuda)\n",
    "        approx_post = self.model.encode(x)\n",
    "        if sample:\n",
    "            z_sample = approx_post.sample()\n",
    "        else:\n",
    "            z_sample = approx_post.loc\n",
    "        rec_params = self.model.decode(z_sample)\n",
    "\n",
    "        vlb = self.model.vlb(self.prior, approx_post, x, rec_params)\n",
    "\n",
    "        return vlb.mean().item(), rec_params\n",
    "\n",
    "    def eval_iw(self, x, k=50):\n",
    "        self.set_mode_train(train=False)\n",
    "        x,  = to_variable(var=(x, ), cuda=self.cuda)\n",
    "\n",
    "        approx_post = self.model.recognition_encode(x)\n",
    "\n",
    "        iw_lb = self.model.iwlb(self.prior, approx_post, x, k)\n",
    "        return iw_lb.mean().item()\n",
    "\n",
    "    def recongnition(self, x, grad=False):\n",
    "        self.set_mode_train(train=False)\n",
    "        if grad:\n",
    "            if not x.requires_grad:\n",
    "                x.requires_grad = True\n",
    "        else:\n",
    "            x, = to_variable(var=(x,), volatile=True, cuda=self.cuda)\n",
    "        approx_post = self.model.encode(x)\n",
    "        return approx_post\n",
    "\n",
    "    def regenerate(self, z, grad=False):\n",
    "        self.set_mode_train(train=False)\n",
    "        if grad:\n",
    "            if not z.requires_grad:\n",
    "                z.requires_grad = True\n",
    "        else:\n",
    "            z, = to_variable(var=(z,), volatile=True, cuda=self.cuda)\n",
    "        out = self.model.decode(z)\n",
    "        if self.pred_sig:\n",
    "            return normal_parse_params(out, 1e-2)\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0a15675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_VAE(net, name, batch_size, nb_epochs, trainset, valset, cuda, flat_ims=False,\n",
    "              train_plot=False, Nclass=None, early_stop=None, script_mode=False):\n",
    "\n",
    "    models_dir = name + '_models'\n",
    "    results_dir = name + '_results'\n",
    "    mkdir(models_dir)\n",
    "    mkdir(results_dir)\n",
    "\n",
    "    if cuda:\n",
    "        trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True,\n",
    "                                                  num_workers=0)\n",
    "        valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True,\n",
    "                                                num_workers=0)\n",
    "\n",
    "    else:\n",
    "        trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n",
    "                                                  num_workers=0)\n",
    "        valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                                num_workers=0)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# net dims\n",
    "    cprint('c', '\\nNetwork:')\n",
    "\n",
    "    epoch = 0\n",
    "\n",
    "    ## ---------------------------------------------------------------------------------------------------------------------\n",
    "    # train\n",
    "    cprint('c', '\\nTrain:')\n",
    "\n",
    "    print('  init cost variables:')\n",
    "    vlb_train = np.zeros(nb_epochs)\n",
    "    vlb_dev = np.zeros(nb_epochs)\n",
    "    best_vlb = -np.inf\n",
    "    best_vlb_train = -np.inf\n",
    "    best_epoch = 0\n",
    "\n",
    "    nb_its_dev = 1\n",
    "\n",
    "    tic0 = time.time()\n",
    "    i = 0\n",
    "    for i in range(epoch, nb_epochs):\n",
    "        net.set_mode_train(True)\n",
    "\n",
    "        tic = time.time()\n",
    "        nb_samples = 0\n",
    "        for x, y in trainloader:\n",
    "\n",
    "            if flat_ims:\n",
    "                x = x.view(x.shape[0], -1)\n",
    "            if Nclass is not None:\n",
    "                y_oh = torch_onehot(y, Nclass).type(x.type())\n",
    "                x = torch.cat([x, y_oh], 1)\n",
    "\n",
    "            cost, _ = net.fit(x)\n",
    "\n",
    "            vlb_train[i] += cost * len(x)\n",
    "            nb_samples += len(x)\n",
    "\n",
    "        vlb_train[i] /= nb_samples\n",
    "\n",
    "        toc = time.time()\n",
    "\n",
    "        # ---- print\n",
    "        print(\"it %d/%d, vlb %f, \" % (i, nb_epochs, vlb_train[i]), end=\"\")\n",
    "        cprint('r', '   time: %f seconds\\n' % (toc - tic))\n",
    "        net.update_lr(i)\n",
    "\n",
    "        if vlb_train[i] > best_vlb_train:\n",
    "            best_vlb_train = vlb_train[i]\n",
    "\n",
    "        # ---- dev\n",
    "        if i % nb_its_dev == 0:\n",
    "            nb_samples = 0\n",
    "            for j, (x, y) in enumerate(valloader):\n",
    "\n",
    "                if flat_ims:\n",
    "                    x = x.view(x.shape[0], -1)\n",
    "                if Nclass is not None:\n",
    "                    y_oh = torch_onehot(y, Nclass).type(x.type())\n",
    "                    x = torch.cat([x, y_oh], 1)\n",
    "\n",
    "                cost, _ = net.eval(x)\n",
    "\n",
    "                vlb_dev[i] += cost * len(x)\n",
    "                nb_samples += len(x)\n",
    "\n",
    "            vlb_dev[i] /= nb_samples\n",
    "\n",
    "            cprint('g', '    vlb %f (%f)\\n' % (vlb_dev[i], best_vlb))\n",
    "\n",
    "            if train_plot:\n",
    "                zz = net.recongnition(x).sample()\n",
    "                o = net.regenerate(zz)\n",
    "                try:\n",
    "                    o = o.cpu()\n",
    "                except:\n",
    "                    o = o.loc.cpu()\n",
    "                if len(x.shape) == 2:\n",
    "                    side = int(np.sqrt(x.shape[1]))\n",
    "                    x = x.view(-1, 1, side, side).data\n",
    "                    o = o.view(-1, 1, side, side).data\n",
    "\n",
    "                # save_image(torch.cat([x[:8], o[:8]]), results_dir + '/rec_%d.png' % i, nrow=8)\n",
    "                import matplotlib.pyplot as plt\n",
    "                plt.figure()\n",
    "                dd = make_grid(torch.cat([x[:10], o[:10]]), nrow=10).numpy()\n",
    "                plt.imshow(np.transpose(dd, (1, 2, 0)), interpolation='nearest')\n",
    "                if script_mode:\n",
    "                    plt.savefig(results_dir + '/rec%d.png' % i)\n",
    "                else:\n",
    "                    plt.show()\n",
    "\n",
    "                z_sample = normal(loc=0.0, scale=1.0, size=(36, net.latent_dim))\n",
    "                x_rec = net.regenerate(z_sample)\n",
    "                try:\n",
    "                    x_rec = x_rec.cpu()\n",
    "                except:\n",
    "                    x_rec = x_rec.loc.cpu()\n",
    "                if len(x_rec.shape) == 2:\n",
    "                    side = int(np.sqrt(x_rec.shape[1]))\n",
    "                    x_rec = x_rec.view(-1, 1, side, side)\n",
    "                plt.figure()\n",
    "                dd = make_grid(x_rec, nrow=6).numpy()\n",
    "                plt.imshow(np.transpose(dd, (1, 2, 0)), interpolation='nearest')\n",
    "                if script_mode:\n",
    "                    plt.savefig(results_dir + '/sample%d.png' % i)\n",
    "                else:\n",
    "                    plt.show()\n",
    "\n",
    "        if vlb_dev[i] > best_vlb:\n",
    "            best_vlb = vlb_dev[i]\n",
    "            best_epoch = i\n",
    "            #net.save(models_dir + '/theta_best.dat')\n",
    "\n",
    "        if early_stop is not None and (i - best_epoch) > early_stop:\n",
    "            break\n",
    "\n",
    "\n",
    "    #net.save(models_dir + '/theta_last.dat')\n",
    "    toc0 = time.time()\n",
    "    runtime_per_it = (toc0 - tic0) / float(nb_epochs)\n",
    "    cprint('r', '   average time: %f seconds\\n' % runtime_per_it)\n",
    "\n",
    "    ## ---------------------------------------------------------------------------------------------------------------------\n",
    "    # results\n",
    "    \"\"\"\n",
    "    cprint('c', '\\nRESULTS:')\n",
    "    nb_parameters = net.get_nb_parameters()\n",
    "    best_cost_dev = best_vlb\n",
    "    best_cost_train = best_vlb_train\n",
    "\n",
    "    print('  best_vlb_dev: %f' % best_cost_dev)\n",
    "    print('  best_vlb_train: %f' % best_cost_train)\n",
    "    print('  nb_parameters: %d (%s)\\n' % (nb_parameters, humansize(nb_parameters)))\n",
    "\n",
    "    ## ---------------------------------------------------------------------------------------------------------------------\n",
    "    # fig cost vs its\n",
    "    if not train_plot:\n",
    "        import matplotlib\n",
    "        matplotlib.use('agg')\n",
    "    import matplotlib.pyplot as plt\n",
    "    if train_plot:\n",
    "        plt.figure()\n",
    "        plt.plot(np.clip(vlb_train, -1000, 1000), 'r')\n",
    "        plt.plot(np.clip(vlb_dev[::nb_its_dev], -1000, 1000), 'b')\n",
    "        plt.legend(['cost_train', 'cost_dev'])\n",
    "        plt.ylabel('vlb')\n",
    "        plt.xlabel('it')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(results_dir+'/train_cost.png')\n",
    "        if train_plot:\n",
    "            plt.show()\n",
    "    \"\"\"\n",
    "    return vlb_train, vlb_dev, best_epoch, best_vlb, i\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c04fea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class under_VAEAC(BaseNet):\n",
    "    def __init__(self, base_VAE, width, depth, latent_dim, lr=1e-3, cuda=True):\n",
    "        super(under_VAEAC, self).__init__()\n",
    "        cprint('y', 'VAE_gauss_net')\n",
    "\n",
    "        self.base_VAEAC = base_VAE\n",
    "        self.pred_sig = False\n",
    "\n",
    "        self.cuda = cuda\n",
    "\n",
    "        self.input_dim = self.base_VAEAC.latent_dim\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.latent_dim = latent_dim\n",
    "        self.lr = lr\n",
    "\n",
    "        self.create_net()\n",
    "        self.create_opt()\n",
    "        self.epoch = 0\n",
    "        self.schedule = None\n",
    "\n",
    "        if self.cuda:\n",
    "            self.prior = self.prior = Normal(loc=torch.zeros(latent_dim).cuda(), scale=torch.ones(latent_dim).cuda())\n",
    "        else:\n",
    "            self.prior = Normal(loc=torch.zeros(latent_dim), scale=torch.ones(latent_dim))\n",
    "        self.vlb_scale = 1 / self.input_dim  # scale for dimensions of input so we can use same LR always\n",
    "\n",
    "    def create_net(self):\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed(42)\n",
    "        self.model = VAE_gauss(self.input_dim, self.width, self.depth, self.latent_dim, self.pred_sig)\n",
    "        if self.cuda:\n",
    "            self.model = self.model.cuda()\n",
    "            cudnn.benchmark = True\n",
    "        print('    Total params: %.2fM' % (self.get_nb_parameters() / 1000000.0))\n",
    "\n",
    "    def create_opt(self):\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def fit(self, x):\n",
    "        self.set_mode_train(train=True)\n",
    "\n",
    "        x, = to_variable(var=(x, ), cuda=self.cuda)\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        z_sample = self.base_VAEAC.recognition_encode(x).sample()\n",
    "        approx_post = self.model.encode(z_sample)\n",
    "        u_sample = approx_post.rsample()\n",
    "        rec_params = self.model.decode(u_sample)\n",
    "\n",
    "        vlb = self.model.vlb(self.prior, approx_post, z_sample, rec_params)\n",
    "        loss = (- vlb * self.vlb_scale).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return vlb.mean().item(), rec_params\n",
    "\n",
    "    def eval(self, x):\n",
    "\n",
    "        self.set_mode_train(train=False)\n",
    "\n",
    "        x, = to_variable(var=(x, ), cuda=self.cuda)\n",
    "        z_sample = self.base_VAEAC.recognition_encode(x).sample()\n",
    "        approx_post = self.model.encode(z_sample)\n",
    "        u_sample = approx_post.rsample()\n",
    "        rec_params = self.model.decode(u_sample)\n",
    "\n",
    "        vlb = self.model.vlb(self.prior, approx_post, z_sample, rec_params)\n",
    "\n",
    "        return vlb.mean().item(), rec_params\n",
    "\n",
    "    def eval_iw(self, x, k=50):\n",
    "        self.set_mode_train(train=False)\n",
    "        x,  = to_variable(var=(x, ), cuda=self.cuda)\n",
    "        z_sample = self.base_VAEAC.recognition_encode(x).sample()\n",
    "        approx_post = self.model.recognition_encode(z_sample)\n",
    "\n",
    "        iw_lb = self.model.iwlb(self.prior, approx_post, z_sample, k)\n",
    "        return iw_lb.mean().item()\n",
    "\n",
    "    def recongnition(self, x, grad=False):\n",
    "        self.set_mode_train(train=False)\n",
    "        if grad:\n",
    "            if not x.requires_grad:\n",
    "                x.requires_grad = True\n",
    "        else:\n",
    "            x, = to_variable(var=(x,), volatile=True, cuda=self.cuda)\n",
    "        approx_post = self.model.encode(x)\n",
    "        return approx_post\n",
    "\n",
    "    def regenerate(self, z, grad=False):\n",
    "        self.set_mode_train(train=False)\n",
    "        if grad:\n",
    "            if not z.requires_grad:\n",
    "                z.requires_grad = True\n",
    "        else:\n",
    "            z, = to_variable(var=(z,), volatile=True, cuda=self.cuda)\n",
    "        out = self.model.decode(z)\n",
    "        return out.data\n",
    "\n",
    "    def u_recongnition(self, x, grad=False):\n",
    "        self.set_mode_train(train=False)\n",
    "        if grad:\n",
    "            if not x.requires_grad:\n",
    "                x.requires_grad = True\n",
    "        else:\n",
    "            x, = to_variable(var=(x,), volatile=True, cuda=self.cuda)\n",
    "\n",
    "        z = self.base_VAEAC.recognition_encode(x).loc\n",
    "        approx_post = self.model.encode(z)\n",
    "        return approx_post\n",
    "\n",
    "    def u_mask_recongnition(self, x, mask, grad=False):\n",
    "        self.set_mode_train(train=False)\n",
    "        if grad:\n",
    "            if not x.requires_grad:\n",
    "                x.requires_grad = True\n",
    "            mask, = to_variable(var=(mask, ), cuda=self.cuda)\n",
    "        else:\n",
    "            x, mask = to_variable(var=(x, mask), cuda=self.cuda)\n",
    "\n",
    "        z = self.base_VAEAC.prior_encode(x, mask).loc\n",
    "        approx_post = self.model.encode(z)\n",
    "        return approx_post\n",
    "\n",
    "    def u_regenerate(self, u, grad=False):\n",
    "        self.set_mode_train(train=False)\n",
    "        if grad:\n",
    "            if not u.requires_grad:\n",
    "                u.requires_grad = True\n",
    "        else:\n",
    "            u, = to_variable(var=(u,), volatile=True, cuda=self.cuda)\n",
    "\n",
    "        z = self.model.decode(u)\n",
    "        out = self.base_VAEAC.decode(z)\n",
    "        if self.base_VAEAC.pred_sig:\n",
    "            return normal_parse_params(out, 1e-2)\n",
    "        else:\n",
    "            return out.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ac94e51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compas (5554, 19) (618, 19)\n",
      "[3 6 2 2 2 1 1 2]\n",
      "compas\n",
      "\u001b[36m\n",
      "Net:\u001b[0m\n",
      "\u001b[33mVAE_gauss_net\u001b[0m\n",
      "    Total params: 0.05M\n",
      "\u001b[36m\n",
      "Network:\u001b[0m\n",
      "\u001b[36m\n",
      "Train:\u001b[0m\n",
      "  init cost variables:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mortimer\\AppData\\Local\\Temp/ipykernel_22248/1070947545.py:6: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return np.sum(p.numel() for p in self.model.parameters())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 0/2000, vlb -16.552909, \u001b[31m   time: 0.395074 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -13.125182 (-inf)\n",
      "\u001b[0m\n",
      "it 1/2000, vlb -11.521138, \u001b[31m   time: 0.439851 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -10.333074 (-13.125182)\n",
      "\u001b[0m\n",
      "it 2/2000, vlb -9.540297, \u001b[31m   time: 0.381172 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -8.538390 (-10.333074)\n",
      "\u001b[0m\n",
      "it 3/2000, vlb -8.522815, \u001b[31m   time: 0.381980 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -7.963790 (-8.538390)\n",
      "\u001b[0m\n",
      "it 4/2000, vlb -8.115296, \u001b[31m   time: 0.402921 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -7.601742 (-7.963790)\n",
      "\u001b[0m\n",
      "it 5/2000, vlb -7.811508, \u001b[31m   time: 0.401987 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -7.994112 (-7.601742)\n",
      "\u001b[0m\n",
      "it 6/2000, vlb -7.819426, \u001b[31m   time: 0.386507 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -7.426943 (-7.601742)\n",
      "\u001b[0m\n",
      "it 7/2000, vlb -7.728694, \u001b[31m   time: 0.411934 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -7.557994 (-7.426943)\n",
      "\u001b[0m\n",
      "it 8/2000, vlb -7.512555, \u001b[31m   time: 0.385640 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -7.320632 (-7.426943)\n",
      "\u001b[0m\n",
      "it 9/2000, vlb -7.544233, \u001b[31m   time: 0.402277 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -7.062811 (-7.320632)\n",
      "\u001b[0m\n",
      "it 10/2000, vlb -7.437240, \u001b[31m   time: 0.409840 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -7.406143 (-7.062811)\n",
      "\u001b[0m\n",
      "it 11/2000, vlb -7.417204, \u001b[31m   time: 0.393978 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -7.213525 (-7.062811)\n",
      "\u001b[0m\n",
      "it 12/2000, vlb -7.404126, \u001b[31m   time: 0.429936 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -7.213014 (-7.062811)\n",
      "\u001b[0m\n",
      "it 13/2000, vlb -7.396077, \u001b[31m   time: 0.355642 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.975285 (-7.062811)\n",
      "\u001b[0m\n",
      "it 14/2000, vlb -7.331409, \u001b[31m   time: 0.378987 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -7.204815 (-6.975285)\n",
      "\u001b[0m\n",
      "it 15/2000, vlb -7.227986, \u001b[31m   time: 0.381942 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -7.089600 (-6.975285)\n",
      "\u001b[0m\n",
      "it 16/2000, vlb -7.222151, \u001b[31m   time: 0.371420 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -7.150144 (-6.975285)\n",
      "\u001b[0m\n",
      "it 17/2000, vlb -7.162850, \u001b[31m   time: 0.395913 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.765425 (-6.975285)\n",
      "\u001b[0m\n",
      "it 18/2000, vlb -7.021180, \u001b[31m   time: 0.391953 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.762005 (-6.765425)\n",
      "\u001b[0m\n",
      "it 19/2000, vlb -7.039490, \u001b[31m   time: 0.431944 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.740104 (-6.762005)\n",
      "\u001b[0m\n",
      "it 20/2000, vlb -6.939981, \u001b[31m   time: 0.404676 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.683744 (-6.740104)\n",
      "\u001b[0m\n",
      "it 21/2000, vlb -6.944543, \u001b[31m   time: 0.420846 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.646193 (-6.683744)\n",
      "\u001b[0m\n",
      "it 22/2000, vlb -6.910217, \u001b[31m   time: 0.431795 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.603833 (-6.646193)\n",
      "\u001b[0m\n",
      "it 23/2000, vlb -6.873925, \u001b[31m   time: 0.409849 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.710406 (-6.603833)\n",
      "\u001b[0m\n",
      "it 24/2000, vlb -6.874819, \u001b[31m   time: 0.418469 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.651274 (-6.603833)\n",
      "\u001b[0m\n",
      "it 25/2000, vlb -6.783685, \u001b[31m   time: 0.365993 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.577234 (-6.603833)\n",
      "\u001b[0m\n",
      "it 26/2000, vlb -6.742657, \u001b[31m   time: 0.399628 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.380570 (-6.577234)\n",
      "\u001b[0m\n",
      "it 27/2000, vlb -6.691139, \u001b[31m   time: 0.449899 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.515298 (-6.380570)\n",
      "\u001b[0m\n",
      "it 28/2000, vlb -6.681735, \u001b[31m   time: 0.514583 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.474241 (-6.380570)\n",
      "\u001b[0m\n",
      "it 29/2000, vlb -6.712404, \u001b[31m   time: 0.411242 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.443189 (-6.380570)\n",
      "\u001b[0m\n",
      "it 30/2000, vlb -6.626315, \u001b[31m   time: 0.378147 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.385933 (-6.380570)\n",
      "\u001b[0m\n",
      "it 31/2000, vlb -6.656162, \u001b[31m   time: 0.364205 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.648254 (-6.380570)\n",
      "\u001b[0m\n",
      "it 32/2000, vlb -6.611814, \u001b[31m   time: 0.368673 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.508271 (-6.380570)\n",
      "\u001b[0m\n",
      "it 33/2000, vlb -6.596259, \u001b[31m   time: 0.392977 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.325978 (-6.380570)\n",
      "\u001b[0m\n",
      "it 34/2000, vlb -6.592748, \u001b[31m   time: 0.373416 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.378967 (-6.325978)\n",
      "\u001b[0m\n",
      "it 35/2000, vlb -6.553716, \u001b[31m   time: 0.389798 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.507366 (-6.325978)\n",
      "\u001b[0m\n",
      "it 36/2000, vlb -6.512616, \u001b[31m   time: 0.398341 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.447881 (-6.325978)\n",
      "\u001b[0m\n",
      "it 37/2000, vlb -6.562255, \u001b[31m   time: 0.389652 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.394116 (-6.325978)\n",
      "\u001b[0m\n",
      "it 38/2000, vlb -6.558998, \u001b[31m   time: 0.392955 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.363107 (-6.325978)\n",
      "\u001b[0m\n",
      "it 39/2000, vlb -6.483736, \u001b[31m   time: 0.379251 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.217495 (-6.325978)\n",
      "\u001b[0m\n",
      "it 40/2000, vlb -6.514738, \u001b[31m   time: 0.400482 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.266735 (-6.217495)\n",
      "\u001b[0m\n",
      "it 41/2000, vlb -6.532075, \u001b[31m   time: 0.381011 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.274365 (-6.217495)\n",
      "\u001b[0m\n",
      "it 42/2000, vlb -6.511413, \u001b[31m   time: 0.424444 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.237981 (-6.217495)\n",
      "\u001b[0m\n",
      "it 43/2000, vlb -6.493618, \u001b[31m   time: 0.415015 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.377398 (-6.217495)\n",
      "\u001b[0m\n",
      "it 44/2000, vlb -6.504534, \u001b[31m   time: 0.386060 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.464860 (-6.217495)\n",
      "\u001b[0m\n",
      "it 45/2000, vlb -6.496123, \u001b[31m   time: 0.385972 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.450758 (-6.217495)\n",
      "\u001b[0m\n",
      "it 46/2000, vlb -6.543968, \u001b[31m   time: 0.377297 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.216572 (-6.217495)\n",
      "\u001b[0m\n",
      "it 47/2000, vlb -6.510723, \u001b[31m   time: 0.402151 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.389056 (-6.216572)\n",
      "\u001b[0m\n",
      "it 48/2000, vlb -6.488713, \u001b[31m   time: 0.384998 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.399987 (-6.216572)\n",
      "\u001b[0m\n",
      "it 49/2000, vlb -6.474642, \u001b[31m   time: 0.390053 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.288565 (-6.216572)\n",
      "\u001b[0m\n",
      "it 50/2000, vlb -6.470610, \u001b[31m   time: 0.399248 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.337709 (-6.216572)\n",
      "\u001b[0m\n",
      "it 51/2000, vlb -6.485561, \u001b[31m   time: 0.393606 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.289849 (-6.216572)\n",
      "\u001b[0m\n",
      "it 52/2000, vlb -6.464087, \u001b[31m   time: 0.398593 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.245654 (-6.216572)\n",
      "\u001b[0m\n",
      "it 53/2000, vlb -6.515031, \u001b[31m   time: 0.432816 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.291599 (-6.216572)\n",
      "\u001b[0m\n",
      "it 54/2000, vlb -6.492559, \u001b[31m   time: 0.436835 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.258985 (-6.216572)\n",
      "\u001b[0m\n",
      "it 55/2000, vlb -6.501834, \u001b[31m   time: 0.379085 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.355245 (-6.216572)\n",
      "\u001b[0m\n",
      "it 56/2000, vlb -6.449416, \u001b[31m   time: 0.394454 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.450884 (-6.216572)\n",
      "\u001b[0m\n",
      "it 57/2000, vlb -6.550675, \u001b[31m   time: 0.374028 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.168288 (-6.216572)\n",
      "\u001b[0m\n",
      "it 58/2000, vlb -6.492596, \u001b[31m   time: 0.360115 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.219998 (-6.168288)\n",
      "\u001b[0m\n",
      "it 59/2000, vlb -6.508332, \u001b[31m   time: 0.392271 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.368841 (-6.168288)\n",
      "\u001b[0m\n",
      "it 60/2000, vlb -6.481346, \u001b[31m   time: 0.377970 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.279795 (-6.168288)\n",
      "\u001b[0m\n",
      "it 61/2000, vlb -6.518623, \u001b[31m   time: 0.388374 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.234138 (-6.168288)\n",
      "\u001b[0m\n",
      "it 62/2000, vlb -6.505557, \u001b[31m   time: 0.397022 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.268100 (-6.168288)\n",
      "\u001b[0m\n",
      "it 63/2000, vlb -6.492509, \u001b[31m   time: 0.371542 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.296509 (-6.168288)\n",
      "\u001b[0m\n",
      "it 64/2000, vlb -6.453134, \u001b[31m   time: 0.395938 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.252588 (-6.168288)\n",
      "\u001b[0m\n",
      "it 65/2000, vlb -6.481254, \u001b[31m   time: 0.382918 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.262339 (-6.168288)\n",
      "\u001b[0m\n",
      "it 66/2000, vlb -6.513003, \u001b[31m   time: 0.382006 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.234205 (-6.168288)\n",
      "\u001b[0m\n",
      "it 67/2000, vlb -6.432446, \u001b[31m   time: 0.400696 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.359850 (-6.168288)\n",
      "\u001b[0m\n",
      "it 68/2000, vlb -6.419825, \u001b[31m   time: 0.389089 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.247389 (-6.168288)\n",
      "\u001b[0m\n",
      "it 69/2000, vlb -6.458601, \u001b[31m   time: 0.404002 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.257127 (-6.168288)\n",
      "\u001b[0m\n",
      "it 70/2000, vlb -6.484802, \u001b[31m   time: 0.372077 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.217332 (-6.168288)\n",
      "\u001b[0m\n",
      "it 71/2000, vlb -6.463741, \u001b[31m   time: 0.390657 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.201847 (-6.168288)\n",
      "\u001b[0m\n",
      "it 72/2000, vlb -6.435058, \u001b[31m   time: 0.388275 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.237096 (-6.168288)\n",
      "\u001b[0m\n",
      "it 73/2000, vlb -6.475032, \u001b[31m   time: 0.396106 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.318640 (-6.168288)\n",
      "\u001b[0m\n",
      "it 74/2000, vlb -6.457218, \u001b[31m   time: 0.395350 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.245541 (-6.168288)\n",
      "\u001b[0m\n",
      "it 75/2000, vlb -6.438645, \u001b[31m   time: 0.382007 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.392649 (-6.168288)\n",
      "\u001b[0m\n",
      "it 76/2000, vlb -6.441512, \u001b[31m   time: 0.396013 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.191979 (-6.168288)\n",
      "\u001b[0m\n",
      "it 77/2000, vlb -6.483278, \u001b[31m   time: 0.382117 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.306088 (-6.168288)\n",
      "\u001b[0m\n",
      "it 78/2000, vlb -6.489907, \u001b[31m   time: 0.384312 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.280094 (-6.168288)\n",
      "\u001b[0m\n",
      "it 79/2000, vlb -6.472279, \u001b[31m   time: 0.393821 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.308244 (-6.168288)\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 80/2000, vlb -6.473291, \u001b[31m   time: 0.380642 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.231806 (-6.168288)\n",
      "\u001b[0m\n",
      "it 81/2000, vlb -6.386450, \u001b[31m   time: 0.393340 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.284911 (-6.168288)\n",
      "\u001b[0m\n",
      "it 82/2000, vlb -6.471514, \u001b[31m   time: 0.415109 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.351651 (-6.168288)\n",
      "\u001b[0m\n",
      "it 83/2000, vlb -6.398108, \u001b[31m   time: 0.404891 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.361766 (-6.168288)\n",
      "\u001b[0m\n",
      "it 84/2000, vlb -6.409984, \u001b[31m   time: 0.378587 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.253311 (-6.168288)\n",
      "\u001b[0m\n",
      "it 85/2000, vlb -6.472214, \u001b[31m   time: 0.387001 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.142238 (-6.168288)\n",
      "\u001b[0m\n",
      "it 86/2000, vlb -6.500485, \u001b[31m   time: 0.393407 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.310709 (-6.142238)\n",
      "\u001b[0m\n",
      "it 87/2000, vlb -6.443244, \u001b[31m   time: 0.392277 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.206509 (-6.142238)\n",
      "\u001b[0m\n",
      "it 88/2000, vlb -6.431979, \u001b[31m   time: 0.391954 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.260895 (-6.142238)\n",
      "\u001b[0m\n",
      "it 89/2000, vlb -6.426457, \u001b[31m   time: 0.387619 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.168997 (-6.142238)\n",
      "\u001b[0m\n",
      "it 90/2000, vlb -6.472029, \u001b[31m   time: 0.379095 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.236875 (-6.142238)\n",
      "\u001b[0m\n",
      "it 91/2000, vlb -6.448637, \u001b[31m   time: 0.379656 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.276963 (-6.142238)\n",
      "\u001b[0m\n",
      "it 92/2000, vlb -6.371496, \u001b[31m   time: 0.377418 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.189138 (-6.142238)\n",
      "\u001b[0m\n",
      "it 93/2000, vlb -6.436463, \u001b[31m   time: 0.404902 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.294056 (-6.142238)\n",
      "\u001b[0m\n",
      "it 94/2000, vlb -6.462766, \u001b[31m   time: 0.397779 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.295352 (-6.142238)\n",
      "\u001b[0m\n",
      "it 95/2000, vlb -6.462404, \u001b[31m   time: 0.412825 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.343885 (-6.142238)\n",
      "\u001b[0m\n",
      "it 96/2000, vlb -6.452613, \u001b[31m   time: 0.383406 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.359772 (-6.142238)\n",
      "\u001b[0m\n",
      "it 97/2000, vlb -6.426744, \u001b[31m   time: 0.388556 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.309960 (-6.142238)\n",
      "\u001b[0m\n",
      "it 98/2000, vlb -6.453786, \u001b[31m   time: 0.387430 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.274856 (-6.142238)\n",
      "\u001b[0m\n",
      "it 99/2000, vlb -6.433890, \u001b[31m   time: 0.384098 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.163259 (-6.142238)\n",
      "\u001b[0m\n",
      "it 100/2000, vlb -6.438575, \u001b[31m   time: 0.391375 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.230149 (-6.142238)\n",
      "\u001b[0m\n",
      "it 101/2000, vlb -6.372888, \u001b[31m   time: 0.375843 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.276348 (-6.142238)\n",
      "\u001b[0m\n",
      "it 102/2000, vlb -6.436566, \u001b[31m   time: 0.386595 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.191784 (-6.142238)\n",
      "\u001b[0m\n",
      "it 103/2000, vlb -6.415372, \u001b[31m   time: 0.398541 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.132022 (-6.142238)\n",
      "\u001b[0m\n",
      "it 104/2000, vlb -6.403253, \u001b[31m   time: 0.383773 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.295188 (-6.132022)\n",
      "\u001b[0m\n",
      "it 105/2000, vlb -6.400572, \u001b[31m   time: 0.397909 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.238157 (-6.132022)\n",
      "\u001b[0m\n",
      "it 106/2000, vlb -6.351787, \u001b[31m   time: 0.374219 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.320235 (-6.132022)\n",
      "\u001b[0m\n",
      "it 107/2000, vlb -6.395084, \u001b[31m   time: 0.389076 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.263817 (-6.132022)\n",
      "\u001b[0m\n",
      "it 108/2000, vlb -6.472834, \u001b[31m   time: 0.399319 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.255226 (-6.132022)\n",
      "\u001b[0m\n",
      "it 109/2000, vlb -6.426515, \u001b[31m   time: 0.373335 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.264329 (-6.132022)\n",
      "\u001b[0m\n",
      "it 110/2000, vlb -6.429921, \u001b[31m   time: 0.400898 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.122314 (-6.132022)\n",
      "\u001b[0m\n",
      "it 111/2000, vlb -6.389681, \u001b[31m   time: 0.382467 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.246449 (-6.122314)\n",
      "\u001b[0m\n",
      "it 112/2000, vlb -6.460275, \u001b[31m   time: 0.387711 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.247409 (-6.122314)\n",
      "\u001b[0m\n",
      "it 113/2000, vlb -6.380900, \u001b[31m   time: 0.384088 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.396470 (-6.122314)\n",
      "\u001b[0m\n",
      "it 114/2000, vlb -6.392265, \u001b[31m   time: 0.388064 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.266059 (-6.122314)\n",
      "\u001b[0m\n",
      "it 115/2000, vlb -6.432129, \u001b[31m   time: 0.396311 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.310625 (-6.122314)\n",
      "\u001b[0m\n",
      "it 116/2000, vlb -6.453289, \u001b[31m   time: 0.386472 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.287644 (-6.122314)\n",
      "\u001b[0m\n",
      "it 117/2000, vlb -6.399974, \u001b[31m   time: 0.393009 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.317756 (-6.122314)\n",
      "\u001b[0m\n",
      "it 118/2000, vlb -6.439119, \u001b[31m   time: 0.389219 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.303693 (-6.122314)\n",
      "\u001b[0m\n",
      "it 119/2000, vlb -6.410205, \u001b[31m   time: 0.389029 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.206802 (-6.122314)\n",
      "\u001b[0m\n",
      "it 120/2000, vlb -6.455951, \u001b[31m   time: 0.452789 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.184259 (-6.122314)\n",
      "\u001b[0m\n",
      "it 121/2000, vlb -6.435151, \u001b[31m   time: 0.398966 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.344493 (-6.122314)\n",
      "\u001b[0m\n",
      "it 122/2000, vlb -6.364682, \u001b[31m   time: 0.391607 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.242186 (-6.122314)\n",
      "\u001b[0m\n",
      "it 123/2000, vlb -6.379863, \u001b[31m   time: 0.382360 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.218091 (-6.122314)\n",
      "\u001b[0m\n",
      "it 124/2000, vlb -6.441211, \u001b[31m   time: 0.402922 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.299200 (-6.122314)\n",
      "\u001b[0m\n",
      "it 125/2000, vlb -6.385339, \u001b[31m   time: 0.386533 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.246787 (-6.122314)\n",
      "\u001b[0m\n",
      "it 126/2000, vlb -6.419877, \u001b[31m   time: 0.388535 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.289606 (-6.122314)\n",
      "\u001b[0m\n",
      "it 127/2000, vlb -6.411112, \u001b[31m   time: 0.414649 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.258572 (-6.122314)\n",
      "\u001b[0m\n",
      "it 128/2000, vlb -6.401424, \u001b[31m   time: 0.484084 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.178901 (-6.122314)\n",
      "\u001b[0m\n",
      "it 129/2000, vlb -6.455891, \u001b[31m   time: 0.477151 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.194531 (-6.122314)\n",
      "\u001b[0m\n",
      "it 130/2000, vlb -6.426508, \u001b[31m   time: 0.464701 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.228510 (-6.122314)\n",
      "\u001b[0m\n",
      "it 131/2000, vlb -6.444887, \u001b[31m   time: 0.496442 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.239888 (-6.122314)\n",
      "\u001b[0m\n",
      "it 132/2000, vlb -6.372680, \u001b[31m   time: 0.500408 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.195884 (-6.122314)\n",
      "\u001b[0m\n",
      "it 133/2000, vlb -6.368354, \u001b[31m   time: 0.456093 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.326393 (-6.122314)\n",
      "\u001b[0m\n",
      "it 134/2000, vlb -6.405924, \u001b[31m   time: 0.409307 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.247243 (-6.122314)\n",
      "\u001b[0m\n",
      "it 135/2000, vlb -6.426176, \u001b[31m   time: 0.394004 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.176351 (-6.122314)\n",
      "\u001b[0m\n",
      "it 136/2000, vlb -6.342021, \u001b[31m   time: 0.380017 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.210643 (-6.122314)\n",
      "\u001b[0m\n",
      "it 137/2000, vlb -6.373737, \u001b[31m   time: 0.390469 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.104055 (-6.122314)\n",
      "\u001b[0m\n",
      "it 138/2000, vlb -6.418978, \u001b[31m   time: 0.430685 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.374614 (-6.104055)\n",
      "\u001b[0m\n",
      "it 139/2000, vlb -6.373557, \u001b[31m   time: 0.394944 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.173970 (-6.104055)\n",
      "\u001b[0m\n",
      "it 140/2000, vlb -6.393095, \u001b[31m   time: 0.440637 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.240303 (-6.104055)\n",
      "\u001b[0m\n",
      "it 141/2000, vlb -6.405152, \u001b[31m   time: 0.388980 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.189251 (-6.104055)\n",
      "\u001b[0m\n",
      "it 142/2000, vlb -6.475075, \u001b[31m   time: 0.429338 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.207600 (-6.104055)\n",
      "\u001b[0m\n",
      "it 143/2000, vlb -6.377919, \u001b[31m   time: 0.377022 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.244335 (-6.104055)\n",
      "\u001b[0m\n",
      "it 144/2000, vlb -6.405451, \u001b[31m   time: 0.372065 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.189909 (-6.104055)\n",
      "\u001b[0m\n",
      "it 145/2000, vlb -6.324549, \u001b[31m   time: 0.360845 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.221133 (-6.104055)\n",
      "\u001b[0m\n",
      "it 146/2000, vlb -6.386234, \u001b[31m   time: 0.375964 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.258700 (-6.104055)\n",
      "\u001b[0m\n",
      "it 147/2000, vlb -6.369527, \u001b[31m   time: 0.386561 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.166249 (-6.104055)\n",
      "\u001b[0m\n",
      "it 148/2000, vlb -6.414779, \u001b[31m   time: 0.381731 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.176611 (-6.104055)\n",
      "\u001b[0m\n",
      "it 149/2000, vlb -6.425179, \u001b[31m   time: 0.416695 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.232351 (-6.104055)\n",
      "\u001b[0m\n",
      "it 150/2000, vlb -6.386235, \u001b[31m   time: 0.394487 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.193677 (-6.104055)\n",
      "\u001b[0m\n",
      "it 151/2000, vlb -6.382286, \u001b[31m   time: 0.428060 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.286171 (-6.104055)\n",
      "\u001b[0m\n",
      "it 152/2000, vlb -6.351092, \u001b[31m   time: 0.403765 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.323278 (-6.104055)\n",
      "\u001b[0m\n",
      "it 153/2000, vlb -6.358364, \u001b[31m   time: 0.500418 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.273022 (-6.104055)\n",
      "\u001b[0m\n",
      "it 154/2000, vlb -6.377086, \u001b[31m   time: 0.552522 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.178838 (-6.104055)\n",
      "\u001b[0m\n",
      "it 155/2000, vlb -6.357854, \u001b[31m   time: 0.429969 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.139759 (-6.104055)\n",
      "\u001b[0m\n",
      "it 156/2000, vlb -6.364808, \u001b[31m   time: 0.422173 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.258726 (-6.104055)\n",
      "\u001b[0m\n",
      "it 157/2000, vlb -6.402130, \u001b[31m   time: 0.428874 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.182375 (-6.104055)\n",
      "\u001b[0m\n",
      "it 158/2000, vlb -6.426406, \u001b[31m   time: 0.426861 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.165292 (-6.104055)\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 159/2000, vlb -6.401927, \u001b[31m   time: 0.418896 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.223312 (-6.104055)\n",
      "\u001b[0m\n",
      "it 160/2000, vlb -6.393282, \u001b[31m   time: 0.397519 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.289355 (-6.104055)\n",
      "\u001b[0m\n",
      "it 161/2000, vlb -6.371887, \u001b[31m   time: 0.370357 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.202980 (-6.104055)\n",
      "\u001b[0m\n",
      "it 162/2000, vlb -6.346342, \u001b[31m   time: 0.402110 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.243317 (-6.104055)\n",
      "\u001b[0m\n",
      "it 163/2000, vlb -6.413532, \u001b[31m   time: 0.373903 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.352031 (-6.104055)\n",
      "\u001b[0m\n",
      "it 164/2000, vlb -6.375266, \u001b[31m   time: 0.394070 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.286810 (-6.104055)\n",
      "\u001b[0m\n",
      "it 165/2000, vlb -6.395371, \u001b[31m   time: 0.384570 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.279281 (-6.104055)\n",
      "\u001b[0m\n",
      "it 166/2000, vlb -6.443645, \u001b[31m   time: 0.379833 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.256909 (-6.104055)\n",
      "\u001b[0m\n",
      "it 167/2000, vlb -6.383839, \u001b[31m   time: 0.424783 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.160509 (-6.104055)\n",
      "\u001b[0m\n",
      "it 168/2000, vlb -6.364690, \u001b[31m   time: 0.417272 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.263319 (-6.104055)\n",
      "\u001b[0m\n",
      "it 169/2000, vlb -6.395268, \u001b[31m   time: 0.386472 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.280783 (-6.104055)\n",
      "\u001b[0m\n",
      "it 170/2000, vlb -6.359037, \u001b[31m   time: 0.390060 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.175438 (-6.104055)\n",
      "\u001b[0m\n",
      "it 171/2000, vlb -6.389065, \u001b[31m   time: 0.377904 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.166153 (-6.104055)\n",
      "\u001b[0m\n",
      "it 172/2000, vlb -6.382263, \u001b[31m   time: 0.394020 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.157849 (-6.104055)\n",
      "\u001b[0m\n",
      "it 173/2000, vlb -6.396658, \u001b[31m   time: 0.384227 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.196717 (-6.104055)\n",
      "\u001b[0m\n",
      "it 174/2000, vlb -6.430009, \u001b[31m   time: 0.394648 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.104762 (-6.104055)\n",
      "\u001b[0m\n",
      "it 175/2000, vlb -6.445462, \u001b[31m   time: 0.380708 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.202377 (-6.104055)\n",
      "\u001b[0m\n",
      "it 176/2000, vlb -6.362474, \u001b[31m   time: 0.385940 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.218806 (-6.104055)\n",
      "\u001b[0m\n",
      "it 177/2000, vlb -6.346675, \u001b[31m   time: 0.403500 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.202567 (-6.104055)\n",
      "\u001b[0m\n",
      "it 178/2000, vlb -6.319335, \u001b[31m   time: 0.401873 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.248425 (-6.104055)\n",
      "\u001b[0m\n",
      "it 179/2000, vlb -6.322217, \u001b[31m   time: 0.395137 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.164777 (-6.104055)\n",
      "\u001b[0m\n",
      "it 180/2000, vlb -6.373084, \u001b[31m   time: 0.382483 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.158966 (-6.104055)\n",
      "\u001b[0m\n",
      "it 181/2000, vlb -6.342805, \u001b[31m   time: 0.384519 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.186039 (-6.104055)\n",
      "\u001b[0m\n",
      "it 182/2000, vlb -6.370642, \u001b[31m   time: 0.394657 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.116267 (-6.104055)\n",
      "\u001b[0m\n",
      "it 183/2000, vlb -6.394186, \u001b[31m   time: 0.385245 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.183878 (-6.104055)\n",
      "\u001b[0m\n",
      "it 184/2000, vlb -6.397528, \u001b[31m   time: 0.388961 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.260019 (-6.104055)\n",
      "\u001b[0m\n",
      "it 185/2000, vlb -6.313308, \u001b[31m   time: 0.375787 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.261697 (-6.104055)\n",
      "\u001b[0m\n",
      "it 186/2000, vlb -6.402401, \u001b[31m   time: 0.398309 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.162201 (-6.104055)\n",
      "\u001b[0m\n",
      "it 187/2000, vlb -6.420705, \u001b[31m   time: 0.382928 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.199998 (-6.104055)\n",
      "\u001b[0m\n",
      "it 188/2000, vlb -6.387948, \u001b[31m   time: 0.390631 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.215988 (-6.104055)\n",
      "\u001b[0m\n",
      "it 189/2000, vlb -6.417575, \u001b[31m   time: 0.393943 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.265003 (-6.104055)\n",
      "\u001b[0m\n",
      "it 190/2000, vlb -6.397620, \u001b[31m   time: 0.379413 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.273739 (-6.104055)\n",
      "\u001b[0m\n",
      "it 191/2000, vlb -6.349389, \u001b[31m   time: 0.383561 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.062206 (-6.104055)\n",
      "\u001b[0m\n",
      "it 192/2000, vlb -6.360220, \u001b[31m   time: 0.385241 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.337243 (-6.062206)\n",
      "\u001b[0m\n",
      "it 193/2000, vlb -6.360075, \u001b[31m   time: 0.382976 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.193726 (-6.062206)\n",
      "\u001b[0m\n",
      "it 194/2000, vlb -6.395409, \u001b[31m   time: 0.399309 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.263071 (-6.062206)\n",
      "\u001b[0m\n",
      "it 195/2000, vlb -6.322039, \u001b[31m   time: 0.376533 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.202262 (-6.062206)\n",
      "\u001b[0m\n",
      "it 196/2000, vlb -6.379585, \u001b[31m   time: 0.401914 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.257507 (-6.062206)\n",
      "\u001b[0m\n",
      "it 197/2000, vlb -6.398529, \u001b[31m   time: 0.378850 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.136273 (-6.062206)\n",
      "\u001b[0m\n",
      "it 198/2000, vlb -6.365523, \u001b[31m   time: 0.383976 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.309719 (-6.062206)\n",
      "\u001b[0m\n",
      "it 199/2000, vlb -6.372401, \u001b[31m   time: 0.378881 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.166941 (-6.062206)\n",
      "\u001b[0m\n",
      "it 200/2000, vlb -6.376848, \u001b[31m   time: 0.383735 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.183154 (-6.062206)\n",
      "\u001b[0m\n",
      "it 201/2000, vlb -6.401118, \u001b[31m   time: 0.411716 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.186520 (-6.062206)\n",
      "\u001b[0m\n",
      "it 202/2000, vlb -6.347909, \u001b[31m   time: 0.434430 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.213429 (-6.062206)\n",
      "\u001b[0m\n",
      "it 203/2000, vlb -6.347407, \u001b[31m   time: 0.412158 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.193879 (-6.062206)\n",
      "\u001b[0m\n",
      "it 204/2000, vlb -6.343787, \u001b[31m   time: 0.384918 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.186280 (-6.062206)\n",
      "\u001b[0m\n",
      "it 205/2000, vlb -6.354960, \u001b[31m   time: 0.421834 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.210656 (-6.062206)\n",
      "\u001b[0m\n",
      "it 206/2000, vlb -6.347369, \u001b[31m   time: 0.399900 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.176444 (-6.062206)\n",
      "\u001b[0m\n",
      "it 207/2000, vlb -6.392636, \u001b[31m   time: 0.397688 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.111709 (-6.062206)\n",
      "\u001b[0m\n",
      "it 208/2000, vlb -6.340210, \u001b[31m   time: 0.397975 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.157828 (-6.062206)\n",
      "\u001b[0m\n",
      "it 209/2000, vlb -6.329437, \u001b[31m   time: 0.381954 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.198686 (-6.062206)\n",
      "\u001b[0m\n",
      "it 210/2000, vlb -6.328495, \u001b[31m   time: 0.393949 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.208847 (-6.062206)\n",
      "\u001b[0m\n",
      "it 211/2000, vlb -6.401650, \u001b[31m   time: 0.391062 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.247310 (-6.062206)\n",
      "\u001b[0m\n",
      "it 212/2000, vlb -6.346339, \u001b[31m   time: 0.392674 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.356152 (-6.062206)\n",
      "\u001b[0m\n",
      "it 213/2000, vlb -6.369077, \u001b[31m   time: 0.396800 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.274774 (-6.062206)\n",
      "\u001b[0m\n",
      "it 214/2000, vlb -6.371627, \u001b[31m   time: 0.375628 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.216052 (-6.062206)\n",
      "\u001b[0m\n",
      "it 215/2000, vlb -6.385592, \u001b[31m   time: 0.399603 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.361849 (-6.062206)\n",
      "\u001b[0m\n",
      "it 216/2000, vlb -6.376894, \u001b[31m   time: 0.373105 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.264883 (-6.062206)\n",
      "\u001b[0m\n",
      "it 217/2000, vlb -6.356783, \u001b[31m   time: 0.386038 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.299995 (-6.062206)\n",
      "\u001b[0m\n",
      "it 218/2000, vlb -6.366188, \u001b[31m   time: 0.420446 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.258332 (-6.062206)\n",
      "\u001b[0m\n",
      "it 219/2000, vlb -6.330685, \u001b[31m   time: 0.379700 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.181437 (-6.062206)\n",
      "\u001b[0m\n",
      "it 220/2000, vlb -6.338761, \u001b[31m   time: 0.398400 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.194184 (-6.062206)\n",
      "\u001b[0m\n",
      "it 221/2000, vlb -6.357276, \u001b[31m   time: 0.387906 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.144634 (-6.062206)\n",
      "\u001b[0m\n",
      "it 222/2000, vlb -6.374689, \u001b[31m   time: 0.386971 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.276395 (-6.062206)\n",
      "\u001b[0m\n",
      "it 223/2000, vlb -6.367491, \u001b[31m   time: 0.385677 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.243279 (-6.062206)\n",
      "\u001b[0m\n",
      "it 224/2000, vlb -6.404842, \u001b[31m   time: 0.386599 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.266894 (-6.062206)\n",
      "\u001b[0m\n",
      "it 225/2000, vlb -6.380295, \u001b[31m   time: 0.393035 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.209883 (-6.062206)\n",
      "\u001b[0m\n",
      "it 226/2000, vlb -6.363793, \u001b[31m   time: 0.374000 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.258653 (-6.062206)\n",
      "\u001b[0m\n",
      "it 227/2000, vlb -6.336550, \u001b[31m   time: 0.401063 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.244826 (-6.062206)\n",
      "\u001b[0m\n",
      "it 228/2000, vlb -6.375894, \u001b[31m   time: 0.392116 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.313524 (-6.062206)\n",
      "\u001b[0m\n",
      "it 229/2000, vlb -6.347680, \u001b[31m   time: 0.384009 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.120619 (-6.062206)\n",
      "\u001b[0m\n",
      "it 230/2000, vlb -6.326194, \u001b[31m   time: 0.385956 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.204850 (-6.062206)\n",
      "\u001b[0m\n",
      "it 231/2000, vlb -6.345333, \u001b[31m   time: 0.380985 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.217616 (-6.062206)\n",
      "\u001b[0m\n",
      "it 232/2000, vlb -6.305329, \u001b[31m   time: 0.403388 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.109033 (-6.062206)\n",
      "\u001b[0m\n",
      "it 233/2000, vlb -6.380270, \u001b[31m   time: 0.380763 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.361197 (-6.062206)\n",
      "\u001b[0m\n",
      "it 234/2000, vlb -6.377004, \u001b[31m   time: 0.386572 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.332112 (-6.062206)\n",
      "\u001b[0m\n",
      "it 235/2000, vlb -6.372848, \u001b[31m   time: 0.389532 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.112258 (-6.062206)\n",
      "\u001b[0m\n",
      "it 236/2000, vlb -6.383532, \u001b[31m   time: 0.378554 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.015011 (-6.062206)\n",
      "\u001b[0m\n",
      "it 237/2000, vlb -6.380528, \u001b[31m   time: 0.391459 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.144919 (-6.015011)\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 238/2000, vlb -6.343128, \u001b[31m   time: 0.380256 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.114589 (-6.015011)\n",
      "\u001b[0m\n",
      "it 239/2000, vlb -6.356722, \u001b[31m   time: 0.392980 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.149192 (-6.015011)\n",
      "\u001b[0m\n",
      "it 240/2000, vlb -6.351412, \u001b[31m   time: 0.420211 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.104270 (-6.015011)\n",
      "\u001b[0m\n",
      "it 241/2000, vlb -6.393960, \u001b[31m   time: 0.403611 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.233919 (-6.015011)\n",
      "\u001b[0m\n",
      "it 242/2000, vlb -6.368896, \u001b[31m   time: 0.387712 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.113433 (-6.015011)\n",
      "\u001b[0m\n",
      "it 243/2000, vlb -6.347166, \u001b[31m   time: 0.398816 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.105509 (-6.015011)\n",
      "\u001b[0m\n",
      "it 244/2000, vlb -6.328558, \u001b[31m   time: 0.415889 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.390550 (-6.015011)\n",
      "\u001b[0m\n",
      "it 245/2000, vlb -6.358860, \u001b[31m   time: 0.407058 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.256924 (-6.015011)\n",
      "\u001b[0m\n",
      "it 246/2000, vlb -6.391318, \u001b[31m   time: 0.394818 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.109604 (-6.015011)\n",
      "\u001b[0m\n",
      "it 247/2000, vlb -6.410911, \u001b[31m   time: 0.381304 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.171204 (-6.015011)\n",
      "\u001b[0m\n",
      "it 248/2000, vlb -6.345385, \u001b[31m   time: 0.377602 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.259896 (-6.015011)\n",
      "\u001b[0m\n",
      "it 249/2000, vlb -6.341314, \u001b[31m   time: 0.384977 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.195500 (-6.015011)\n",
      "\u001b[0m\n",
      "it 250/2000, vlb -6.355583, \u001b[31m   time: 0.385865 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.238656 (-6.015011)\n",
      "\u001b[0m\n",
      "it 251/2000, vlb -6.358279, \u001b[31m   time: 0.394893 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.170745 (-6.015011)\n",
      "\u001b[0m\n",
      "it 252/2000, vlb -6.337807, \u001b[31m   time: 0.382796 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.233476 (-6.015011)\n",
      "\u001b[0m\n",
      "it 253/2000, vlb -6.335090, \u001b[31m   time: 0.386161 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.051350 (-6.015011)\n",
      "\u001b[0m\n",
      "it 254/2000, vlb -6.332237, \u001b[31m   time: 0.395314 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.220340 (-6.015011)\n",
      "\u001b[0m\n",
      "it 255/2000, vlb -6.368393, \u001b[31m   time: 0.374885 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.270860 (-6.015011)\n",
      "\u001b[0m\n",
      "it 256/2000, vlb -6.345861, \u001b[31m   time: 0.411348 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.125951 (-6.015011)\n",
      "\u001b[0m\n",
      "it 257/2000, vlb -6.343883, \u001b[31m   time: 0.388439 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.237266 (-6.015011)\n",
      "\u001b[0m\n",
      "it 258/2000, vlb -6.374729, \u001b[31m   time: 0.392658 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.229874 (-6.015011)\n",
      "\u001b[0m\n",
      "it 259/2000, vlb -6.383869, \u001b[31m   time: 0.386050 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.301381 (-6.015011)\n",
      "\u001b[0m\n",
      "it 260/2000, vlb -6.340481, \u001b[31m   time: 0.389054 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.376119 (-6.015011)\n",
      "\u001b[0m\n",
      "it 261/2000, vlb -6.326301, \u001b[31m   time: 0.398965 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.296151 (-6.015011)\n",
      "\u001b[0m\n",
      "it 262/2000, vlb -6.401118, \u001b[31m   time: 0.377494 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.207584 (-6.015011)\n",
      "\u001b[0m\n",
      "it 263/2000, vlb -6.358273, \u001b[31m   time: 0.397002 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.112452 (-6.015011)\n",
      "\u001b[0m\n",
      "it 264/2000, vlb -6.387786, \u001b[31m   time: 0.385966 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.219449 (-6.015011)\n",
      "\u001b[0m\n",
      "it 265/2000, vlb -6.330563, \u001b[31m   time: 0.382998 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.162625 (-6.015011)\n",
      "\u001b[0m\n",
      "it 266/2000, vlb -6.291264, \u001b[31m   time: 0.391289 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.192212 (-6.015011)\n",
      "\u001b[0m\n",
      "it 267/2000, vlb -6.326100, \u001b[31m   time: 0.387319 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.239599 (-6.015011)\n",
      "\u001b[0m\n",
      "it 268/2000, vlb -6.323055, \u001b[31m   time: 0.393518 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.224506 (-6.015011)\n",
      "\u001b[0m\n",
      "it 269/2000, vlb -6.343997, \u001b[31m   time: 0.382194 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.286340 (-6.015011)\n",
      "\u001b[0m\n",
      "it 270/2000, vlb -6.374232, \u001b[31m   time: 0.387890 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.287655 (-6.015011)\n",
      "\u001b[0m\n",
      "it 271/2000, vlb -6.298626, \u001b[31m   time: 0.400469 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.193278 (-6.015011)\n",
      "\u001b[0m\n",
      "it 272/2000, vlb -6.313522, \u001b[31m   time: 0.399443 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.207081 (-6.015011)\n",
      "\u001b[0m\n",
      "it 273/2000, vlb -6.318982, \u001b[31m   time: 0.477956 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.154690 (-6.015011)\n",
      "\u001b[0m\n",
      "it 274/2000, vlb -6.307695, \u001b[31m   time: 0.438060 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.306392 (-6.015011)\n",
      "\u001b[0m\n",
      "it 275/2000, vlb -6.403013, \u001b[31m   time: 0.448451 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.200747 (-6.015011)\n",
      "\u001b[0m\n",
      "it 276/2000, vlb -6.315960, \u001b[31m   time: 0.425473 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.255857 (-6.015011)\n",
      "\u001b[0m\n",
      "it 277/2000, vlb -6.338354, \u001b[31m   time: 0.435398 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.169512 (-6.015011)\n",
      "\u001b[0m\n",
      "it 278/2000, vlb -6.292313, \u001b[31m   time: 0.428122 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.208299 (-6.015011)\n",
      "\u001b[0m\n",
      "it 279/2000, vlb -6.336522, \u001b[31m   time: 0.499388 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.295178 (-6.015011)\n",
      "\u001b[0m\n",
      "it 280/2000, vlb -6.332445, \u001b[31m   time: 0.477723 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.157454 (-6.015011)\n",
      "\u001b[0m\n",
      "it 281/2000, vlb -6.368458, \u001b[31m   time: 0.462305 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.321591 (-6.015011)\n",
      "\u001b[0m\n",
      "it 282/2000, vlb -6.370698, \u001b[31m   time: 0.455785 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.339466 (-6.015011)\n",
      "\u001b[0m\n",
      "it 283/2000, vlb -6.352298, \u001b[31m   time: 0.464074 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.280672 (-6.015011)\n",
      "\u001b[0m\n",
      "it 284/2000, vlb -6.340931, \u001b[31m   time: 0.416453 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.274566 (-6.015011)\n",
      "\u001b[0m\n",
      "it 285/2000, vlb -6.339817, \u001b[31m   time: 0.429622 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.279434 (-6.015011)\n",
      "\u001b[0m\n",
      "it 286/2000, vlb -6.380624, \u001b[31m   time: 0.429317 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.236518 (-6.015011)\n",
      "\u001b[0m\n",
      "it 287/2000, vlb -6.368270, \u001b[31m   time: 0.426260 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.206866 (-6.015011)\n",
      "\u001b[0m\n",
      "it 288/2000, vlb -6.382415, \u001b[31m   time: 0.437120 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.134177 (-6.015011)\n",
      "\u001b[0m\n",
      "it 289/2000, vlb -6.317781, \u001b[31m   time: 0.422023 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.188003 (-6.015011)\n",
      "\u001b[0m\n",
      "it 290/2000, vlb -6.352612, \u001b[31m   time: 0.437484 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.241715 (-6.015011)\n",
      "\u001b[0m\n",
      "it 291/2000, vlb -6.382113, \u001b[31m   time: 0.370010 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.088479 (-6.015011)\n",
      "\u001b[0m\n",
      "it 292/2000, vlb -6.408338, \u001b[31m   time: 0.369017 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.232669 (-6.015011)\n",
      "\u001b[0m\n",
      "it 293/2000, vlb -6.327528, \u001b[31m   time: 0.364372 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.177947 (-6.015011)\n",
      "\u001b[0m\n",
      "it 294/2000, vlb -6.361823, \u001b[31m   time: 0.364124 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.272213 (-6.015011)\n",
      "\u001b[0m\n",
      "it 295/2000, vlb -6.365094, \u001b[31m   time: 0.397965 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.281102 (-6.015011)\n",
      "\u001b[0m\n",
      "it 296/2000, vlb -6.323489, \u001b[31m   time: 0.378233 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.086105 (-6.015011)\n",
      "\u001b[0m\n",
      "it 297/2000, vlb -6.292382, \u001b[31m   time: 0.397032 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.296881 (-6.015011)\n",
      "\u001b[0m\n",
      "it 298/2000, vlb -6.349419, \u001b[31m   time: 0.390299 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.195363 (-6.015011)\n",
      "\u001b[0m\n",
      "it 299/2000, vlb -6.401409, \u001b[31m   time: 0.385844 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.035292 (-6.015011)\n",
      "\u001b[0m\n",
      "it 300/2000, vlb -6.311899, \u001b[31m   time: 0.394691 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.158713 (-6.015011)\n",
      "\u001b[0m\n",
      "it 301/2000, vlb -6.298153, \u001b[31m   time: 0.373537 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.259697 (-6.015011)\n",
      "\u001b[0m\n",
      "it 302/2000, vlb -6.357277, \u001b[31m   time: 0.396498 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.227777 (-6.015011)\n",
      "\u001b[0m\n",
      "it 303/2000, vlb -6.380558, \u001b[31m   time: 0.392182 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.260493 (-6.015011)\n",
      "\u001b[0m\n",
      "it 304/2000, vlb -6.332351, \u001b[31m   time: 0.387798 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.114526 (-6.015011)\n",
      "\u001b[0m\n",
      "it 305/2000, vlb -6.343712, \u001b[31m   time: 0.394465 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.147013 (-6.015011)\n",
      "\u001b[0m\n",
      "it 306/2000, vlb -6.325901, \u001b[31m   time: 0.376987 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.184828 (-6.015011)\n",
      "\u001b[0m\n",
      "it 307/2000, vlb -6.370399, \u001b[31m   time: 0.395080 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.030743 (-6.015011)\n",
      "\u001b[0m\n",
      "it 308/2000, vlb -6.334390, \u001b[31m   time: 0.379543 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.165606 (-6.015011)\n",
      "\u001b[0m\n",
      "it 309/2000, vlb -6.246794, \u001b[31m   time: 0.374191 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.188937 (-6.015011)\n",
      "\u001b[0m\n",
      "it 310/2000, vlb -6.336130, \u001b[31m   time: 0.383514 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.088597 (-6.015011)\n",
      "\u001b[0m\n",
      "it 311/2000, vlb -6.342999, \u001b[31m   time: 0.430880 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.243946 (-6.015011)\n",
      "\u001b[0m\n",
      "it 312/2000, vlb -6.356103, \u001b[31m   time: 0.460833 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.143040 (-6.015011)\n",
      "\u001b[0m\n",
      "it 313/2000, vlb -6.349585, \u001b[31m   time: 0.422533 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.187931 (-6.015011)\n",
      "\u001b[0m\n",
      "it 314/2000, vlb -6.347944, \u001b[31m   time: 0.439824 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.176369 (-6.015011)\n",
      "\u001b[0m\n",
      "it 315/2000, vlb -6.312444, \u001b[31m   time: 0.421873 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.167535 (-6.015011)\n",
      "\u001b[0m\n",
      "it 316/2000, vlb -6.341972, \u001b[31m   time: 0.426042 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -5.985919 (-6.015011)\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 317/2000, vlb -6.298844, \u001b[31m   time: 0.416886 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.169161 (-5.985919)\n",
      "\u001b[0m\n",
      "it 318/2000, vlb -6.355716, \u001b[31m   time: 0.433840 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.295353 (-5.985919)\n",
      "\u001b[0m\n",
      "it 319/2000, vlb -6.294968, \u001b[31m   time: 0.388863 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.254072 (-5.985919)\n",
      "\u001b[0m\n",
      "it 320/2000, vlb -6.300523, \u001b[31m   time: 0.372594 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.125876 (-5.985919)\n",
      "\u001b[0m\n",
      "it 321/2000, vlb -6.360593, \u001b[31m   time: 0.365350 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.196890 (-5.985919)\n",
      "\u001b[0m\n",
      "it 322/2000, vlb -6.369689, \u001b[31m   time: 0.391980 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.195996 (-5.985919)\n",
      "\u001b[0m\n",
      "it 323/2000, vlb -6.339726, \u001b[31m   time: 0.361067 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.193942 (-5.985919)\n",
      "\u001b[0m\n",
      "it 324/2000, vlb -6.336729, \u001b[31m   time: 0.376097 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.255024 (-5.985919)\n",
      "\u001b[0m\n",
      "it 325/2000, vlb -6.325942, \u001b[31m   time: 0.376993 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.106762 (-5.985919)\n",
      "\u001b[0m\n",
      "it 326/2000, vlb -6.329658, \u001b[31m   time: 0.392950 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.235408 (-5.985919)\n",
      "\u001b[0m\n",
      "it 327/2000, vlb -6.338459, \u001b[31m   time: 0.420706 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.144817 (-5.985919)\n",
      "\u001b[0m\n",
      "it 328/2000, vlb -6.316151, \u001b[31m   time: 0.404135 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.179990 (-5.985919)\n",
      "\u001b[0m\n",
      "it 329/2000, vlb -6.346917, \u001b[31m   time: 0.384459 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.155696 (-5.985919)\n",
      "\u001b[0m\n",
      "it 330/2000, vlb -6.375738, \u001b[31m   time: 0.376149 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.162234 (-5.985919)\n",
      "\u001b[0m\n",
      "it 331/2000, vlb -6.266425, \u001b[31m   time: 0.492614 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.223790 (-5.985919)\n",
      "\u001b[0m\n",
      "it 332/2000, vlb -6.341465, \u001b[31m   time: 0.429567 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.124891 (-5.985919)\n",
      "\u001b[0m\n",
      "it 333/2000, vlb -6.359886, \u001b[31m   time: 0.427857 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.248870 (-5.985919)\n",
      "\u001b[0m\n",
      "it 334/2000, vlb -6.313821, \u001b[31m   time: 0.396939 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.164349 (-5.985919)\n",
      "\u001b[0m\n",
      "it 335/2000, vlb -6.319760, \u001b[31m   time: 0.408437 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.159092 (-5.985919)\n",
      "\u001b[0m\n",
      "it 336/2000, vlb -6.333445, \u001b[31m   time: 0.422195 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.124408 (-5.985919)\n",
      "\u001b[0m\n",
      "it 337/2000, vlb -6.337615, \u001b[31m   time: 0.533087 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.353255 (-5.985919)\n",
      "\u001b[0m\n",
      "it 338/2000, vlb -6.325216, \u001b[31m   time: 0.464659 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.208817 (-5.985919)\n",
      "\u001b[0m\n",
      "it 339/2000, vlb -6.397146, \u001b[31m   time: 0.377560 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.154831 (-5.985919)\n",
      "\u001b[0m\n",
      "it 340/2000, vlb -6.314601, \u001b[31m   time: 0.375996 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.208879 (-5.985919)\n",
      "\u001b[0m\n",
      "it 341/2000, vlb -6.338189, \u001b[31m   time: 0.372964 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.214077 (-5.985919)\n",
      "\u001b[0m\n",
      "it 342/2000, vlb -6.339288, \u001b[31m   time: 0.380650 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.209295 (-5.985919)\n",
      "\u001b[0m\n",
      "it 343/2000, vlb -6.294591, \u001b[31m   time: 0.376090 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.205683 (-5.985919)\n",
      "\u001b[0m\n",
      "it 344/2000, vlb -6.296348, \u001b[31m   time: 0.399522 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.101107 (-5.985919)\n",
      "\u001b[0m\n",
      "it 345/2000, vlb -6.364709, \u001b[31m   time: 0.379579 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.204109 (-5.985919)\n",
      "\u001b[0m\n",
      "it 346/2000, vlb -6.343353, \u001b[31m   time: 0.390591 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.080511 (-5.985919)\n",
      "\u001b[0m\n",
      "it 347/2000, vlb -6.365459, \u001b[31m   time: 0.384139 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.199126 (-5.985919)\n",
      "\u001b[0m\n",
      "it 348/2000, vlb -6.379494, \u001b[31m   time: 0.379363 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.080064 (-5.985919)\n",
      "\u001b[0m\n",
      "it 349/2000, vlb -6.308123, \u001b[31m   time: 0.397288 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.134110 (-5.985919)\n",
      "\u001b[0m\n",
      "it 350/2000, vlb -6.306727, \u001b[31m   time: 0.377724 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.138655 (-5.985919)\n",
      "\u001b[0m\n",
      "it 351/2000, vlb -6.311297, \u001b[31m   time: 0.398846 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.146894 (-5.985919)\n",
      "\u001b[0m\n",
      "it 352/2000, vlb -6.321314, \u001b[31m   time: 0.404917 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.269183 (-5.985919)\n",
      "\u001b[0m\n",
      "it 353/2000, vlb -6.291220, \u001b[31m   time: 0.404469 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.231616 (-5.985919)\n",
      "\u001b[0m\n",
      "it 354/2000, vlb -6.322114, \u001b[31m   time: 0.423867 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.089065 (-5.985919)\n",
      "\u001b[0m\n",
      "it 355/2000, vlb -6.335825, \u001b[31m   time: 0.429469 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.321035 (-5.985919)\n",
      "\u001b[0m\n",
      "it 356/2000, vlb -6.308957, \u001b[31m   time: 0.446320 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.235956 (-5.985919)\n",
      "\u001b[0m\n",
      "it 357/2000, vlb -6.341237, \u001b[31m   time: 0.384805 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.219237 (-5.985919)\n",
      "\u001b[0m\n",
      "it 358/2000, vlb -6.331090, \u001b[31m   time: 0.402143 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.016883 (-5.985919)\n",
      "\u001b[0m\n",
      "it 359/2000, vlb -6.330204, \u001b[31m   time: 0.382418 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.275769 (-5.985919)\n",
      "\u001b[0m\n",
      "it 360/2000, vlb -6.308128, \u001b[31m   time: 0.381886 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.103370 (-5.985919)\n",
      "\u001b[0m\n",
      "it 361/2000, vlb -6.303768, \u001b[31m   time: 0.399732 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.118261 (-5.985919)\n",
      "\u001b[0m\n",
      "it 362/2000, vlb -6.278822, \u001b[31m   time: 0.379691 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.155582 (-5.985919)\n",
      "\u001b[0m\n",
      "it 363/2000, vlb -6.365789, \u001b[31m   time: 0.394115 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.169156 (-5.985919)\n",
      "\u001b[0m\n",
      "it 364/2000, vlb -6.378207, \u001b[31m   time: 0.392711 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.275194 (-5.985919)\n",
      "\u001b[0m\n",
      "it 365/2000, vlb -6.365727, \u001b[31m   time: 0.400911 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.215122 (-5.985919)\n",
      "\u001b[0m\n",
      "it 366/2000, vlb -6.323095, \u001b[31m   time: 0.389762 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.140134 (-5.985919)\n",
      "\u001b[0m\n",
      "it 367/2000, vlb -6.326839, \u001b[31m   time: 0.380455 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.194047 (-5.985919)\n",
      "\u001b[0m\n",
      "it 368/2000, vlb -6.331384, \u001b[31m   time: 0.397603 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.119842 (-5.985919)\n",
      "\u001b[0m\n",
      "it 369/2000, vlb -6.284337, \u001b[31m   time: 0.384701 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.097561 (-5.985919)\n",
      "\u001b[0m\n",
      "it 370/2000, vlb -6.323424, \u001b[31m   time: 0.401357 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.050320 (-5.985919)\n",
      "\u001b[0m\n",
      "it 371/2000, vlb -6.353771, \u001b[31m   time: 0.383754 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.115123 (-5.985919)\n",
      "\u001b[0m\n",
      "it 372/2000, vlb -6.341134, \u001b[31m   time: 0.391694 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.306105 (-5.985919)\n",
      "\u001b[0m\n",
      "it 373/2000, vlb -6.304254, \u001b[31m   time: 0.389418 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.111339 (-5.985919)\n",
      "\u001b[0m\n",
      "it 374/2000, vlb -6.291065, \u001b[31m   time: 0.383006 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.111791 (-5.985919)\n",
      "\u001b[0m\n",
      "it 375/2000, vlb -6.316310, \u001b[31m   time: 0.406940 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.198892 (-5.985919)\n",
      "\u001b[0m\n",
      "it 376/2000, vlb -6.345097, \u001b[31m   time: 0.392979 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.144392 (-5.985919)\n",
      "\u001b[0m\n",
      "it 377/2000, vlb -6.314745, \u001b[31m   time: 0.380486 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.190179 (-5.985919)\n",
      "\u001b[0m\n",
      "it 378/2000, vlb -6.346327, \u001b[31m   time: 0.393553 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.211349 (-5.985919)\n",
      "\u001b[0m\n",
      "it 379/2000, vlb -6.351883, \u001b[31m   time: 0.375919 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.145118 (-5.985919)\n",
      "\u001b[0m\n",
      "it 380/2000, vlb -6.343488, \u001b[31m   time: 0.396101 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.118621 (-5.985919)\n",
      "\u001b[0m\n",
      "it 381/2000, vlb -6.316203, \u001b[31m   time: 0.378561 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.104592 (-5.985919)\n",
      "\u001b[0m\n",
      "it 382/2000, vlb -6.330198, \u001b[31m   time: 0.439827 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.249996 (-5.985919)\n",
      "\u001b[0m\n",
      "it 383/2000, vlb -6.306186, \u001b[31m   time: 0.399525 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.245165 (-5.985919)\n",
      "\u001b[0m\n",
      "it 384/2000, vlb -6.357968, \u001b[31m   time: 0.387034 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.143719 (-5.985919)\n",
      "\u001b[0m\n",
      "it 385/2000, vlb -6.291429, \u001b[31m   time: 0.385545 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.274694 (-5.985919)\n",
      "\u001b[0m\n",
      "it 386/2000, vlb -6.339009, \u001b[31m   time: 0.390954 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.009661 (-5.985919)\n",
      "\u001b[0m\n",
      "it 387/2000, vlb -6.317761, \u001b[31m   time: 0.401985 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.197726 (-5.985919)\n",
      "\u001b[0m\n",
      "it 388/2000, vlb -6.327256, \u001b[31m   time: 0.381607 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.200181 (-5.985919)\n",
      "\u001b[0m\n",
      "it 389/2000, vlb -6.363551, \u001b[31m   time: 0.395820 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.301900 (-5.985919)\n",
      "\u001b[0m\n",
      "it 390/2000, vlb -6.296875, \u001b[31m   time: 0.391956 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.174623 (-5.985919)\n",
      "\u001b[0m\n",
      "it 391/2000, vlb -6.301892, \u001b[31m   time: 0.376976 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.066102 (-5.985919)\n",
      "\u001b[0m\n",
      "it 392/2000, vlb -6.284206, \u001b[31m   time: 0.406740 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.203519 (-5.985919)\n",
      "\u001b[0m\n",
      "it 393/2000, vlb -6.321986, \u001b[31m   time: 0.403796 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.133015 (-5.985919)\n",
      "\u001b[0m\n",
      "it 394/2000, vlb -6.294945, \u001b[31m   time: 0.427066 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.170207 (-5.985919)\n",
      "\u001b[0m\n",
      "it 395/2000, vlb -6.298400, \u001b[31m   time: 0.393429 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.136761 (-5.985919)\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 396/2000, vlb -6.337680, \u001b[31m   time: 0.392287 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.201905 (-5.985919)\n",
      "\u001b[0m\n",
      "it 397/2000, vlb -6.356670, \u001b[31m   time: 0.396938 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.068730 (-5.985919)\n",
      "\u001b[0m\n",
      "it 398/2000, vlb -6.299813, \u001b[31m   time: 0.389664 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.153541 (-5.985919)\n",
      "\u001b[0m\n",
      "it 399/2000, vlb -6.339164, \u001b[31m   time: 0.403922 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.101538 (-5.985919)\n",
      "\u001b[0m\n",
      "it 400/2000, vlb -6.286968, \u001b[31m   time: 0.380425 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.198867 (-5.985919)\n",
      "\u001b[0m\n",
      "it 401/2000, vlb -6.300365, \u001b[31m   time: 0.396438 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.205887 (-5.985919)\n",
      "\u001b[0m\n",
      "it 402/2000, vlb -6.366295, \u001b[31m   time: 0.393799 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.207232 (-5.985919)\n",
      "\u001b[0m\n",
      "it 403/2000, vlb -6.311619, \u001b[31m   time: 0.390012 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.112798 (-5.985919)\n",
      "\u001b[0m\n",
      "it 404/2000, vlb -6.337098, \u001b[31m   time: 0.398120 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.247663 (-5.985919)\n",
      "\u001b[0m\n",
      "it 405/2000, vlb -6.348216, \u001b[31m   time: 0.392493 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.125947 (-5.985919)\n",
      "\u001b[0m\n",
      "it 406/2000, vlb -6.381601, \u001b[31m   time: 0.407861 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.179163 (-5.985919)\n",
      "\u001b[0m\n",
      "it 407/2000, vlb -6.305793, \u001b[31m   time: 0.382572 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.107998 (-5.985919)\n",
      "\u001b[0m\n",
      "it 408/2000, vlb -6.305728, \u001b[31m   time: 0.393977 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.185402 (-5.985919)\n",
      "\u001b[0m\n",
      "it 409/2000, vlb -6.411217, \u001b[31m   time: 0.394206 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.097513 (-5.985919)\n",
      "\u001b[0m\n",
      "it 410/2000, vlb -6.365861, \u001b[31m   time: 0.377738 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.171004 (-5.985919)\n",
      "\u001b[0m\n",
      "it 411/2000, vlb -6.350843, \u001b[31m   time: 0.408925 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.100734 (-5.985919)\n",
      "\u001b[0m\n",
      "it 412/2000, vlb -6.283760, \u001b[31m   time: 0.382050 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.139628 (-5.985919)\n",
      "\u001b[0m\n",
      "it 413/2000, vlb -6.302349, \u001b[31m   time: 0.415932 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.177685 (-5.985919)\n",
      "\u001b[0m\n",
      "it 414/2000, vlb -6.329539, \u001b[31m   time: 0.401048 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.333333 (-5.985919)\n",
      "\u001b[0m\n",
      "it 415/2000, vlb -6.373340, \u001b[31m   time: 0.396385 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.185726 (-5.985919)\n",
      "\u001b[0m\n",
      "it 416/2000, vlb -6.356490, \u001b[31m   time: 0.395911 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.176600 (-5.985919)\n",
      "\u001b[0m\n",
      "it 417/2000, vlb -6.332496, \u001b[31m   time: 0.380052 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.159158 (-5.985919)\n",
      "\u001b[0m\n",
      "it 418/2000, vlb -6.320804, \u001b[31m   time: 0.401563 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.242664 (-5.985919)\n",
      "\u001b[0m\n",
      "it 419/2000, vlb -6.288965, \u001b[31m   time: 0.379706 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.065871 (-5.985919)\n",
      "\u001b[0m\n",
      "it 420/2000, vlb -6.318382, \u001b[31m   time: 0.396709 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.130052 (-5.985919)\n",
      "\u001b[0m\n",
      "it 421/2000, vlb -6.332497, \u001b[31m   time: 0.391589 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.152125 (-5.985919)\n",
      "\u001b[0m\n",
      "it 422/2000, vlb -6.305777, \u001b[31m   time: 0.383041 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.265123 (-5.985919)\n",
      "\u001b[0m\n",
      "it 423/2000, vlb -6.343689, \u001b[31m   time: 0.395238 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.124064 (-5.985919)\n",
      "\u001b[0m\n",
      "it 424/2000, vlb -6.330642, \u001b[31m   time: 0.381817 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.216644 (-5.985919)\n",
      "\u001b[0m\n",
      "it 425/2000, vlb -6.325055, \u001b[31m   time: 0.393780 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.184222 (-5.985919)\n",
      "\u001b[0m\n",
      "it 426/2000, vlb -6.309607, \u001b[31m   time: 0.380243 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.168944 (-5.985919)\n",
      "\u001b[0m\n",
      "it 427/2000, vlb -6.343885, \u001b[31m   time: 0.387002 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.216706 (-5.985919)\n",
      "\u001b[0m\n",
      "it 428/2000, vlb -6.308379, \u001b[31m   time: 0.436889 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.214878 (-5.985919)\n",
      "\u001b[0m\n",
      "it 429/2000, vlb -6.331465, \u001b[31m   time: 0.388833 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.022234 (-5.985919)\n",
      "\u001b[0m\n",
      "it 430/2000, vlb -6.253268, \u001b[31m   time: 0.414891 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.185041 (-5.985919)\n",
      "\u001b[0m\n",
      "it 431/2000, vlb -6.326424, \u001b[31m   time: 0.406694 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.183885 (-5.985919)\n",
      "\u001b[0m\n",
      "it 432/2000, vlb -6.281514, \u001b[31m   time: 0.410273 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.222094 (-5.985919)\n",
      "\u001b[0m\n",
      "it 433/2000, vlb -6.368256, \u001b[31m   time: 0.378570 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.149981 (-5.985919)\n",
      "\u001b[0m\n",
      "it 434/2000, vlb -6.328356, \u001b[31m   time: 0.380753 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.133273 (-5.985919)\n",
      "\u001b[0m\n",
      "it 435/2000, vlb -6.311161, \u001b[31m   time: 0.394928 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.186958 (-5.985919)\n",
      "\u001b[0m\n",
      "it 436/2000, vlb -6.312536, \u001b[31m   time: 0.388792 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.180505 (-5.985919)\n",
      "\u001b[0m\n",
      "it 437/2000, vlb -6.271369, \u001b[31m   time: 0.400795 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.176894 (-5.985919)\n",
      "\u001b[0m\n",
      "it 438/2000, vlb -6.317625, \u001b[31m   time: 0.390355 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.252627 (-5.985919)\n",
      "\u001b[0m\n",
      "it 439/2000, vlb -6.309923, \u001b[31m   time: 0.480505 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.153783 (-5.985919)\n",
      "\u001b[0m\n",
      "it 440/2000, vlb -6.322716, \u001b[31m   time: 0.464127 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.271938 (-5.985919)\n",
      "\u001b[0m\n",
      "it 441/2000, vlb -6.302651, \u001b[31m   time: 0.510636 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.122370 (-5.985919)\n",
      "\u001b[0m\n",
      "it 442/2000, vlb -6.289602, \u001b[31m   time: 0.433875 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.130091 (-5.985919)\n",
      "\u001b[0m\n",
      "it 443/2000, vlb -6.325253, \u001b[31m   time: 0.619076 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.068087 (-5.985919)\n",
      "\u001b[0m\n",
      "it 444/2000, vlb -6.278457, \u001b[31m   time: 0.389019 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.100268 (-5.985919)\n",
      "\u001b[0m\n",
      "it 445/2000, vlb -6.336435, \u001b[31m   time: 0.405516 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.060740 (-5.985919)\n",
      "\u001b[0m\n",
      "it 446/2000, vlb -6.314182, \u001b[31m   time: 0.401739 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.224208 (-5.985919)\n",
      "\u001b[0m\n",
      "it 447/2000, vlb -6.295183, \u001b[31m   time: 0.382368 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.136406 (-5.985919)\n",
      "\u001b[0m\n",
      "it 448/2000, vlb -6.303371, \u001b[31m   time: 0.401897 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.176783 (-5.985919)\n",
      "\u001b[0m\n",
      "it 449/2000, vlb -6.301868, \u001b[31m   time: 0.388023 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.293419 (-5.985919)\n",
      "\u001b[0m\n",
      "it 450/2000, vlb -6.362441, \u001b[31m   time: 0.404171 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.215661 (-5.985919)\n",
      "\u001b[0m\n",
      "it 451/2000, vlb -6.288510, \u001b[31m   time: 0.379022 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.186328 (-5.985919)\n",
      "\u001b[0m\n",
      "it 452/2000, vlb -6.308761, \u001b[31m   time: 0.401726 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.118476 (-5.985919)\n",
      "\u001b[0m\n",
      "it 453/2000, vlb -6.370328, \u001b[31m   time: 0.426197 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.157889 (-5.985919)\n",
      "\u001b[0m\n",
      "it 454/2000, vlb -6.339492, \u001b[31m   time: 0.448035 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.111872 (-5.985919)\n",
      "\u001b[0m\n",
      "it 455/2000, vlb -6.347413, \u001b[31m   time: 0.457287 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.171846 (-5.985919)\n",
      "\u001b[0m\n",
      "it 456/2000, vlb -6.337876, \u001b[31m   time: 0.458774 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -5.944186 (-5.985919)\n",
      "\u001b[0m\n",
      "it 457/2000, vlb -6.339350, \u001b[31m   time: 0.420875 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.168910 (-5.944186)\n",
      "\u001b[0m\n",
      "it 458/2000, vlb -6.310299, \u001b[31m   time: 0.460220 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.174387 (-5.944186)\n",
      "\u001b[0m\n",
      "it 459/2000, vlb -6.339088, \u001b[31m   time: 0.474731 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.244743 (-5.944186)\n",
      "\u001b[0m\n",
      "it 460/2000, vlb -6.311846, \u001b[31m   time: 0.444598 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.196285 (-5.944186)\n",
      "\u001b[0m\n",
      "it 461/2000, vlb -6.273851, \u001b[31m   time: 0.468874 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.141982 (-5.944186)\n",
      "\u001b[0m\n",
      "it 462/2000, vlb -6.332270, \u001b[31m   time: 0.372628 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.089163 (-5.944186)\n",
      "\u001b[0m\n",
      "it 463/2000, vlb -6.310334, \u001b[31m   time: 0.452789 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.255591 (-5.944186)\n",
      "\u001b[0m\n",
      "it 464/2000, vlb -6.313498, \u001b[31m   time: 0.383974 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.329075 (-5.944186)\n",
      "\u001b[0m\n",
      "it 465/2000, vlb -6.347525, \u001b[31m   time: 0.465400 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.073351 (-5.944186)\n",
      "\u001b[0m\n",
      "it 466/2000, vlb -6.250948, \u001b[31m   time: 0.455301 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.099348 (-5.944186)\n",
      "\u001b[0m\n",
      "it 467/2000, vlb -6.292716, \u001b[31m   time: 0.417883 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.141062 (-5.944186)\n",
      "\u001b[0m\n",
      "it 468/2000, vlb -6.311843, \u001b[31m   time: 0.394916 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.113091 (-5.944186)\n",
      "\u001b[0m\n",
      "it 469/2000, vlb -6.332513, \u001b[31m   time: 0.398821 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.125399 (-5.944186)\n",
      "\u001b[0m\n",
      "it 470/2000, vlb -6.334336, \u001b[31m   time: 0.374030 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.175715 (-5.944186)\n",
      "\u001b[0m\n",
      "it 471/2000, vlb -6.290741, \u001b[31m   time: 0.372504 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.217237 (-5.944186)\n",
      "\u001b[0m\n",
      "it 472/2000, vlb -6.330143, \u001b[31m   time: 0.384973 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.230728 (-5.944186)\n",
      "\u001b[0m\n",
      "it 473/2000, vlb -6.269876, \u001b[31m   time: 0.360072 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.217446 (-5.944186)\n",
      "\u001b[0m\n",
      "it 474/2000, vlb -6.309983, \u001b[31m   time: 0.366992 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.290969 (-5.944186)\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 475/2000, vlb -6.266431, \u001b[31m   time: 0.376988 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.075492 (-5.944186)\n",
      "\u001b[0m\n",
      "it 476/2000, vlb -6.302665, \u001b[31m   time: 0.364635 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.074968 (-5.944186)\n",
      "\u001b[0m\n",
      "it 477/2000, vlb -6.326942, \u001b[31m   time: 0.381737 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.208656 (-5.944186)\n",
      "\u001b[0m\n",
      "it 478/2000, vlb -6.353093, \u001b[31m   time: 0.380991 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.146125 (-5.944186)\n",
      "\u001b[0m\n",
      "it 479/2000, vlb -6.354736, \u001b[31m   time: 0.418498 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.063373 (-5.944186)\n",
      "\u001b[0m\n",
      "it 480/2000, vlb -6.315715, \u001b[31m   time: 0.399220 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.159573 (-5.944186)\n",
      "\u001b[0m\n",
      "it 481/2000, vlb -6.312483, \u001b[31m   time: 0.364000 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.246387 (-5.944186)\n",
      "\u001b[0m\n",
      "it 482/2000, vlb -6.286203, \u001b[31m   time: 0.403903 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.357959 (-5.944186)\n",
      "\u001b[0m\n",
      "it 483/2000, vlb -6.338617, \u001b[31m   time: 0.378731 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.099934 (-5.944186)\n",
      "\u001b[0m\n",
      "it 484/2000, vlb -6.325305, \u001b[31m   time: 0.358737 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.113967 (-5.944186)\n",
      "\u001b[0m\n",
      "it 485/2000, vlb -6.308981, \u001b[31m   time: 0.388527 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.190396 (-5.944186)\n",
      "\u001b[0m\n",
      "it 486/2000, vlb -6.316736, \u001b[31m   time: 0.390979 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.237138 (-5.944186)\n",
      "\u001b[0m\n",
      "it 487/2000, vlb -6.323379, \u001b[31m   time: 0.419843 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.141934 (-5.944186)\n",
      "\u001b[0m\n",
      "it 488/2000, vlb -6.330752, \u001b[31m   time: 0.388482 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.151652 (-5.944186)\n",
      "\u001b[0m\n",
      "it 489/2000, vlb -6.285708, \u001b[31m   time: 0.481746 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.107705 (-5.944186)\n",
      "\u001b[0m\n",
      "it 490/2000, vlb -6.297774, \u001b[31m   time: 0.383981 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.256642 (-5.944186)\n",
      "\u001b[0m\n",
      "it 491/2000, vlb -6.347098, \u001b[31m   time: 0.371845 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.102143 (-5.944186)\n",
      "\u001b[0m\n",
      "it 492/2000, vlb -6.321931, \u001b[31m   time: 0.380714 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.122026 (-5.944186)\n",
      "\u001b[0m\n",
      "it 493/2000, vlb -6.328727, \u001b[31m   time: 0.368761 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.021212 (-5.944186)\n",
      "\u001b[0m\n",
      "it 494/2000, vlb -6.276333, \u001b[31m   time: 0.388119 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.192471 (-5.944186)\n",
      "\u001b[0m\n",
      "it 495/2000, vlb -6.266058, \u001b[31m   time: 0.367016 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.260349 (-5.944186)\n",
      "\u001b[0m\n",
      "it 496/2000, vlb -6.300280, \u001b[31m   time: 0.378548 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.148376 (-5.944186)\n",
      "\u001b[0m\n",
      "it 497/2000, vlb -6.294429, \u001b[31m   time: 0.386967 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.166132 (-5.944186)\n",
      "\u001b[0m\n",
      "it 498/2000, vlb -6.283113, \u001b[31m   time: 0.370011 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.205121 (-5.944186)\n",
      "\u001b[0m\n",
      "it 499/2000, vlb -6.330016, \u001b[31m   time: 0.393433 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.135734 (-5.944186)\n",
      "\u001b[0m\n",
      "it 500/2000, vlb -6.375207, \u001b[31m   time: 0.373131 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.144248 (-5.944186)\n",
      "\u001b[0m\n",
      "it 501/2000, vlb -6.287234, \u001b[31m   time: 0.377989 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.133902 (-5.944186)\n",
      "\u001b[0m\n",
      "it 502/2000, vlb -6.329241, \u001b[31m   time: 0.390407 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.107311 (-5.944186)\n",
      "\u001b[0m\n",
      "it 503/2000, vlb -6.264371, \u001b[31m   time: 0.374220 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.180421 (-5.944186)\n",
      "\u001b[0m\n",
      "it 504/2000, vlb -6.362779, \u001b[31m   time: 0.401506 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.113613 (-5.944186)\n",
      "\u001b[0m\n",
      "it 505/2000, vlb -6.260177, \u001b[31m   time: 0.433806 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.092094 (-5.944186)\n",
      "\u001b[0m\n",
      "it 506/2000, vlb -6.294986, \u001b[31m   time: 0.396937 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.281388 (-5.944186)\n",
      "\u001b[0m\n",
      "it 507/2000, vlb -6.302971, \u001b[31m   time: 0.376771 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.221054 (-5.944186)\n",
      "\u001b[0m\n",
      "it 508/2000, vlb -6.317096, \u001b[31m   time: 0.381011 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.033317 (-5.944186)\n",
      "\u001b[0m\n",
      "it 509/2000, vlb -6.288585, \u001b[31m   time: 0.396973 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.205709 (-5.944186)\n",
      "\u001b[0m\n",
      "it 510/2000, vlb -6.216888, \u001b[31m   time: 0.372904 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.135651 (-5.944186)\n",
      "\u001b[0m\n",
      "it 511/2000, vlb -6.366407, \u001b[31m   time: 0.389934 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.081962 (-5.944186)\n",
      "\u001b[0m\n",
      "it 512/2000, vlb -6.268185, \u001b[31m   time: 0.382283 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.172498 (-5.944186)\n",
      "\u001b[0m\n",
      "it 513/2000, vlb -6.293052, \u001b[31m   time: 0.378212 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.249093 (-5.944186)\n",
      "\u001b[0m\n",
      "it 514/2000, vlb -6.307036, \u001b[31m   time: 0.392551 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.156630 (-5.944186)\n",
      "\u001b[0m\n",
      "it 515/2000, vlb -6.330212, \u001b[31m   time: 0.369020 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.230027 (-5.944186)\n",
      "\u001b[0m\n",
      "it 516/2000, vlb -6.315435, \u001b[31m   time: 0.385424 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.118614 (-5.944186)\n",
      "\u001b[0m\n",
      "it 517/2000, vlb -6.340012, \u001b[31m   time: 0.378121 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.208008 (-5.944186)\n",
      "\u001b[0m\n",
      "it 518/2000, vlb -6.262641, \u001b[31m   time: 0.382576 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.191209 (-5.944186)\n",
      "\u001b[0m\n",
      "it 519/2000, vlb -6.291662, \u001b[31m   time: 0.389948 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.245450 (-5.944186)\n",
      "\u001b[0m\n",
      "it 520/2000, vlb -6.272903, \u001b[31m   time: 0.375402 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.077877 (-5.944186)\n",
      "\u001b[0m\n",
      "it 521/2000, vlb -6.330074, \u001b[31m   time: 0.394978 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.230627 (-5.944186)\n",
      "\u001b[0m\n",
      "it 522/2000, vlb -6.325182, \u001b[31m   time: 0.374593 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.228217 (-5.944186)\n",
      "\u001b[0m\n",
      "it 523/2000, vlb -6.351776, \u001b[31m   time: 0.383007 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.138794 (-5.944186)\n",
      "\u001b[0m\n",
      "it 524/2000, vlb -6.279736, \u001b[31m   time: 0.388283 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.064291 (-5.944186)\n",
      "\u001b[0m\n",
      "it 525/2000, vlb -6.302885, \u001b[31m   time: 0.371042 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.188395 (-5.944186)\n",
      "\u001b[0m\n",
      "it 526/2000, vlb -6.329152, \u001b[31m   time: 0.397196 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.146586 (-5.944186)\n",
      "\u001b[0m\n",
      "it 527/2000, vlb -6.339011, \u001b[31m   time: 0.369742 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.165948 (-5.944186)\n",
      "\u001b[0m\n",
      "it 528/2000, vlb -6.308571, \u001b[31m   time: 0.381885 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.184834 (-5.944186)\n",
      "\u001b[0m\n",
      "it 529/2000, vlb -6.297895, \u001b[31m   time: 0.389999 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.251781 (-5.944186)\n",
      "\u001b[0m\n",
      "it 530/2000, vlb -6.297193, \u001b[31m   time: 0.375662 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.222684 (-5.944186)\n",
      "\u001b[0m\n",
      "it 531/2000, vlb -6.244570, \u001b[31m   time: 0.389304 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.111484 (-5.944186)\n",
      "\u001b[0m\n",
      "it 532/2000, vlb -6.352001, \u001b[31m   time: 0.373950 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.049182 (-5.944186)\n",
      "\u001b[0m\n",
      "it 533/2000, vlb -6.284675, \u001b[31m   time: 0.386564 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.095306 (-5.944186)\n",
      "\u001b[0m\n",
      "it 534/2000, vlb -6.325557, \u001b[31m   time: 0.379019 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.235073 (-5.944186)\n",
      "\u001b[0m\n",
      "it 535/2000, vlb -6.301766, \u001b[31m   time: 0.376815 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.072127 (-5.944186)\n",
      "\u001b[0m\n",
      "it 536/2000, vlb -6.288804, \u001b[31m   time: 0.391471 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.048066 (-5.944186)\n",
      "\u001b[0m\n",
      "it 537/2000, vlb -6.273440, \u001b[31m   time: 0.374522 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.124178 (-5.944186)\n",
      "\u001b[0m\n",
      "it 538/2000, vlb -6.303268, \u001b[31m   time: 0.383170 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.234148 (-5.944186)\n",
      "\u001b[0m\n",
      "it 539/2000, vlb -6.286332, \u001b[31m   time: 0.378662 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.061276 (-5.944186)\n",
      "\u001b[0m\n",
      "it 540/2000, vlb -6.287990, \u001b[31m   time: 0.375007 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.116035 (-5.944186)\n",
      "\u001b[0m\n",
      "it 541/2000, vlb -6.324038, \u001b[31m   time: 0.393574 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.011877 (-5.944186)\n",
      "\u001b[0m\n",
      "it 542/2000, vlb -6.287415, \u001b[31m   time: 0.370955 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.274992 (-5.944186)\n",
      "\u001b[0m\n",
      "it 543/2000, vlb -6.370276, \u001b[31m   time: 0.453947 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.232758 (-5.944186)\n",
      "\u001b[0m\n",
      "it 544/2000, vlb -6.286701, \u001b[31m   time: 0.397041 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.266417 (-5.944186)\n",
      "\u001b[0m\n",
      "it 545/2000, vlb -6.293209, \u001b[31m   time: 0.380863 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.218461 (-5.944186)\n",
      "\u001b[0m\n",
      "it 546/2000, vlb -6.317600, \u001b[31m   time: 0.396044 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.125126 (-5.944186)\n",
      "\u001b[0m\n",
      "it 547/2000, vlb -6.261740, \u001b[31m   time: 0.381517 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -5.985355 (-5.944186)\n",
      "\u001b[0m\n",
      "it 548/2000, vlb -6.337926, \u001b[31m   time: 0.386992 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.248494 (-5.944186)\n",
      "\u001b[0m\n",
      "it 549/2000, vlb -6.292656, \u001b[31m   time: 0.377014 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.172890 (-5.944186)\n",
      "\u001b[0m\n",
      "it 550/2000, vlb -6.286056, \u001b[31m   time: 0.374791 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.153163 (-5.944186)\n",
      "\u001b[0m\n",
      "it 551/2000, vlb -6.302444, \u001b[31m   time: 0.393743 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.125647 (-5.944186)\n",
      "\u001b[0m\n",
      "it 552/2000, vlb -6.287056, \u001b[31m   time: 0.369796 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.101319 (-5.944186)\n",
      "\u001b[0m\n",
      "it 553/2000, vlb -6.364720, \u001b[31m   time: 0.387038 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.174646 (-5.944186)\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 554/2000, vlb -6.296418, \u001b[31m   time: 0.368608 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.103982 (-5.944186)\n",
      "\u001b[0m\n",
      "it 555/2000, vlb -6.364489, \u001b[31m   time: 0.383630 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.214694 (-5.944186)\n",
      "\u001b[0m\n",
      "it 556/2000, vlb -6.344497, \u001b[31m   time: 0.380013 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.153323 (-5.944186)\n",
      "\u001b[0m\n",
      "it 557/2000, vlb -6.259218, \u001b[31m   time: 0.366991 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.135377 (-5.944186)\n",
      "\u001b[0m\n",
      "it 558/2000, vlb -6.297971, \u001b[31m   time: 0.393235 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.062052 (-5.944186)\n",
      "\u001b[0m\n",
      "it 559/2000, vlb -6.328966, \u001b[31m   time: 0.378282 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.218480 (-5.944186)\n",
      "\u001b[0m\n",
      "it 560/2000, vlb -6.294514, \u001b[31m   time: 0.379635 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.272411 (-5.944186)\n",
      "\u001b[0m\n",
      "it 561/2000, vlb -6.332949, \u001b[31m   time: 0.376925 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.228172 (-5.944186)\n",
      "\u001b[0m\n",
      "it 562/2000, vlb -6.280023, \u001b[31m   time: 0.376023 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.140863 (-5.944186)\n",
      "\u001b[0m\n",
      "it 563/2000, vlb -6.320916, \u001b[31m   time: 0.387816 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.172093 (-5.944186)\n",
      "\u001b[0m\n",
      "it 564/2000, vlb -6.290190, \u001b[31m   time: 0.368666 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.217259 (-5.944186)\n",
      "\u001b[0m\n",
      "it 565/2000, vlb -6.272964, \u001b[31m   time: 0.382601 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.081873 (-5.944186)\n",
      "\u001b[0m\n",
      "it 566/2000, vlb -6.326305, \u001b[31m   time: 0.375995 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.264816 (-5.944186)\n",
      "\u001b[0m\n",
      "it 567/2000, vlb -6.348403, \u001b[31m   time: 0.368016 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.184401 (-5.944186)\n",
      "\u001b[0m\n",
      "it 568/2000, vlb -6.314657, \u001b[31m   time: 0.394720 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.070108 (-5.944186)\n",
      "\u001b[0m\n",
      "it 569/2000, vlb -6.281051, \u001b[31m   time: 0.374003 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.064259 (-5.944186)\n",
      "\u001b[0m\n",
      "it 570/2000, vlb -6.347666, \u001b[31m   time: 0.376000 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.318660 (-5.944186)\n",
      "\u001b[0m\n",
      "it 571/2000, vlb -6.331143, \u001b[31m   time: 0.382078 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.119126 (-5.944186)\n",
      "\u001b[0m\n",
      "it 572/2000, vlb -6.328521, \u001b[31m   time: 0.371666 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.224829 (-5.944186)\n",
      "\u001b[0m\n",
      "it 573/2000, vlb -6.288405, \u001b[31m   time: 0.382975 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.074828 (-5.944186)\n",
      "\u001b[0m\n",
      "it 574/2000, vlb -6.327465, \u001b[31m   time: 0.364999 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.179945 (-5.944186)\n",
      "\u001b[0m\n",
      "it 575/2000, vlb -6.278231, \u001b[31m   time: 0.383847 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.064258 (-5.944186)\n",
      "\u001b[0m\n",
      "it 576/2000, vlb -6.288394, \u001b[31m   time: 0.381185 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.204664 (-5.944186)\n",
      "\u001b[0m\n",
      "it 577/2000, vlb -6.311590, \u001b[31m   time: 0.376201 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.248965 (-5.944186)\n",
      "\u001b[0m\n",
      "it 578/2000, vlb -6.307792, \u001b[31m   time: 0.387963 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.188685 (-5.944186)\n",
      "\u001b[0m\n",
      "it 579/2000, vlb -6.266539, \u001b[31m   time: 0.413894 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.247819 (-5.944186)\n",
      "\u001b[0m\n",
      "it 580/2000, vlb -6.317373, \u001b[31m   time: 0.409797 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.138071 (-5.944186)\n",
      "\u001b[0m\n",
      "it 581/2000, vlb -6.227426, \u001b[31m   time: 0.377999 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.070985 (-5.944186)\n",
      "\u001b[0m\n",
      "it 582/2000, vlb -6.267983, \u001b[31m   time: 0.391956 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.269807 (-5.944186)\n",
      "\u001b[0m\n",
      "it 583/2000, vlb -6.310341, \u001b[31m   time: 0.407813 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.200998 (-5.944186)\n",
      "\u001b[0m\n",
      "it 584/2000, vlb -6.309874, \u001b[31m   time: 0.365331 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.146198 (-5.944186)\n",
      "\u001b[0m\n",
      "it 585/2000, vlb -6.340342, \u001b[31m   time: 0.380196 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.207087 (-5.944186)\n",
      "\u001b[0m\n",
      "it 586/2000, vlb -6.299802, \u001b[31m   time: 0.375995 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.143030 (-5.944186)\n",
      "\u001b[0m\n",
      "it 587/2000, vlb -6.291876, \u001b[31m   time: 0.374587 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.012743 (-5.944186)\n",
      "\u001b[0m\n",
      "it 588/2000, vlb -6.260163, \u001b[31m   time: 0.378637 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.069836 (-5.944186)\n",
      "\u001b[0m\n",
      "it 589/2000, vlb -6.311407, \u001b[31m   time: 0.365532 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.192106 (-5.944186)\n",
      "\u001b[0m\n",
      "it 590/2000, vlb -6.302859, \u001b[31m   time: 0.386769 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.015451 (-5.944186)\n",
      "\u001b[0m\n",
      "it 591/2000, vlb -6.303930, \u001b[31m   time: 0.373306 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.298035 (-5.944186)\n",
      "\u001b[0m\n",
      "it 592/2000, vlb -6.331831, \u001b[31m   time: 0.377499 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.073003 (-5.944186)\n",
      "\u001b[0m\n",
      "it 593/2000, vlb -6.255277, \u001b[31m   time: 0.376767 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.167586 (-5.944186)\n",
      "\u001b[0m\n",
      "it 594/2000, vlb -6.294907, \u001b[31m   time: 0.374001 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.143351 (-5.944186)\n",
      "\u001b[0m\n",
      "it 595/2000, vlb -6.279712, \u001b[31m   time: 0.387024 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.173864 (-5.944186)\n",
      "\u001b[0m\n",
      "it 596/2000, vlb -6.307493, \u001b[31m   time: 0.374059 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.164834 (-5.944186)\n",
      "\u001b[0m\n",
      "it 597/2000, vlb -6.267221, \u001b[31m   time: 0.376399 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.166534 (-5.944186)\n",
      "\u001b[0m\n",
      "it 598/2000, vlb -6.307298, \u001b[31m   time: 0.381226 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.145133 (-5.944186)\n",
      "\u001b[0m\n",
      "it 599/2000, vlb -6.310439, \u001b[31m   time: 0.376063 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.122788 (-5.944186)\n",
      "\u001b[0m\n",
      "it 600/2000, vlb -6.298709, \u001b[31m   time: 0.389024 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.196580 (-5.944186)\n",
      "\u001b[0m\n",
      "it 601/2000, vlb -6.314084, \u001b[31m   time: 0.376593 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.146923 (-5.944186)\n",
      "\u001b[0m\n",
      "it 602/2000, vlb -6.303349, \u001b[31m   time: 0.379834 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.101060 (-5.944186)\n",
      "\u001b[0m\n",
      "it 603/2000, vlb -6.333627, \u001b[31m   time: 0.383946 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.192675 (-5.944186)\n",
      "\u001b[0m\n",
      "it 604/2000, vlb -6.325409, \u001b[31m   time: 0.362069 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.148635 (-5.944186)\n",
      "\u001b[0m\n",
      "it 605/2000, vlb -6.302537, \u001b[31m   time: 0.382793 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.285499 (-5.944186)\n",
      "\u001b[0m\n",
      "it 606/2000, vlb -6.288267, \u001b[31m   time: 0.373044 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.053151 (-5.944186)\n",
      "\u001b[0m\n",
      "it 607/2000, vlb -6.379344, \u001b[31m   time: 0.380305 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.200456 (-5.944186)\n",
      "\u001b[0m\n",
      "it 608/2000, vlb -6.300830, \u001b[31m   time: 0.383705 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.144170 (-5.944186)\n",
      "\u001b[0m\n",
      "it 609/2000, vlb -6.241735, \u001b[31m   time: 0.380932 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.187408 (-5.944186)\n",
      "\u001b[0m\n",
      "it 610/2000, vlb -6.278468, \u001b[31m   time: 0.388973 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.188645 (-5.944186)\n",
      "\u001b[0m\n",
      "it 611/2000, vlb -6.313014, \u001b[31m   time: 0.376990 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.182596 (-5.944186)\n",
      "\u001b[0m\n",
      "it 612/2000, vlb -6.322107, \u001b[31m   time: 0.380352 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.199915 (-5.944186)\n",
      "\u001b[0m\n",
      "it 613/2000, vlb -6.311442, \u001b[31m   time: 0.380648 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.255984 (-5.944186)\n",
      "\u001b[0m\n",
      "it 614/2000, vlb -6.260647, \u001b[31m   time: 0.375558 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.096247 (-5.944186)\n",
      "\u001b[0m\n",
      "it 615/2000, vlb -6.301117, \u001b[31m   time: 0.385702 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.167376 (-5.944186)\n",
      "\u001b[0m\n",
      "it 616/2000, vlb -6.276501, \u001b[31m   time: 0.369014 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.124442 (-5.944186)\n",
      "\u001b[0m\n",
      "it 617/2000, vlb -6.292772, \u001b[31m   time: 0.382352 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.156053 (-5.944186)\n",
      "\u001b[0m\n",
      "it 618/2000, vlb -6.299123, \u001b[31m   time: 0.374163 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.102428 (-5.944186)\n",
      "\u001b[0m\n",
      "it 619/2000, vlb -6.336412, \u001b[31m   time: 0.369982 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.214363 (-5.944186)\n",
      "\u001b[0m\n",
      "it 620/2000, vlb -6.309576, \u001b[31m   time: 0.431680 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.052368 (-5.944186)\n",
      "\u001b[0m\n",
      "it 621/2000, vlb -6.314877, \u001b[31m   time: 0.398106 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.185452 (-5.944186)\n",
      "\u001b[0m\n",
      "it 622/2000, vlb -6.311537, \u001b[31m   time: 0.407283 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.133667 (-5.944186)\n",
      "\u001b[0m\n",
      "it 623/2000, vlb -6.328483, \u001b[31m   time: 0.385942 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.258705 (-5.944186)\n",
      "\u001b[0m\n",
      "it 624/2000, vlb -6.282887, \u001b[31m   time: 0.372062 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.197132 (-5.944186)\n",
      "\u001b[0m\n",
      "it 625/2000, vlb -6.317385, \u001b[31m   time: 0.391241 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.193419 (-5.944186)\n",
      "\u001b[0m\n",
      "it 626/2000, vlb -6.348614, \u001b[31m   time: 0.372720 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.115879 (-5.944186)\n",
      "\u001b[0m\n",
      "it 627/2000, vlb -6.316467, \u001b[31m   time: 0.389552 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.244099 (-5.944186)\n",
      "\u001b[0m\n",
      "it 628/2000, vlb -6.315301, \u001b[31m   time: 0.375644 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.226717 (-5.944186)\n",
      "\u001b[0m\n",
      "it 629/2000, vlb -6.296813, \u001b[31m   time: 0.382771 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.053893 (-5.944186)\n",
      "\u001b[0m\n",
      "it 630/2000, vlb -6.371707, \u001b[31m   time: 0.381487 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.151165 (-5.944186)\n",
      "\u001b[0m\n",
      "it 631/2000, vlb -6.292202, \u001b[31m   time: 0.373916 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.165792 (-5.944186)\n",
      "\u001b[0m\n",
      "it 632/2000, vlb -6.265116, \u001b[31m   time: 0.390960 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.091088 (-5.944186)\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 633/2000, vlb -6.289485, \u001b[31m   time: 0.376577 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.143195 (-5.944186)\n",
      "\u001b[0m\n",
      "it 634/2000, vlb -6.299899, \u001b[31m   time: 0.376743 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.130884 (-5.944186)\n",
      "\u001b[0m\n",
      "it 635/2000, vlb -6.300354, \u001b[31m   time: 0.383073 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.119686 (-5.944186)\n",
      "\u001b[0m\n",
      "it 636/2000, vlb -6.329898, \u001b[31m   time: 0.372152 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.023461 (-5.944186)\n",
      "\u001b[0m\n",
      "it 637/2000, vlb -6.285444, \u001b[31m   time: 0.385941 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.202297 (-5.944186)\n",
      "\u001b[0m\n",
      "it 638/2000, vlb -6.342248, \u001b[31m   time: 0.370718 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.142872 (-5.944186)\n",
      "\u001b[0m\n",
      "it 639/2000, vlb -6.258252, \u001b[31m   time: 0.378566 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.187391 (-5.944186)\n",
      "\u001b[0m\n",
      "it 640/2000, vlb -6.317720, \u001b[31m   time: 0.382976 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.137143 (-5.944186)\n",
      "\u001b[0m\n",
      "it 641/2000, vlb -6.268360, \u001b[31m   time: 0.371034 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.156444 (-5.944186)\n",
      "\u001b[0m\n",
      "it 642/2000, vlb -6.365222, \u001b[31m   time: 0.389587 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.207402 (-5.944186)\n",
      "\u001b[0m\n",
      "it 643/2000, vlb -6.310038, \u001b[31m   time: 0.367078 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.070249 (-5.944186)\n",
      "\u001b[0m\n",
      "it 644/2000, vlb -6.288770, \u001b[31m   time: 0.369845 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.133433 (-5.944186)\n",
      "\u001b[0m\n",
      "it 645/2000, vlb -6.289199, \u001b[31m   time: 0.389426 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.064063 (-5.944186)\n",
      "\u001b[0m\n",
      "it 646/2000, vlb -6.315604, \u001b[31m   time: 0.370389 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.202442 (-5.944186)\n",
      "\u001b[0m\n",
      "it 647/2000, vlb -6.270501, \u001b[31m   time: 0.389923 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.116970 (-5.944186)\n",
      "\u001b[0m\n",
      "it 648/2000, vlb -6.298362, \u001b[31m   time: 0.373515 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.266019 (-5.944186)\n",
      "\u001b[0m\n",
      "it 649/2000, vlb -6.305977, \u001b[31m   time: 0.378874 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.131203 (-5.944186)\n",
      "\u001b[0m\n",
      "it 650/2000, vlb -6.374764, \u001b[31m   time: 0.386996 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.224341 (-5.944186)\n",
      "\u001b[0m\n",
      "it 651/2000, vlb -6.310297, \u001b[31m   time: 0.361042 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.072740 (-5.944186)\n",
      "\u001b[0m\n",
      "it 652/2000, vlb -6.279099, \u001b[31m   time: 0.392219 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.104026 (-5.944186)\n",
      "\u001b[0m\n",
      "it 653/2000, vlb -6.274133, \u001b[31m   time: 0.373653 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.143808 (-5.944186)\n",
      "\u001b[0m\n",
      "it 654/2000, vlb -6.282276, \u001b[31m   time: 0.386674 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.228866 (-5.944186)\n",
      "\u001b[0m\n",
      "it 655/2000, vlb -6.330292, \u001b[31m   time: 0.385742 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.229119 (-5.944186)\n",
      "\u001b[0m\n",
      "it 656/2000, vlb -6.273063, \u001b[31m   time: 0.384218 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.171684 (-5.944186)\n",
      "\u001b[0m\n",
      "it 657/2000, vlb -6.331948, \u001b[31m   time: 0.391121 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    vlb -6.145339 (-5.944186)\n",
      "\u001b[0m\n",
      "\u001b[31m   average time: 0.139197 seconds\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Credit\n",
    "\"\"\"\n",
    "dname = 'default_credit'\n",
    "print(dname)\n",
    "\n",
    "x_train, x_test, x_means, x_stds = \\\n",
    "load_UCI(dset_name=dname, splits=10, seed=42, separate_targets=False, save_dir='../data/')\n",
    "\n",
    "input_dim_vec = [1, 2, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ,1 ,1 ,1 ,1 ,1, 2] #has this form for targets\n",
    "print(input_dim_vec)\n",
    "\n",
    "x_train = unnormalise_cat_vars(x_train, x_means, x_stds, input_dim_vec)\n",
    "x_test = unnormalise_cat_vars(x_test, x_means, x_stds, input_dim_vec)\n",
    "\n",
    "x_train_flat = gauss_cat_to_flat(torch.Tensor(x_train), input_dim_vec)\n",
    "x_test_flat = gauss_cat_to_flat(torch.Tensor(x_test), input_dim_vec)\n",
    "\n",
    "\n",
    "print(x_train_flat.shape)\n",
    "print(x_test_flat.shape)\n",
    "\n",
    "trainset = Datafeed(x_train_flat, x_train_flat, transform=None)\n",
    "valset = Datafeed(x_test_flat, x_test_flat, transform=None)\n",
    "\n",
    "\n",
    "width = widths[names.index(dname)]\n",
    "depth = depths[names.index(dname)] # number of hidden layers\n",
    "latent_dim = latent_dims[names.index(dname)]\n",
    "lr = 1e-4\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "#base_network_wraper = VAEAC_gauss_cat_net(input_dim_vec, width, depth, latent_dim, pred_sig=False, lr=lr, cuda=cuda, flatten=True)\n",
    "#base_network_wraper.load('../saves/fc_preact_VAEAC_NEW2_' + dname + '_models/theta_best.dat')\n",
    "\n",
    "base_network = net.model\n",
    "width = 150\n",
    "depth = 2\n",
    "latent_dim = under_latent_dims2[names.index(dname)]\n",
    "\n",
    "batch_size = 128\n",
    "nb_epochs = 2000\n",
    "early_stop = 200\n",
    "lr = 1e-4\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# COMPAS\n",
    "#\"\"\"\n",
    "x_train, x_test, x_means, x_stds, y_train, y_test, feature_names, X_dims = \\\n",
    "    get_my_COMPAS(rseed=42, separate_test=True, test_ratio=0.1, save_dir='../data/')\n",
    "\n",
    "x_train, x_test, input_dim_vec = join_compas_targets(x_train, x_test, y_train, y_test, X_dims)\n",
    "print('Compas', x_train.shape, x_test.shape)\n",
    "print(input_dim_vec)\n",
    "\n",
    "dname = 'compas'\n",
    "print(dname)\n",
    "\n",
    "trainset = Datafeed(x_train, x_train, transform=None)\n",
    "valset = Datafeed(x_test, x_test, transform=None)\n",
    "\n",
    "width = 150\n",
    "depth = 2\n",
    "latent_dim = under_latent_dims[names.index(dname)]\n",
    "\n",
    "batch_size = 128\n",
    "nb_epochs = 2000\n",
    "early_stop = 200\n",
    "lr = 1e-4\n",
    "\n",
    "cuda = False\n",
    "\n",
    "#base_network_wraper = VAEAC_gauss_cat_net(input_dim_vec, width, depth, latent_dim, pred_sig=False, lr=lr, cuda=cuda, flatten=True)\n",
    "#base_network_wraper.load('../saves/fc_preact_VAEAC_NEW_' + dname + '_models/theta_best.dat')\n",
    "\n",
    "base_network = net.model\n",
    "\n",
    "#VAEAC = VAEAC_gauss_cat_net(input_dim_vec, width, depth, latent_dim, pred_sig=False, lr=lr, cuda=cuda, flatten=flat_vaeac_bools[d_idx])\n",
    "#VAEAC.load('../saves/fc_preact_VAEAC_NEW_' + dname + '_models/theta_best.dat')\n",
    "\n",
    "#base_network = base_network_wraper.model\n",
    "\n",
    "savedir = './torch_vlb/fc_VAEAC_NEW_under_' + dname  # remove the 2 to remove dimensionality reduction\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "under_VAEAC_net = under_VAEAC(base_network, width, depth, latent_dim, lr, cuda=cuda)\n",
    "\n",
    "vlb_train, vlb_dev, best_epoch, best_vlb, curr_epoch = train_VAE(under_VAEAC_net, savedir,\n",
    "                               batch_size, nb_epochs, trainset, valset, cuda=cuda,\n",
    "                               flat_ims=False, train_plot=False, Nclass=None, early_stop=early_stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e250ba56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f722fb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best vlb  -5.944185536270388\n",
      "best epoch  456\n",
      "curr epoch 657\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "np.save(\"./torch_vlb/\" + str(dname) + \"_under_vlb_train_lr_\" + str(lr), vlb_train)\n",
    "np.save(\"./torch_vlb/\" + str(dname) + \"_under_vlb_val_lr_\" + str(lr), vlb_dev)\n",
    "\n",
    "# best_epoch = 321 (pos 320 in train_val)\n",
    "#curr_epoch = 522\n",
    "\n",
    "print(\"best vlb \", best_vlb)\n",
    "print(\"best epoch \", best_epoch)\n",
    "print(\"curr epoch\", curr_epoch)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.clip(vlb_train[:curr_epoch], -1000, 1000), 'r')\n",
    "plt.plot(np.clip(vlb_dev[:curr_epoch], -1000, 1000), 'b')\n",
    "plt.legend(['train', 'validation'], fontsize =15)\n",
    "plt.ylabel('Variational Lower Bound', fontweight='bold', fontsize =15)\n",
    "plt.xlabel('Epoch', fontweight='bold', fontsize =15)\n",
    "plt.yticks(fontsize = 13)\n",
    "plt.xticks(fontsize=13)\n",
    "plt.title('under_VAEAC Original', fontweight='bold', fontsize= 15)\n",
    "plt.grid(True)\n",
    "plt.savefig('./torch_vlb/' +  'torch_compas_under_vaeac' + '.png', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "668de08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best vlb  -5.8112255037795375\n",
      "best epoch  316\n",
      "curr epoch 517\n"
     ]
    }
   ],
   "source": [
    "print(\"best vlb \", best_vlb)\n",
    "print(\"best epoch \", best_epoch)\n",
    "print(\"curr epoch\", curr_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac7cfc3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.clip(vlb_train[:curr_epoch], -1000, 1000), 'r')\n",
    "plt.plot(np.clip(vlb_dev[:curr_epoch], -1000, 1000), 'b')\n",
    "plt.legend(['cost_train', 'cost_dev'])\n",
    "plt.ylabel('Variational lower bound', fontweight ='bold', fontsize = 15)\n",
    "plt.xlabel('epoch', fontweight ='bold', fontsize = 15)\n",
    "plt.grid(True)\n",
    "#plt.savefig( str(dname) + '_under_vlb_lr_' + str(lr) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32142966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadc4845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5fc8f2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_10249/1070947545.py:6: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return np.sum(p.numel() for p in self.model.parameters())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49812"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "under_VAEAC_net.get_nb_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9d9790a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "778085\n"
     ]
    }
   ],
   "source": [
    "number_p = 0\n",
    "for p in net.model.parameters():\n",
    "    for i in range(len(p.data)):\n",
    "        #print(len(p.data))\n",
    "        try:\n",
    "            number_p += len(p.data[i])\n",
    "        except:\n",
    "            number_p += 1\n",
    "print(number_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0bee9aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1953,  0.1680, -0.0635,  ...,  0.1309,  0.0337, -0.1151],\n",
      "        [ 0.0867, -0.1051, -0.0316,  ...,  0.1187,  0.0668,  0.1751],\n",
      "        [ 0.0473, -0.1043,  0.0623,  ...,  0.0617,  0.0875,  0.0812],\n",
      "        ...,\n",
      "        [-0.1474,  0.2562, -0.1020,  ...,  0.1505,  0.1124,  0.0671],\n",
      "        [-0.0777,  0.1288,  0.0823,  ...,  0.0869,  0.1883, -0.0504],\n",
      "        [ 0.0840,  0.1529,  0.0006,  ...,  0.0037, -0.1448, -0.0819]])\n",
      "tensor([-0.0420,  0.0951,  0.0312, -0.0081,  0.0587, -0.1466,  0.2107,  0.1487,\n",
      "         0.1520, -0.1944,  0.2045,  0.0413, -0.2018,  0.2153, -0.0358,  0.0860,\n",
      "         0.1789,  0.1633,  0.2059, -0.2002,  0.2350,  0.0600,  0.0048, -0.1823,\n",
      "        -0.1383,  0.2303,  0.1632,  0.1677, -0.0273, -0.1063,  0.0492, -0.0949,\n",
      "        -0.1412,  0.0960,  0.2115, -0.1593, -0.2120,  0.0359, -0.0733, -0.1013,\n",
      "        -0.0405,  0.0021, -0.0675,  0.1887,  0.0983,  0.0666, -0.0523, -0.0020,\n",
      "         0.1542,  0.0323, -0.2225,  0.0058,  0.0914, -0.0639, -0.0398, -0.0056,\n",
      "         0.1752,  0.1858, -0.1554,  0.1764,  0.1144,  0.0876, -0.0972, -0.1501,\n",
      "         0.2246,  0.0807, -0.0747, -0.0354,  0.1914, -0.0531,  0.1662, -0.0600,\n",
      "         0.0779,  0.0240, -0.1035,  0.0095,  0.0736, -0.2000, -0.0374,  0.0016,\n",
      "         0.2160,  0.0627, -0.1768, -0.0429,  0.2389, -0.0291,  0.1740,  0.0294,\n",
      "         0.1521, -0.1208,  0.2155,  0.0587, -0.0382,  0.1923,  0.1369, -0.1661,\n",
      "        -0.0383,  0.1908,  0.0487, -0.0722, -0.1540, -0.1394,  0.1258,  0.0887,\n",
      "        -0.2133, -0.0617,  0.0390, -0.1926,  0.1767, -0.0184, -0.1178,  0.0142,\n",
      "         0.0668,  0.0635,  0.2471,  0.1226, -0.0080,  0.0483,  0.0743, -0.0576,\n",
      "        -0.2042, -0.0077, -0.0815, -0.2545,  0.0935, -0.1144,  0.1985,  0.1057,\n",
      "         0.0985,  0.0948,  0.1168,  0.1817,  0.1892, -0.0115, -0.0900,  0.0812,\n",
      "        -0.0763,  0.0173,  0.0227,  0.1199, -0.2372,  0.1071, -0.0741, -0.1400,\n",
      "        -0.0908,  0.0041,  0.1035, -0.1785,  0.0705,  0.2223, -0.1057,  0.0658,\n",
      "        -0.0599,  0.0539,  0.1126, -0.1399,  0.0630,  0.2510,  0.0171, -0.1483,\n",
      "         0.0133,  0.0837,  0.0842, -0.0138, -0.0706, -0.0942,  0.1554, -0.1260,\n",
      "        -0.0869,  0.1748, -0.0714, -0.0172, -0.0855, -0.2041,  0.1035, -0.1029,\n",
      "        -0.0657, -0.0483,  0.0300,  0.2015,  0.2305,  0.0392, -0.1829, -0.0027,\n",
      "        -0.0087,  0.2097, -0.0060, -0.2116,  0.0826, -0.0370,  0.0819,  0.0447,\n",
      "         0.0854,  0.1396, -0.1457, -0.2050,  0.2179,  0.2240, -0.0849,  0.2222,\n",
      "         0.0769, -0.0500, -0.0975, -0.1374,  0.2060, -0.1493, -0.0998,  0.2307,\n",
      "        -0.1120,  0.2623,  0.0647,  0.1365,  0.0568, -0.0112,  0.0854,  0.0873,\n",
      "        -0.1450, -0.1785, -0.1525, -0.0049,  0.0812, -0.1227,  0.0298,  0.1519,\n",
      "         0.1340, -0.0955,  0.0767, -0.1878,  0.1554, -0.2018,  0.0120, -0.0688,\n",
      "        -0.2146, -0.2159, -0.1413, -0.1513,  0.1841, -0.0091, -0.1306,  0.0117,\n",
      "        -0.0905, -0.0299,  0.1972, -0.1873, -0.1885,  0.0476,  0.1555, -0.2267,\n",
      "         0.1065,  0.2575, -0.0319, -0.1194,  0.1209,  0.1444, -0.0176, -0.1857,\n",
      "         0.0285, -0.1385, -0.1897,  0.1472,  0.1508,  0.1102, -0.1667,  0.2024,\n",
      "         0.1495,  0.1775, -0.0183, -0.0266, -0.1959,  0.1660,  0.0427, -0.2218,\n",
      "         0.2647, -0.0276,  0.1316, -0.0358, -0.0556,  0.1974,  0.1943, -0.0783,\n",
      "         0.0486,  0.1609, -0.0061, -0.0325,  0.2107,  0.2260, -0.0718,  0.1607,\n",
      "        -0.0186,  0.0005,  0.1658, -0.0142, -0.0119, -0.0325, -0.1750,  0.1724,\n",
      "        -0.1811, -0.1445,  0.0497, -0.0234, -0.0048,  0.0098,  0.2449, -0.1720,\n",
      "        -0.1738,  0.0535, -0.2071, -0.1699,  0.1711,  0.0805, -0.0405,  0.0264,\n",
      "         0.0411,  0.0076,  0.2176,  0.0218,  0.1697, -0.1927, -0.0611,  0.0660,\n",
      "         0.1005, -0.0769,  0.2028, -0.1890, -0.1019,  0.1857, -0.1811, -0.2128,\n",
      "        -0.0502, -0.1674,  0.0790, -0.0419, -0.0695,  0.1500,  0.0051, -0.2067,\n",
      "        -0.2143,  0.1683,  0.1079,  0.0974, -0.1670, -0.0572,  0.2179,  0.2138,\n",
      "         0.1447,  0.2220, -0.0732, -0.1904,  0.1612,  0.1183])\n",
      "tensor([0.9876, 0.9732, 0.9717, 0.9571, 0.9969, 0.8806, 0.9383, 0.9872, 0.9477,\n",
      "        0.9999, 0.9958, 1.0090, 1.0272, 0.9836, 0.9664, 1.0089, 1.0163, 0.9851,\n",
      "        1.0318, 0.9381, 0.9890, 0.9987, 0.9585, 0.9903, 0.9450, 1.0055, 0.9883,\n",
      "        1.0069, 0.9798, 0.9367, 1.0301, 0.8865, 0.9855, 0.9663, 1.0016, 1.0088,\n",
      "        0.9436, 1.0309, 0.9903, 1.0137, 0.9859, 0.9917, 0.9678, 1.0066, 1.0132,\n",
      "        0.9745, 1.0426, 1.0319, 0.9452, 0.9263, 0.9539, 0.9884, 1.1107, 0.9634,\n",
      "        0.9438, 0.9904, 0.9994, 0.9958, 0.9554, 1.0168, 0.9822, 0.9986, 0.9905,\n",
      "        0.9490, 1.0077, 0.9666, 1.0173, 1.0055, 1.0086, 0.9943, 1.0355, 0.9284,\n",
      "        0.9905, 1.0190, 0.9525, 1.0012, 1.0274, 1.0323, 1.0351, 0.9576, 0.9871,\n",
      "        0.9554, 0.9525, 1.0019, 0.9971, 0.9638, 1.0101, 1.0171, 0.9996, 1.0180,\n",
      "        0.9854, 1.0099, 1.0209, 1.0005, 0.9331, 0.9726, 0.9744, 0.9592, 1.0168,\n",
      "        1.0218, 0.9647, 0.9997, 1.0280, 1.0161, 0.9868, 1.0282, 0.9760, 1.0335,\n",
      "        1.0120, 1.0005, 1.0416, 1.0144, 0.9715, 0.9642, 1.0234, 0.9686, 0.9775,\n",
      "        0.9882, 0.9016, 0.9810, 0.9528, 1.0044, 0.9909, 0.9800, 1.0269, 0.9687,\n",
      "        1.0287, 1.0331, 0.9892, 0.9905, 0.9840, 1.0151, 0.9665, 0.9681, 0.9741,\n",
      "        1.0685, 1.0326, 1.0224, 1.0117, 0.9803, 1.0398, 0.9917, 0.9640, 0.9844,\n",
      "        0.9658, 0.9616, 0.9712, 0.9383, 0.9605, 0.9733, 0.9847, 0.9394, 0.9815,\n",
      "        0.9873, 0.9633, 1.0141, 0.9572, 0.9889, 0.9757, 0.9768, 0.9471, 1.0227,\n",
      "        0.9793, 1.0163, 0.9610, 1.0159, 0.9665, 0.9939, 0.9979, 0.9825, 0.9825,\n",
      "        1.0311, 0.9825, 0.9387, 0.9971, 1.0116, 0.9784, 0.9990, 1.0233, 0.9883,\n",
      "        0.9847, 1.0040, 1.0182, 1.0052, 1.0286, 0.9638, 0.9834, 0.9327, 1.0020,\n",
      "        0.9888, 1.0293, 1.0108, 1.0154, 0.9932, 0.9569, 0.9970, 1.0317, 0.9356,\n",
      "        0.9586, 0.9773, 1.0184, 1.0135, 0.9889, 0.9646, 0.9962, 0.9580, 0.9724,\n",
      "        1.0467, 1.0254, 1.0492, 0.9878, 0.9785, 0.9797, 1.0324, 0.9661, 1.0153,\n",
      "        1.1279, 1.0275, 0.9710, 1.0122, 1.0071, 0.9557, 1.0304, 0.9727, 0.9929,\n",
      "        0.9788, 0.9790, 0.9538, 1.0066, 0.9330, 0.9869, 1.0231, 0.9023, 1.0602,\n",
      "        0.9983, 0.9848, 0.9980, 0.9792, 1.0043, 1.0105, 1.0139, 1.0067, 0.9718,\n",
      "        0.9786, 1.0083, 1.0180, 0.9944, 1.0019, 0.9856, 0.9976, 1.0322, 1.0159,\n",
      "        1.0089, 0.9851, 0.9921, 1.0216, 0.9849, 1.0107, 0.9207, 0.9506, 0.9598,\n",
      "        0.9959, 1.0032, 1.0031, 0.9599, 1.0341, 1.0138, 1.0077, 1.0555, 0.9791,\n",
      "        0.9861, 0.9860, 1.0536, 0.9600, 0.9666, 0.9889, 0.9602, 1.0032, 0.9970,\n",
      "        0.9496, 1.0005, 0.9759, 0.9785, 1.0330, 0.9781, 0.9771, 0.9923, 0.9996,\n",
      "        1.0021, 1.0380, 0.9738, 0.9838, 0.9986, 0.9733, 1.0243, 0.9883, 0.9801,\n",
      "        1.0121, 0.9954, 0.9608, 1.0066, 0.9687, 1.0039, 0.9619, 0.9652, 0.9475,\n",
      "        1.0092, 0.9501, 0.9932, 0.9762, 1.0101, 0.9583, 1.0459, 0.9962, 1.0045,\n",
      "        0.9610, 1.0021, 0.9717, 0.9444, 0.9808, 0.9838, 1.0179, 0.9713, 0.9820,\n",
      "        0.9592, 0.9755, 1.0178, 0.9685, 0.9045, 0.9556, 0.9841, 1.0322, 1.0321,\n",
      "        1.0045, 0.9795, 1.0122, 0.9814, 0.9705, 1.0038, 0.9853, 0.9874, 0.9861,\n",
      "        1.0022, 1.0543, 1.0052, 0.9722, 1.0072, 0.9843, 1.0134, 1.0498])\n",
      "tensor([-1.9262e-02,  6.8808e-02, -2.7709e-02, -7.9781e-03,  5.2086e-04,\n",
      "        -2.9641e-02,  1.2010e-02,  6.0496e-02,  9.3870e-03, -2.3443e-02,\n",
      "        -2.9899e-03,  9.0187e-03, -7.2212e-03,  2.9913e-02, -4.5215e-02,\n",
      "        -7.9863e-03, -2.5696e-02,  1.2925e-02,  2.7612e-02,  5.9847e-02,\n",
      "         5.9145e-02, -5.9123e-02, -3.0768e-02, -4.0015e-02, -3.8799e-02,\n",
      "         1.8588e-02, -7.5539e-03,  1.1407e-02, -7.9261e-02,  4.2057e-02,\n",
      "        -3.1660e-02,  7.5483e-04, -7.2974e-03,  6.6604e-02,  4.0746e-02,\n",
      "         5.8240e-03, -4.4103e-04,  5.7515e-02, -1.5165e-03, -3.2856e-03,\n",
      "        -2.1731e-02, -3.7585e-02, -1.2027e-02,  8.2008e-03, -2.8148e-02,\n",
      "        -6.8415e-02,  4.9045e-02, -5.1184e-03, -9.7163e-03, -3.9261e-02,\n",
      "         5.4607e-02, -3.0264e-02,  6.2059e-02, -4.5310e-02, -1.0581e-02,\n",
      "         5.4883e-02,  7.2326e-02,  6.4441e-03, -1.6695e-03, -1.3685e-02,\n",
      "         2.9680e-02,  4.5392e-02, -7.7423e-03,  1.5334e-02, -1.2557e-02,\n",
      "        -1.9720e-02, -6.5372e-02,  2.2941e-02,  4.6141e-02,  4.5977e-02,\n",
      "        -2.0632e-02,  2.4224e-03,  2.5363e-02,  1.8250e-02,  8.8735e-03,\n",
      "        -1.9366e-02,  3.0012e-02, -8.5280e-03,  2.8335e-02, -5.7338e-02,\n",
      "        -2.1394e-02, -5.4109e-03,  2.8922e-02,  2.3728e-02,  7.0688e-02,\n",
      "         2.2296e-02,  1.4364e-03, -3.7522e-02, -2.4035e-02,  2.5778e-02,\n",
      "        -4.2108e-02, -4.6456e-02,  1.2510e-02,  6.2311e-04,  3.5344e-03,\n",
      "         2.7247e-03, -5.3089e-03,  2.2763e-02,  5.1632e-02, -1.7072e-02,\n",
      "        -4.5645e-03, -5.1766e-02, -1.6763e-03,  7.8325e-03,  4.9228e-02,\n",
      "        -1.6008e-02,  1.8120e-02, -8.1276e-03,  1.6122e-02, -1.6607e-02,\n",
      "        -2.0517e-02,  1.3496e-02, -4.8073e-02, -5.6107e-02, -1.9892e-02,\n",
      "         6.2271e-02,  2.3159e-02, -3.6944e-02, -8.4918e-03,  2.2747e-02,\n",
      "         3.9289e-02, -3.4095e-02,  2.9631e-02,  4.2823e-02,  4.0381e-02,\n",
      "         1.6333e-02,  1.6300e-02,  1.3065e-02,  1.5338e-05,  3.2305e-02,\n",
      "         3.3070e-02,  5.6642e-03,  1.7595e-02, -2.7387e-02,  1.0444e-02,\n",
      "         2.5394e-02,  6.8261e-02,  5.8893e-02, -4.9872e-02, -1.5014e-02,\n",
      "        -1.4407e-02, -3.7862e-02,  7.1918e-03,  2.7607e-02, -6.7100e-03,\n",
      "         1.5155e-03, -2.5426e-02,  4.9831e-02, -2.0772e-02,  4.7268e-02,\n",
      "        -6.7425e-03, -3.7260e-02,  3.1881e-03, -4.7137e-02, -6.2286e-02,\n",
      "        -2.1851e-02, -6.3547e-02,  5.3632e-03,  5.4303e-02, -2.8698e-02,\n",
      "        -5.7086e-02,  4.1721e-02,  6.2883e-02,  3.5069e-03, -5.2867e-02,\n",
      "         4.0767e-02, -4.3008e-03,  2.2340e-02, -3.4104e-02, -1.0424e-02,\n",
      "         1.3295e-02, -8.9642e-03, -4.4534e-02, -2.2958e-02, -5.6079e-03,\n",
      "         1.9897e-03,  7.5531e-02,  1.9228e-02, -8.9603e-03, -4.1270e-02,\n",
      "         1.4835e-03,  4.6044e-02,  4.3999e-02, -1.2212e-02, -2.3107e-02,\n",
      "         4.4801e-04,  4.0472e-02,  4.3104e-02,  6.6777e-03, -4.2531e-03,\n",
      "         1.6991e-02,  2.1619e-02,  3.4818e-02,  7.6285e-03,  4.7863e-03,\n",
      "        -1.5506e-03, -2.5538e-02,  2.8417e-03, -9.8136e-03, -5.9801e-03,\n",
      "         8.0864e-02,  3.1428e-02,  7.5658e-03,  2.7218e-02,  4.6474e-02,\n",
      "         3.2926e-02, -7.1620e-03,  2.7260e-02,  2.4225e-03,  8.3858e-02,\n",
      "        -3.3439e-02, -9.5333e-03, -2.1884e-02,  6.9214e-02,  1.4129e-02,\n",
      "         1.1217e-02,  4.2525e-02,  3.2221e-02, -7.3203e-03,  3.9458e-02,\n",
      "        -5.6014e-02, -2.3412e-02,  3.0586e-02,  1.2310e-02,  2.1869e-02,\n",
      "         1.4144e-02, -7.4028e-03,  1.9991e-02, -7.8582e-03,  6.1067e-02,\n",
      "        -7.4784e-02, -1.2307e-02,  2.0536e-02,  8.2082e-03,  6.3744e-02,\n",
      "        -3.1241e-02,  9.5708e-03,  4.7774e-02,  2.4200e-02,  4.1907e-02,\n",
      "        -1.0739e-02,  8.8580e-03, -1.1545e-02, -3.9819e-02,  4.2797e-02,\n",
      "         1.7787e-02, -8.8647e-03,  5.6656e-02,  2.8186e-02, -3.8903e-03,\n",
      "         2.5539e-02,  9.4593e-03,  4.0041e-02,  3.4093e-02, -5.8309e-03,\n",
      "         5.9342e-04, -2.4656e-03, -3.3458e-03, -5.2046e-02,  2.9881e-02,\n",
      "         3.7359e-02,  1.3456e-02,  8.4703e-04, -6.1181e-04,  4.1887e-02,\n",
      "        -5.4249e-04, -2.7966e-02,  3.7510e-02,  2.2977e-02,  5.8251e-02,\n",
      "         3.2375e-02,  9.7487e-03, -8.5164e-03,  2.6924e-02,  4.9820e-02,\n",
      "        -1.3774e-02, -1.5499e-02,  1.6504e-02, -1.8427e-02,  5.2729e-03,\n",
      "        -1.0275e-02, -5.3591e-02, -1.8835e-02,  8.2620e-02, -2.6378e-02,\n",
      "         4.1757e-02,  3.7953e-02, -2.0136e-02,  2.1273e-02, -5.5847e-03,\n",
      "         3.5576e-02, -4.5933e-03,  1.6068e-03,  7.6787e-03, -2.7052e-02,\n",
      "         9.7230e-03,  1.1144e-02, -1.0734e-02,  3.5131e-04,  3.3855e-02,\n",
      "        -5.8945e-02, -1.8785e-02, -3.8598e-03,  4.9779e-02, -2.4780e-02,\n",
      "        -1.3345e-02,  4.9903e-02, -4.5732e-02, -1.1392e-02,  2.5641e-02,\n",
      "         1.4635e-02,  2.0806e-02,  5.4765e-02, -5.3246e-02, -9.5838e-03,\n",
      "        -1.7550e-02,  4.4272e-02, -1.3776e-02,  8.6036e-03, -6.5184e-03,\n",
      "         4.2923e-02, -1.9712e-02,  4.4422e-02,  3.8219e-02, -6.1315e-03,\n",
      "         2.4058e-02,  2.3787e-02,  7.7424e-03, -1.0414e-02,  2.1284e-02,\n",
      "         2.5402e-02,  4.5648e-03,  7.3262e-03,  8.0529e-02,  4.3396e-03,\n",
      "         6.5539e-03,  3.7631e-02,  8.4009e-03, -4.8994e-02,  3.6077e-02,\n",
      "        -1.8451e-02, -3.1641e-02, -4.3811e-03,  2.7165e-02,  4.0062e-02,\n",
      "        -3.3648e-02,  3.3398e-02,  4.2256e-02,  7.1011e-04,  3.7615e-03])\n",
      "tensor([[ 4.5963e-05, -4.7313e-02,  7.9125e-03,  ..., -9.2213e-03,\n",
      "          1.9835e-02, -1.7928e-02],\n",
      "        [ 8.3867e-02, -5.5072e-02,  3.5573e-03,  ..., -1.7408e-02,\n",
      "          1.0537e-02, -7.5854e-02],\n",
      "        [ 5.3811e-02, -3.3852e-02,  2.5569e-02,  ..., -4.8031e-02,\n",
      "         -3.3776e-02,  5.0088e-04],\n",
      "        ...,\n",
      "        [-2.1221e-02,  1.2443e-02, -1.2909e-02,  ...,  8.8199e-03,\n",
      "          2.2996e-02,  1.7484e-02],\n",
      "        [ 9.5624e-03, -3.3259e-02, -5.0231e-02,  ...,  5.4299e-03,\n",
      "          1.5280e-02, -3.6116e-02],\n",
      "        [-6.6424e-03, -5.2713e-02,  4.1261e-02,  ..., -1.1734e-02,\n",
      "         -4.2039e-02, -2.2081e-02]])\n",
      "tensor([ 0.0347,  0.0051,  0.0095, -0.0613, -0.0357,  0.0105,  0.0558,  0.0516,\n",
      "        -0.0672,  0.0390,  0.0753, -0.0219,  0.0642, -0.0380, -0.0130, -0.0293,\n",
      "        -0.0111,  0.0084,  0.0216, -0.1348,  0.0702,  0.0642, -0.0356, -0.0238,\n",
      "         0.0159, -0.0588,  0.0169,  0.0559,  0.0066, -0.0361,  0.0125,  0.0777,\n",
      "        -0.0338, -0.0348,  0.0715, -0.0424,  0.0228,  0.0021, -0.0316,  0.0101,\n",
      "        -0.0504,  0.0497,  0.0408, -0.0326, -0.0774, -0.0157, -0.0209, -0.0683,\n",
      "         0.0394,  0.0198, -0.0669,  0.0210,  0.0219, -0.0103, -0.0269, -0.0158,\n",
      "         0.0443,  0.0436, -0.0870, -0.0329,  0.0184, -0.0381,  0.0431, -0.0308,\n",
      "         0.0525, -0.0458,  0.0003, -0.0304,  0.0855,  0.0427, -0.0133, -0.0588,\n",
      "         0.0564, -0.0172, -0.0126, -0.0234, -0.0274, -0.0195, -0.0215,  0.0219,\n",
      "         0.0293,  0.0440, -0.0358,  0.0239, -0.0181,  0.0361,  0.0215, -0.0282,\n",
      "        -0.0839,  0.0183, -0.0273,  0.0317,  0.0186, -0.0165,  0.0730, -0.0456,\n",
      "        -0.0149,  0.0167, -0.0494,  0.0617, -0.0428,  0.0837,  0.0480,  0.0170,\n",
      "        -0.0738, -0.0597, -0.0234,  0.0124,  0.0297,  0.0357,  0.0571, -0.0033,\n",
      "        -0.0206,  0.0211,  0.0249,  0.0055,  0.0006, -0.0271,  0.0320,  0.0601,\n",
      "         0.0125, -0.0066, -0.0151,  0.0358,  0.0264, -0.0367,  0.0319, -0.0425,\n",
      "         0.0514,  0.0604,  0.0755, -0.0156, -0.0084, -0.0600, -0.0036, -0.0446,\n",
      "         0.0149, -0.0488,  0.0371,  0.0392, -0.0498,  0.0624, -0.0452,  0.0200,\n",
      "         0.0122, -0.0649, -0.0452,  0.0282,  0.0489,  0.0159, -0.0111,  0.1017,\n",
      "         0.0014,  0.0320,  0.0262,  0.0120,  0.0253,  0.0421, -0.0434,  0.0794,\n",
      "         0.0467,  0.0132,  0.0410, -0.0359,  0.0328,  0.0358, -0.0372,  0.0429,\n",
      "        -0.0420,  0.0654,  0.0344, -0.0057,  0.0499,  0.0241, -0.0843, -0.0776,\n",
      "         0.0463,  0.0297, -0.0286, -0.0033,  0.0049,  0.0754,  0.0847, -0.0822,\n",
      "         0.0425,  0.0146, -0.0111, -0.0633,  0.1065, -0.0200,  0.0361, -0.0030,\n",
      "         0.0325, -0.0193, -0.0583,  0.0008, -0.0360, -0.0694, -0.0402,  0.0691,\n",
      "        -0.0140,  0.0602,  0.0436, -0.0034,  0.0502,  0.0076,  0.0306,  0.0126,\n",
      "         0.0901,  0.0141, -0.0055,  0.0210,  0.0327, -0.0166,  0.0368,  0.0364,\n",
      "         0.1112, -0.0185,  0.0527, -0.0076,  0.0768, -0.0624, -0.0256,  0.0326,\n",
      "         0.0331,  0.0153,  0.0553, -0.0018, -0.0062,  0.0068, -0.0296,  0.0860,\n",
      "         0.0062, -0.0264,  0.0100,  0.0301, -0.0052, -0.0138, -0.0790,  0.0482,\n",
      "        -0.0117,  0.0628,  0.0103, -0.0185, -0.0306, -0.0096, -0.0322,  0.0600,\n",
      "         0.0560,  0.0210, -0.0396, -0.0521,  0.0719,  0.0526, -0.0093, -0.0116,\n",
      "        -0.0015,  0.0415,  0.0326, -0.0556,  0.0059, -0.0111,  0.0377, -0.0462,\n",
      "         0.0083,  0.0363, -0.0184, -0.0326,  0.0386, -0.0022,  0.0232, -0.0720,\n",
      "        -0.0076,  0.0488,  0.0310,  0.0105, -0.1059,  0.0330,  0.0413,  0.0241,\n",
      "         0.0617,  0.0466,  0.0069, -0.0063, -0.0048,  0.0447, -0.0254, -0.0395,\n",
      "         0.0316,  0.0522,  0.0195,  0.0515,  0.0340,  0.0359, -0.0091,  0.0488,\n",
      "        -0.0247, -0.0172, -0.0188,  0.0517, -0.0312,  0.0474, -0.0233, -0.0150,\n",
      "        -0.0277,  0.0050,  0.0154, -0.0784,  0.0475, -0.0153,  0.0793,  0.0154,\n",
      "        -0.0446, -0.0227,  0.0759, -0.0089, -0.0088,  0.0011, -0.0180,  0.0703,\n",
      "        -0.0581,  0.0309,  0.0466,  0.0260,  0.0026, -0.0349, -0.0324, -0.0205,\n",
      "        -0.0131, -0.0231,  0.0632,  0.0289, -0.0298, -0.0033, -0.0059,  0.0091,\n",
      "         0.0696,  0.0361, -0.0467,  0.0453,  0.0816, -0.0069, -0.0285, -0.0046,\n",
      "        -0.0467, -0.0251,  0.0364, -0.0050,  0.0135, -0.0198])\n",
      "tensor([1.0276, 0.9734, 0.9957, 1.0078, 1.0151, 1.0305, 0.9878, 1.0024, 0.9848,\n",
      "        0.9885, 0.9671, 0.9577, 0.9735, 0.9976, 0.9931, 1.0471, 1.0200, 1.0076,\n",
      "        1.0147, 1.0976, 1.0151, 1.0188, 1.0208, 1.0023, 0.9693, 0.9917, 1.0252,\n",
      "        1.0278, 0.9826, 1.0219, 0.9958, 0.9717, 1.0553, 0.9846, 1.0318, 1.0027,\n",
      "        0.9813, 1.0103, 1.0031, 0.9662, 0.9919, 0.9906, 0.9656, 0.9655, 1.0095,\n",
      "        0.9703, 1.0270, 1.0539, 0.9987, 1.0018, 1.0788, 1.0008, 1.0141, 1.0225,\n",
      "        0.9876, 1.0063, 1.0317, 1.0050, 0.9833, 0.9641, 1.0407, 1.0003, 1.0027,\n",
      "        1.0242, 1.0027, 1.0354, 1.0505, 0.9687, 0.9794, 0.9811, 0.9985, 0.9983,\n",
      "        0.9880, 1.0080, 0.9740, 0.9943, 0.9915, 1.0074, 1.0049, 0.9850, 1.0111,\n",
      "        0.9620, 1.0144, 1.0112, 0.9646, 0.9897, 1.0078, 1.0050, 1.0339, 1.0247,\n",
      "        0.9894, 0.9860, 1.0322, 1.0284, 0.9617, 0.9929, 0.9691, 0.9840, 0.9773,\n",
      "        1.0174, 1.0004, 0.9786, 1.0732, 1.0262, 1.0148, 1.0317, 1.0114, 1.0083,\n",
      "        0.9789, 1.0044, 0.9950, 1.0026, 0.9888, 1.0235, 0.9867, 0.9979, 1.0289,\n",
      "        0.9779, 0.9652, 1.0048, 0.9974, 0.9973, 1.0047, 0.9879, 1.0275, 1.0569,\n",
      "        1.0324, 0.9800, 0.9852, 0.9513, 0.9791, 1.0330, 1.0028, 0.9975, 0.9552,\n",
      "        1.0100, 1.0201, 0.9832, 0.9597, 1.0090, 1.0094, 0.9727, 0.9789, 0.9863,\n",
      "        0.9616, 0.9913, 1.0209, 0.9954, 1.0049, 0.9845, 1.0359, 1.0044, 1.0041,\n",
      "        0.9999, 0.9971, 0.9938, 0.9813, 1.0475, 0.9600, 0.9691, 0.9972, 1.0279,\n",
      "        0.9597, 0.9878, 0.9768, 0.9905, 1.0116, 0.9689, 0.9886, 1.0035, 1.0130,\n",
      "        1.0079, 0.9864, 1.0016, 1.0102, 0.9657, 0.9814, 1.0073, 0.9785, 0.9734,\n",
      "        1.0022, 0.9741, 0.9904, 0.9756, 0.9953, 0.9937, 0.9845, 0.9992, 0.9822,\n",
      "        0.9847, 0.9808, 0.9834, 1.0167, 0.9824, 0.9566, 1.0134, 0.9866, 1.0094,\n",
      "        0.9836, 1.0023, 1.0117, 1.0030, 0.9680, 0.9864, 1.0629, 0.9799, 0.9989,\n",
      "        0.9914, 1.0161, 0.9927, 1.0228, 1.0208, 1.0056, 0.9903, 0.9812, 1.0128,\n",
      "        0.9802, 0.9686, 0.9837, 0.9587, 0.9947, 1.0057, 1.0099, 0.9736, 0.9867,\n",
      "        1.0106, 1.0019, 0.9777, 1.0266, 0.9839, 1.0092, 0.9791, 1.0120, 0.9951,\n",
      "        0.9892, 1.0080, 0.9924, 1.0151, 1.0514, 0.9986, 0.9767, 0.9932, 1.0072,\n",
      "        1.0096, 0.9693, 0.9893, 0.9941, 1.0092, 0.9834, 0.9917, 1.0273, 1.0228,\n",
      "        0.9961, 0.9971, 0.9910, 1.0073, 1.0079, 0.9845, 0.9848, 0.9819, 0.9515,\n",
      "        0.9866, 0.9800, 1.0095, 1.0334, 1.0131, 0.9525, 0.9887, 1.0006, 0.9878,\n",
      "        1.0286, 0.9987, 0.9834, 0.9541, 0.9848, 0.9973, 1.1001, 0.9669, 0.9908,\n",
      "        0.9613, 1.0240, 0.9648, 1.0000, 1.0100, 0.9762, 1.0238, 0.9801, 0.9944,\n",
      "        1.0194, 0.9932, 0.9892, 0.9858, 1.0204, 1.0209, 1.0051, 0.9804, 0.9982,\n",
      "        1.0412, 0.9724, 0.9410, 1.0054, 1.0076, 1.0047, 0.9616, 1.0797, 0.9864,\n",
      "        0.9831, 1.0180, 0.9940, 1.0127, 0.9759, 1.0594, 0.9919, 0.9868, 0.9981,\n",
      "        0.9792, 1.0028, 0.9713, 0.9710, 0.9807, 1.0127, 0.9971, 0.9967, 0.9640,\n",
      "        0.9967, 0.9670, 0.9862, 0.9880, 0.9866, 0.9486, 0.9962, 1.0195, 1.0055,\n",
      "        1.0060, 1.0463, 0.9530, 1.0020, 0.9688, 1.0120, 1.0150, 1.0151, 0.9696,\n",
      "        1.0005, 1.0241, 0.9665, 0.9750, 1.0198, 1.0150, 0.9713, 1.0306])\n",
      "tensor([-3.5350e-02,  1.4544e-02,  1.3029e-02, -4.5930e-02,  1.1225e-02,\n",
      "         1.1952e-02, -2.6069e-03, -2.4680e-02,  1.3814e-02, -5.9603e-03,\n",
      "         3.6558e-02,  2.7692e-02, -6.4048e-03,  1.0037e-02, -1.0070e-02,\n",
      "        -1.7275e-02,  1.6674e-02, -2.7602e-02, -4.0532e-02,  4.6684e-02,\n",
      "        -3.0559e-02, -2.1456e-02,  3.3872e-02, -2.3979e-02,  7.0430e-04,\n",
      "         7.2112e-03,  1.2325e-02, -3.6426e-02, -2.0055e-02,  4.4589e-02,\n",
      "        -9.8862e-03,  2.4613e-02, -1.7372e-02,  1.1600e-02, -1.6265e-02,\n",
      "        -2.4330e-02,  4.6533e-03,  5.4278e-02, -2.9920e-03,  2.8152e-02,\n",
      "        -2.0078e-02,  2.3122e-02,  2.5459e-02, -2.8040e-02, -8.7775e-03,\n",
      "         1.8702e-02,  2.7632e-03, -4.6198e-02,  2.9821e-02,  2.2518e-02,\n",
      "         5.2416e-02, -1.1100e-02,  1.0222e-02,  1.5708e-02,  2.8826e-02,\n",
      "         1.2678e-02,  2.2529e-02, -3.5432e-02,  3.4163e-02, -2.9744e-03,\n",
      "        -1.1407e-02, -2.2160e-02,  3.1458e-02,  3.6269e-02,  6.3464e-03,\n",
      "         8.0826e-03,  1.7367e-02,  2.9274e-02, -3.4223e-02,  5.6659e-03,\n",
      "        -1.0145e-02,  3.9194e-04, -6.9186e-03, -2.0809e-02,  4.4090e-02,\n",
      "        -1.0095e-02, -2.8433e-02,  2.2805e-02,  2.9220e-02,  1.3711e-02,\n",
      "        -5.9642e-02,  1.4878e-02,  3.5361e-02, -2.7359e-02,  1.8694e-02,\n",
      "         1.5357e-02, -1.2007e-02, -8.3162e-03, -1.1112e-02, -2.2560e-02,\n",
      "         7.3673e-04, -6.8736e-03, -3.9925e-03, -2.3595e-02,  2.7152e-02,\n",
      "        -1.8236e-02,  4.7175e-02,  1.2756e-02,  6.7276e-03,  4.3013e-02,\n",
      "         3.2018e-02,  4.6339e-03, -3.7869e-03, -5.3183e-02, -8.4378e-03,\n",
      "         2.2388e-02,  1.6571e-02, -3.7221e-02, -1.2483e-02,  2.1794e-03,\n",
      "         2.2247e-02, -7.0703e-04, -3.0174e-02,  2.5105e-02, -2.7229e-02,\n",
      "        -4.7152e-02, -2.7540e-02, -2.2321e-02,  1.1478e-02,  2.0905e-02,\n",
      "        -3.0561e-02, -2.3679e-02,  2.1322e-02, -2.3602e-03, -5.9481e-02,\n",
      "        -1.6024e-02, -1.7491e-02, -1.1836e-03, -3.1539e-03, -1.3152e-02,\n",
      "         5.6931e-02, -2.6342e-02, -1.2109e-02, -8.2168e-03,  5.0936e-02,\n",
      "        -3.3346e-02,  4.2103e-02, -2.3183e-02,  3.3327e-02,  4.6228e-02,\n",
      "        -3.4010e-02, -2.2589e-02, -1.4561e-03,  2.6052e-02,  3.9765e-02,\n",
      "        -5.0117e-02,  2.3261e-04,  7.1917e-03, -1.5897e-02,  1.8670e-02,\n",
      "        -2.5867e-02, -4.0815e-02,  5.3983e-03, -2.0648e-02, -1.3693e-02,\n",
      "        -1.7737e-03, -2.7294e-02,  5.2415e-03, -1.6004e-02,  5.4567e-02,\n",
      "        -4.5197e-02, -3.4419e-02,  9.3330e-03,  3.2222e-02, -2.8663e-03,\n",
      "         1.3755e-02,  2.2604e-02, -5.0868e-03, -3.6231e-02, -3.9697e-04,\n",
      "         1.4922e-02, -5.7933e-02,  1.9755e-02, -2.3488e-02,  1.7444e-02,\n",
      "         4.5066e-02,  4.5853e-03,  6.8704e-03,  8.0424e-03, -3.4096e-02,\n",
      "        -3.7074e-02, -1.4356e-02, -3.6809e-02,  6.3309e-03,  1.8600e-02,\n",
      "         1.2181e-03,  9.5870e-03,  4.6199e-02, -2.0432e-02,  4.0745e-02,\n",
      "        -1.9347e-02,  1.6188e-02, -1.7725e-03, -4.5266e-02,  6.9166e-04,\n",
      "         1.7140e-03,  4.3890e-02, -4.2235e-02, -3.1289e-02,  1.4253e-02,\n",
      "         6.0712e-02, -7.4071e-02,  3.0854e-03, -1.1153e-03, -2.3178e-02,\n",
      "         1.4160e-02,  1.0558e-02, -1.0060e-02, -8.1522e-03,  2.4040e-03,\n",
      "        -8.1727e-04, -1.2081e-04,  2.2357e-02,  3.5634e-02,  8.0477e-03,\n",
      "         1.7610e-02,  1.1644e-02, -8.5299e-03, -1.2960e-02,  1.3231e-02,\n",
      "         4.2892e-02, -8.0465e-03,  2.3163e-02, -2.7743e-02,  1.5291e-03,\n",
      "         3.0255e-02,  6.4247e-03,  3.0443e-02, -3.9913e-03,  2.3711e-02,\n",
      "        -1.0543e-02, -1.9637e-02, -1.9586e-02, -1.0059e-02,  3.0858e-02,\n",
      "        -2.7976e-02, -1.8511e-02, -2.7751e-02, -2.1409e-02, -2.6789e-02,\n",
      "         3.6398e-03, -2.3267e-03,  2.8849e-02, -1.5588e-02, -1.6449e-02,\n",
      "        -8.7423e-03, -2.2012e-02, -6.6562e-03,  3.0040e-02,  9.0777e-03,\n",
      "         2.4204e-03, -2.1653e-02, -3.8930e-02, -2.4384e-02,  1.4804e-02,\n",
      "         3.4419e-02, -3.7753e-02, -1.3193e-03,  8.1317e-03,  2.0632e-02,\n",
      "         8.0026e-03, -7.0535e-03,  8.9526e-03, -3.6372e-02, -6.1832e-03,\n",
      "        -1.1020e-02,  3.0251e-02,  2.0859e-03,  3.0837e-02, -9.9613e-03,\n",
      "        -1.5744e-02,  2.5834e-03, -2.9797e-02, -2.1005e-02,  1.4507e-02,\n",
      "        -1.1474e-02, -1.8025e-02,  1.4232e-02,  5.2274e-03, -1.2668e-02,\n",
      "        -9.8186e-04,  4.0867e-03, -8.1167e-03, -3.4265e-02,  6.0216e-03,\n",
      "         5.6454e-03, -1.5457e-03,  1.6552e-02,  3.9761e-02, -1.0269e-02,\n",
      "         1.0324e-02, -1.6654e-03,  3.3594e-03,  2.1380e-02,  1.7027e-02,\n",
      "         7.5875e-02,  1.0034e-02, -1.7652e-02,  1.1878e-02, -1.9520e-03,\n",
      "         2.4688e-03,  1.6417e-02,  1.3519e-02, -1.3754e-02, -3.2320e-02,\n",
      "        -1.3942e-02, -2.8841e-02,  2.1106e-02, -2.8115e-03, -2.7969e-02,\n",
      "        -1.1159e-02, -1.4443e-02, -8.7004e-03, -5.0777e-03, -2.0218e-02,\n",
      "        -1.5662e-02,  1.0083e-02,  1.0443e-02,  1.6605e-02, -1.9987e-02,\n",
      "         3.2370e-03, -6.3985e-02, -1.8802e-02, -1.9201e-02,  3.3417e-02,\n",
      "        -4.8423e-02, -1.1690e-02, -1.5085e-02,  4.6856e-03, -3.2940e-02,\n",
      "        -1.3441e-02, -2.2589e-03,  2.2053e-02,  1.9735e-02, -1.7714e-02,\n",
      "        -5.1835e-03,  4.4337e-03,  3.8068e-02,  4.9846e-06, -7.1225e-02,\n",
      "         3.4681e-02,  6.9338e-03, -1.8490e-02, -2.1908e-02,  7.6246e-03,\n",
      "         1.0069e-02, -2.1550e-02,  1.0254e-02, -1.8278e-02, -2.6174e-03])\n",
      "tensor([[-0.0770,  0.0347,  0.0074,  ..., -0.0336,  0.0143,  0.0743],\n",
      "        [-0.0192,  0.0327,  0.0788,  ..., -0.0576, -0.0430,  0.0561],\n",
      "        [-0.0069,  0.0412, -0.0471,  ..., -0.0413, -0.0583,  0.0006],\n",
      "        ...,\n",
      "        [-0.0324, -0.0533, -0.0025,  ...,  0.0153,  0.0354, -0.0625],\n",
      "        [-0.0351,  0.0120, -0.0232,  ...,  0.0581, -0.0030,  0.0478],\n",
      "        [ 0.0375, -0.0448, -0.0452,  ...,  0.0433, -0.0350, -0.0641]])\n",
      "tensor([ 3.1220e-02, -6.4367e-02, -5.0851e-02,  2.5917e-02, -4.5012e-02,\n",
      "         2.9362e-02,  1.9246e-02,  6.8199e-02, -4.2455e-03,  5.2977e-02,\n",
      "         2.5955e-02, -1.9022e-02,  8.4306e-02,  7.8209e-03,  1.0526e-02,\n",
      "        -2.5088e-02,  8.5779e-02, -2.4915e-02, -3.8811e-02, -1.5853e-02,\n",
      "         7.4089e-03,  5.5927e-02,  4.8042e-02,  3.3678e-02, -1.2334e-02,\n",
      "        -1.0018e-02,  2.4896e-02,  1.2468e-02,  4.9789e-02,  1.7878e-02,\n",
      "        -1.3859e-02, -3.1982e-03, -4.6200e-02,  4.8855e-02,  9.1821e-02,\n",
      "         4.9626e-02, -4.2201e-02,  2.3957e-02,  4.0595e-03, -3.7398e-02,\n",
      "         3.7986e-02,  4.5612e-02, -1.6361e-03, -3.9586e-02,  1.7108e-02,\n",
      "        -4.2307e-02,  3.8448e-02,  5.3015e-03,  3.5367e-02, -3.5653e-02,\n",
      "         1.0286e-02, -4.9044e-02, -3.8599e-02, -5.5869e-02,  1.1401e-02,\n",
      "         2.6030e-02, -1.6078e-02, -9.1632e-03, -1.1468e-02,  4.9287e-02,\n",
      "        -1.2999e-02,  4.6635e-02,  2.8266e-02, -1.1957e-03, -4.4946e-02,\n",
      "         2.2004e-02, -3.7596e-02,  2.6022e-02,  6.0958e-02,  2.8824e-02,\n",
      "         6.2551e-02, -2.0607e-02, -5.0752e-03,  7.7025e-02,  4.8286e-02,\n",
      "         6.0685e-02,  5.0243e-02, -1.3424e-03, -3.2029e-02,  1.2732e-02,\n",
      "         6.8487e-02,  1.6726e-02,  2.5311e-02,  4.0938e-02, -3.7663e-02,\n",
      "        -6.9774e-03, -2.6668e-02,  4.3938e-02,  1.9049e-02,  2.5011e-02,\n",
      "         5.0649e-03,  2.7766e-02, -1.8677e-02, -6.5871e-03, -1.0926e-02,\n",
      "        -4.5367e-02, -1.4554e-02, -8.9167e-03,  1.2815e-02,  1.9678e-02,\n",
      "        -6.5054e-02,  7.9996e-03, -5.6171e-04,  2.2121e-02, -7.9475e-02,\n",
      "        -1.4613e-02,  1.1928e-02, -7.1760e-03,  2.1885e-02,  7.4417e-02,\n",
      "        -3.4661e-02,  4.3362e-02, -8.3427e-03,  1.1697e-02, -4.1059e-02,\n",
      "        -2.2433e-02,  2.9944e-02, -4.1845e-02, -3.2012e-02,  2.7519e-02,\n",
      "         3.0520e-02, -6.6486e-03,  1.9922e-02,  4.1011e-05,  2.4321e-02,\n",
      "        -3.4112e-02, -8.1252e-03, -1.3817e-03,  4.1487e-02,  5.5383e-03,\n",
      "         7.4264e-02,  5.8538e-02, -1.2504e-02, -5.8637e-02, -3.2831e-02,\n",
      "        -3.8373e-02,  3.7144e-02, -4.6573e-02, -4.4477e-03,  1.4093e-02,\n",
      "         2.4116e-02,  3.7189e-02,  1.8752e-02,  2.8613e-02,  3.1591e-02,\n",
      "        -5.8729e-02, -2.5585e-02,  1.1315e-02,  3.4952e-02,  4.1815e-02,\n",
      "        -4.2700e-02,  9.5975e-02, -5.4345e-02,  4.4778e-02,  3.4699e-02,\n",
      "        -1.4143e-03, -4.2060e-02,  4.5085e-03,  3.6229e-02,  8.6210e-02,\n",
      "         5.6177e-02, -2.5983e-02,  5.1932e-02, -2.1423e-02,  1.7566e-02,\n",
      "         8.7870e-02,  5.8296e-02,  7.4138e-02,  8.4713e-02,  2.6736e-02,\n",
      "         2.2297e-02, -3.9376e-02,  2.6866e-02,  1.8884e-02, -6.2928e-02,\n",
      "        -1.1927e-01,  7.9880e-03,  2.8906e-02,  4.5301e-02, -7.4961e-02,\n",
      "         3.2721e-02,  7.2418e-02,  6.7919e-02, -3.9072e-02, -4.5895e-03,\n",
      "        -1.5853e-03, -3.0371e-02, -3.7048e-02,  5.1904e-02,  4.0411e-02,\n",
      "        -8.3111e-04,  7.5006e-02, -1.7369e-02, -1.9430e-02,  8.6174e-03,\n",
      "        -1.6283e-02, -1.4449e-02, -4.4124e-03,  6.7219e-02,  2.5801e-03,\n",
      "         3.5575e-02,  2.3887e-02,  7.3724e-02, -2.7450e-02,  5.9700e-02,\n",
      "        -4.4520e-02,  1.5025e-02,  4.0414e-02,  1.2516e-02, -6.9264e-02,\n",
      "        -5.9417e-02, -2.6525e-02,  6.7678e-02, -1.2832e-02,  5.0105e-03,\n",
      "         4.0910e-02,  1.0097e-01, -3.5914e-02,  2.8993e-02, -4.8939e-03,\n",
      "         1.0175e-01, -4.1305e-02,  1.1802e-02, -3.7813e-02, -2.6506e-02,\n",
      "         2.8449e-02,  7.3423e-02,  1.0151e-02, -6.7958e-02,  3.9225e-03,\n",
      "        -2.4804e-02, -3.9276e-03,  5.4167e-02, -5.4194e-02,  4.5707e-02,\n",
      "         7.6104e-03, -1.4399e-02,  1.5504e-02, -1.5404e-02,  4.8668e-02,\n",
      "        -6.3860e-02,  7.4544e-02, -7.2998e-02,  2.2758e-02,  1.6362e-02,\n",
      "         4.3762e-02, -2.2281e-03,  4.9973e-02,  4.8246e-02,  5.3134e-02,\n",
      "        -3.1200e-02,  1.8368e-03, -1.1808e-02, -8.7060e-03,  2.1497e-02,\n",
      "        -5.0635e-02, -5.4812e-02,  7.8076e-02,  8.1736e-02,  2.0540e-02,\n",
      "         4.8063e-02, -1.1393e-02, -1.6909e-02,  3.8772e-02,  2.5120e-03,\n",
      "        -1.0431e-02, -5.7217e-02, -2.5625e-02, -1.8534e-03,  3.7000e-02,\n",
      "        -6.7892e-04, -8.4087e-03, -3.5301e-02,  5.0963e-02,  4.7199e-02,\n",
      "        -1.0314e-02, -5.1786e-02, -1.9911e-02,  7.9702e-02, -4.2759e-02,\n",
      "         4.7687e-02, -1.4493e-02, -1.4170e-03,  4.3998e-02, -4.6188e-02,\n",
      "         3.0044e-02, -1.5390e-02, -3.2464e-02, -2.9193e-03, -1.5669e-02,\n",
      "        -8.2792e-03,  3.3157e-02,  2.0329e-02,  1.2330e-02, -1.9292e-02,\n",
      "         3.7264e-02,  4.6310e-02,  2.3605e-02, -1.8521e-03,  5.1313e-02,\n",
      "        -2.3791e-02, -2.9857e-02, -4.8621e-02, -5.4848e-03,  3.3813e-03,\n",
      "         5.3147e-02,  4.2042e-02, -5.6409e-02,  8.6589e-02,  6.4586e-02,\n",
      "         3.0701e-02, -4.3648e-02,  8.4899e-03, -2.5204e-02,  1.7927e-02,\n",
      "         3.1091e-02, -4.4601e-02, -2.4615e-02, -2.9417e-02, -1.9232e-02,\n",
      "        -6.1471e-02, -5.9971e-02, -9.8965e-03,  2.1335e-02, -6.8786e-03,\n",
      "        -1.6642e-02,  1.9179e-02, -1.7309e-02,  1.4843e-03,  4.9123e-02,\n",
      "         7.0532e-02,  4.9847e-02,  7.3972e-02,  7.2877e-02, -6.6248e-03,\n",
      "        -3.3546e-02,  6.9865e-02, -3.2413e-02,  4.0398e-02,  2.2823e-02,\n",
      "         7.1041e-02,  1.2250e-02,  2.2351e-02,  6.0800e-03,  1.6917e-03,\n",
      "        -2.5096e-02, -3.3802e-03, -4.1258e-02,  7.3368e-02,  1.6909e-02])\n",
      "tensor([0.9919, 0.9576, 0.9811, 0.9651, 0.9635, 0.9802, 0.9853, 1.0039, 0.9933,\n",
      "        0.9623, 0.9718, 0.9842, 0.9897, 0.9812, 0.9484, 0.9757, 0.9711, 0.9700,\n",
      "        0.9854, 0.9577, 0.9423, 0.9521, 0.9719, 0.9733, 0.9581, 0.9735, 0.9702,\n",
      "        0.9868, 0.9906, 0.9787, 1.0252, 0.9752, 0.9649, 0.9821, 0.9929, 0.9642,\n",
      "        0.9635, 1.0294, 0.9881, 0.9807, 0.9316, 0.9410, 0.9825, 0.9654, 0.9852,\n",
      "        0.9745, 0.9801, 0.9946, 0.9821, 0.9462, 0.9836, 0.9553, 0.9902, 0.9726,\n",
      "        1.0154, 0.9567, 0.9701, 0.9774, 0.9715, 0.9715, 0.9402, 0.9711, 0.9590,\n",
      "        0.9445, 0.9643, 0.9546, 0.9834, 0.9694, 0.9546, 0.9643, 0.9710, 1.0158,\n",
      "        0.9915, 0.9437, 0.9822, 0.9569, 0.9762, 0.9644, 1.0053, 0.9721, 0.9582,\n",
      "        0.9626, 0.9433, 0.9915, 0.9699, 0.9544, 0.9871, 0.9702, 0.9716, 0.9834,\n",
      "        0.9841, 0.9715, 0.9696, 0.9884, 0.9725, 0.9669, 0.9538, 1.0165, 0.9846,\n",
      "        0.9789, 0.9878, 0.9689, 0.9682, 0.9725, 1.0017, 0.9300, 0.9205, 0.9698,\n",
      "        0.9573, 0.9611, 0.9688, 0.9783, 0.9792, 0.9616, 0.9552, 0.9684, 0.9823,\n",
      "        1.0161, 0.9550, 0.9911, 0.9747, 0.9865, 0.9784, 0.9777, 0.9633, 0.9911,\n",
      "        0.9986, 0.9640, 0.9686, 0.9618, 0.9600, 0.9759, 0.9681, 0.9452, 0.9651,\n",
      "        0.9637, 0.9612, 0.9723, 0.9511, 0.9703, 0.9575, 0.9596, 0.9730, 0.9417,\n",
      "        0.9548, 0.9622, 0.9938, 0.9807, 0.9678, 0.9602, 0.9780, 0.9688, 0.9867,\n",
      "        0.9611, 0.9707, 0.9819, 0.9795, 0.9631, 0.9705, 0.9447, 0.9796, 0.9750,\n",
      "        0.9664, 0.9696, 0.9893, 0.9526, 0.9593, 0.9587, 1.0137, 1.0085, 0.9536,\n",
      "        0.9850, 0.9744, 0.9593, 0.9750, 1.0203, 0.9568, 0.9746, 0.9660, 1.0010,\n",
      "        0.9763, 0.9566, 0.9680, 0.9614, 0.9804, 0.9702, 0.9783, 0.9279, 0.9528,\n",
      "        0.9855, 0.9604, 0.9749, 0.9695, 0.9855, 0.9655, 0.9736, 0.9755, 0.9745,\n",
      "        0.9705, 0.9568, 0.9915, 0.9834, 0.9710, 0.9521, 0.9838, 0.9748, 0.9512,\n",
      "        0.9922, 0.9635, 0.9585, 1.0037, 0.9800, 0.9741, 0.9556, 0.9806, 0.9967,\n",
      "        0.9761, 0.9767, 0.9504, 0.9778, 0.9653, 0.9860, 0.9464, 0.9659, 0.9700,\n",
      "        1.0087, 0.9511, 0.9576, 1.0116, 0.9804, 0.9625, 0.9418, 0.9561, 0.9623,\n",
      "        0.9672, 0.9622, 0.9743, 0.9694, 0.9770, 0.9532, 0.9772, 0.9615, 0.9806,\n",
      "        0.9910, 0.9729, 0.9506, 1.0117, 0.9687, 0.9665, 0.9522, 1.0053, 0.9685,\n",
      "        0.9557, 0.9974, 0.9603, 0.9674, 0.9676, 0.9719, 0.9860, 0.9819, 0.9524,\n",
      "        0.9787, 0.9420, 0.9621, 1.0045, 0.9771, 0.9721, 0.9713, 0.9587, 0.9708,\n",
      "        0.9509, 0.9782, 0.9734, 0.9727, 0.9706, 0.9772, 1.0704, 0.9742, 0.9759,\n",
      "        0.9552, 0.9682, 0.9605, 0.9692, 0.9558, 0.9604, 1.0157, 1.0005, 0.9906,\n",
      "        0.9638, 0.9870, 0.9871, 0.9754, 0.9550, 0.9604, 0.9325, 0.9642, 0.9680,\n",
      "        0.9502, 0.9823, 0.9651, 0.9645, 0.9758, 0.9553, 0.9847, 0.9827, 0.9744,\n",
      "        0.9791, 0.9517, 0.9694, 0.9378, 0.9612, 1.0090, 0.9761, 0.9719, 0.9685,\n",
      "        0.9395, 0.9740, 0.9664, 0.9775, 0.9613, 0.9796, 0.9979, 0.9767, 0.9550,\n",
      "        0.9737, 0.9677, 0.9341, 0.9586, 0.9828, 0.9518, 0.9707, 0.9615, 0.9807,\n",
      "        0.9683, 0.9838, 0.9513, 0.9552, 0.9736, 0.9624, 0.9662, 0.9757, 0.9819,\n",
      "        0.9939, 0.9508, 0.9852, 0.9840, 0.9838, 0.9646, 0.9495, 0.9699])\n",
      "tensor([-0.0226,  0.0538, -0.0304, -0.0098, -0.0627, -0.0219,  0.0288, -0.0105,\n",
      "        -0.0439,  0.0502, -0.0290,  0.0008,  0.0318, -0.0215,  0.0049,  0.0006,\n",
      "        -0.0370, -0.0284, -0.0326, -0.0080,  0.0376, -0.0410, -0.0019, -0.0089,\n",
      "         0.0106,  0.0117,  0.0672,  0.0755, -0.0321, -0.0181, -0.0018, -0.0104,\n",
      "        -0.0053,  0.0507, -0.0463, -0.0272,  0.0375,  0.0580,  0.0277, -0.0274,\n",
      "         0.0584,  0.0443,  0.0370, -0.0239,  0.0466,  0.0283, -0.0166,  0.0271,\n",
      "        -0.0219,  0.0404, -0.0206, -0.0433, -0.0419, -0.0040, -0.0287, -0.0216,\n",
      "        -0.0360,  0.0113, -0.0292, -0.0121,  0.0128,  0.0073,  0.0142, -0.0043,\n",
      "         0.0455, -0.0239,  0.0687, -0.0356, -0.0038, -0.0295, -0.0279,  0.0225,\n",
      "        -0.0423,  0.0161, -0.0070,  0.0353,  0.0574,  0.0290, -0.0213,  0.0148,\n",
      "         0.0412,  0.0224,  0.0521,  0.0329, -0.0406,  0.0632,  0.0655, -0.0137,\n",
      "        -0.0107, -0.0483, -0.0255,  0.0113, -0.0397, -0.0186, -0.0219, -0.0272,\n",
      "         0.0295, -0.0540,  0.0249, -0.0104, -0.0544,  0.0181, -0.0064,  0.0421,\n",
      "         0.0527, -0.0031,  0.0209, -0.0273,  0.0445, -0.0115, -0.0180, -0.0525,\n",
      "         0.0684,  0.0268,  0.0297,  0.0037, -0.0334,  0.0559, -0.0135,  0.0262,\n",
      "        -0.0073, -0.0536,  0.0367,  0.0370,  0.0492, -0.0243, -0.0249,  0.0564,\n",
      "        -0.0323,  0.0475,  0.0380,  0.0363,  0.0292,  0.0296, -0.0212, -0.0284,\n",
      "         0.0024, -0.0171, -0.0420, -0.0194,  0.0465,  0.0109,  0.0269,  0.0133,\n",
      "        -0.0097,  0.0419, -0.0456,  0.0554, -0.0141,  0.0081, -0.0290,  0.0180,\n",
      "        -0.0461,  0.0228, -0.0122, -0.0166, -0.0348, -0.0175,  0.0140,  0.0294,\n",
      "         0.0454,  0.0232, -0.0644, -0.0007, -0.0214, -0.0726, -0.0321,  0.0073,\n",
      "         0.0493, -0.0533,  0.0176, -0.0137,  0.0167,  0.0321, -0.0149,  0.0079,\n",
      "         0.0196,  0.0377, -0.0253,  0.0492, -0.0208, -0.0314, -0.0249, -0.0498,\n",
      "        -0.0063,  0.0046, -0.0281,  0.0252,  0.0158,  0.0198, -0.0183, -0.0288,\n",
      "         0.0306, -0.0366, -0.0438,  0.0158, -0.0165, -0.0193,  0.0122, -0.0367,\n",
      "        -0.0310,  0.0207, -0.0164, -0.0295,  0.0243, -0.0184, -0.0240, -0.0269,\n",
      "         0.0260,  0.0326, -0.0128,  0.0201, -0.0422, -0.0027, -0.0115, -0.0209,\n",
      "         0.0332,  0.0414,  0.0384, -0.0347,  0.0167,  0.0446, -0.0073, -0.0060,\n",
      "        -0.0205,  0.0087,  0.0570, -0.0283,  0.0373,  0.0302, -0.0311,  0.0443,\n",
      "         0.0146, -0.0538,  0.0091, -0.0333, -0.0389, -0.0028, -0.0088, -0.0593,\n",
      "         0.0176, -0.0075, -0.0379, -0.0507, -0.0304, -0.0203, -0.0194, -0.0297,\n",
      "         0.0260,  0.0254,  0.0131,  0.0297, -0.0343, -0.0046,  0.0265,  0.0380,\n",
      "        -0.0683, -0.0740, -0.0397, -0.0493,  0.0575,  0.0233,  0.0267, -0.0381,\n",
      "         0.0405, -0.0256, -0.0002,  0.0137,  0.0168,  0.0080, -0.0579, -0.0265,\n",
      "        -0.0727, -0.0255, -0.0308,  0.0350,  0.0894,  0.0109,  0.0200, -0.0303,\n",
      "        -0.0425,  0.0093, -0.0321,  0.0097, -0.0300, -0.0347,  0.0307, -0.0310,\n",
      "        -0.0165, -0.0126, -0.0199,  0.0453,  0.0281,  0.0370,  0.0057, -0.0380,\n",
      "        -0.0212, -0.0268, -0.0235,  0.0277,  0.0200,  0.0242,  0.0421,  0.0045,\n",
      "        -0.0489, -0.0019,  0.0130,  0.0218,  0.0299, -0.0126,  0.0185, -0.0074,\n",
      "        -0.0175,  0.0084, -0.0208,  0.0642, -0.0090, -0.0129,  0.0289,  0.0152,\n",
      "        -0.0528, -0.0220,  0.0456,  0.0353,  0.0185, -0.0272,  0.0164, -0.0344,\n",
      "         0.0223, -0.0085, -0.0148,  0.0416, -0.0436, -0.0175,  0.0229, -0.0189,\n",
      "         0.0194, -0.0445,  0.0047,  0.0414, -0.0046, -0.0292, -0.0073,  0.0510,\n",
      "         0.0293,  0.0477, -0.0332, -0.0036,  0.0384, -0.0208])\n",
      "tensor([[ 0.0197,  0.0170, -0.0218,  ...,  0.0335,  0.0120,  0.0485],\n",
      "        [ 0.0874,  0.0221, -0.0034,  ..., -0.0036, -0.0230, -0.0055],\n",
      "        [ 0.0230,  0.0156, -0.0149,  ...,  0.0041,  0.0165, -0.0266],\n",
      "        ...,\n",
      "        [ 0.0295,  0.0033,  0.0144,  ...,  0.0327, -0.0194,  0.0147],\n",
      "        [ 0.0323,  0.0046, -0.0191,  ...,  0.0060,  0.0001,  0.0367],\n",
      "        [-0.0169, -0.0113,  0.0556,  ...,  0.0321, -0.0220, -0.0177]])\n",
      "tensor([ 0.0466,  0.0187,  0.0439, -0.0222, -0.0308, -0.0481, -0.0520,  0.0114])\n",
      "tensor([[-0.0739,  0.1198, -0.0276,  ..., -0.0088,  0.0488, -0.0371],\n",
      "        [ 0.0008, -0.1393, -0.1285,  ...,  0.0470, -0.0160,  0.1164],\n",
      "        [ 0.1466,  0.0598,  0.1393,  ..., -0.0500, -0.0836, -0.0630],\n",
      "        ...,\n",
      "        [ 0.0226,  0.0698, -0.0759,  ..., -0.0509, -0.0376,  0.1457],\n",
      "        [-0.0373,  0.0848, -0.1624,  ...,  0.1388, -0.0036, -0.0975],\n",
      "        [-0.0968, -0.1052,  0.0692,  ...,  0.0342,  0.1335,  0.0043]])\n",
      "tensor([-3.0543e-02, -3.5821e-02,  1.2162e-01, -1.0187e-01, -9.1161e-02,\n",
      "         4.5207e-02,  2.0954e-01,  5.8366e-02,  2.3705e-02,  9.1012e-02,\n",
      "        -1.0781e-02,  2.1154e-01,  1.2733e-01,  1.2345e-01, -6.8523e-02,\n",
      "        -8.5949e-02,  2.3186e-01, -4.0249e-02,  8.9672e-02, -1.7098e-01,\n",
      "         1.2167e-01, -1.4403e-01,  1.9679e-01,  1.8696e-01, -3.0602e-02,\n",
      "         1.1708e-01,  2.7519e-01,  9.7253e-02, -7.5421e-02,  1.1782e-01,\n",
      "         1.4252e-01, -1.1547e-01,  1.7381e-01,  2.0104e-01, -5.9491e-02,\n",
      "         1.3054e-01,  7.6997e-02, -5.6495e-03, -9.8097e-02,  1.6119e-02,\n",
      "         1.6657e-01, -2.2475e-02,  6.2619e-02,  9.1626e-02,  6.2916e-02,\n",
      "        -4.4277e-02, -5.7589e-02,  7.4381e-02,  2.6277e-01, -7.9904e-02,\n",
      "        -1.4272e-01,  3.5406e-02, -1.3599e-01,  1.2097e-01,  7.7743e-02,\n",
      "         2.4754e-01,  1.6101e-01,  7.9228e-02,  1.1073e-01, -4.1832e-02,\n",
      "         1.1375e-01,  1.4721e-01,  2.6687e-01, -7.3177e-02,  1.0892e-01,\n",
      "        -1.0496e-01, -6.2271e-02,  8.5179e-02,  1.6241e-01,  3.8793e-02,\n",
      "         9.3146e-04,  1.0429e-01,  1.8227e-01,  5.8587e-02,  1.2392e-01,\n",
      "         6.3384e-02,  3.1772e-02,  2.2902e-01, -7.6355e-02, -4.2899e-02,\n",
      "        -3.7909e-02,  1.9008e-01,  1.8625e-01,  1.5118e-01,  5.3555e-02,\n",
      "         1.0429e-01, -8.5525e-02,  6.0124e-02,  9.5218e-02,  8.3532e-02,\n",
      "         1.0526e-03, -8.8173e-02,  7.5813e-02,  1.1198e-01, -1.2428e-01,\n",
      "        -1.7935e-02,  2.0867e-01,  1.5171e-01,  2.6043e-01,  1.0289e-02,\n",
      "        -1.0227e-01,  1.8176e-01, -6.5464e-02,  1.7260e-01, -4.4620e-02,\n",
      "         1.7386e-01, -9.9804e-02,  2.7085e-02,  4.8649e-02,  9.0978e-02,\n",
      "        -1.4522e-02,  5.0144e-02,  2.7064e-02,  1.1646e-01,  9.5457e-02,\n",
      "         3.4164e-02, -1.0861e-01,  1.5659e-01, -1.5228e-02,  8.3819e-02,\n",
      "         1.7339e-01, -1.5891e-02, -2.1988e-03,  1.2582e-02,  2.0374e-01,\n",
      "         1.5293e-02, -1.2125e-01,  1.8066e-01, -2.8012e-02,  8.5298e-02,\n",
      "        -1.1308e-01,  2.7320e-01,  2.5258e-02, -8.8448e-02,  1.9618e-01,\n",
      "         4.8639e-02, -9.2476e-03,  6.7800e-02,  9.2085e-02, -1.1441e-01,\n",
      "         5.9660e-02, -5.6994e-02,  2.6814e-01,  1.1946e-01,  2.9347e-02,\n",
      "         2.6254e-01,  5.9447e-02,  1.3827e-01,  5.1153e-02, -5.9081e-02,\n",
      "        -5.3996e-02,  1.3619e-01,  1.2323e-01,  1.8285e-01, -8.4436e-03,\n",
      "         1.4477e-01,  4.1536e-02, -1.0732e-01,  3.2855e-02,  2.0624e-01,\n",
      "        -1.0505e-01,  9.9968e-03,  1.6997e-01, -1.0132e-01,  7.3262e-02,\n",
      "         1.1771e-01, -5.6171e-02, -9.7614e-02, -9.3018e-02,  7.7950e-02,\n",
      "         8.8889e-02, -9.8612e-02,  2.5723e-02,  9.8365e-02,  4.7671e-02,\n",
      "         2.1980e-01, -1.5436e-01, -1.0643e-01,  4.0180e-02,  1.1196e-01,\n",
      "         1.6786e-01,  1.0621e-01,  2.3366e-01, -4.0035e-02,  1.2793e-01,\n",
      "         3.3726e-02,  4.1785e-02, -5.1227e-02,  7.1775e-02,  2.0989e-01,\n",
      "         1.3254e-01,  1.6535e-01,  2.1443e-01, -1.2260e-01,  2.1879e-01,\n",
      "         1.2227e-01,  1.1674e-01,  7.1581e-03,  1.8211e-01,  1.1956e-01,\n",
      "         2.4998e-02,  1.9623e-01, -1.5321e-02,  1.0718e-01, -7.8355e-02,\n",
      "        -1.1326e-01,  1.9297e-01,  1.2302e-01,  2.1488e-02,  1.8375e-01,\n",
      "        -7.0644e-02,  2.0471e-01,  3.0901e-03, -1.4418e-02,  1.3463e-01,\n",
      "         1.1034e-01,  6.4821e-02,  2.3115e-01,  2.0275e-01,  1.3634e-01,\n",
      "        -1.3591e-01,  1.6902e-02, -1.9240e-02,  1.1929e-01,  1.3207e-01,\n",
      "         2.0507e-01,  4.3778e-03, -4.0236e-02, -4.2740e-04,  2.6730e-01,\n",
      "        -2.2275e-01, -6.4235e-02, -9.8890e-02,  2.9866e-02,  2.2385e-01,\n",
      "         1.7711e-01,  2.0513e-01, -4.7634e-02, -1.0972e-01, -1.9416e-02,\n",
      "         7.4810e-02,  7.7781e-02,  1.5393e-02,  1.4977e-01,  7.3896e-02,\n",
      "         2.9498e-02,  1.4788e-01, -4.6658e-02,  1.6507e-02, -1.4274e-02,\n",
      "         1.6919e-01,  7.9606e-02,  1.3096e-01,  1.0997e-01,  9.3153e-02,\n",
      "        -2.1021e-03,  1.4450e-01,  6.0856e-02, -1.0615e-02,  2.4149e-02,\n",
      "        -4.2761e-02,  2.6228e-02, -1.1105e-01,  1.4887e-01, -1.2350e-02,\n",
      "         1.2003e-01,  2.3322e-01,  4.1488e-02, -1.1564e-02, -3.2250e-02,\n",
      "         1.1118e-01, -1.0945e-01, -7.6545e-03, -6.3192e-02,  1.7922e-01,\n",
      "         1.8046e-01,  7.7163e-02, -3.8353e-02,  2.1646e-01,  1.4877e-01,\n",
      "         1.8486e-01,  1.6874e-01, -1.0730e-01,  1.0086e-01, -8.8698e-02,\n",
      "         5.3817e-03,  9.7886e-02, -6.4851e-02,  2.6151e-01,  4.5403e-02,\n",
      "        -2.0648e-02,  2.1013e-01, -3.7825e-02, -5.4148e-02,  1.6665e-02,\n",
      "         3.7667e-02, -1.2040e-02,  3.7119e-02,  8.5423e-02,  1.4835e-01,\n",
      "         1.8971e-01, -3.2630e-02,  9.5849e-02,  2.5131e-01,  4.1384e-02,\n",
      "        -8.5663e-02,  1.5133e-01, -3.2198e-03,  5.7627e-02,  2.5137e-01,\n",
      "         8.8389e-02, -9.0132e-02,  6.6890e-02,  3.0833e-02,  1.2972e-01,\n",
      "         2.1958e-01,  1.2693e-01,  1.7336e-01, -1.3838e-01,  1.7328e-01,\n",
      "        -9.3433e-02, -8.9950e-03,  8.1280e-02,  2.0431e-01,  1.2210e-01,\n",
      "        -4.3468e-03, -3.9147e-02,  3.5477e-02,  7.5938e-02,  1.8368e-01,\n",
      "         2.0307e-01,  8.4911e-02, -6.0621e-02,  4.1936e-03, -6.6686e-02,\n",
      "         4.7311e-02,  2.0628e-02,  1.4621e-01,  1.6897e-01,  1.6376e-01,\n",
      "         1.4580e-01,  5.2465e-02, -4.7107e-02,  6.4156e-05,  1.0928e-01,\n",
      "         2.1441e-01,  1.9604e-01,  1.2788e-01, -1.1484e-01,  9.2929e-02])\n",
      "tensor([0.8370, 1.0221, 0.9643, 0.9487, 0.9680, 0.9880, 1.0300, 0.9946, 1.0491,\n",
      "        1.0613, 0.9966, 0.9778, 1.0537, 1.0010, 1.0088, 0.9859, 1.0182, 0.9610,\n",
      "        0.9706, 0.9879, 0.9871, 0.8480, 0.9949, 0.9483, 0.9744, 0.9835, 1.0292,\n",
      "        1.0012, 0.9129, 0.9844, 1.0035, 0.9612, 0.9120, 1.0149, 0.9712, 1.0041,\n",
      "        0.9834, 0.9493, 1.0498, 1.0433, 1.0328, 1.0107, 0.9573, 0.9878, 0.9903,\n",
      "        0.9961, 0.9495, 0.9479, 1.0765, 1.0338, 0.8552, 0.9552, 0.8884, 1.0010,\n",
      "        1.0111, 1.0366, 0.9958, 0.9753, 0.9013, 0.9925, 0.9529, 0.9425, 1.0064,\n",
      "        0.9120, 0.9254, 0.9822, 0.8394, 0.9850, 0.9944, 0.9843, 0.9361, 0.9779,\n",
      "        1.0303, 0.9631, 1.0454, 0.9669, 0.9724, 1.0070, 0.9705, 0.9247, 0.9423,\n",
      "        0.9700, 0.9599, 0.9972, 1.0038, 0.9821, 1.0182, 0.9316, 1.0535, 1.0187,\n",
      "        0.9785, 0.9937, 1.0310, 0.9272, 0.9466, 0.9002, 0.9571, 1.0134, 1.0091,\n",
      "        0.9566, 0.8773, 0.9298, 0.9477, 1.0468, 0.9303, 1.0157, 0.9693, 0.9573,\n",
      "        0.9196, 1.0130, 0.9548, 0.9565, 0.9654, 1.0298, 0.9356, 1.0103, 1.0068,\n",
      "        0.9468, 0.9871, 0.9526, 0.9386, 0.9482, 1.0074, 0.9868, 0.9948, 1.0155,\n",
      "        0.9523, 0.8772, 0.9884, 0.9778, 0.8733, 1.0624, 0.8389, 0.8882, 1.0002,\n",
      "        0.9853, 0.8586, 1.0254, 0.9911, 0.9320, 1.0054, 0.9765, 1.0595, 0.9842,\n",
      "        1.0129, 1.0574, 1.0000, 0.9989, 0.9136, 0.9450, 0.9892, 0.9939, 0.9437,\n",
      "        0.9699, 1.0203, 1.0223, 0.9352, 0.8951, 0.9947, 0.9286, 0.9288, 1.0160,\n",
      "        0.9936, 0.9106, 1.0284, 0.9070, 0.9832, 0.9564, 0.9573, 0.9880, 0.9783,\n",
      "        0.9561, 0.9438, 0.9883, 0.9353, 0.9880, 0.9083, 0.9769, 0.9582, 0.9529,\n",
      "        1.0698, 0.8368, 0.9761, 1.0341, 0.9798, 0.9696, 0.9683, 0.9386, 0.9465,\n",
      "        1.0129, 0.9554, 0.9825, 1.0318, 0.9247, 1.0344, 0.9937, 0.9700, 0.9778,\n",
      "        1.0553, 0.9864, 0.9859, 1.0122, 0.9990, 1.0022, 0.9684, 1.0514, 1.0284,\n",
      "        0.9689, 0.9975, 0.9975, 0.9609, 0.9656, 0.9820, 1.0384, 0.9788, 1.0250,\n",
      "        0.8833, 1.0267, 1.0498, 1.0383, 0.9076, 1.0305, 0.8974, 0.9997, 0.9006,\n",
      "        1.0337, 0.9680, 0.9258, 0.9348, 1.0909, 0.9921, 0.9691, 0.9979, 1.0362,\n",
      "        1.0264, 1.0361, 0.9368, 1.0192, 0.9917, 0.9943, 0.9959, 0.9517, 0.9784,\n",
      "        0.9831, 1.0035, 0.9686, 1.0070, 0.9539, 0.9805, 0.9559, 0.9421, 1.0040,\n",
      "        0.9912, 1.0161, 0.9428, 0.9832, 1.0089, 0.9156, 0.9662, 0.9420, 0.8753,\n",
      "        0.9870, 0.9779, 1.0154, 1.0317, 1.0312, 1.0223, 0.9640, 1.0440, 0.9761,\n",
      "        0.9412, 0.9771, 1.0514, 0.9157, 0.9665, 0.9501, 0.9356, 1.0660, 1.0329,\n",
      "        0.9822, 1.0183, 0.9916, 0.9992, 0.9586, 0.9702, 0.9340, 0.9576, 0.9409,\n",
      "        1.0127, 0.9395, 0.9584, 1.0333, 0.9963, 0.9300, 0.9801, 1.0262, 1.0208,\n",
      "        0.9722, 0.9517, 0.9605, 1.0541, 0.9659, 1.0307, 1.0707, 0.9970, 1.0347,\n",
      "        1.0109, 1.0004, 0.9633, 1.0177, 0.9524, 0.9063, 0.9568, 0.9308, 0.9855,\n",
      "        1.0098, 0.9938, 1.0740, 0.8708, 1.0050, 0.9169, 1.0769, 0.9522, 0.9060,\n",
      "        0.9898, 0.9358, 0.8765, 1.0051, 1.0079, 0.9404, 1.0156, 0.9816, 1.0031,\n",
      "        1.0247, 0.9829, 1.0289, 0.9706, 0.9765, 1.0295, 0.9969, 1.0025, 0.9270,\n",
      "        0.9036, 1.0194, 0.8235, 1.0202, 1.0172, 0.9887, 0.9781, 1.0222])\n",
      "tensor([-8.1144e-03, -1.1477e-01,  6.5374e-02,  7.1216e-02,  1.6664e-01,\n",
      "        -1.9901e-02, -5.4041e-02,  1.2363e-01, -2.1770e-03,  1.3605e-01,\n",
      "        -6.4105e-02, -5.7969e-02, -1.2041e-01, -1.2723e-01, -1.4060e-01,\n",
      "        -1.1969e-01,  2.2015e-01, -1.3179e-03, -7.3500e-02,  8.6607e-02,\n",
      "        -1.4935e-02,  1.0440e-01,  8.7954e-03,  1.3460e-02,  1.0356e-02,\n",
      "        -4.8980e-02,  6.7917e-02, -6.1129e-02, -2.0193e-02,  1.1643e-01,\n",
      "        -4.1386e-02, -8.3423e-02,  2.2341e-03,  1.7840e-01,  5.0502e-02,\n",
      "        -4.3699e-02, -1.3264e-01, -4.2652e-02, -8.1347e-02, -6.8727e-02,\n",
      "        -1.6505e-02,  1.8421e-03,  4.6652e-02,  4.0345e-02,  2.5943e-02,\n",
      "        -5.1128e-02,  1.4282e-01,  3.2128e-02, -2.1327e-02, -1.3249e-01,\n",
      "        -1.7046e-02,  9.7437e-02,  1.1447e-01,  5.9549e-02,  1.6295e-01,\n",
      "         3.8790e-02,  8.5921e-02,  6.9810e-02,  6.6768e-02, -3.2995e-02,\n",
      "         9.4957e-02,  8.5988e-03, -6.4181e-02, -4.5814e-02, -8.1845e-03,\n",
      "        -1.3149e-01, -2.7017e-02,  4.3474e-02, -1.0657e-01, -1.0830e-02,\n",
      "         1.1168e-01, -1.8270e-02,  1.8009e-01, -1.2698e-02, -2.2515e-02,\n",
      "         5.3589e-02, -7.4970e-02,  1.9552e-02,  5.2082e-02,  2.2937e-03,\n",
      "        -3.2120e-02,  1.7009e-01,  6.4640e-02, -1.4250e-01, -4.7977e-02,\n",
      "         9.5050e-02, -4.1076e-02,  4.3931e-02,  4.1493e-02, -7.0248e-04,\n",
      "         1.0054e-01,  3.7532e-02, -6.1912e-03,  3.3785e-02,  3.4195e-02,\n",
      "         1.0011e-01, -9.3150e-04, -1.9278e-01, -9.2620e-02, -6.6880e-02,\n",
      "        -3.4881e-02,  7.4292e-02,  9.4718e-02, -4.3925e-03, -1.1759e-02,\n",
      "        -9.3252e-02,  2.4573e-02,  1.1865e-03, -7.2230e-02, -7.3656e-02,\n",
      "         1.5216e-01,  9.8712e-02,  2.9949e-02,  6.4871e-03, -6.1234e-02,\n",
      "        -6.3716e-04, -1.8832e-01,  1.6440e-01, -3.4098e-02, -7.9614e-02,\n",
      "        -3.2163e-02, -2.8197e-02, -5.4475e-02,  7.7480e-02,  1.3228e-01,\n",
      "        -4.7472e-02,  8.4632e-02,  2.5547e-02, -3.4490e-02, -8.1728e-02,\n",
      "         9.1207e-02,  5.8275e-03,  1.2863e-01, -1.3331e-02, -3.6641e-02,\n",
      "        -5.8843e-02,  5.8967e-02, -1.0698e-01,  1.3907e-01, -1.3259e-02,\n",
      "        -8.7982e-02, -7.4879e-02,  6.6379e-02, -2.1053e-02,  1.4107e-01,\n",
      "        -1.0160e-01, -1.1111e-01, -4.2767e-02, -1.4831e-01,  5.4159e-02,\n",
      "        -1.4913e-01, -2.3089e-02,  1.5194e-01,  1.5491e-02, -1.4185e-01,\n",
      "         5.8096e-02,  2.0699e-03,  3.6854e-02,  5.0500e-02, -4.6747e-02,\n",
      "        -5.9488e-03,  1.0648e-01, -1.2815e-01, -8.4211e-04,  3.8928e-02,\n",
      "        -2.0866e-02, -8.0412e-03,  5.5991e-02,  9.1247e-02, -1.3911e-01,\n",
      "         1.3721e-01, -5.6909e-02, -9.6634e-02, -1.4635e-01,  7.3759e-02,\n",
      "         1.4711e-01, -5.7070e-02, -5.9861e-02,  2.1037e-02, -6.1670e-02,\n",
      "        -4.8901e-02, -1.9954e-03,  1.9451e-01, -1.3784e-01,  1.2642e-01,\n",
      "        -1.4795e-01,  9.9879e-02,  3.6984e-02, -3.1923e-02, -1.3582e-01,\n",
      "         5.4003e-02, -7.6928e-03, -2.9329e-03,  1.9424e-01, -2.5528e-04,\n",
      "         7.3750e-02,  4.3357e-02,  6.3562e-02, -1.4393e-01, -5.7903e-02,\n",
      "         3.2249e-02, -1.8477e-02, -1.0983e-01,  1.7166e-01,  1.0049e-01,\n",
      "        -9.0256e-02,  1.1341e-01,  4.2288e-02, -1.0401e-03, -1.6779e-01,\n",
      "         8.2802e-02, -5.3941e-02,  1.8284e-03, -5.8344e-04,  8.3532e-02,\n",
      "        -1.7525e-02, -2.3756e-02, -8.2716e-02,  6.0895e-02, -1.6640e-01,\n",
      "         3.3623e-02,  6.4700e-02,  5.0722e-02,  8.0817e-02,  2.2480e-01,\n",
      "        -1.4670e-01, -1.3324e-01,  1.6592e-02,  4.8687e-02, -5.3927e-02,\n",
      "        -1.4498e-01, -8.2600e-02, -1.4261e-01, -1.1942e-02, -1.3534e-02,\n",
      "         3.3677e-03,  3.1670e-02,  2.5212e-02, -4.5288e-02, -2.2336e-02,\n",
      "        -1.0292e-01, -4.2005e-02,  5.0229e-02, -6.1796e-02,  1.3958e-01,\n",
      "        -1.1141e-02, -6.2033e-03,  1.4374e-01, -2.1762e-02, -1.0884e-01,\n",
      "        -5.9575e-02, -5.5757e-02, -1.5575e-01, -5.6297e-02,  2.5913e-01,\n",
      "        -7.1701e-02,  2.5687e-03,  5.9178e-02, -1.1603e-01, -3.7085e-02,\n",
      "        -1.0919e-02, -1.8227e-01, -1.3923e-01,  1.5664e-01, -2.7265e-02,\n",
      "         1.3327e-01, -5.2999e-02,  8.1670e-02, -9.2690e-02, -5.9331e-02,\n",
      "         1.6527e-02, -1.0694e-01, -1.0637e-02,  4.3608e-02,  1.3251e-01,\n",
      "         1.0316e-01,  2.2908e-02, -7.3523e-02,  5.8124e-02,  2.8313e-02,\n",
      "        -6.1745e-02,  9.9176e-02, -1.7280e-02,  4.8946e-02, -5.0848e-02,\n",
      "         1.1215e-01,  3.3671e-03, -1.1047e-01,  1.3350e-01,  7.9876e-02,\n",
      "        -3.9340e-02, -4.9876e-02, -6.2516e-02, -1.2915e-02,  5.9618e-02,\n",
      "         1.4237e-02, -4.4857e-02, -9.1821e-02,  2.4345e-02,  7.4726e-02,\n",
      "         1.2336e-01,  1.0910e-01,  2.3629e-01,  6.1586e-02,  6.3471e-02,\n",
      "        -1.6998e-01,  1.8545e-02,  4.5022e-02,  1.8300e-01,  7.2371e-02,\n",
      "         1.3217e-01,  8.1778e-03,  1.0253e-02, -2.4598e-02, -4.7790e-02,\n",
      "        -1.2665e-01, -6.5223e-03, -2.6463e-02,  3.2004e-02, -6.0030e-02,\n",
      "        -5.6529e-02, -6.7958e-02,  6.9051e-02, -5.2029e-02, -2.0657e-02,\n",
      "        -8.7977e-02,  4.0982e-02, -2.0531e-03, -1.2463e-01,  3.6412e-02,\n",
      "        -8.6987e-03,  4.6096e-02,  7.2024e-02, -2.1125e-01,  8.7709e-02,\n",
      "         3.5188e-02,  4.7298e-02,  2.2774e-02, -3.1049e-02,  1.9253e-02,\n",
      "        -3.1815e-02,  9.9070e-03,  3.2314e-02, -9.8031e-02,  4.4015e-02,\n",
      "        -3.9436e-02, -5.3085e-02, -1.6634e-01,  1.6482e-02,  1.0628e-01])\n",
      "tensor([[ 0.0045,  0.0479, -0.0243,  ...,  0.0017,  0.0420,  0.0303],\n",
      "        [-0.0112,  0.0099, -0.0159,  ..., -0.0154, -0.0031, -0.0407],\n",
      "        [ 0.0504, -0.0389, -0.0315,  ...,  0.0269,  0.0071, -0.0259],\n",
      "        ...,\n",
      "        [ 0.0177, -0.0305, -0.0268,  ...,  0.0076,  0.0512,  0.0038],\n",
      "        [ 0.0047,  0.0543, -0.0521,  ...,  0.0069, -0.0438, -0.0285],\n",
      "        [ 0.0054,  0.0565,  0.0435,  ..., -0.0210, -0.0063, -0.0416]])\n",
      "tensor([-0.0304, -0.0259,  0.0006, -0.0254,  0.0673,  0.0076, -0.0397, -0.0381,\n",
      "        -0.0411, -0.0570, -0.0126, -0.0166, -0.0312, -0.0345,  0.0069,  0.0278,\n",
      "        -0.0293,  0.0541,  0.1054,  0.0705,  0.0199, -0.0928,  0.0654, -0.0828,\n",
      "        -0.0079, -0.0337,  0.0607, -0.0004, -0.0502,  0.1061, -0.0520,  0.0047,\n",
      "         0.0089, -0.1322,  0.0213, -0.0380,  0.0107, -0.0041,  0.0212,  0.0564,\n",
      "        -0.0140, -0.0122, -0.0468, -0.0085,  0.0847, -0.0289, -0.0222, -0.0288,\n",
      "         0.0344,  0.0006, -0.0755, -0.1015,  0.0648, -0.0095, -0.0099, -0.0080,\n",
      "         0.0709,  0.0134, -0.0177, -0.0768, -0.0337, -0.0227,  0.0568,  0.0330,\n",
      "        -0.0115, -0.0334, -0.1446,  0.0177,  0.0119, -0.1012,  0.0380, -0.0203,\n",
      "        -0.0245, -0.0384, -0.0106, -0.0129,  0.0492, -0.0038,  0.0255,  0.0551,\n",
      "         0.0090, -0.0741,  0.0491, -0.0539, -0.0135, -0.0647, -0.0437, -0.0484,\n",
      "        -0.1294, -0.0757,  0.0880, -0.0575, -0.0074,  0.0407,  0.0468, -0.1713,\n",
      "        -0.0152,  0.0428,  0.0776, -0.0802,  0.0363,  0.0770, -0.0689, -0.0245,\n",
      "         0.0263, -0.0031,  0.0788,  0.0051,  0.0207, -0.0369, -0.0171, -0.0236,\n",
      "        -0.1416, -0.0010, -0.0200, -0.0266, -0.0494, -0.0361,  0.0471, -0.0189,\n",
      "        -0.0166, -0.0037, -0.0313, -0.0034,  0.0139, -0.0441,  0.0146, -0.0140,\n",
      "         0.0315,  0.0461, -0.1807,  0.0696,  0.0514, -0.0909, -0.0622, -0.0036,\n",
      "         0.0152,  0.0432, -0.0044, -0.0188,  0.0016, -0.0570,  0.1194,  0.0533,\n",
      "        -0.0242, -0.0587,  0.0413, -0.0508,  0.0362,  0.0680,  0.0425,  0.0300,\n",
      "        -0.0651, -0.0587,  0.0446, -0.0082, -0.0307,  0.0430,  0.0897, -0.0247,\n",
      "        -0.0077, -0.0530,  0.0051, -0.0199, -0.0196,  0.0013, -0.0278,  0.0409,\n",
      "         0.0101, -0.0332,  0.0453, -0.0542, -0.0219, -0.0277, -0.0403,  0.0150,\n",
      "        -0.0187, -0.0579, -0.0675,  0.0321,  0.0252,  0.0177, -0.0123,  0.0534,\n",
      "        -0.0099,  0.0033,  0.0732,  0.0472, -0.0346, -0.0547,  0.0227, -0.0831,\n",
      "         0.0696, -0.0329,  0.0606,  0.0225, -0.1579,  0.0851,  0.0176, -0.0045,\n",
      "        -0.0773,  0.1072,  0.0544,  0.0157, -0.1035,  0.0164,  0.0237, -0.0281,\n",
      "        -0.0105,  0.0016, -0.0124,  0.0711,  0.0130, -0.1629,  0.0423, -0.0298,\n",
      "        -0.0368, -0.0352,  0.0448,  0.0215,  0.0625, -0.1029,  0.0150,  0.0519,\n",
      "        -0.0206,  0.0098,  0.0134,  0.0355, -0.0206,  0.1163, -0.0632,  0.0625,\n",
      "         0.0359, -0.1378, -0.0287, -0.0469,  0.0124, -0.1485, -0.0760,  0.0556,\n",
      "         0.0716, -0.1722,  0.0267,  0.0544,  0.0016, -0.0019, -0.1193,  0.0520,\n",
      "        -0.0083, -0.0526,  0.0921, -0.0146,  0.0465, -0.1075, -0.0407, -0.0494,\n",
      "        -0.0080,  0.1083, -0.0139,  0.0290,  0.0757, -0.0482, -0.0794, -0.0409,\n",
      "        -0.0278,  0.1029, -0.0326, -0.0264, -0.0257, -0.0650, -0.0816, -0.0036,\n",
      "        -0.0316,  0.0143, -0.0356,  0.0149, -0.0155,  0.0578,  0.0381,  0.0025,\n",
      "         0.0231, -0.0380,  0.0594,  0.0226, -0.0200,  0.0804,  0.0841, -0.0577,\n",
      "         0.0182, -0.0517, -0.1953, -0.0243, -0.0247, -0.0274,  0.0359, -0.0899,\n",
      "         0.0156,  0.0212, -0.0534, -0.0315,  0.0624, -0.0055, -0.0245,  0.0303,\n",
      "        -0.0021, -0.0571, -0.0627,  0.0491,  0.1401,  0.1133, -0.0540, -0.0852,\n",
      "        -0.0402,  0.0331,  0.0440,  0.0845,  0.0642, -0.0145, -0.0297, -0.2073,\n",
      "        -0.1083,  0.0754, -0.0226,  0.0336, -0.0243, -0.1141, -0.0795,  0.0166,\n",
      "        -0.0037, -0.0360,  0.0333,  0.0233,  0.0533, -0.0403, -0.1543,  0.0134,\n",
      "         0.0880, -0.0506,  0.0095,  0.0206,  0.0316, -0.0759, -0.0027, -0.0014,\n",
      "         0.0044,  0.0700, -0.0142, -0.0792,  0.0037,  0.0897])\n",
      "tensor([0.9773, 1.0180, 0.9657, 0.9664, 0.9859, 0.9889, 1.0059, 0.9501, 1.0717,\n",
      "        1.0873, 1.0021, 0.9841, 0.9734, 0.9902, 0.9558, 1.0329, 1.0050, 1.0458,\n",
      "        0.9666, 0.9482, 0.9712, 1.1008, 0.9727, 1.0203, 0.9933, 0.9694, 0.9899,\n",
      "        1.0129, 1.0232, 1.0106, 0.9734, 0.9549, 1.0045, 1.0593, 0.9628, 0.9459,\n",
      "        0.9828, 0.9327, 1.0186, 1.0187, 0.9979, 0.9824, 1.0711, 1.0706, 0.9905,\n",
      "        0.9849, 0.9892, 0.9855, 0.9690, 0.9776, 1.0665, 0.9744, 1.0045, 0.9832,\n",
      "        0.9740, 0.9855, 0.9908, 1.0356, 0.9873, 1.0128, 0.9729, 1.0878, 0.9827,\n",
      "        0.9961, 0.9564, 0.9539, 1.0524, 1.0281, 1.0108, 1.0142, 0.9549, 0.9774,\n",
      "        1.0707, 0.9920, 0.9589, 0.9830, 1.0264, 0.9766, 0.9733, 0.9637, 1.0279,\n",
      "        1.0677, 0.9805, 1.0235, 0.9811, 1.0182, 0.9820, 1.0378, 1.1031, 1.0191,\n",
      "        0.9674, 1.1976, 0.9978, 0.9481, 1.0194, 1.0284, 0.9988, 0.9349, 0.9895,\n",
      "        1.0412, 0.9863, 0.9674, 1.0434, 0.9988, 0.9690, 0.9823, 1.0015, 0.9412,\n",
      "        1.0693, 1.0450, 1.0368, 1.0766, 1.0858, 0.9559, 0.9082, 0.9808, 1.0013,\n",
      "        0.9810, 0.9779, 0.9855, 0.9828, 1.1215, 0.9613, 1.0408, 1.1043, 0.9883,\n",
      "        1.0107, 0.9640, 0.9663, 0.9802, 1.0189, 0.9814, 0.9523, 1.0413, 0.9916,\n",
      "        0.9903, 1.0234, 0.9914, 0.9556, 0.9892, 1.1197, 1.0289, 0.9727, 0.9704,\n",
      "        0.9506, 0.9940, 1.0009, 0.9988, 0.9642, 1.0177, 0.9516, 0.9872, 1.0066,\n",
      "        0.9606, 0.9954, 0.9653, 0.9158, 1.1389, 0.9857, 0.9738, 0.9574, 0.9989,\n",
      "        1.0022, 1.0289, 1.0337, 1.0204, 0.9695, 0.9774, 0.9992, 0.9685, 1.0588,\n",
      "        1.0037, 0.9411, 1.0685, 1.0302, 1.0028, 0.9778, 1.0043, 0.9845, 0.9650,\n",
      "        0.9603, 1.0447, 1.0437, 1.0170, 0.9762, 0.9627, 0.9966, 0.9692, 1.0182,\n",
      "        0.9715, 0.9788, 0.9902, 1.0042, 0.9594, 0.9882, 1.0183, 0.9993, 0.9718,\n",
      "        0.9785, 0.9869, 1.0187, 0.9740, 1.0059, 0.9994, 1.0186, 0.9648, 0.9907,\n",
      "        0.9893, 0.9657, 0.9990, 0.9323, 0.9639, 1.0256, 1.0827, 1.0057, 1.0061,\n",
      "        0.9818, 0.9609, 0.9498, 1.0053, 0.9558, 1.0565, 0.9810, 0.9907, 0.9615,\n",
      "        0.9580, 1.0378, 0.9673, 0.9524, 0.9887, 1.0312, 0.9869, 0.9338, 1.1124,\n",
      "        0.9954, 0.9752, 1.0069, 1.0399, 0.9764, 0.9713, 1.0072, 1.0681, 1.0491,\n",
      "        1.0006, 0.9871, 0.9931, 0.9556, 1.0069, 0.9974, 1.0354, 1.0055, 1.0101,\n",
      "        0.9754, 1.0314, 1.0016, 0.9906, 0.9716, 0.9668, 1.0110, 0.9092, 0.9737,\n",
      "        0.9893, 1.0221, 0.9898, 1.0187, 0.9942, 1.0067, 0.9789, 0.9605, 0.9899,\n",
      "        1.0691, 0.9645, 1.0108, 0.9953, 0.9998, 0.9396, 0.9452, 0.9772, 0.9839,\n",
      "        0.9492, 1.0158, 0.9458, 0.9996, 0.9598, 0.9885, 0.9549, 1.0054, 1.0739,\n",
      "        0.9731, 1.0789, 1.1329, 1.0086, 1.0475, 1.0289, 1.0332, 1.0420, 0.9626,\n",
      "        0.9424, 1.0043, 0.9748, 0.9579, 1.0684, 1.0307, 0.9588, 0.9748, 1.0065,\n",
      "        0.9850, 0.9859, 1.0342, 0.9817, 0.9908, 0.9846, 0.9733, 0.9588, 0.9485,\n",
      "        1.0645, 1.0141, 1.0070, 1.1046, 1.1741, 1.0385, 0.9683, 1.0114, 0.9335,\n",
      "        0.9741, 1.1568, 1.1207, 1.0231, 1.0103, 0.9802, 0.9571, 0.9520, 0.9869,\n",
      "        0.9984, 1.0452, 1.0117, 0.9749, 1.0114, 0.9913, 0.9860, 0.9722, 0.9980,\n",
      "        0.9742, 0.9794, 0.9306, 0.9517, 0.9469, 1.0021, 0.9977, 0.9707])\n",
      "tensor([-0.0981, -0.1447,  0.0858,  0.0857,  0.0792,  0.0315,  0.0168, -0.0729,\n",
      "         0.0654,  0.0756,  0.0943, -0.0968, -0.1231, -0.1121,  0.0367, -0.1001,\n",
      "        -0.0402, -0.1235, -0.0498, -0.0215,  0.0777,  0.0414,  0.0203,  0.0377,\n",
      "        -0.1197, -0.0743,  0.1209, -0.0150, -0.0553,  0.0143, -0.0307, -0.0098,\n",
      "         0.0031, -0.1200, -0.0469,  0.0359, -0.0814,  0.0627, -0.0880, -0.0736,\n",
      "        -0.0101, -0.0163,  0.0216,  0.0166,  0.0747,  0.0865,  0.1388,  0.0492,\n",
      "         0.0340,  0.1141, -0.0075,  0.0336,  0.0557,  0.0406, -0.1004, -0.0900,\n",
      "         0.0335,  0.0148, -0.0056, -0.0401, -0.0708,  0.0655, -0.0283, -0.0146,\n",
      "        -0.0911,  0.0502, -0.0892,  0.1004,  0.0131, -0.1103,  0.1092, -0.0285,\n",
      "         0.0258,  0.0412, -0.0049,  0.0454, -0.0017,  0.1613,  0.0095,  0.1778,\n",
      "         0.0621, -0.0085, -0.0204, -0.0685, -0.0605, -0.0300, -0.0391,  0.1249,\n",
      "        -0.0720,  0.0903,  0.0437, -0.0250,  0.0491, -0.0487,  0.0571, -0.0659,\n",
      "         0.0130, -0.0342,  0.0984,  0.0754,  0.0509,  0.1435, -0.0024, -0.0186,\n",
      "         0.0188,  0.0220, -0.0937, -0.0261,  0.0018, -0.0124,  0.0376, -0.0957,\n",
      "        -0.1117,  0.0366, -0.0006, -0.0930,  0.0086, -0.0678,  0.1132,  0.0272,\n",
      "         0.0050, -0.0173, -0.1866,  0.0477, -0.0935, -0.0240,  0.0280, -0.1194,\n",
      "        -0.0565, -0.0578, -0.0674, -0.0688, -0.0259,  0.0385, -0.0942,  0.0284,\n",
      "        -0.0206,  0.1064,  0.0614, -0.0139, -0.0218, -0.1134,  0.0802,  0.0083,\n",
      "         0.0271, -0.1470, -0.0353, -0.0268,  0.0242,  0.0214,  0.0421,  0.1293,\n",
      "        -0.0404, -0.0633, -0.0012, -0.0991,  0.0874, -0.0036,  0.0077, -0.0232,\n",
      "        -0.0234, -0.0165,  0.1568, -0.0514,  0.0205, -0.0121, -0.0654, -0.0052,\n",
      "        -0.0491, -0.0748,  0.0240,  0.0500, -0.0049,  0.0648,  0.0817, -0.0026,\n",
      "         0.1139, -0.1146,  0.0170, -0.0393,  0.0365, -0.0167,  0.0531, -0.0074,\n",
      "        -0.0772,  0.0809,  0.0567,  0.0528, -0.0310,  0.0361,  0.1002, -0.1192,\n",
      "        -0.0128,  0.0864,  0.0826, -0.0320,  0.0506, -0.0253, -0.0090, -0.0159,\n",
      "         0.0648, -0.0650, -0.0275, -0.0258, -0.0523, -0.0038, -0.0532,  0.0810,\n",
      "         0.0442, -0.0270,  0.0093,  0.0249,  0.1049,  0.0393, -0.0028, -0.0350,\n",
      "        -0.0285,  0.0526,  0.0231, -0.0878,  0.0239,  0.0573,  0.0138,  0.0188,\n",
      "         0.0466,  0.0260, -0.0127, -0.0833,  0.1594,  0.0759, -0.0604,  0.0546,\n",
      "        -0.0071, -0.0754,  0.0630, -0.0357, -0.0226, -0.2310,  0.0098,  0.0182,\n",
      "        -0.0724,  0.0173,  0.1451, -0.0251,  0.0810,  0.0097, -0.1290, -0.0591,\n",
      "        -0.0549,  0.0751, -0.0862, -0.0156, -0.0534, -0.1199, -0.0837, -0.1032,\n",
      "         0.0306, -0.0107, -0.0436,  0.0070,  0.0044, -0.0763, -0.0839,  0.0097,\n",
      "        -0.0509, -0.0051, -0.0104, -0.0435,  0.1566, -0.1259,  0.1162,  0.0427,\n",
      "        -0.0356,  0.0227,  0.0845, -0.0675, -0.0741,  0.0307,  0.0206,  0.0084,\n",
      "         0.0643,  0.0195,  0.0107, -0.0696,  0.0410,  0.0042,  0.0275, -0.0082,\n",
      "         0.0689,  0.0087,  0.1199,  0.0123,  0.0680,  0.0649,  0.0720, -0.0175,\n",
      "         0.0771, -0.0680, -0.0118, -0.0700,  0.0816, -0.0166,  0.0826,  0.1277,\n",
      "         0.0515, -0.0849,  0.0226,  0.0588,  0.0013, -0.1070, -0.0513, -0.0154,\n",
      "        -0.0392,  0.0598,  0.0029,  0.0473, -0.0660, -0.0715,  0.0610,  0.0209,\n",
      "         0.0140, -0.0459, -0.0520,  0.0216,  0.0028, -0.0537, -0.0282,  0.0597,\n",
      "        -0.0679, -0.1188,  0.0381,  0.0701, -0.0548, -0.0148,  0.0219, -0.0056,\n",
      "         0.0664,  0.0441, -0.0045,  0.0527, -0.0439,  0.0737,  0.1422,  0.0320,\n",
      "         0.0540,  0.0893,  0.0403, -0.0629,  0.0502,  0.0888])\n",
      "tensor([[ 0.0206,  0.0586, -0.0106,  ..., -0.0037,  0.0276, -0.0502],\n",
      "        [ 0.0392, -0.0088,  0.0330,  ...,  0.0184, -0.0493,  0.0568],\n",
      "        [-0.0090, -0.0361, -0.0015,  ...,  0.0018, -0.0065,  0.0037],\n",
      "        ...,\n",
      "        [-0.0088,  0.0149,  0.0138,  ..., -0.0069,  0.0211,  0.0133],\n",
      "        [-0.0255,  0.0366, -0.0025,  ..., -0.0355, -0.0220,  0.0022],\n",
      "        [ 0.0318,  0.0662,  0.0542,  ...,  0.0263,  0.0183, -0.0109]])\n",
      "tensor([-0.0720, -0.0885,  0.0625, -0.0260,  0.0721, -0.0023, -0.0389,  0.0767,\n",
      "        -0.0258, -0.1227,  0.0468,  0.0079,  0.0777, -0.0407,  0.0593,  0.0539,\n",
      "         0.0205,  0.0290,  0.0422,  0.0405,  0.0446, -0.0914,  0.0093, -0.0645,\n",
      "        -0.0303,  0.0303,  0.0347,  0.0379,  0.0383,  0.0005, -0.0035,  0.0052,\n",
      "         0.0606, -0.0305, -0.0146,  0.0429,  0.0107, -0.0187,  0.0583,  0.0173,\n",
      "         0.0677, -0.0616,  0.0253, -0.0033,  0.0542,  0.0097,  0.0112,  0.0363,\n",
      "         0.0560, -0.0436,  0.1094, -0.0219,  0.0424,  0.0415,  0.0027,  0.0186,\n",
      "         0.0473,  0.0023, -0.0708,  0.0295, -0.0069,  0.0356,  0.0562, -0.0656,\n",
      "         0.0390,  0.0706, -0.1649,  0.0204,  0.0958, -0.0100,  0.0053,  0.0141,\n",
      "         0.0155, -0.0022, -0.0050,  0.1083,  0.0767,  0.0387,  0.0132,  0.0727,\n",
      "         0.1003,  0.0316,  0.0484,  0.0101,  0.0230, -0.0057,  0.0682,  0.0148,\n",
      "        -0.0892, -0.0353,  0.0422,  0.0310, -0.0262,  0.0361,  0.0023, -0.1123,\n",
      "        -0.1262, -0.0662, -0.0110, -0.0543,  0.0741,  0.0337,  0.0665,  0.0021,\n",
      "        -0.0411,  0.0445,  0.0647,  0.0333, -0.0271, -0.1226, -0.0169, -0.0555,\n",
      "         0.0055, -0.0108, -0.0274, -0.0330, -0.0263,  0.0460,  0.0926,  0.0037,\n",
      "        -0.0746, -0.0133,  0.0051, -0.0338, -0.0204,  0.0627,  0.0134, -0.0173,\n",
      "         0.0462,  0.0211, -0.1683, -0.0380, -0.0011, -0.1388, -0.0297,  0.0363,\n",
      "         0.0161,  0.0037,  0.0330, -0.0626,  0.0347,  0.0126,  0.0425,  0.0228,\n",
      "         0.0024,  0.0116,  0.0527,  0.0236, -0.0331,  0.0776,  0.0648,  0.0632,\n",
      "         0.0218,  0.0173,  0.0761,  0.0127,  0.0490,  0.0602,  0.0217,  0.0063,\n",
      "        -0.0204,  0.0963, -0.0152, -0.0132,  0.0216,  0.0017,  0.0326,  0.0787,\n",
      "         0.0386,  0.0067, -0.0080, -0.0184,  0.0623,  0.0043, -0.0607, -0.0383,\n",
      "        -0.0195, -0.1301, -0.0003,  0.0522,  0.0050,  0.0864,  0.0999,  0.0511,\n",
      "         0.0364, -0.0083,  0.0826,  0.0396,  0.0424,  0.0276, -0.0024, -0.0824,\n",
      "         0.0698,  0.0045,  0.0694,  0.0929, -0.1128,  0.0569, -0.0332,  0.0006,\n",
      "        -0.0434,  0.0202,  0.0575,  0.0257, -0.0956, -0.0036, -0.0017,  0.0144,\n",
      "        -0.0522, -0.0140,  0.0454,  0.0170,  0.0542, -0.1030,  0.0767, -0.0700,\n",
      "        -0.0798, -0.0221, -0.0162, -0.0140,  0.0166, -0.1165, -0.0437,  0.0488,\n",
      "         0.0213,  0.0554, -0.0431,  0.0579,  0.0465,  0.0321,  0.0322,  0.0257,\n",
      "        -0.0191, -0.0721,  0.0538, -0.0756,  0.0200,  0.0329, -0.0714,  0.0236,\n",
      "         0.0283, -0.1400, -0.0463,  0.0041,  0.0066, -0.0171, -0.0407,  0.0704,\n",
      "         0.0813,  0.0251,  0.0022,  0.0231,  0.0393, -0.0236,  0.0775, -0.0562,\n",
      "         0.0427, -0.0113,  0.0104,  0.0560, -0.0443, -0.0455, -0.0931, -0.0176,\n",
      "        -0.0755,  0.1408,  0.0372, -0.0503, -0.0040, -0.0410,  0.0679, -0.0018,\n",
      "         0.0277, -0.0299, -0.0869,  0.0617,  0.0132,  0.0652,  0.0316,  0.0191,\n",
      "         0.0769, -0.0066,  0.0884, -0.0073, -0.0923, -0.0182,  0.0424,  0.0037,\n",
      "        -0.0120,  0.0510, -0.0211, -0.0378,  0.1332, -0.0468,  0.0524,  0.0493,\n",
      "        -0.0325,  0.0047,  0.0095,  0.0044,  0.0990, -0.0559, -0.0379,  0.0937,\n",
      "         0.0312, -0.0159,  0.0361,  0.0325,  0.1214,  0.0470, -0.0960,  0.0297,\n",
      "         0.0068,  0.0501, -0.0023,  0.0809,  0.0420,  0.0040, -0.0105,  0.0095,\n",
      "        -0.0206, -0.0619, -0.0602,  0.0799, -0.0731, -0.0937, -0.1228, -0.0057,\n",
      "        -0.0017,  0.0359,  0.0469,  0.0217,  0.0471,  0.0480,  0.0280, -0.0177,\n",
      "         0.0769,  0.0271,  0.0708,  0.0151,  0.0058,  0.0381, -0.0405,  0.0244,\n",
      "         0.0114,  0.1012,  0.0775, -0.0390,  0.0427,  0.1021])\n",
      "tensor([0.9961, 0.9584, 0.9426, 0.9500, 0.9654, 0.9326, 0.9693, 0.9571, 1.0326,\n",
      "        1.0008, 0.9414, 0.9597, 0.9726, 0.9570, 0.9361, 0.9946, 0.9409, 0.9289,\n",
      "        0.9575, 0.9475, 0.9542, 1.0505, 0.9575, 0.9725, 0.9631, 0.9650, 0.9311,\n",
      "        0.9729, 0.9574, 0.9538, 0.9619, 0.9862, 0.9742, 0.9685, 0.9466, 0.9448,\n",
      "        0.9627, 1.0173, 0.9651, 0.9649, 0.9549, 0.9689, 1.0485, 1.0501, 0.9455,\n",
      "        0.9671, 0.9883, 0.9359, 0.9414, 1.0024, 0.9736, 0.9211, 0.9498, 0.9460,\n",
      "        0.9549, 0.9440, 0.9352, 0.9972, 0.9706, 0.9764, 0.9477, 0.9768, 0.9382,\n",
      "        1.0597, 0.9656, 0.9437, 1.0058, 0.9800, 0.9392, 0.9844, 0.9473, 0.9609,\n",
      "        0.9557, 0.9668, 0.9589, 0.9656, 0.9699, 0.9720, 0.9476, 0.9293, 0.9494,\n",
      "        0.9957, 0.9568, 0.9883, 0.9945, 0.9470, 0.9572, 1.0071, 1.0400, 0.9659,\n",
      "        0.9459, 1.0267, 0.9769, 0.9457, 0.9499, 1.0073, 1.0144, 0.9699, 0.9541,\n",
      "        1.0645, 0.9565, 0.9775, 0.9354, 0.9681, 1.0138, 0.9370, 0.9666, 0.9526,\n",
      "        1.0948, 1.1175, 1.0283, 1.0012, 1.0328, 0.9434, 0.9504, 0.9669, 0.9906,\n",
      "        0.9699, 0.9294, 0.9728, 0.9584, 1.0057, 0.9492, 0.9797, 1.0070, 0.9496,\n",
      "        0.9728, 0.9578, 0.9453, 0.9598, 1.0352, 0.9408, 0.9566, 0.9612, 0.9955,\n",
      "        0.9683, 0.9528, 0.9340, 0.9494, 1.0311, 1.0197, 0.9610, 0.9530, 0.9629,\n",
      "        0.9547, 1.0039, 0.9956, 0.9528, 0.9487, 0.9574, 0.9480, 0.9440, 0.9479,\n",
      "        0.9605, 0.9781, 0.9411, 0.9374, 1.0678, 0.9337, 0.9386, 0.9323, 0.9647,\n",
      "        0.9652, 0.9995, 0.9330, 0.9550, 0.9483, 0.9587, 0.9702, 0.9618, 0.9714,\n",
      "        1.0259, 0.9614, 1.0171, 0.9871, 0.9653, 0.9343, 0.9761, 0.9656, 0.9384,\n",
      "        0.9531, 1.0183, 0.9639, 0.9621, 0.9725, 0.9151, 0.9494, 0.9403, 0.9404,\n",
      "        0.9963, 0.9673, 0.9601, 0.9609, 0.9369, 0.9661, 0.9636, 1.0350, 0.9416,\n",
      "        0.9626, 0.9570, 0.9681, 0.9482, 0.9673, 0.9595, 0.9817, 1.0039, 0.9683,\n",
      "        0.9901, 0.9777, 0.9663, 0.9413, 0.9686, 0.9010, 1.0026, 0.9611, 0.9661,\n",
      "        0.9653, 0.9400, 0.9407, 0.9717, 0.9264, 0.9965, 0.9436, 0.9374, 0.9206,\n",
      "        0.9180, 0.9930, 0.9620, 0.9190, 0.9621, 0.9978, 0.9543, 0.9716, 1.0587,\n",
      "        0.9357, 1.0004, 0.9297, 0.9591, 0.9870, 0.9477, 0.9598, 1.0270, 1.0004,\n",
      "        1.0188, 0.9497, 0.9463, 0.9714, 1.0221, 0.9971, 0.9334, 0.9290, 0.9624,\n",
      "        0.9527, 0.9739, 0.9642, 0.9659, 0.9688, 0.8866, 0.9741, 0.9200, 0.9545,\n",
      "        0.9719, 1.0034, 0.9612, 0.9598, 0.9589, 0.9446, 0.9985, 0.9667, 0.9904,\n",
      "        0.9732, 0.9476, 0.9652, 0.9209, 0.9693, 0.9834, 0.9082, 0.9768, 0.9288,\n",
      "        0.9520, 0.9773, 0.9791, 0.9521, 0.9873, 0.9785, 0.9132, 0.9402, 0.9927,\n",
      "        0.9502, 1.0130, 0.9530, 0.9596, 0.9704, 0.9623, 0.9619, 0.9532, 0.9428,\n",
      "        0.9142, 0.9603, 0.9439, 0.9632, 0.9913, 0.9740, 0.9680, 0.9555, 0.9783,\n",
      "        0.9540, 0.9325, 0.9536, 0.9366, 0.9504, 0.9495, 0.9548, 0.9590, 1.0076,\n",
      "        0.9690, 0.9622, 0.9678, 0.9899, 0.9737, 0.9648, 0.9095, 0.9605, 0.9262,\n",
      "        1.0008, 1.0936, 1.0966, 0.9631, 1.0013, 0.9533, 0.9690, 0.9210, 0.9362,\n",
      "        0.9558, 0.9406, 1.0009, 0.9723, 0.9568, 0.9492, 0.9617, 0.9689, 0.9516,\n",
      "        0.9673, 0.9513, 0.9387, 0.9292, 0.9580, 0.9734, 0.9472, 0.9582])\n",
      "tensor([ 7.1255e-02, -1.1446e-01, -1.4952e-01, -4.9643e-02,  1.1153e-01,\n",
      "        -1.0721e-01,  1.6778e-01,  1.9671e-02,  1.3573e-01, -6.6729e-04,\n",
      "        -1.8647e-01,  1.6062e-01,  5.7368e-02,  1.0469e-01,  1.5631e-01,\n",
      "        -1.1661e-01, -1.1810e-01,  6.6659e-02, -2.0056e-02, -1.4072e-01,\n",
      "         5.6093e-02,  7.6967e-02,  8.6200e-02, -1.4530e-01, -1.3944e-01,\n",
      "        -7.2337e-02, -7.7419e-02, -6.2910e-02, -5.2494e-02,  1.3175e-02,\n",
      "         1.2075e-01, -1.0314e-01,  3.7735e-02, -2.0240e-02, -1.0003e-01,\n",
      "         1.5289e-01, -1.1382e-01, -4.4267e-03, -4.8755e-02,  3.1001e-02,\n",
      "        -1.0690e-01,  2.3532e-02, -1.5242e-01, -2.3668e-02, -1.1323e-01,\n",
      "        -2.4979e-03,  9.0862e-02, -1.3700e-01, -7.3049e-02,  1.0472e-01,\n",
      "        -3.1433e-02, -1.5827e-02,  1.3542e-01, -1.7294e-01,  1.3904e-01,\n",
      "        -1.4445e-01, -1.9507e-01,  1.5866e-01, -8.8607e-02,  1.6912e-01,\n",
      "         7.4109e-02,  1.4530e-01, -7.7246e-02,  9.5019e-02,  1.2795e-01,\n",
      "         2.7948e-02,  1.0764e-01, -9.3208e-02,  1.0894e-01, -1.0231e-02,\n",
      "        -1.7440e-01,  1.5083e-02, -1.6680e-01, -8.5034e-02, -3.8688e-02,\n",
      "        -7.4587e-02,  3.2319e-02, -4.4447e-02, -1.2976e-01, -1.0644e-01,\n",
      "        -4.1961e-02,  5.1871e-02,  1.1935e-01, -4.3845e-02, -4.0445e-02,\n",
      "        -9.8009e-03, -1.6025e-01,  5.4008e-02,  1.3999e-01,  9.2516e-02,\n",
      "         1.3982e-02, -4.9031e-02,  1.1082e-01,  9.0290e-02, -3.0474e-02,\n",
      "         1.1164e-01, -7.7280e-02,  7.0085e-02,  1.9428e-02,  5.5313e-02,\n",
      "         1.1106e-01, -1.1883e-02,  3.7335e-02, -7.9301e-02,  1.0811e-01,\n",
      "        -1.1222e-01,  1.5074e-01, -3.5795e-02,  2.7488e-02, -3.5600e-03,\n",
      "         8.4681e-02,  3.7762e-02, -9.7633e-03, -1.8905e-01, -7.1461e-02,\n",
      "         1.2992e-01, -1.3973e-01, -4.4679e-02, -1.0386e-01,  9.7433e-02,\n",
      "         7.8750e-02,  8.2856e-02, -1.1721e-01, -4.1345e-02,  9.3564e-02,\n",
      "        -9.2854e-02,  1.1935e-01, -1.1652e-01,  1.6174e-02,  1.2212e-01,\n",
      "         8.6132e-02, -1.3363e-01,  8.2934e-02, -9.2710e-02, -3.3534e-02,\n",
      "         8.3959e-02,  1.3878e-01, -1.5862e-01,  9.1869e-03,  5.9080e-02,\n",
      "         7.9278e-02,  1.7799e-01,  8.4116e-02, -1.3343e-01,  9.9810e-02,\n",
      "        -9.6545e-02,  4.7383e-02, -8.6331e-02,  7.1847e-03,  1.6491e-01,\n",
      "         1.4662e-01,  1.2823e-01, -2.2107e-02,  7.9926e-02, -1.3234e-01,\n",
      "         1.6597e-01,  1.3737e-01, -3.7415e-02,  1.4595e-01,  5.1886e-02,\n",
      "        -5.1679e-02,  8.8925e-02, -1.1383e-01, -2.4260e-02, -1.0519e-01,\n",
      "         1.7529e-01, -3.5633e-02, -3.8340e-02, -1.1919e-01,  3.9863e-02,\n",
      "        -7.1174e-02,  8.5152e-02, -6.4063e-02, -1.0907e-01,  1.3946e-02,\n",
      "        -7.5489e-02, -1.2254e-01,  1.6496e-02, -1.3728e-02, -1.0907e-01,\n",
      "        -1.2434e-01,  1.2942e-02,  3.7925e-02,  1.0458e-01,  1.1242e-02,\n",
      "        -4.5544e-02, -9.1723e-02, -6.7543e-02,  6.1198e-02,  1.5548e-01,\n",
      "        -1.1915e-01,  2.7597e-02, -1.8035e-01,  1.2251e-01,  1.4531e-01,\n",
      "         4.5695e-02,  1.2519e-01,  1.6632e-01, -9.0415e-02, -9.5918e-02,\n",
      "        -8.9977e-02, -1.7291e-02, -9.7382e-02, -1.4566e-02, -7.2189e-02,\n",
      "         1.8011e-03,  1.3028e-01,  4.5420e-02,  2.5145e-02, -9.2173e-02,\n",
      "        -5.8469e-02,  9.2367e-02, -1.1986e-01, -1.3530e-01, -1.3963e-01,\n",
      "         1.2209e-01,  5.5936e-04, -5.0808e-02, -7.4634e-02,  4.9620e-02,\n",
      "        -1.1516e-01,  1.3946e-01, -1.3667e-01,  1.4245e-02,  1.7096e-01,\n",
      "         6.4940e-03, -1.3598e-01, -1.3759e-01,  1.3729e-01,  4.7029e-02,\n",
      "        -6.9926e-02,  1.2544e-01, -5.5896e-02,  1.0131e-01, -1.4268e-01,\n",
      "        -6.6309e-02, -7.4353e-02,  1.7377e-01, -4.7109e-02, -1.3729e-02,\n",
      "        -8.0726e-03,  9.5052e-02, -1.4615e-01, -5.8619e-02, -1.2865e-01,\n",
      "        -1.2545e-01,  7.6196e-02, -1.9686e-02,  1.4760e-01,  8.0157e-02,\n",
      "         9.4014e-02, -4.0117e-02,  1.2042e-01,  1.1177e-01,  1.3027e-01,\n",
      "        -3.0185e-02, -1.2567e-01, -1.3577e-01,  1.5464e-01, -1.4710e-01,\n",
      "         4.3994e-02, -9.9803e-02, -4.5403e-02, -1.1377e-01, -8.6547e-03,\n",
      "        -2.3723e-02,  1.1733e-01, -1.7608e-01, -9.1242e-02,  1.0616e-02,\n",
      "        -8.0084e-02, -1.1333e-01, -5.4316e-02, -1.0779e-01, -1.8977e-02,\n",
      "         4.6597e-02, -3.8798e-02,  4.7470e-05,  2.6071e-02, -2.3994e-02,\n",
      "         9.1402e-02,  1.9966e-02,  7.3941e-02,  5.2774e-02, -1.7478e-02,\n",
      "         1.2389e-01, -1.1391e-01,  2.6951e-02, -1.1606e-01, -1.3344e-01,\n",
      "        -1.4759e-01, -5.7651e-03, -1.4172e-01, -8.3470e-02, -8.9579e-04,\n",
      "         9.7065e-02, -7.3963e-02,  1.5741e-01, -1.3600e-01,  4.3365e-03,\n",
      "        -3.8324e-02,  9.2979e-02,  8.6552e-02,  4.5212e-02, -1.2754e-01,\n",
      "         1.2311e-01,  1.4091e-01, -1.7458e-01,  1.0042e-01,  4.4123e-02,\n",
      "         1.0499e-01, -1.0710e-01, -7.5058e-02,  5.3561e-02, -2.5272e-02,\n",
      "        -5.1344e-02, -9.7772e-02,  1.6128e-01, -9.1485e-02,  1.0397e-01,\n",
      "         1.2419e-02, -2.3559e-02, -8.6567e-02,  1.3798e-01,  8.0930e-02,\n",
      "        -4.8980e-02, -8.7553e-02, -1.5975e-01,  3.0054e-02, -3.2420e-02,\n",
      "         2.0201e-02,  7.8407e-02, -1.3206e-01, -6.7589e-03,  4.9661e-02,\n",
      "         1.3081e-01, -1.4031e-01, -1.8108e-01,  1.5774e-01, -1.2043e-01,\n",
      "        -7.0231e-02,  1.2344e-01,  1.4412e-01,  6.9671e-03, -9.3140e-02,\n",
      "        -1.1692e-01, -1.0187e-01,  1.2905e-01,  7.2723e-03,  1.6543e-01])\n",
      "tensor([[ 4.9389e-04, -3.6792e-02, -2.1432e-02,  ..., -1.9962e-02,\n",
      "         -5.2565e-03,  1.6962e-02],\n",
      "        [ 2.0479e-03,  1.6671e-03, -1.4008e-03,  ..., -3.2802e-02,\n",
      "         -3.2941e-03,  2.8445e-02],\n",
      "        [-6.1091e-02,  2.4868e-02, -1.1177e-02,  ..., -1.0235e-02,\n",
      "         -1.8755e-02, -2.2258e-04],\n",
      "        ...,\n",
      "        [ 2.3627e-02, -1.8557e-02, -3.2102e-02,  ...,  4.4563e-02,\n",
      "          2.9497e-02,  1.1738e-02],\n",
      "        [ 5.0182e-05,  5.9447e-03, -3.5349e-03,  ...,  5.9359e-03,\n",
      "          1.6808e-02,  1.7560e-02],\n",
      "        [ 5.9696e-03, -1.0979e-02, -1.9197e-02,  ...,  6.1531e-02,\n",
      "         -2.2316e-02,  3.5027e-02]])\n",
      "tensor([ 0.0116, -0.0288,  0.0308,  0.0226,  0.2309,  0.1195,  0.1605,  0.1157])\n",
      "tensor([[-0.0784, -0.1295, -0.5098,  0.0275],\n",
      "        [-0.2481,  0.1075, -0.2494,  0.0249],\n",
      "        [ 0.1772,  0.2915,  0.1700, -0.1111],\n",
      "        ...,\n",
      "        [ 0.4124,  0.3857, -0.4344, -0.0914],\n",
      "        [-0.3666, -0.3978,  0.3891, -0.0222],\n",
      "        [ 0.2643,  0.2642,  0.4051, -0.1835]])\n",
      "tensor([-4.3020e-02,  2.9969e-01,  6.5399e-02,  3.0189e-01, -5.0614e-01,\n",
      "         3.7838e-01, -4.2457e-01, -2.2316e-01,  1.9417e-01,  3.0755e-01,\n",
      "        -1.8473e-01,  4.2207e-01, -1.0232e-01, -1.4918e-01, -2.6085e-01,\n",
      "         3.2919e-01,  1.6166e-01, -1.4737e-01, -4.7426e-01, -4.7206e-01,\n",
      "         5.2078e-01,  1.3810e-01, -2.7125e-01,  1.5792e-01, -4.7280e-01,\n",
      "         7.3925e-02,  1.8158e-01, -1.4815e-02, -2.8621e-01, -2.0399e-01,\n",
      "         4.1990e-02, -1.1153e-01,  3.2641e-01, -4.2025e-01, -4.5760e-02,\n",
      "         4.0333e-01,  4.8967e-01, -6.3954e-02, -2.1781e-01,  3.8280e-01,\n",
      "        -2.6189e-01, -2.7963e-02,  3.6660e-01,  1.2413e-02,  1.3932e-01,\n",
      "         4.9994e-01,  3.8468e-01, -1.4283e-01,  1.2342e-01, -4.6008e-01,\n",
      "        -4.6035e-01, -8.6044e-02,  3.2241e-02, -2.3757e-01,  8.4951e-02,\n",
      "        -2.4551e-01,  2.3315e-01,  6.3135e-02,  2.4338e-01, -4.4695e-02,\n",
      "         2.6393e-01,  4.6519e-01, -4.8289e-01,  3.4122e-01,  1.7276e-01,\n",
      "        -3.0560e-01, -4.5741e-01, -4.6753e-01,  2.6667e-01,  1.4752e-01,\n",
      "        -2.4959e-01,  2.2586e-01, -4.6134e-01, -1.4954e-01,  5.1866e-01,\n",
      "        -4.8048e-01, -4.0899e-01, -1.7042e-01,  3.4021e-01,  4.9576e-01,\n",
      "        -4.2467e-01,  3.9106e-01, -2.9685e-01,  5.6869e-02,  1.3299e-01,\n",
      "         9.8934e-02,  1.5276e-01, -7.8478e-02,  4.5528e-01,  3.0151e-02,\n",
      "        -4.9500e-01,  3.0414e-01,  1.7387e-01,  4.5944e-04, -3.8290e-01,\n",
      "        -2.7455e-02, -1.4940e-01,  3.8179e-01, -2.5497e-01,  2.6123e-02,\n",
      "         4.3612e-01, -1.9063e-01,  1.9382e-01, -5.8208e-02,  9.2423e-02,\n",
      "        -2.5205e-01,  8.0242e-02, -5.3147e-01,  3.5583e-01,  1.3991e-01,\n",
      "         3.4884e-01,  8.5409e-02, -3.0843e-01, -1.1506e-01, -3.4391e-01,\n",
      "         7.8779e-02, -4.4042e-01,  9.7868e-02, -2.8637e-01, -3.7152e-01,\n",
      "         8.1405e-03, -3.3772e-01, -5.1432e-01,  4.0029e-01,  7.1581e-03,\n",
      "        -3.9896e-02, -4.4091e-01,  1.9275e-01, -2.0327e-01, -1.8298e-01,\n",
      "         2.1414e-01, -3.7660e-01, -2.8724e-01, -4.7778e-01,  4.4487e-01,\n",
      "        -2.1097e-01, -2.1568e-01,  3.8005e-01, -2.9934e-01, -4.7102e-01,\n",
      "        -6.9711e-02, -3.2850e-01,  4.2776e-01,  3.7263e-02,  1.4695e-01,\n",
      "         3.4278e-01,  1.4984e-01,  2.1059e-01, -4.0187e-01,  2.5999e-01,\n",
      "        -2.4620e-01, -1.1500e-02,  3.8363e-02, -1.5345e-01, -2.0955e-01,\n",
      "         1.5547e-01, -3.0834e-01,  4.8383e-01,  1.8081e-01, -2.6497e-01,\n",
      "         3.8297e-01, -1.1881e-01, -3.5884e-01, -2.1329e-01,  4.5147e-01,\n",
      "        -2.6230e-01, -1.8329e-01, -4.8954e-01,  1.1010e-01,  3.1031e-01,\n",
      "        -1.8755e-01,  2.0964e-02, -5.4057e-03, -2.2887e-01,  2.9438e-01,\n",
      "        -2.7214e-01, -4.5021e-01, -5.2709e-01,  4.9499e-01,  1.9709e-01,\n",
      "        -3.6606e-01, -3.4032e-01, -1.8129e-01,  3.2917e-01, -1.2291e-01,\n",
      "        -4.2586e-01,  3.9982e-01, -2.7707e-01, -2.9440e-01, -8.2126e-02,\n",
      "         6.0296e-02, -3.6475e-02,  4.0823e-01,  2.4409e-01,  2.4589e-01,\n",
      "        -4.0850e-02, -2.7215e-02, -1.2572e-01, -3.5818e-02,  2.9811e-01,\n",
      "        -2.6556e-01, -3.6270e-02,  1.4641e-01,  1.0953e-01, -5.8666e-02,\n",
      "         2.1830e-01,  1.0167e-02, -3.4519e-01, -1.9280e-01,  3.7310e-01,\n",
      "         3.3991e-01,  3.5430e-01, -9.8209e-02, -5.0136e-01, -1.4529e-01,\n",
      "        -3.2692e-01, -4.8623e-01,  5.1324e-01, -3.4279e-01,  4.4396e-01,\n",
      "         2.7882e-02, -3.1055e-01, -5.0080e-01, -3.7689e-01,  4.2322e-01,\n",
      "        -4.6641e-02, -3.9868e-01, -2.9844e-01, -2.3105e-01,  3.1161e-01,\n",
      "         2.6747e-02, -5.8259e-02, -5.8371e-01,  1.6868e-01, -5.0295e-01,\n",
      "        -2.1525e-01, -2.5489e-01, -2.8370e-01,  4.1358e-01, -4.8061e-02,\n",
      "         1.1933e-01, -3.3910e-01, -4.3503e-01, -9.6355e-02,  4.3248e-01,\n",
      "         5.4411e-02, -3.6087e-01,  5.8935e-02,  4.1195e-01, -4.6014e-02,\n",
      "        -4.4776e-02,  1.1352e-01, -1.9175e-01, -4.3912e-01,  2.4847e-01,\n",
      "        -2.0782e-01,  4.2239e-01, -2.4262e-01,  4.0668e-01, -4.0237e-01,\n",
      "         2.9072e-01, -3.3332e-01, -2.9129e-01, -4.9142e-01, -5.4206e-01,\n",
      "         1.7218e-01, -2.5421e-01,  4.9640e-01, -4.4749e-02,  3.0032e-02,\n",
      "        -4.6002e-01, -4.5387e-02,  3.4294e-01, -4.4748e-02,  7.7214e-02,\n",
      "        -4.2339e-01, -1.0649e-01,  1.8151e-01,  1.3043e-01,  7.7580e-02,\n",
      "        -1.0599e-03, -5.9093e-02,  2.3774e-01, -2.8147e-01, -1.4200e-01,\n",
      "        -2.7043e-01,  1.3340e-01,  2.2301e-01, -4.5934e-03, -2.1648e-01,\n",
      "        -4.1928e-01, -3.6548e-01,  2.8747e-01, -4.8238e-01,  1.1651e-02,\n",
      "         3.0597e-01, -2.9933e-01, -4.1778e-01, -2.3436e-01, -3.5503e-01,\n",
      "         1.1685e-01,  3.1811e-01, -3.5806e-01,  1.2905e-01,  1.7959e-02,\n",
      "        -1.4593e-01,  1.3884e-01,  2.4195e-01,  2.4333e-01,  1.8740e-01,\n",
      "        -1.0046e-01,  2.5516e-01, -2.2813e-01, -4.8857e-01, -1.8231e-01,\n",
      "        -5.0858e-01,  4.3496e-01, -2.3021e-01,  4.4912e-01,  4.6785e-01,\n",
      "        -3.0700e-01, -8.5895e-03,  2.1984e-01, -5.3838e-01, -2.6413e-01,\n",
      "        -2.5318e-02,  1.2573e-01,  2.3074e-01, -1.2955e-01, -4.4345e-01,\n",
      "         2.1200e-01, -1.9676e-01, -1.4202e-01,  1.8499e-01, -4.0162e-01,\n",
      "         4.1014e-02,  4.3653e-01, -3.8940e-01,  3.3421e-02, -8.5253e-02,\n",
      "         4.4427e-01,  4.4633e-01, -4.5393e-01,  3.7066e-01, -4.0457e-01,\n",
      "         3.7730e-01, -5.3893e-01,  2.4798e-01,  2.5763e-01, -2.3542e-01])\n",
      "tensor([1.0214, 0.9607, 0.9753, 0.9525, 0.9841, 1.0175, 0.9466, 1.0020, 0.9733,\n",
      "        0.9353, 0.9889, 0.9632, 0.9799, 0.9367, 0.9546, 0.9776, 0.9271, 1.0126,\n",
      "        0.9684, 0.9806, 0.9855, 0.9571, 0.9970, 0.9661, 0.9900, 0.9763, 0.9787,\n",
      "        0.9664, 1.0212, 0.9590, 0.9766, 1.0290, 0.9634, 1.0003, 0.9690, 1.0052,\n",
      "        0.9625, 0.9661, 1.0015, 0.9565, 0.9995, 0.9442, 1.0138, 1.0041, 0.9874,\n",
      "        0.9901, 0.9443, 0.9799, 0.9638, 0.9981, 0.9909, 0.9542, 0.9780, 0.9394,\n",
      "        0.9751, 1.0121, 0.9627, 1.0053, 0.9772, 0.9654, 0.9879, 0.9595, 0.9794,\n",
      "        0.9583, 0.9640, 0.9683, 0.9291, 0.9645, 0.9467, 0.9437, 1.0086, 0.9789,\n",
      "        0.9937, 0.9638, 0.9621, 0.9702, 0.9637, 0.9546, 0.9457, 0.9364, 0.9875,\n",
      "        0.9772, 1.0080, 0.9940, 0.9943, 0.9446, 0.9418, 0.9863, 0.9592, 0.9714,\n",
      "        1.0097, 0.9746, 1.0206, 0.9791, 0.9685, 0.9607, 1.0621, 0.9502, 0.9719,\n",
      "        0.9686, 0.9776, 0.9408, 0.9587, 0.9993, 0.9898, 0.9913, 1.0089, 0.9942,\n",
      "        0.9770, 0.9643, 1.0060, 0.9728, 0.9395, 0.9816, 0.9987, 0.9799, 1.0013,\n",
      "        0.9507, 0.9725, 0.9871, 0.9726, 0.9823, 0.9817, 0.9956, 0.9648, 0.9513,\n",
      "        0.9469, 1.0043, 1.0093, 0.9838, 0.9581, 0.9980, 0.9586, 0.9341, 0.9498,\n",
      "        0.9360, 0.9692, 0.9791, 0.9952, 0.8891, 0.9517, 1.0032, 0.9788, 1.0254,\n",
      "        0.9926, 0.9951, 0.9957, 0.9889, 0.9567, 0.9878, 0.9763, 0.9736, 0.9523,\n",
      "        1.0265, 0.9872, 0.9872, 0.9862, 1.0008, 0.9674, 0.9797, 0.9618, 0.9817,\n",
      "        1.0299, 0.9613, 0.9606, 0.9636, 1.0804, 0.9932, 0.9638, 0.9713, 1.0084,\n",
      "        0.9574, 0.9995, 0.9877, 0.9436, 0.9922, 0.9414, 0.9972, 0.9651, 1.0031,\n",
      "        0.9894, 0.9650, 0.9786, 0.9754, 0.9883, 1.0024, 0.9721, 0.9772, 0.9417,\n",
      "        0.9540, 0.9689, 0.9947, 0.9573, 0.9793, 0.9658, 0.9786, 0.9807, 1.0685,\n",
      "        0.9678, 0.9633, 0.9811, 0.9694, 0.9778, 0.9784, 0.9675, 0.9522, 0.9669,\n",
      "        0.9736, 0.9887, 0.9506, 0.9556, 1.0429, 0.9548, 0.9908, 0.9534, 0.9973,\n",
      "        0.9722, 0.9594, 1.0234, 0.9782, 0.9918, 1.0366, 0.9978, 1.0172, 0.9702,\n",
      "        0.9470, 1.0002, 1.0043, 1.0556, 0.9546, 0.9626, 0.9640, 1.0128, 0.9227,\n",
      "        1.0228, 0.9630, 1.0923, 0.9534, 0.9497, 0.9575, 0.9307, 0.9842, 0.9799,\n",
      "        0.9514, 0.9744, 0.9964, 0.9791, 0.9983, 0.9338, 1.0304, 0.9687, 0.9793,\n",
      "        0.9414, 0.9556, 0.9418, 0.9726, 1.0133, 0.9682, 0.9624, 0.9522, 0.9750,\n",
      "        0.9746, 0.9742, 0.9715, 0.9995, 0.9534, 0.9812, 0.9298, 0.9679, 0.9761,\n",
      "        0.9820, 0.9611, 0.9784, 1.0509, 0.9581, 0.9741, 0.9699, 1.0246, 1.0269,\n",
      "        0.9768, 1.0561, 1.0022, 0.9554, 0.9633, 1.0148, 1.0124, 0.9731, 0.9418,\n",
      "        0.9586, 1.0183, 0.9640, 1.0010, 0.9368, 1.0142, 1.0028, 0.9431, 0.9629,\n",
      "        0.9711, 0.9746, 0.9797, 0.9707, 0.9861, 0.9514, 0.9577, 0.9402, 0.9643,\n",
      "        0.9719, 0.9409, 1.0413, 0.9679, 1.0423, 0.9732, 0.9561, 0.9585, 0.9716,\n",
      "        0.9739, 0.9695, 0.9349, 0.9455, 0.9541, 0.9518, 0.9774, 1.0202, 0.9810,\n",
      "        1.0103, 0.9516, 0.9575, 0.9936, 0.9593, 0.9706, 0.9716, 1.0004, 0.9593,\n",
      "        0.9749, 0.9647, 0.9730, 1.0134, 0.9926, 0.9746, 0.9837, 0.9848, 0.9700,\n",
      "        1.0456, 0.9505, 0.9845, 0.9519, 1.0104, 0.9886, 0.9549, 0.9849])\n",
      "tensor([ 0.0488,  0.0820, -0.0549,  0.0379,  0.0119,  0.0141,  0.0049,  0.0967,\n",
      "        -0.0088, -0.0090,  0.0291, -0.0280, -0.0598,  0.0181,  0.0076,  0.0065,\n",
      "         0.0115,  0.0235, -0.0380,  0.0255,  0.0322, -0.0392,  0.0023, -0.0052,\n",
      "        -0.0989,  0.0308, -0.0584,  0.0343,  0.0648, -0.0091, -0.0136,  0.0784,\n",
      "         0.0263,  0.0371, -0.0014,  0.0035, -0.0264,  0.1138,  0.0549, -0.0071,\n",
      "        -0.0208, -0.0527, -0.0504, -0.0629,  0.0162, -0.0363, -0.0358, -0.0645,\n",
      "        -0.0849, -0.0016,  0.0156, -0.0902, -0.0423,  0.0186, -0.0459, -0.0027,\n",
      "         0.0036,  0.0683,  0.0496,  0.0838, -0.0061, -0.0594, -0.0665,  0.0081,\n",
      "        -0.0146, -0.0643,  0.0135, -0.0214,  0.0075, -0.0309,  0.0130,  0.0389,\n",
      "        -0.0064, -0.0578, -0.0212,  0.0116, -0.0498, -0.0069, -0.0468, -0.0036,\n",
      "         0.0356, -0.0161, -0.0862,  0.0184, -0.0019, -0.0308,  0.0051,  0.0064,\n",
      "         0.0146, -0.0334,  0.0065, -0.0598, -0.0285,  0.0496, -0.0619, -0.0031,\n",
      "         0.0921,  0.0216,  0.0147,  0.0402, -0.0502,  0.0119, -0.1240,  0.0120,\n",
      "         0.0310,  0.0257,  0.0450,  0.0808, -0.0893, -0.0384,  0.0248, -0.0243,\n",
      "         0.0896, -0.0400, -0.0874,  0.0193, -0.0502,  0.0280, -0.0704,  0.0060,\n",
      "        -0.0207, -0.0131,  0.0021,  0.0303, -0.0267,  0.0089, -0.0189,  0.0200,\n",
      "        -0.0105,  0.0243,  0.0221, -0.0040,  0.0567, -0.0263, -0.0798,  0.0232,\n",
      "        -0.0419, -0.0252,  0.0264,  0.0596, -0.0134,  0.0443, -0.0569, -0.0400,\n",
      "         0.0345, -0.0321,  0.0265,  0.0032,  0.0462,  0.0026,  0.0058,  0.0778,\n",
      "        -0.0107,  0.0237, -0.0246,  0.0677,  0.0334,  0.0023,  0.0329, -0.0039,\n",
      "        -0.0657, -0.0281, -0.0512,  0.0586,  0.0116, -0.0163,  0.0583,  0.0248,\n",
      "         0.0117, -0.0200,  0.0103,  0.0567, -0.0736, -0.0940, -0.0574,  0.0160,\n",
      "         0.0685, -0.0544, -0.0161, -0.0147, -0.0137,  0.0357, -0.0145, -0.0185,\n",
      "         0.0126,  0.0170,  0.0355, -0.0348,  0.0333, -0.0147, -0.0524, -0.0258,\n",
      "        -0.0217,  0.0374, -0.0029, -0.0369,  0.0358, -0.0432,  0.0047, -0.0268,\n",
      "         0.0187,  0.0189,  0.0775, -0.0161,  0.0373,  0.0035, -0.0651, -0.0031,\n",
      "        -0.0560,  0.0141, -0.0172, -0.0387, -0.0049,  0.0447, -0.0283,  0.0310,\n",
      "        -0.0122, -0.0771,  0.0084,  0.0383, -0.0182,  0.0588,  0.0323, -0.0089,\n",
      "         0.0117, -0.0462, -0.0441, -0.0307,  0.0336,  0.0416, -0.0540, -0.0496,\n",
      "         0.0024, -0.1044, -0.0312, -0.0071,  0.0705, -0.0173, -0.0912,  0.0395,\n",
      "        -0.0239,  0.0047,  0.0379, -0.0210, -0.0298,  0.0107,  0.0150,  0.0258,\n",
      "        -0.0217,  0.0667, -0.0590, -0.0578, -0.0206, -0.0086, -0.0740,  0.0654,\n",
      "         0.0261, -0.0151, -0.0536,  0.0036, -0.0478,  0.0096, -0.0363, -0.0140,\n",
      "        -0.0173,  0.0389, -0.0813, -0.0603,  0.0012, -0.0014, -0.0011,  0.0354,\n",
      "         0.0588, -0.0531,  0.0156, -0.0762,  0.0138,  0.0003, -0.0225,  0.0383,\n",
      "        -0.0350,  0.0076, -0.0648, -0.0609, -0.0326,  0.0076, -0.0064,  0.0059,\n",
      "        -0.0252,  0.0428, -0.0310,  0.0339, -0.0116, -0.0207,  0.0082, -0.0336,\n",
      "        -0.0006, -0.0714,  0.0014, -0.0133,  0.0509, -0.0076, -0.0505,  0.0008,\n",
      "        -0.0561, -0.0119,  0.0180, -0.0930, -0.0345, -0.0565,  0.0919,  0.0472,\n",
      "         0.1384, -0.0189,  0.0298,  0.0276,  0.0493,  0.0147, -0.0002, -0.0120,\n",
      "         0.0032, -0.0037,  0.0341,  0.0535,  0.0770,  0.0357, -0.0287, -0.0154,\n",
      "         0.0335, -0.0668, -0.0227, -0.0123, -0.0855, -0.0028, -0.0627,  0.0157,\n",
      "        -0.0378, -0.0165, -0.0730, -0.0157, -0.0699,  0.0549,  0.0308,  0.0397,\n",
      "        -0.0565,  0.0091,  0.0140,  0.0361,  0.0060,  0.0498])\n",
      "tensor([[-0.0562, -0.0275,  0.0060,  ..., -0.0115, -0.0477, -0.0365],\n",
      "        [-0.0033,  0.0109, -0.0192,  ..., -0.0086, -0.0638, -0.0599],\n",
      "        [-0.0561,  0.0195, -0.0375,  ..., -0.0382,  0.0477, -0.0713],\n",
      "        ...,\n",
      "        [-0.0341,  0.0589,  0.0352,  ...,  0.0022, -0.0411, -0.0056],\n",
      "        [-0.0303, -0.0055, -0.0358,  ...,  0.0133, -0.0170,  0.0268],\n",
      "        [ 0.0500,  0.0515, -0.0328,  ...,  0.0040, -0.0572,  0.0202]])\n",
      "tensor([-0.0516, -0.0315, -0.0649, -0.0264, -0.0465, -0.0282, -0.0249,  0.0140,\n",
      "        -0.0140, -0.0606,  0.0795, -0.0463, -0.0802, -0.0683,  0.0423, -0.0379,\n",
      "        -0.0756,  0.0067, -0.0238, -0.0600,  0.0647, -0.0417, -0.0580, -0.0487,\n",
      "        -0.0377, -0.0712, -0.0115,  0.0677, -0.1367, -0.0193, -0.0922, -0.0460,\n",
      "         0.0222,  0.0022, -0.0546, -0.0021,  0.0124,  0.0243, -0.1006,  0.0265,\n",
      "        -0.0838, -0.0336,  0.0300, -0.0539, -0.0034,  0.1055, -0.0186, -0.0067,\n",
      "         0.0198, -0.0454, -0.0604,  0.0229, -0.0157, -0.0006, -0.0288,  0.0174,\n",
      "         0.0269, -0.0319,  0.0072, -0.0128,  0.0296,  0.0067, -0.0252, -0.1206,\n",
      "        -0.0445, -0.0155, -0.0021,  0.0393,  0.0393, -0.0284, -0.0207,  0.0328,\n",
      "        -0.0869, -0.0853,  0.1081,  0.0111, -0.0234, -0.0162, -0.0535,  0.0835,\n",
      "         0.0227,  0.0159, -0.0214,  0.0144, -0.1333,  0.0442, -0.0564, -0.0527,\n",
      "        -0.0875,  0.0147, -0.0630, -0.0221, -0.0141,  0.0493, -0.0129, -0.0489,\n",
      "        -0.0315,  0.0431,  0.0095, -0.0986,  0.0444,  0.0235,  0.0093, -0.0177,\n",
      "        -0.0168, -0.0642,  0.0143, -0.0590,  0.0023,  0.0115, -0.0146,  0.0476,\n",
      "         0.0221, -0.1196, -0.0074, -0.0386, -0.0219,  0.0390, -0.0058,  0.0463,\n",
      "         0.0578,  0.0361,  0.0137,  0.0045, -0.0316, -0.0184,  0.0243,  0.0360,\n",
      "        -0.0412,  0.0102, -0.0116,  0.0214,  0.0013, -0.0484,  0.0185, -0.0100,\n",
      "         0.0407, -0.0302, -0.0698, -0.0775,  0.0424, -0.0132, -0.0401, -0.0349,\n",
      "        -0.1585, -0.0277,  0.0492, -0.0412, -0.0259, -0.0109, -0.0444, -0.0546,\n",
      "         0.0507, -0.0110, -0.0712,  0.0713, -0.0502,  0.0775, -0.0290, -0.0380,\n",
      "         0.0292, -0.1119, -0.1075, -0.0159, -0.0094, -0.1256, -0.0179, -0.0658,\n",
      "        -0.0102, -0.0295,  0.0303, -0.0316, -0.0083, -0.0454,  0.0132, -0.0017,\n",
      "        -0.0682, -0.0135,  0.0481, -0.0999, -0.0119, -0.0396,  0.0280,  0.0619,\n",
      "        -0.0382, -0.0210,  0.0075, -0.0248, -0.0703,  0.0410, -0.0458, -0.0060,\n",
      "        -0.0332, -0.0511, -0.0547, -0.0614,  0.0732, -0.0325,  0.0287,  0.0206,\n",
      "         0.0304, -0.0706, -0.0056, -0.0141, -0.0373,  0.0520, -0.0478, -0.0722,\n",
      "         0.0770,  0.0381, -0.0056,  0.0513, -0.0307, -0.0395, -0.0078, -0.0432,\n",
      "         0.0168, -0.0488,  0.0091,  0.0526,  0.0755, -0.0087, -0.0094, -0.1160,\n",
      "         0.0433, -0.0192, -0.0290, -0.0046,  0.0399,  0.0195,  0.0206,  0.0225,\n",
      "        -0.1350, -0.0784, -0.0151, -0.0293, -0.0325, -0.0198, -0.0338, -0.0058,\n",
      "        -0.0611,  0.0136, -0.0412, -0.1146, -0.0347,  0.0399,  0.0269, -0.0095,\n",
      "        -0.0064, -0.0064,  0.0095,  0.0391, -0.0406,  0.0294,  0.0096, -0.0533,\n",
      "        -0.0717, -0.0263,  0.0889,  0.0132, -0.0124, -0.0304, -0.0049,  0.0533,\n",
      "         0.0185, -0.0531,  0.0072,  0.0628, -0.0334, -0.0475, -0.0007,  0.0092,\n",
      "         0.0100, -0.1176, -0.0268,  0.0273, -0.0303, -0.0633, -0.0578, -0.0502,\n",
      "         0.0209,  0.0257,  0.0897, -0.0270, -0.0587,  0.0162, -0.0280, -0.0796,\n",
      "         0.0118, -0.0998,  0.0527,  0.0140, -0.0566, -0.1184,  0.0262,  0.0704,\n",
      "        -0.0224, -0.0155, -0.0184, -0.0867,  0.0138,  0.0345, -0.0886, -0.0243,\n",
      "        -0.0185, -0.0134,  0.0279,  0.0372, -0.0233, -0.0243, -0.0835,  0.0070,\n",
      "        -0.0879, -0.0186, -0.0254, -0.0918, -0.0111,  0.0395, -0.0692, -0.0192,\n",
      "        -0.0111, -0.0070,  0.0169, -0.0404, -0.0346, -0.0487,  0.0467, -0.0058,\n",
      "         0.0258,  0.0350,  0.0144, -0.0068, -0.0341, -0.0500, -0.0047, -0.0787,\n",
      "        -0.0293, -0.0772, -0.0164,  0.0194, -0.0194,  0.0057,  0.0038,  0.0138,\n",
      "        -0.0567, -0.0607, -0.0023, -0.0083, -0.0300,  0.0390])\n",
      "tensor([1.0421, 1.0052, 0.9821, 1.0109, 1.0011, 1.0182, 0.9673, 1.0031, 1.0448,\n",
      "        1.0042, 0.9768, 1.0079, 0.9980, 0.9977, 1.0166, 0.9862, 0.9788, 1.0313,\n",
      "        0.9901, 1.0279, 0.9840, 0.9668, 0.9685, 0.9738, 1.0242, 1.0028, 0.9881,\n",
      "        0.9920, 0.9912, 1.0130, 1.0404, 1.0210, 1.0019, 1.0073, 1.0333, 0.9885,\n",
      "        0.9964, 1.0199, 0.9895, 0.9538, 1.0097, 1.0626, 1.0214, 0.9877, 1.0074,\n",
      "        1.0198, 0.9500, 1.0195, 0.9594, 1.0381, 1.0440, 1.0199, 1.0195, 1.0008,\n",
      "        1.0116, 0.9543, 0.9907, 1.0012, 1.0003, 0.9826, 1.0133, 1.0091, 1.0398,\n",
      "        1.0366, 0.9840, 1.0357, 0.9990, 1.0268, 1.0155, 1.0042, 0.9763, 1.0158,\n",
      "        0.9877, 0.9888, 1.0160, 0.9973, 1.0046, 1.0354, 1.0204, 0.9468, 1.0313,\n",
      "        1.0227, 0.9981, 0.9776, 1.0895, 1.0115, 1.0001, 0.9868, 0.9902, 1.0011,\n",
      "        1.0465, 0.9968, 0.9801, 0.9933, 1.0131, 1.0490, 1.0446, 0.9746, 1.0454,\n",
      "        0.9952, 0.9932, 0.9972, 0.9942, 0.9809, 0.9904, 0.9857, 0.9930, 1.0312,\n",
      "        0.9825, 0.9870, 0.9534, 0.9576, 1.0338, 1.0042, 0.9712, 1.0389, 0.9895,\n",
      "        1.0234, 1.0247, 1.0169, 0.9827, 1.0188, 0.9663, 1.0121, 1.0220, 1.0139,\n",
      "        1.0064, 1.0251, 0.9865, 0.9819, 0.9872, 0.9999, 0.9970, 1.0669, 0.9892,\n",
      "        0.9882, 1.0282, 0.9871, 1.0112, 1.0374, 1.0138, 1.0172, 1.0148, 0.9546,\n",
      "        1.0293, 0.9937, 0.9643, 0.9802, 0.9792, 1.0203, 1.0069, 0.9592, 1.0038,\n",
      "        1.0281, 0.9906, 1.0092, 1.0229, 1.0044, 0.9852, 0.9992, 1.0271, 1.0139,\n",
      "        1.0485, 0.9785, 0.9797, 1.0496, 0.9701, 1.0131, 1.0059, 1.0013, 1.0112,\n",
      "        1.0231, 1.0297, 1.0077, 1.0097, 1.0927, 1.0705, 1.0244, 0.9862, 0.9695,\n",
      "        0.9905, 0.9753, 1.0094, 0.9623, 0.9782, 0.9994, 1.0348, 0.9868, 1.0093,\n",
      "        0.9958, 0.9881, 1.0126, 0.9821, 1.0298, 1.0068, 1.0041, 0.9519, 0.9651,\n",
      "        0.9660, 0.9731, 0.9855, 0.9911, 1.0130, 1.0235, 1.0284, 0.9930, 0.9959,\n",
      "        0.9971, 0.9505, 0.9900, 0.9990, 0.9766, 1.0389, 0.9969, 0.9763, 1.0360,\n",
      "        1.0168, 1.0222, 1.0021, 0.9618, 1.0065, 0.9793, 1.0168, 1.0096, 1.0105,\n",
      "        1.0069, 1.0019, 0.9796, 0.9995, 0.9798, 0.9835, 0.9857, 1.0191, 1.0066,\n",
      "        0.9913, 0.9869, 1.0015, 0.9966, 0.9836, 0.9932, 0.9971, 1.0423, 1.0455,\n",
      "        1.0071, 0.9449, 1.0022, 0.9723, 1.0068, 0.9856, 1.0601, 0.9691, 1.0021,\n",
      "        0.9626, 1.0200, 0.9809, 1.0581, 1.0350, 1.0231, 1.0058, 0.9835, 1.0340,\n",
      "        1.0331, 0.9997, 0.9998, 0.9977, 0.9904, 0.9929, 0.9577, 1.0225, 1.0397,\n",
      "        1.0755, 0.9925, 0.9765, 0.9841, 1.0426, 1.0421, 1.0265, 1.0267, 1.0135,\n",
      "        1.0038, 0.9826, 1.0029, 0.9843, 1.0085, 1.0392, 1.0236, 0.9665, 0.9861,\n",
      "        0.9772, 1.0345, 1.0119, 1.0195, 1.0142, 1.0125, 1.0071, 0.9839, 0.9707,\n",
      "        1.0563, 1.0329, 0.9912, 0.9655, 1.0086, 1.0114, 0.9801, 0.9540, 1.0059,\n",
      "        0.9840, 0.9769, 1.0116, 1.0123, 1.0375, 0.9744, 1.0247, 0.9986, 0.9987,\n",
      "        1.0158, 0.9761, 1.0329, 1.0465, 0.9888, 1.0308, 1.0301, 1.0077, 1.0303,\n",
      "        0.9967, 1.0198, 1.0119, 1.0182, 0.9968, 0.9613, 0.9739, 1.0142, 1.0130,\n",
      "        1.0615, 1.0537, 1.0306, 1.0320, 0.9998, 0.9781, 1.0428, 0.9837, 0.9831,\n",
      "        1.0055, 0.9791, 1.0217, 0.9556, 0.9823, 1.0007, 0.9994, 1.0041])\n",
      "tensor([ 2.6530e-02,  3.4068e-02,  6.7312e-02, -7.8475e-03, -1.3262e-02,\n",
      "         7.5315e-03,  1.1240e-02, -4.1953e-02,  4.3788e-03,  1.3003e-02,\n",
      "        -1.0826e-02, -3.8767e-02,  2.1830e-02,  2.0539e-02,  4.0463e-03,\n",
      "         2.3981e-02, -4.3788e-02, -9.6494e-03,  4.1471e-02, -3.5621e-04,\n",
      "         1.8546e-02, -6.3270e-02, -3.5969e-03,  1.1481e-02, -3.9631e-03,\n",
      "        -6.5629e-03,  9.8632e-02, -1.2366e-02, -4.4768e-03,  1.6067e-02,\n",
      "        -2.1741e-02, -8.0580e-03, -4.1869e-02, -2.9693e-02,  3.8821e-02,\n",
      "        -9.4130e-03, -4.1922e-02,  2.9888e-03,  1.4397e-02, -7.6406e-04,\n",
      "         2.3460e-02, -3.8164e-02,  4.5126e-02,  5.8815e-02,  6.8374e-02,\n",
      "         3.8053e-02,  1.4067e-02,  8.3054e-02, -6.1149e-02, -4.5795e-02,\n",
      "        -2.5191e-02,  1.8380e-02,  7.3665e-02,  1.2341e-02,  1.0799e-02,\n",
      "        -7.4914e-02, -3.9902e-02, -7.8940e-03, -1.2031e-03,  3.0925e-02,\n",
      "         2.3278e-02,  4.8276e-02, -4.7751e-02, -3.0060e-02, -5.1525e-02,\n",
      "        -2.1637e-02,  3.5585e-02,  5.4899e-02,  2.7492e-02,  5.9021e-02,\n",
      "         9.9745e-02,  2.2124e-02,  7.6397e-04, -6.2784e-02,  4.6928e-03,\n",
      "         1.7939e-02, -5.6119e-02,  7.1678e-02, -1.4697e-02,  3.8676e-02,\n",
      "         8.9099e-02, -2.7013e-02,  4.9152e-02, -1.8754e-02, -5.1652e-02,\n",
      "        -5.6167e-02, -1.3397e-03,  1.1871e-02, -5.2695e-02,  1.6686e-02,\n",
      "        -2.6515e-02, -1.9629e-02, -3.1140e-02, -1.8567e-02, -5.4428e-02,\n",
      "        -1.7093e-02,  2.2931e-02,  1.7468e-02,  1.0947e-02, -1.4821e-02,\n",
      "         4.6458e-02, -2.8857e-02, -9.8483e-03, -2.4354e-02,  4.0958e-03,\n",
      "         1.1553e-02,  1.4093e-02,  1.8364e-02, -1.9876e-03,  1.5568e-02,\n",
      "        -5.4005e-02,  3.2357e-03, -2.3236e-02,  5.6477e-03, -1.0799e-02,\n",
      "        -8.8480e-03, -3.9217e-02, -8.9828e-03,  3.4407e-02,  6.3522e-03,\n",
      "        -1.4279e-02,  2.3888e-02,  2.6430e-02,  3.4631e-02,  5.9434e-02,\n",
      "         5.6107e-03, -3.7069e-02, -9.6610e-03,  4.2594e-02,  2.5945e-02,\n",
      "         6.9011e-03, -2.0582e-02, -8.9410e-03, -5.3910e-02,  3.9308e-02,\n",
      "         4.4552e-02, -2.9592e-02, -6.2558e-02,  1.3149e-02,  5.5392e-03,\n",
      "         1.4186e-02, -1.9986e-02,  8.8822e-03, -1.4202e-02,  2.7615e-02,\n",
      "         5.3317e-02,  2.4163e-02, -2.8994e-03,  9.8716e-03,  6.1870e-02,\n",
      "         5.0399e-02, -1.7055e-02,  1.5262e-02, -5.9504e-02,  1.0424e-02,\n",
      "        -5.2957e-02,  2.7312e-02,  2.4682e-02, -9.2089e-03,  1.4456e-02,\n",
      "         5.5120e-03, -7.7923e-03, -3.3280e-02, -3.1693e-02,  3.6217e-02,\n",
      "         1.9619e-02,  4.4898e-02, -3.6008e-02,  1.1219e-02, -1.7697e-02,\n",
      "         2.2847e-02,  1.5782e-02,  6.2070e-02, -2.2819e-02,  8.4239e-02,\n",
      "         7.4501e-03, -9.7937e-03, -6.3666e-02,  4.5607e-02, -3.1805e-02,\n",
      "         3.6856e-02,  3.9972e-02, -4.2071e-02, -4.5021e-03, -3.1917e-02,\n",
      "         2.2301e-02,  2.9831e-02, -2.4223e-02,  4.2356e-02,  4.3691e-03,\n",
      "        -2.2190e-02,  2.5498e-02,  4.9704e-05,  4.6759e-02,  4.6778e-02,\n",
      "        -3.7739e-02, -1.9008e-02, -1.3249e-02, -5.3888e-03, -5.6318e-02,\n",
      "        -1.7213e-02,  2.5543e-02, -3.2097e-02,  5.9015e-02,  3.4878e-02,\n",
      "         4.6962e-02,  7.8576e-02, -9.5014e-03,  1.6976e-02, -9.1342e-02,\n",
      "        -1.4488e-02,  3.2907e-02,  7.1920e-02,  3.9064e-02,  8.3003e-03,\n",
      "         1.2628e-02, -3.6837e-02,  3.9729e-02,  3.3719e-02,  2.0454e-02,\n",
      "         2.8438e-02,  9.0228e-03, -1.4908e-02,  6.6780e-02,  1.2661e-02,\n",
      "         5.1994e-02, -1.4851e-02,  3.0051e-02,  3.6721e-02,  5.2340e-03,\n",
      "         3.0619e-03,  3.0778e-02, -9.4560e-03,  3.2280e-02, -1.1014e-02,\n",
      "         4.3687e-02, -6.7893e-02, -4.6850e-02,  9.7631e-03,  3.2313e-02,\n",
      "         2.8892e-02, -1.3917e-02, -3.4229e-02,  1.6975e-02, -1.3078e-02,\n",
      "        -3.7331e-02,  1.0147e-02, -4.0304e-02, -5.5163e-03, -3.6798e-02,\n",
      "         3.0228e-02,  1.9561e-02, -4.3691e-03, -3.7808e-02, -7.8656e-03,\n",
      "         9.1757e-02,  3.0811e-02, -1.4044e-02,  4.7375e-02,  4.7435e-02,\n",
      "        -4.1955e-02, -2.9351e-02,  1.0494e-02,  3.7287e-02, -2.3301e-02,\n",
      "        -2.1342e-02,  2.9134e-02, -1.6506e-02,  3.6097e-02,  3.2266e-02,\n",
      "         2.9259e-02, -4.7643e-02, -5.4631e-03, -1.9915e-02, -8.0284e-02,\n",
      "         4.2786e-02,  3.2800e-02, -5.5490e-03,  2.5245e-02,  1.3271e-02,\n",
      "        -2.0215e-02,  2.7012e-02,  5.7934e-02,  1.6542e-02, -1.8728e-02,\n",
      "        -2.2956e-02, -3.0397e-02, -4.5016e-03,  5.1769e-02, -3.7549e-02,\n",
      "         6.8749e-03,  4.1289e-03,  3.3011e-02, -5.8532e-02,  5.0159e-02,\n",
      "         3.2656e-02,  2.1113e-03, -8.2517e-03,  2.4149e-02,  3.8592e-02,\n",
      "        -4.5529e-02, -3.7461e-02,  5.1672e-02,  1.5865e-02,  2.1098e-02,\n",
      "         9.4689e-03,  2.8948e-02, -3.2434e-02,  1.9499e-02, -1.7465e-02,\n",
      "         6.4700e-02,  6.2926e-02, -5.3374e-02,  6.8129e-03,  5.6141e-02,\n",
      "         1.6101e-02, -3.6139e-02,  2.8665e-02,  3.5079e-02, -7.9447e-03,\n",
      "        -1.3293e-02,  7.3510e-03, -1.2716e-02,  6.3565e-02, -1.6406e-02,\n",
      "        -5.9021e-02,  7.5282e-03, -1.8560e-02,  8.6926e-02, -9.3193e-03,\n",
      "        -3.4289e-02, -2.4119e-02, -4.7480e-02,  4.7163e-03, -3.2394e-02,\n",
      "         1.8527e-02,  3.9744e-02,  3.7574e-02, -2.8846e-02,  7.7723e-02,\n",
      "         1.2077e-02, -1.2148e-02, -3.3115e-02,  3.1379e-02,  9.3003e-03,\n",
      "        -1.2808e-04, -3.7130e-02,  4.0527e-02, -3.4974e-02,  5.0961e-02])\n",
      "tensor([[-0.0680, -0.0349, -0.0519,  ...,  0.0206,  0.0345, -0.0268],\n",
      "        [ 0.0047, -0.0663, -0.0292,  ..., -0.0101, -0.0019,  0.0266],\n",
      "        [ 0.0143,  0.0309, -0.0437,  ...,  0.0046,  0.0637,  0.0335],\n",
      "        ...,\n",
      "        [ 0.0597, -0.0180,  0.0300,  ..., -0.0700,  0.0120, -0.0122],\n",
      "        [-0.0315, -0.0946,  0.0342,  ...,  0.0589, -0.0048, -0.0078],\n",
      "        [ 0.0026, -0.0551,  0.0204,  ..., -0.0432, -0.0185, -0.0374]])\n",
      "tensor([-0.1055, -0.0142, -0.0393,  0.0444, -0.0327, -0.0736,  0.0404,  0.0006,\n",
      "        -0.0120, -0.0433,  0.0433, -0.0873,  0.0098, -0.0172,  0.0238,  0.0351,\n",
      "        -0.0554,  0.0007,  0.0221, -0.0263,  0.0825,  0.0188, -0.0299, -0.0494,\n",
      "        -0.0481, -0.0058, -0.0290, -0.0067, -0.0858,  0.0847, -0.0294,  0.0431,\n",
      "         0.0653, -0.0078,  0.0702,  0.0192, -0.0056,  0.0226, -0.0197,  0.0203,\n",
      "        -0.0706,  0.0192,  0.0412, -0.0388,  0.0244,  0.0077, -0.0041,  0.0260,\n",
      "         0.0444, -0.0028, -0.0072,  0.0027, -0.0439, -0.0287,  0.0158, -0.0058,\n",
      "         0.0410,  0.0288,  0.0214, -0.0343, -0.0290, -0.0196,  0.0147, -0.0677,\n",
      "        -0.0884, -0.0366,  0.0171, -0.0307,  0.0060, -0.0103, -0.0016, -0.0442,\n",
      "        -0.0925, -0.0045,  0.0720, -0.0186, -0.0189, -0.0284, -0.0600,  0.0052,\n",
      "        -0.0461,  0.0585, -0.0529,  0.0008,  0.0269, -0.0658, -0.0359,  0.0241,\n",
      "         0.0103, -0.0448, -0.0299, -0.0399,  0.0312,  0.0670, -0.0133, -0.0372,\n",
      "         0.0604,  0.0157,  0.0628,  0.0110, -0.0048, -0.0053, -0.0391, -0.0380,\n",
      "        -0.0891,  0.0016, -0.0404,  0.0066,  0.0463, -0.0500, -0.0026,  0.0100,\n",
      "        -0.0095, -0.0106, -0.0084, -0.0128,  0.0305, -0.0434,  0.0403,  0.0264,\n",
      "         0.0672,  0.0486,  0.0070, -0.0218,  0.0331,  0.0556, -0.0398, -0.0167,\n",
      "         0.0311, -0.0303, -0.0097, -0.0624, -0.0065,  0.0167,  0.0568, -0.0315,\n",
      "        -0.0321, -0.1150,  0.0399, -0.0150,  0.0113, -0.0092, -0.0132, -0.0478,\n",
      "        -0.1763,  0.0009,  0.0977,  0.0017,  0.0432, -0.0249, -0.0431, -0.0084,\n",
      "        -0.0065,  0.0039,  0.0545, -0.0258,  0.0111,  0.0406,  0.0111, -0.0100,\n",
      "         0.0041, -0.0345, -0.0063, -0.1012, -0.0153, -0.0805, -0.0614,  0.0287,\n",
      "        -0.0741, -0.0514,  0.0869,  0.0093,  0.0411,  0.0040, -0.0192, -0.0256,\n",
      "         0.0723,  0.0023,  0.0191, -0.0661,  0.0339, -0.0070, -0.0060,  0.0082,\n",
      "         0.0317,  0.0561, -0.0061,  0.0309, -0.0371, -0.0516, -0.0030, -0.0294,\n",
      "        -0.0451, -0.0686, -0.0615, -0.1006,  0.0965, -0.0179,  0.0126,  0.0063,\n",
      "         0.0412, -0.0626, -0.0437, -0.0226, -0.0665,  0.0180, -0.0498,  0.0086,\n",
      "         0.0561,  0.0168, -0.0678,  0.0538,  0.0212,  0.0275,  0.0150, -0.0136,\n",
      "         0.0471,  0.0281, -0.0290,  0.0136,  0.0512,  0.0289, -0.0465, -0.0798,\n",
      "        -0.0129,  0.0411, -0.0499,  0.0238,  0.0515, -0.0375,  0.0591, -0.0040,\n",
      "        -0.0931,  0.0242, -0.0879,  0.0112,  0.0210, -0.0221, -0.0094, -0.0397,\n",
      "        -0.0430, -0.0786, -0.0291, -0.0040, -0.0663,  0.0125, -0.0236, -0.0347,\n",
      "        -0.0112, -0.0275,  0.0040, -0.0528, -0.0341, -0.0186, -0.0188,  0.0525,\n",
      "        -0.0095, -0.0579,  0.0095,  0.0653,  0.0087, -0.0051,  0.0278,  0.0508,\n",
      "        -0.0061,  0.0438, -0.0568,  0.0160, -0.0082, -0.0027,  0.0080,  0.0138,\n",
      "         0.0377, -0.0665, -0.0346, -0.0241,  0.0690,  0.0299, -0.0091, -0.0560,\n",
      "         0.0597,  0.0254,  0.0548, -0.0487, -0.0418, -0.0264, -0.0036, -0.0265,\n",
      "        -0.0823, -0.1047, -0.0287,  0.0706, -0.1070, -0.1340, -0.0259,  0.0291,\n",
      "        -0.0178,  0.0379, -0.0066, -0.0564, -0.0215, -0.0454, -0.0660, -0.0810,\n",
      "        -0.0612, -0.0351,  0.0312, -0.0427,  0.0196,  0.0258, -0.0330, -0.0069,\n",
      "        -0.0731, -0.0620,  0.0282, -0.0913, -0.0217, -0.0255, -0.0352,  0.0288,\n",
      "        -0.0027, -0.0114, -0.0426,  0.0135, -0.0418, -0.0348, -0.0058,  0.0331,\n",
      "         0.0378,  0.0609,  0.0622, -0.0040,  0.0065, -0.0312, -0.0251, -0.0017,\n",
      "        -0.0376,  0.0140, -0.0206,  0.0005, -0.0452,  0.0248, -0.0772, -0.0353,\n",
      "        -0.0640, -0.0321, -0.0169,  0.0729, -0.0877, -0.0501])\n",
      "tensor([1.1148, 1.0141, 1.0548, 1.0368, 1.0481, 1.0464, 1.0362, 1.0098, 1.0570,\n",
      "        1.0138, 1.0650, 1.0548, 1.0176, 1.0360, 0.9849, 1.0148, 1.0273, 1.0450,\n",
      "        0.9993, 0.9943, 1.0344, 1.0194, 1.0069, 1.0280, 1.0707, 1.0314, 0.9807,\n",
      "        1.0301, 1.0656, 1.0639, 1.0158, 1.0774, 0.9911, 1.0202, 1.0284, 1.0395,\n",
      "        1.0048, 1.0222, 0.9598, 1.0272, 1.0131, 1.0464, 1.0313, 1.0447, 1.0433,\n",
      "        1.0491, 1.0265, 1.0357, 1.0277, 1.0640, 1.1031, 1.0382, 1.0608, 1.0049,\n",
      "        1.0169, 1.0651, 1.0599, 1.0296, 1.0190, 1.0565, 1.0975, 1.0964, 1.0636,\n",
      "        1.0858, 1.1103, 1.0148, 1.1150, 1.0316, 1.0196, 1.0764, 1.0368, 1.0305,\n",
      "        1.0579, 1.0245, 1.0122, 1.0240, 1.0243, 1.0361, 1.0408, 1.0258, 1.0534,\n",
      "        1.0300, 1.0150, 1.0148, 1.1104, 1.0179, 1.0375, 1.1150, 1.0061, 1.0883,\n",
      "        1.0451, 1.0324, 1.0150, 1.0291, 1.0310, 0.9939, 1.0330, 1.0334, 1.0532,\n",
      "        1.0646, 1.0297, 1.0111, 1.0311, 1.1973, 1.0490, 1.0353, 1.0394, 1.0500,\n",
      "        1.0635, 1.0529, 1.0230, 1.0270, 0.9999, 1.0281, 1.0255, 1.0577, 0.9895,\n",
      "        1.0642, 1.0232, 1.0373, 1.0520, 1.0139, 1.0136, 1.0102, 1.0114, 1.0849,\n",
      "        1.0295, 1.0333, 1.0479, 1.0028, 1.0530, 1.0175, 1.0257, 1.0313, 1.0125,\n",
      "        1.0514, 1.0123, 1.0544, 1.0061, 1.0032, 1.0078, 1.0259, 1.0279, 1.0048,\n",
      "        1.1384, 1.0247, 1.1074, 1.0146, 1.0376, 1.0450, 1.0196, 1.0425, 1.0546,\n",
      "        1.0394, 1.0445, 1.0280, 1.0564, 1.0031, 1.0220, 1.0195, 1.0217, 1.0132,\n",
      "        1.0571, 1.0498, 1.0699, 1.0769, 1.0432, 1.0593, 0.9973, 1.0127, 1.0188,\n",
      "        1.0346, 1.0429, 1.0588, 1.0302, 1.0309, 1.0700, 1.0541, 1.0049, 1.1042,\n",
      "        1.0160, 1.0060, 1.0411, 0.9783, 1.0621, 0.9909, 1.0407, 1.0125, 1.1026,\n",
      "        1.0127, 1.0452, 1.0576, 1.0514, 1.0759, 1.1319, 1.0630, 1.0454, 0.9996,\n",
      "        1.0311, 1.0308, 1.0389, 1.0083, 1.0693, 1.0669, 1.0640, 1.0365, 1.1174,\n",
      "        1.0213, 1.0392, 1.0146, 1.0710, 1.0647, 1.0396, 1.0135, 1.0627, 1.0278,\n",
      "        1.0304, 1.0123, 1.0725, 1.0405, 1.0650, 1.0202, 1.0154, 1.0154, 1.0354,\n",
      "        1.0536, 1.0213, 1.0656, 1.0390, 1.0398, 1.0160, 0.9966, 1.0843, 1.0373,\n",
      "        1.0001, 1.0423, 1.0084, 1.0398, 1.0356, 1.0320, 1.0907, 1.0578, 1.0450,\n",
      "        1.0264, 1.0476, 1.0479, 1.0325, 1.0899, 1.0582, 1.0416, 1.0448, 1.0274,\n",
      "        1.0658, 0.9916, 1.0130, 1.0361, 1.0178, 1.0288, 1.0943, 1.0254, 1.0932,\n",
      "        1.0225, 0.9969, 1.0483, 1.0138, 1.0475, 1.0560, 1.0308, 1.0149, 1.0659,\n",
      "        1.0496, 1.0327, 1.0585, 1.0088, 1.0533, 1.0587, 1.0017, 1.0372, 1.0104,\n",
      "        1.0476, 1.0089, 1.0262, 1.0588, 1.0031, 1.0393, 1.0153, 1.0527, 1.0350,\n",
      "        1.0230, 1.1048, 1.0438, 1.0562, 1.0787, 1.0798, 1.0069, 1.0465, 1.0197,\n",
      "        1.0574, 1.0887, 1.0507, 1.0307, 1.0410, 1.0302, 1.0299, 1.0315, 1.0135,\n",
      "        1.0214, 1.0707, 1.0321, 1.0357, 1.0057, 1.0579, 1.0603, 1.0529, 1.0457,\n",
      "        1.0758, 1.0392, 1.0133, 1.0939, 1.0209, 0.9829, 1.0161, 1.0406, 1.0188,\n",
      "        1.0195, 1.0696, 1.0549, 1.0186, 1.0516, 1.0240, 1.0055, 1.0768, 1.0526,\n",
      "        1.0950, 1.0771, 1.0412, 1.0531, 1.0068, 1.0706, 1.0415, 1.0456, 1.0440,\n",
      "        1.0662, 1.0423, 1.0919, 1.0246, 1.0229, 1.0328, 1.0970, 1.0104])\n",
      "tensor([-0.0502,  0.1835, -0.1844, -0.0763,  0.0981, -0.0632,  0.1781,  0.1799,\n",
      "        -0.1615, -0.0070,  0.0180, -0.1221,  0.0532, -0.0642,  0.1442, -0.0105,\n",
      "        -0.0110, -0.1797,  0.0989,  0.0290,  0.0524, -0.1219, -0.0814, -0.0964,\n",
      "         0.0850,  0.1046,  0.0888,  0.0730,  0.2475,  0.0987, -0.0807,  0.0227,\n",
      "         0.2099,  0.1751,  0.1787, -0.0726,  0.2360, -0.1719, -0.0662,  0.0993,\n",
      "        -0.1662,  0.1792,  0.0687, -0.0914,  0.2015,  0.1067, -0.1048,  0.2747,\n",
      "         0.0885, -0.2359, -0.1621,  0.0642, -0.0750,  0.0571,  0.1149, -0.1690,\n",
      "         0.1581,  0.0500,  0.1722, -0.1559,  0.1978,  0.1538,  0.2045, -0.2411,\n",
      "         0.1083,  0.1640, -0.1631, -0.0725,  0.1616,  0.0133, -0.0311, -0.2404,\n",
      "         0.0999, -0.1359, -0.1695, -0.1385, -0.0917,  0.1369,  0.0765,  0.1796,\n",
      "         0.1789,  0.1468, -0.2053,  0.0925, -0.0215,  0.1086,  0.2229,  0.0214,\n",
      "        -0.1603,  0.2229,  0.1009, -0.1305, -0.0197, -0.1939, -0.0919, -0.1292,\n",
      "         0.0940,  0.1249, -0.0296,  0.1658,  0.1623,  0.0304,  0.1712, -0.1425,\n",
      "         0.2158,  0.1802,  0.2118,  0.2391, -0.1955,  0.1230,  0.0807,  0.0138,\n",
      "        -0.0447, -0.1215,  0.0992, -0.1799, -0.1511, -0.0713,  0.1787,  0.2354,\n",
      "        -0.0908, -0.0950, -0.0093,  0.1304, -0.1023, -0.1671,  0.1482,  0.2177,\n",
      "        -0.0456, -0.1250,  0.0822, -0.0919,  0.1335,  0.2406, -0.1098, -0.0949,\n",
      "        -0.2020, -0.1000,  0.1127, -0.0588, -0.1748,  0.1471, -0.1005, -0.1136,\n",
      "         0.0628,  0.0628,  0.1542, -0.1733, -0.0977,  0.2574, -0.1552, -0.1288,\n",
      "         0.1105,  0.0325,  0.2301, -0.1985,  0.0388,  0.1362, -0.0735, -0.1593,\n",
      "        -0.1916,  0.1985, -0.1225, -0.1455, -0.1777,  0.2084, -0.0750, -0.2712,\n",
      "        -0.0989, -0.0199, -0.1780,  0.0529,  0.1424,  0.0915, -0.1351, -0.0340,\n",
      "         0.2285, -0.1627,  0.2559,  0.1475, -0.0866, -0.0639,  0.1266, -0.0215,\n",
      "        -0.1581, -0.0855, -0.1540, -0.1085,  0.2112,  0.1174, -0.0925,  0.0609,\n",
      "         0.2068, -0.2392, -0.0464, -0.1811, -0.0253, -0.0691,  0.1281, -0.0803,\n",
      "         0.0856, -0.0878,  0.1700, -0.1894,  0.1141,  0.2125, -0.1014,  0.1147,\n",
      "         0.0724, -0.1748,  0.1498, -0.2617, -0.0550, -0.0813, -0.1159, -0.1020,\n",
      "         0.0562, -0.1539,  0.1956,  0.1433, -0.1581,  0.0814,  0.0280, -0.1372,\n",
      "         0.1417, -0.1184,  0.0162,  0.1014, -0.0249,  0.1090,  0.1286,  0.1625,\n",
      "         0.0682, -0.1908,  0.1283, -0.1346,  0.0377,  0.1623,  0.2513, -0.1629,\n",
      "        -0.0419,  0.1360, -0.2449, -0.1193, -0.1957, -0.2435, -0.1803, -0.1047,\n",
      "        -0.1436, -0.1185, -0.2189, -0.0587, -0.1312, -0.1207, -0.2030,  0.0891,\n",
      "        -0.1623, -0.0724, -0.1750,  0.2263, -0.0869, -0.0677,  0.0577, -0.2483,\n",
      "         0.1628,  0.1870,  0.2058, -0.1071, -0.1557,  0.2590,  0.2499,  0.1801,\n",
      "        -0.1378,  0.1214, -0.2091, -0.1011, -0.1090, -0.1650, -0.0997,  0.2204,\n",
      "        -0.1191,  0.1421,  0.2736, -0.1255, -0.1790,  0.0421, -0.2486, -0.0122,\n",
      "         0.0617,  0.0741, -0.2152, -0.1700,  0.1184, -0.1648, -0.0906,  0.0261,\n",
      "        -0.0988, -0.0908,  0.0077,  0.1079, -0.1607, -0.1200, -0.0441, -0.1713,\n",
      "         0.0992,  0.1517, -0.0167,  0.1583,  0.0858,  0.1436, -0.0345,  0.1662,\n",
      "        -0.1596,  0.0313,  0.1974, -0.1423, -0.1691, -0.1880,  0.1543,  0.1643,\n",
      "         0.1695, -0.2335,  0.1707,  0.1607, -0.1587, -0.0932,  0.1273,  0.0040,\n",
      "        -0.0253, -0.0389, -0.0725,  0.1756, -0.0616, -0.1428, -0.0922, -0.1161,\n",
      "        -0.0915, -0.1616, -0.1734, -0.2097,  0.0257,  0.1436, -0.0120,  0.1366,\n",
      "         0.1910,  0.0631, -0.2381,  0.1449,  0.1477,  0.2067])\n",
      "tensor([[-0.0038,  0.0329, -0.0995,  ...,  0.0798, -0.0322, -0.0546],\n",
      "        [-0.0104,  0.0485,  0.0151,  ..., -0.0214,  0.0907, -0.0470],\n",
      "        [ 0.0438, -0.0056,  0.0693,  ..., -0.0544, -0.0832,  0.0021],\n",
      "        ...,\n",
      "        [-0.0098, -0.0338,  0.0007,  ...,  0.0295, -0.0061,  0.0332],\n",
      "        [-0.1034, -0.1140, -0.0630,  ..., -0.1009,  0.1711, -0.0071],\n",
      "        [ 0.1734,  0.0847,  0.0628,  ...,  0.0281, -0.1722,  0.0327]])\n",
      "tensor([ 0.0382,  0.0118, -0.1258,  0.1912, -0.1298,  0.0823, -0.0080, -0.1503,\n",
      "        -0.1260, -0.2725,  0.2707,  0.0453, -0.0321,  0.0530, -0.0065,  0.0183,\n",
      "         0.0470,  0.0210,  0.0005])\n"
     ]
    }
   ],
   "source": [
    "for p in net.model.parameters():\n",
    "    print(p.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4c8d5cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1953,  0.1680, -0.0635,  ...,  0.1309,  0.0337, -0.1151],\n",
      "        [ 0.0867, -0.1051, -0.0316,  ...,  0.1187,  0.0668,  0.1751],\n",
      "        [ 0.0473, -0.1043,  0.0623,  ...,  0.0617,  0.0875,  0.0812],\n",
      "        ...,\n",
      "        [-0.1474,  0.2562, -0.1020,  ...,  0.1505,  0.1124,  0.0671],\n",
      "        [-0.0777,  0.1288,  0.0823,  ...,  0.0869,  0.1883, -0.0504],\n",
      "        [ 0.0840,  0.1529,  0.0006,  ...,  0.0037, -0.1448, -0.0819]])\n",
      "tensor([-0.0420,  0.0951,  0.0312, -0.0081,  0.0587, -0.1466,  0.2107,  0.1487,\n",
      "         0.1520, -0.1944,  0.2045,  0.0413, -0.2018,  0.2153, -0.0358,  0.0860,\n",
      "         0.1789,  0.1633,  0.2059, -0.2002,  0.2350,  0.0600,  0.0048, -0.1823,\n",
      "        -0.1383,  0.2303,  0.1632,  0.1677, -0.0273, -0.1063,  0.0492, -0.0949,\n",
      "        -0.1412,  0.0960,  0.2115, -0.1593, -0.2120,  0.0359, -0.0733, -0.1013,\n",
      "        -0.0405,  0.0021, -0.0675,  0.1887,  0.0983,  0.0666, -0.0523, -0.0020,\n",
      "         0.1542,  0.0323, -0.2225,  0.0058,  0.0914, -0.0639, -0.0398, -0.0056,\n",
      "         0.1752,  0.1858, -0.1554,  0.1764,  0.1144,  0.0876, -0.0972, -0.1501,\n",
      "         0.2246,  0.0807, -0.0747, -0.0354,  0.1914, -0.0531,  0.1662, -0.0600,\n",
      "         0.0779,  0.0240, -0.1035,  0.0095,  0.0736, -0.2000, -0.0374,  0.0016,\n",
      "         0.2160,  0.0627, -0.1768, -0.0429,  0.2389, -0.0291,  0.1740,  0.0294,\n",
      "         0.1521, -0.1208,  0.2155,  0.0587, -0.0382,  0.1923,  0.1369, -0.1661,\n",
      "        -0.0383,  0.1908,  0.0487, -0.0722, -0.1540, -0.1394,  0.1258,  0.0887,\n",
      "        -0.2133, -0.0617,  0.0390, -0.1926,  0.1767, -0.0184, -0.1178,  0.0142,\n",
      "         0.0668,  0.0635,  0.2471,  0.1226, -0.0080,  0.0483,  0.0743, -0.0576,\n",
      "        -0.2042, -0.0077, -0.0815, -0.2545,  0.0935, -0.1144,  0.1985,  0.1057,\n",
      "         0.0985,  0.0948,  0.1168,  0.1817,  0.1892, -0.0115, -0.0900,  0.0812,\n",
      "        -0.0763,  0.0173,  0.0227,  0.1199, -0.2372,  0.1071, -0.0741, -0.1400,\n",
      "        -0.0908,  0.0041,  0.1035, -0.1785,  0.0705,  0.2223, -0.1057,  0.0658,\n",
      "        -0.0599,  0.0539,  0.1126, -0.1399,  0.0630,  0.2510,  0.0171, -0.1483,\n",
      "         0.0133,  0.0837,  0.0842, -0.0138, -0.0706, -0.0942,  0.1554, -0.1260,\n",
      "        -0.0869,  0.1748, -0.0714, -0.0172, -0.0855, -0.2041,  0.1035, -0.1029,\n",
      "        -0.0657, -0.0483,  0.0300,  0.2015,  0.2305,  0.0392, -0.1829, -0.0027,\n",
      "        -0.0087,  0.2097, -0.0060, -0.2116,  0.0826, -0.0370,  0.0819,  0.0447,\n",
      "         0.0854,  0.1396, -0.1457, -0.2050,  0.2179,  0.2240, -0.0849,  0.2222,\n",
      "         0.0769, -0.0500, -0.0975, -0.1374,  0.2060, -0.1493, -0.0998,  0.2307,\n",
      "        -0.1120,  0.2623,  0.0647,  0.1365,  0.0568, -0.0112,  0.0854,  0.0873,\n",
      "        -0.1450, -0.1785, -0.1525, -0.0049,  0.0812, -0.1227,  0.0298,  0.1519,\n",
      "         0.1340, -0.0955,  0.0767, -0.1878,  0.1554, -0.2018,  0.0120, -0.0688,\n",
      "        -0.2146, -0.2159, -0.1413, -0.1513,  0.1841, -0.0091, -0.1306,  0.0117,\n",
      "        -0.0905, -0.0299,  0.1972, -0.1873, -0.1885,  0.0476,  0.1555, -0.2267,\n",
      "         0.1065,  0.2575, -0.0319, -0.1194,  0.1209,  0.1444, -0.0176, -0.1857,\n",
      "         0.0285, -0.1385, -0.1897,  0.1472,  0.1508,  0.1102, -0.1667,  0.2024,\n",
      "         0.1495,  0.1775, -0.0183, -0.0266, -0.1959,  0.1660,  0.0427, -0.2218,\n",
      "         0.2647, -0.0276,  0.1316, -0.0358, -0.0556,  0.1974,  0.1943, -0.0783,\n",
      "         0.0486,  0.1609, -0.0061, -0.0325,  0.2107,  0.2260, -0.0718,  0.1607,\n",
      "        -0.0186,  0.0005,  0.1658, -0.0142, -0.0119, -0.0325, -0.1750,  0.1724,\n",
      "        -0.1811, -0.1445,  0.0497, -0.0234, -0.0048,  0.0098,  0.2449, -0.1720,\n",
      "        -0.1738,  0.0535, -0.2071, -0.1699,  0.1711,  0.0805, -0.0405,  0.0264,\n",
      "         0.0411,  0.0076,  0.2176,  0.0218,  0.1697, -0.1927, -0.0611,  0.0660,\n",
      "         0.1005, -0.0769,  0.2028, -0.1890, -0.1019,  0.1857, -0.1811, -0.2128,\n",
      "        -0.0502, -0.1674,  0.0790, -0.0419, -0.0695,  0.1500,  0.0051, -0.2067,\n",
      "        -0.2143,  0.1683,  0.1079,  0.0974, -0.1670, -0.0572,  0.2179,  0.2138,\n",
      "         0.1447,  0.2220, -0.0732, -0.1904,  0.1612,  0.1183])\n",
      "tensor([0.9876, 0.9732, 0.9717, 0.9571, 0.9969, 0.8806, 0.9383, 0.9872, 0.9477,\n",
      "        0.9999, 0.9958, 1.0090, 1.0272, 0.9836, 0.9664, 1.0089, 1.0163, 0.9851,\n",
      "        1.0318, 0.9381, 0.9890, 0.9987, 0.9585, 0.9903, 0.9450, 1.0055, 0.9883,\n",
      "        1.0069, 0.9798, 0.9367, 1.0301, 0.8865, 0.9855, 0.9663, 1.0016, 1.0088,\n",
      "        0.9436, 1.0309, 0.9903, 1.0137, 0.9859, 0.9917, 0.9678, 1.0066, 1.0132,\n",
      "        0.9745, 1.0426, 1.0319, 0.9452, 0.9263, 0.9539, 0.9884, 1.1107, 0.9634,\n",
      "        0.9438, 0.9904, 0.9994, 0.9958, 0.9554, 1.0168, 0.9822, 0.9986, 0.9905,\n",
      "        0.9490, 1.0077, 0.9666, 1.0173, 1.0055, 1.0086, 0.9943, 1.0355, 0.9284,\n",
      "        0.9905, 1.0190, 0.9525, 1.0012, 1.0274, 1.0323, 1.0351, 0.9576, 0.9871,\n",
      "        0.9554, 0.9525, 1.0019, 0.9971, 0.9638, 1.0101, 1.0171, 0.9996, 1.0180,\n",
      "        0.9854, 1.0099, 1.0209, 1.0005, 0.9331, 0.9726, 0.9744, 0.9592, 1.0168,\n",
      "        1.0218, 0.9647, 0.9997, 1.0280, 1.0161, 0.9868, 1.0282, 0.9760, 1.0335,\n",
      "        1.0120, 1.0005, 1.0416, 1.0144, 0.9715, 0.9642, 1.0234, 0.9686, 0.9775,\n",
      "        0.9882, 0.9016, 0.9810, 0.9528, 1.0044, 0.9909, 0.9800, 1.0269, 0.9687,\n",
      "        1.0287, 1.0331, 0.9892, 0.9905, 0.9840, 1.0151, 0.9665, 0.9681, 0.9741,\n",
      "        1.0685, 1.0326, 1.0224, 1.0117, 0.9803, 1.0398, 0.9917, 0.9640, 0.9844,\n",
      "        0.9658, 0.9616, 0.9712, 0.9383, 0.9605, 0.9733, 0.9847, 0.9394, 0.9815,\n",
      "        0.9873, 0.9633, 1.0141, 0.9572, 0.9889, 0.9757, 0.9768, 0.9471, 1.0227,\n",
      "        0.9793, 1.0163, 0.9610, 1.0159, 0.9665, 0.9939, 0.9979, 0.9825, 0.9825,\n",
      "        1.0311, 0.9825, 0.9387, 0.9971, 1.0116, 0.9784, 0.9990, 1.0233, 0.9883,\n",
      "        0.9847, 1.0040, 1.0182, 1.0052, 1.0286, 0.9638, 0.9834, 0.9327, 1.0020,\n",
      "        0.9888, 1.0293, 1.0108, 1.0154, 0.9932, 0.9569, 0.9970, 1.0317, 0.9356,\n",
      "        0.9586, 0.9773, 1.0184, 1.0135, 0.9889, 0.9646, 0.9962, 0.9580, 0.9724,\n",
      "        1.0467, 1.0254, 1.0492, 0.9878, 0.9785, 0.9797, 1.0324, 0.9661, 1.0153,\n",
      "        1.1279, 1.0275, 0.9710, 1.0122, 1.0071, 0.9557, 1.0304, 0.9727, 0.9929,\n",
      "        0.9788, 0.9790, 0.9538, 1.0066, 0.9330, 0.9869, 1.0231, 0.9023, 1.0602,\n",
      "        0.9983, 0.9848, 0.9980, 0.9792, 1.0043, 1.0105, 1.0139, 1.0067, 0.9718,\n",
      "        0.9786, 1.0083, 1.0180, 0.9944, 1.0019, 0.9856, 0.9976, 1.0322, 1.0159,\n",
      "        1.0089, 0.9851, 0.9921, 1.0216, 0.9849, 1.0107, 0.9207, 0.9506, 0.9598,\n",
      "        0.9959, 1.0032, 1.0031, 0.9599, 1.0341, 1.0138, 1.0077, 1.0555, 0.9791,\n",
      "        0.9861, 0.9860, 1.0536, 0.9600, 0.9666, 0.9889, 0.9602, 1.0032, 0.9970,\n",
      "        0.9496, 1.0005, 0.9759, 0.9785, 1.0330, 0.9781, 0.9771, 0.9923, 0.9996,\n",
      "        1.0021, 1.0380, 0.9738, 0.9838, 0.9986, 0.9733, 1.0243, 0.9883, 0.9801,\n",
      "        1.0121, 0.9954, 0.9608, 1.0066, 0.9687, 1.0039, 0.9619, 0.9652, 0.9475,\n",
      "        1.0092, 0.9501, 0.9932, 0.9762, 1.0101, 0.9583, 1.0459, 0.9962, 1.0045,\n",
      "        0.9610, 1.0021, 0.9717, 0.9444, 0.9808, 0.9838, 1.0179, 0.9713, 0.9820,\n",
      "        0.9592, 0.9755, 1.0178, 0.9685, 0.9045, 0.9556, 0.9841, 1.0322, 1.0321,\n",
      "        1.0045, 0.9795, 1.0122, 0.9814, 0.9705, 1.0038, 0.9853, 0.9874, 0.9861,\n",
      "        1.0022, 1.0543, 1.0052, 0.9722, 1.0072, 0.9843, 1.0134, 1.0498])\n",
      "tensor([-1.9262e-02,  6.8808e-02, -2.7709e-02, -7.9781e-03,  5.2086e-04,\n",
      "        -2.9641e-02,  1.2010e-02,  6.0496e-02,  9.3870e-03, -2.3443e-02,\n",
      "        -2.9899e-03,  9.0187e-03, -7.2212e-03,  2.9913e-02, -4.5215e-02,\n",
      "        -7.9863e-03, -2.5696e-02,  1.2925e-02,  2.7612e-02,  5.9847e-02,\n",
      "         5.9145e-02, -5.9123e-02, -3.0768e-02, -4.0015e-02, -3.8799e-02,\n",
      "         1.8588e-02, -7.5539e-03,  1.1407e-02, -7.9261e-02,  4.2057e-02,\n",
      "        -3.1660e-02,  7.5483e-04, -7.2974e-03,  6.6604e-02,  4.0746e-02,\n",
      "         5.8240e-03, -4.4103e-04,  5.7515e-02, -1.5165e-03, -3.2856e-03,\n",
      "        -2.1731e-02, -3.7585e-02, -1.2027e-02,  8.2008e-03, -2.8148e-02,\n",
      "        -6.8415e-02,  4.9045e-02, -5.1184e-03, -9.7163e-03, -3.9261e-02,\n",
      "         5.4607e-02, -3.0264e-02,  6.2059e-02, -4.5310e-02, -1.0581e-02,\n",
      "         5.4883e-02,  7.2326e-02,  6.4441e-03, -1.6695e-03, -1.3685e-02,\n",
      "         2.9680e-02,  4.5392e-02, -7.7423e-03,  1.5334e-02, -1.2557e-02,\n",
      "        -1.9720e-02, -6.5372e-02,  2.2941e-02,  4.6141e-02,  4.5977e-02,\n",
      "        -2.0632e-02,  2.4224e-03,  2.5363e-02,  1.8250e-02,  8.8735e-03,\n",
      "        -1.9366e-02,  3.0012e-02, -8.5280e-03,  2.8335e-02, -5.7338e-02,\n",
      "        -2.1394e-02, -5.4109e-03,  2.8922e-02,  2.3728e-02,  7.0688e-02,\n",
      "         2.2296e-02,  1.4364e-03, -3.7522e-02, -2.4035e-02,  2.5778e-02,\n",
      "        -4.2108e-02, -4.6456e-02,  1.2510e-02,  6.2311e-04,  3.5344e-03,\n",
      "         2.7247e-03, -5.3089e-03,  2.2763e-02,  5.1632e-02, -1.7072e-02,\n",
      "        -4.5645e-03, -5.1766e-02, -1.6763e-03,  7.8325e-03,  4.9228e-02,\n",
      "        -1.6008e-02,  1.8120e-02, -8.1276e-03,  1.6122e-02, -1.6607e-02,\n",
      "        -2.0517e-02,  1.3496e-02, -4.8073e-02, -5.6107e-02, -1.9892e-02,\n",
      "         6.2271e-02,  2.3159e-02, -3.6944e-02, -8.4918e-03,  2.2747e-02,\n",
      "         3.9289e-02, -3.4095e-02,  2.9631e-02,  4.2823e-02,  4.0381e-02,\n",
      "         1.6333e-02,  1.6300e-02,  1.3065e-02,  1.5338e-05,  3.2305e-02,\n",
      "         3.3070e-02,  5.6642e-03,  1.7595e-02, -2.7387e-02,  1.0444e-02,\n",
      "         2.5394e-02,  6.8261e-02,  5.8893e-02, -4.9872e-02, -1.5014e-02,\n",
      "        -1.4407e-02, -3.7862e-02,  7.1918e-03,  2.7607e-02, -6.7100e-03,\n",
      "         1.5155e-03, -2.5426e-02,  4.9831e-02, -2.0772e-02,  4.7268e-02,\n",
      "        -6.7425e-03, -3.7260e-02,  3.1881e-03, -4.7137e-02, -6.2286e-02,\n",
      "        -2.1851e-02, -6.3547e-02,  5.3632e-03,  5.4303e-02, -2.8698e-02,\n",
      "        -5.7086e-02,  4.1721e-02,  6.2883e-02,  3.5069e-03, -5.2867e-02,\n",
      "         4.0767e-02, -4.3008e-03,  2.2340e-02, -3.4104e-02, -1.0424e-02,\n",
      "         1.3295e-02, -8.9642e-03, -4.4534e-02, -2.2958e-02, -5.6079e-03,\n",
      "         1.9897e-03,  7.5531e-02,  1.9228e-02, -8.9603e-03, -4.1270e-02,\n",
      "         1.4835e-03,  4.6044e-02,  4.3999e-02, -1.2212e-02, -2.3107e-02,\n",
      "         4.4801e-04,  4.0472e-02,  4.3104e-02,  6.6777e-03, -4.2531e-03,\n",
      "         1.6991e-02,  2.1619e-02,  3.4818e-02,  7.6285e-03,  4.7863e-03,\n",
      "        -1.5506e-03, -2.5538e-02,  2.8417e-03, -9.8136e-03, -5.9801e-03,\n",
      "         8.0864e-02,  3.1428e-02,  7.5658e-03,  2.7218e-02,  4.6474e-02,\n",
      "         3.2926e-02, -7.1620e-03,  2.7260e-02,  2.4225e-03,  8.3858e-02,\n",
      "        -3.3439e-02, -9.5333e-03, -2.1884e-02,  6.9214e-02,  1.4129e-02,\n",
      "         1.1217e-02,  4.2525e-02,  3.2221e-02, -7.3203e-03,  3.9458e-02,\n",
      "        -5.6014e-02, -2.3412e-02,  3.0586e-02,  1.2310e-02,  2.1869e-02,\n",
      "         1.4144e-02, -7.4028e-03,  1.9991e-02, -7.8582e-03,  6.1067e-02,\n",
      "        -7.4784e-02, -1.2307e-02,  2.0536e-02,  8.2082e-03,  6.3744e-02,\n",
      "        -3.1241e-02,  9.5708e-03,  4.7774e-02,  2.4200e-02,  4.1907e-02,\n",
      "        -1.0739e-02,  8.8580e-03, -1.1545e-02, -3.9819e-02,  4.2797e-02,\n",
      "         1.7787e-02, -8.8647e-03,  5.6656e-02,  2.8186e-02, -3.8903e-03,\n",
      "         2.5539e-02,  9.4593e-03,  4.0041e-02,  3.4093e-02, -5.8309e-03,\n",
      "         5.9342e-04, -2.4656e-03, -3.3458e-03, -5.2046e-02,  2.9881e-02,\n",
      "         3.7359e-02,  1.3456e-02,  8.4703e-04, -6.1181e-04,  4.1887e-02,\n",
      "        -5.4249e-04, -2.7966e-02,  3.7510e-02,  2.2977e-02,  5.8251e-02,\n",
      "         3.2375e-02,  9.7487e-03, -8.5164e-03,  2.6924e-02,  4.9820e-02,\n",
      "        -1.3774e-02, -1.5499e-02,  1.6504e-02, -1.8427e-02,  5.2729e-03,\n",
      "        -1.0275e-02, -5.3591e-02, -1.8835e-02,  8.2620e-02, -2.6378e-02,\n",
      "         4.1757e-02,  3.7953e-02, -2.0136e-02,  2.1273e-02, -5.5847e-03,\n",
      "         3.5576e-02, -4.5933e-03,  1.6068e-03,  7.6787e-03, -2.7052e-02,\n",
      "         9.7230e-03,  1.1144e-02, -1.0734e-02,  3.5131e-04,  3.3855e-02,\n",
      "        -5.8945e-02, -1.8785e-02, -3.8598e-03,  4.9779e-02, -2.4780e-02,\n",
      "        -1.3345e-02,  4.9903e-02, -4.5732e-02, -1.1392e-02,  2.5641e-02,\n",
      "         1.4635e-02,  2.0806e-02,  5.4765e-02, -5.3246e-02, -9.5838e-03,\n",
      "        -1.7550e-02,  4.4272e-02, -1.3776e-02,  8.6036e-03, -6.5184e-03,\n",
      "         4.2923e-02, -1.9712e-02,  4.4422e-02,  3.8219e-02, -6.1315e-03,\n",
      "         2.4058e-02,  2.3787e-02,  7.7424e-03, -1.0414e-02,  2.1284e-02,\n",
      "         2.5402e-02,  4.5648e-03,  7.3262e-03,  8.0529e-02,  4.3396e-03,\n",
      "         6.5539e-03,  3.7631e-02,  8.4009e-03, -4.8994e-02,  3.6077e-02,\n",
      "        -1.8451e-02, -3.1641e-02, -4.3811e-03,  2.7165e-02,  4.0062e-02,\n",
      "        -3.3648e-02,  3.3398e-02,  4.2256e-02,  7.1011e-04,  3.7615e-03])\n",
      "tensor([[ 4.5963e-05, -4.7313e-02,  7.9125e-03,  ..., -9.2213e-03,\n",
      "          1.9835e-02, -1.7928e-02],\n",
      "        [ 8.3867e-02, -5.5072e-02,  3.5573e-03,  ..., -1.7408e-02,\n",
      "          1.0537e-02, -7.5854e-02],\n",
      "        [ 5.3811e-02, -3.3852e-02,  2.5569e-02,  ..., -4.8031e-02,\n",
      "         -3.3776e-02,  5.0088e-04],\n",
      "        ...,\n",
      "        [-2.1221e-02,  1.2443e-02, -1.2909e-02,  ...,  8.8199e-03,\n",
      "          2.2996e-02,  1.7484e-02],\n",
      "        [ 9.5624e-03, -3.3259e-02, -5.0231e-02,  ...,  5.4299e-03,\n",
      "          1.5280e-02, -3.6116e-02],\n",
      "        [-6.6424e-03, -5.2713e-02,  4.1261e-02,  ..., -1.1734e-02,\n",
      "         -4.2039e-02, -2.2081e-02]])\n",
      "tensor([ 0.0347,  0.0051,  0.0095, -0.0613, -0.0357,  0.0105,  0.0558,  0.0516,\n",
      "        -0.0672,  0.0390,  0.0753, -0.0219,  0.0642, -0.0380, -0.0130, -0.0293,\n",
      "        -0.0111,  0.0084,  0.0216, -0.1348,  0.0702,  0.0642, -0.0356, -0.0238,\n",
      "         0.0159, -0.0588,  0.0169,  0.0559,  0.0066, -0.0361,  0.0125,  0.0777,\n",
      "        -0.0338, -0.0348,  0.0715, -0.0424,  0.0228,  0.0021, -0.0316,  0.0101,\n",
      "        -0.0504,  0.0497,  0.0408, -0.0326, -0.0774, -0.0157, -0.0209, -0.0683,\n",
      "         0.0394,  0.0198, -0.0669,  0.0210,  0.0219, -0.0103, -0.0269, -0.0158,\n",
      "         0.0443,  0.0436, -0.0870, -0.0329,  0.0184, -0.0381,  0.0431, -0.0308,\n",
      "         0.0525, -0.0458,  0.0003, -0.0304,  0.0855,  0.0427, -0.0133, -0.0588,\n",
      "         0.0564, -0.0172, -0.0126, -0.0234, -0.0274, -0.0195, -0.0215,  0.0219,\n",
      "         0.0293,  0.0440, -0.0358,  0.0239, -0.0181,  0.0361,  0.0215, -0.0282,\n",
      "        -0.0839,  0.0183, -0.0273,  0.0317,  0.0186, -0.0165,  0.0730, -0.0456,\n",
      "        -0.0149,  0.0167, -0.0494,  0.0617, -0.0428,  0.0837,  0.0480,  0.0170,\n",
      "        -0.0738, -0.0597, -0.0234,  0.0124,  0.0297,  0.0357,  0.0571, -0.0033,\n",
      "        -0.0206,  0.0211,  0.0249,  0.0055,  0.0006, -0.0271,  0.0320,  0.0601,\n",
      "         0.0125, -0.0066, -0.0151,  0.0358,  0.0264, -0.0367,  0.0319, -0.0425,\n",
      "         0.0514,  0.0604,  0.0755, -0.0156, -0.0084, -0.0600, -0.0036, -0.0446,\n",
      "         0.0149, -0.0488,  0.0371,  0.0392, -0.0498,  0.0624, -0.0452,  0.0200,\n",
      "         0.0122, -0.0649, -0.0452,  0.0282,  0.0489,  0.0159, -0.0111,  0.1017,\n",
      "         0.0014,  0.0320,  0.0262,  0.0120,  0.0253,  0.0421, -0.0434,  0.0794,\n",
      "         0.0467,  0.0132,  0.0410, -0.0359,  0.0328,  0.0358, -0.0372,  0.0429,\n",
      "        -0.0420,  0.0654,  0.0344, -0.0057,  0.0499,  0.0241, -0.0843, -0.0776,\n",
      "         0.0463,  0.0297, -0.0286, -0.0033,  0.0049,  0.0754,  0.0847, -0.0822,\n",
      "         0.0425,  0.0146, -0.0111, -0.0633,  0.1065, -0.0200,  0.0361, -0.0030,\n",
      "         0.0325, -0.0193, -0.0583,  0.0008, -0.0360, -0.0694, -0.0402,  0.0691,\n",
      "        -0.0140,  0.0602,  0.0436, -0.0034,  0.0502,  0.0076,  0.0306,  0.0126,\n",
      "         0.0901,  0.0141, -0.0055,  0.0210,  0.0327, -0.0166,  0.0368,  0.0364,\n",
      "         0.1112, -0.0185,  0.0527, -0.0076,  0.0768, -0.0624, -0.0256,  0.0326,\n",
      "         0.0331,  0.0153,  0.0553, -0.0018, -0.0062,  0.0068, -0.0296,  0.0860,\n",
      "         0.0062, -0.0264,  0.0100,  0.0301, -0.0052, -0.0138, -0.0790,  0.0482,\n",
      "        -0.0117,  0.0628,  0.0103, -0.0185, -0.0306, -0.0096, -0.0322,  0.0600,\n",
      "         0.0560,  0.0210, -0.0396, -0.0521,  0.0719,  0.0526, -0.0093, -0.0116,\n",
      "        -0.0015,  0.0415,  0.0326, -0.0556,  0.0059, -0.0111,  0.0377, -0.0462,\n",
      "         0.0083,  0.0363, -0.0184, -0.0326,  0.0386, -0.0022,  0.0232, -0.0720,\n",
      "        -0.0076,  0.0488,  0.0310,  0.0105, -0.1059,  0.0330,  0.0413,  0.0241,\n",
      "         0.0617,  0.0466,  0.0069, -0.0063, -0.0048,  0.0447, -0.0254, -0.0395,\n",
      "         0.0316,  0.0522,  0.0195,  0.0515,  0.0340,  0.0359, -0.0091,  0.0488,\n",
      "        -0.0247, -0.0172, -0.0188,  0.0517, -0.0312,  0.0474, -0.0233, -0.0150,\n",
      "        -0.0277,  0.0050,  0.0154, -0.0784,  0.0475, -0.0153,  0.0793,  0.0154,\n",
      "        -0.0446, -0.0227,  0.0759, -0.0089, -0.0088,  0.0011, -0.0180,  0.0703,\n",
      "        -0.0581,  0.0309,  0.0466,  0.0260,  0.0026, -0.0349, -0.0324, -0.0205,\n",
      "        -0.0131, -0.0231,  0.0632,  0.0289, -0.0298, -0.0033, -0.0059,  0.0091,\n",
      "         0.0696,  0.0361, -0.0467,  0.0453,  0.0816, -0.0069, -0.0285, -0.0046,\n",
      "        -0.0467, -0.0251,  0.0364, -0.0050,  0.0135, -0.0198])\n",
      "tensor([1.0276, 0.9734, 0.9957, 1.0078, 1.0151, 1.0305, 0.9878, 1.0024, 0.9848,\n",
      "        0.9885, 0.9671, 0.9577, 0.9735, 0.9976, 0.9931, 1.0471, 1.0200, 1.0076,\n",
      "        1.0147, 1.0976, 1.0151, 1.0188, 1.0208, 1.0023, 0.9693, 0.9917, 1.0252,\n",
      "        1.0278, 0.9826, 1.0219, 0.9958, 0.9717, 1.0553, 0.9846, 1.0318, 1.0027,\n",
      "        0.9813, 1.0103, 1.0031, 0.9662, 0.9919, 0.9906, 0.9656, 0.9655, 1.0095,\n",
      "        0.9703, 1.0270, 1.0539, 0.9987, 1.0018, 1.0788, 1.0008, 1.0141, 1.0225,\n",
      "        0.9876, 1.0063, 1.0317, 1.0050, 0.9833, 0.9641, 1.0407, 1.0003, 1.0027,\n",
      "        1.0242, 1.0027, 1.0354, 1.0505, 0.9687, 0.9794, 0.9811, 0.9985, 0.9983,\n",
      "        0.9880, 1.0080, 0.9740, 0.9943, 0.9915, 1.0074, 1.0049, 0.9850, 1.0111,\n",
      "        0.9620, 1.0144, 1.0112, 0.9646, 0.9897, 1.0078, 1.0050, 1.0339, 1.0247,\n",
      "        0.9894, 0.9860, 1.0322, 1.0284, 0.9617, 0.9929, 0.9691, 0.9840, 0.9773,\n",
      "        1.0174, 1.0004, 0.9786, 1.0732, 1.0262, 1.0148, 1.0317, 1.0114, 1.0083,\n",
      "        0.9789, 1.0044, 0.9950, 1.0026, 0.9888, 1.0235, 0.9867, 0.9979, 1.0289,\n",
      "        0.9779, 0.9652, 1.0048, 0.9974, 0.9973, 1.0047, 0.9879, 1.0275, 1.0569,\n",
      "        1.0324, 0.9800, 0.9852, 0.9513, 0.9791, 1.0330, 1.0028, 0.9975, 0.9552,\n",
      "        1.0100, 1.0201, 0.9832, 0.9597, 1.0090, 1.0094, 0.9727, 0.9789, 0.9863,\n",
      "        0.9616, 0.9913, 1.0209, 0.9954, 1.0049, 0.9845, 1.0359, 1.0044, 1.0041,\n",
      "        0.9999, 0.9971, 0.9938, 0.9813, 1.0475, 0.9600, 0.9691, 0.9972, 1.0279,\n",
      "        0.9597, 0.9878, 0.9768, 0.9905, 1.0116, 0.9689, 0.9886, 1.0035, 1.0130,\n",
      "        1.0079, 0.9864, 1.0016, 1.0102, 0.9657, 0.9814, 1.0073, 0.9785, 0.9734,\n",
      "        1.0022, 0.9741, 0.9904, 0.9756, 0.9953, 0.9937, 0.9845, 0.9992, 0.9822,\n",
      "        0.9847, 0.9808, 0.9834, 1.0167, 0.9824, 0.9566, 1.0134, 0.9866, 1.0094,\n",
      "        0.9836, 1.0023, 1.0117, 1.0030, 0.9680, 0.9864, 1.0629, 0.9799, 0.9989,\n",
      "        0.9914, 1.0161, 0.9927, 1.0228, 1.0208, 1.0056, 0.9903, 0.9812, 1.0128,\n",
      "        0.9802, 0.9686, 0.9837, 0.9587, 0.9947, 1.0057, 1.0099, 0.9736, 0.9867,\n",
      "        1.0106, 1.0019, 0.9777, 1.0266, 0.9839, 1.0092, 0.9791, 1.0120, 0.9951,\n",
      "        0.9892, 1.0080, 0.9924, 1.0151, 1.0514, 0.9986, 0.9767, 0.9932, 1.0072,\n",
      "        1.0096, 0.9693, 0.9893, 0.9941, 1.0092, 0.9834, 0.9917, 1.0273, 1.0228,\n",
      "        0.9961, 0.9971, 0.9910, 1.0073, 1.0079, 0.9845, 0.9848, 0.9819, 0.9515,\n",
      "        0.9866, 0.9800, 1.0095, 1.0334, 1.0131, 0.9525, 0.9887, 1.0006, 0.9878,\n",
      "        1.0286, 0.9987, 0.9834, 0.9541, 0.9848, 0.9973, 1.1001, 0.9669, 0.9908,\n",
      "        0.9613, 1.0240, 0.9648, 1.0000, 1.0100, 0.9762, 1.0238, 0.9801, 0.9944,\n",
      "        1.0194, 0.9932, 0.9892, 0.9858, 1.0204, 1.0209, 1.0051, 0.9804, 0.9982,\n",
      "        1.0412, 0.9724, 0.9410, 1.0054, 1.0076, 1.0047, 0.9616, 1.0797, 0.9864,\n",
      "        0.9831, 1.0180, 0.9940, 1.0127, 0.9759, 1.0594, 0.9919, 0.9868, 0.9981,\n",
      "        0.9792, 1.0028, 0.9713, 0.9710, 0.9807, 1.0127, 0.9971, 0.9967, 0.9640,\n",
      "        0.9967, 0.9670, 0.9862, 0.9880, 0.9866, 0.9486, 0.9962, 1.0195, 1.0055,\n",
      "        1.0060, 1.0463, 0.9530, 1.0020, 0.9688, 1.0120, 1.0150, 1.0151, 0.9696,\n",
      "        1.0005, 1.0241, 0.9665, 0.9750, 1.0198, 1.0150, 0.9713, 1.0306])\n",
      "tensor([-3.5350e-02,  1.4544e-02,  1.3029e-02, -4.5930e-02,  1.1225e-02,\n",
      "         1.1952e-02, -2.6069e-03, -2.4680e-02,  1.3814e-02, -5.9603e-03,\n",
      "         3.6558e-02,  2.7692e-02, -6.4048e-03,  1.0037e-02, -1.0070e-02,\n",
      "        -1.7275e-02,  1.6674e-02, -2.7602e-02, -4.0532e-02,  4.6684e-02,\n",
      "        -3.0559e-02, -2.1456e-02,  3.3872e-02, -2.3979e-02,  7.0430e-04,\n",
      "         7.2112e-03,  1.2325e-02, -3.6426e-02, -2.0055e-02,  4.4589e-02,\n",
      "        -9.8862e-03,  2.4613e-02, -1.7372e-02,  1.1600e-02, -1.6265e-02,\n",
      "        -2.4330e-02,  4.6533e-03,  5.4278e-02, -2.9920e-03,  2.8152e-02,\n",
      "        -2.0078e-02,  2.3122e-02,  2.5459e-02, -2.8040e-02, -8.7775e-03,\n",
      "         1.8702e-02,  2.7632e-03, -4.6198e-02,  2.9821e-02,  2.2518e-02,\n",
      "         5.2416e-02, -1.1100e-02,  1.0222e-02,  1.5708e-02,  2.8826e-02,\n",
      "         1.2678e-02,  2.2529e-02, -3.5432e-02,  3.4163e-02, -2.9744e-03,\n",
      "        -1.1407e-02, -2.2160e-02,  3.1458e-02,  3.6269e-02,  6.3464e-03,\n",
      "         8.0826e-03,  1.7367e-02,  2.9274e-02, -3.4223e-02,  5.6659e-03,\n",
      "        -1.0145e-02,  3.9194e-04, -6.9186e-03, -2.0809e-02,  4.4090e-02,\n",
      "        -1.0095e-02, -2.8433e-02,  2.2805e-02,  2.9220e-02,  1.3711e-02,\n",
      "        -5.9642e-02,  1.4878e-02,  3.5361e-02, -2.7359e-02,  1.8694e-02,\n",
      "         1.5357e-02, -1.2007e-02, -8.3162e-03, -1.1112e-02, -2.2560e-02,\n",
      "         7.3673e-04, -6.8736e-03, -3.9925e-03, -2.3595e-02,  2.7152e-02,\n",
      "        -1.8236e-02,  4.7175e-02,  1.2756e-02,  6.7276e-03,  4.3013e-02,\n",
      "         3.2018e-02,  4.6339e-03, -3.7869e-03, -5.3183e-02, -8.4378e-03,\n",
      "         2.2388e-02,  1.6571e-02, -3.7221e-02, -1.2483e-02,  2.1794e-03,\n",
      "         2.2247e-02, -7.0703e-04, -3.0174e-02,  2.5105e-02, -2.7229e-02,\n",
      "        -4.7152e-02, -2.7540e-02, -2.2321e-02,  1.1478e-02,  2.0905e-02,\n",
      "        -3.0561e-02, -2.3679e-02,  2.1322e-02, -2.3602e-03, -5.9481e-02,\n",
      "        -1.6024e-02, -1.7491e-02, -1.1836e-03, -3.1539e-03, -1.3152e-02,\n",
      "         5.6931e-02, -2.6342e-02, -1.2109e-02, -8.2168e-03,  5.0936e-02,\n",
      "        -3.3346e-02,  4.2103e-02, -2.3183e-02,  3.3327e-02,  4.6228e-02,\n",
      "        -3.4010e-02, -2.2589e-02, -1.4561e-03,  2.6052e-02,  3.9765e-02,\n",
      "        -5.0117e-02,  2.3261e-04,  7.1917e-03, -1.5897e-02,  1.8670e-02,\n",
      "        -2.5867e-02, -4.0815e-02,  5.3983e-03, -2.0648e-02, -1.3693e-02,\n",
      "        -1.7737e-03, -2.7294e-02,  5.2415e-03, -1.6004e-02,  5.4567e-02,\n",
      "        -4.5197e-02, -3.4419e-02,  9.3330e-03,  3.2222e-02, -2.8663e-03,\n",
      "         1.3755e-02,  2.2604e-02, -5.0868e-03, -3.6231e-02, -3.9697e-04,\n",
      "         1.4922e-02, -5.7933e-02,  1.9755e-02, -2.3488e-02,  1.7444e-02,\n",
      "         4.5066e-02,  4.5853e-03,  6.8704e-03,  8.0424e-03, -3.4096e-02,\n",
      "        -3.7074e-02, -1.4356e-02, -3.6809e-02,  6.3309e-03,  1.8600e-02,\n",
      "         1.2181e-03,  9.5870e-03,  4.6199e-02, -2.0432e-02,  4.0745e-02,\n",
      "        -1.9347e-02,  1.6188e-02, -1.7725e-03, -4.5266e-02,  6.9166e-04,\n",
      "         1.7140e-03,  4.3890e-02, -4.2235e-02, -3.1289e-02,  1.4253e-02,\n",
      "         6.0712e-02, -7.4071e-02,  3.0854e-03, -1.1153e-03, -2.3178e-02,\n",
      "         1.4160e-02,  1.0558e-02, -1.0060e-02, -8.1522e-03,  2.4040e-03,\n",
      "        -8.1727e-04, -1.2081e-04,  2.2357e-02,  3.5634e-02,  8.0477e-03,\n",
      "         1.7610e-02,  1.1644e-02, -8.5299e-03, -1.2960e-02,  1.3231e-02,\n",
      "         4.2892e-02, -8.0465e-03,  2.3163e-02, -2.7743e-02,  1.5291e-03,\n",
      "         3.0255e-02,  6.4247e-03,  3.0443e-02, -3.9913e-03,  2.3711e-02,\n",
      "        -1.0543e-02, -1.9637e-02, -1.9586e-02, -1.0059e-02,  3.0858e-02,\n",
      "        -2.7976e-02, -1.8511e-02, -2.7751e-02, -2.1409e-02, -2.6789e-02,\n",
      "         3.6398e-03, -2.3267e-03,  2.8849e-02, -1.5588e-02, -1.6449e-02,\n",
      "        -8.7423e-03, -2.2012e-02, -6.6562e-03,  3.0040e-02,  9.0777e-03,\n",
      "         2.4204e-03, -2.1653e-02, -3.8930e-02, -2.4384e-02,  1.4804e-02,\n",
      "         3.4419e-02, -3.7753e-02, -1.3193e-03,  8.1317e-03,  2.0632e-02,\n",
      "         8.0026e-03, -7.0535e-03,  8.9526e-03, -3.6372e-02, -6.1832e-03,\n",
      "        -1.1020e-02,  3.0251e-02,  2.0859e-03,  3.0837e-02, -9.9613e-03,\n",
      "        -1.5744e-02,  2.5834e-03, -2.9797e-02, -2.1005e-02,  1.4507e-02,\n",
      "        -1.1474e-02, -1.8025e-02,  1.4232e-02,  5.2274e-03, -1.2668e-02,\n",
      "        -9.8186e-04,  4.0867e-03, -8.1167e-03, -3.4265e-02,  6.0216e-03,\n",
      "         5.6454e-03, -1.5457e-03,  1.6552e-02,  3.9761e-02, -1.0269e-02,\n",
      "         1.0324e-02, -1.6654e-03,  3.3594e-03,  2.1380e-02,  1.7027e-02,\n",
      "         7.5875e-02,  1.0034e-02, -1.7652e-02,  1.1878e-02, -1.9520e-03,\n",
      "         2.4688e-03,  1.6417e-02,  1.3519e-02, -1.3754e-02, -3.2320e-02,\n",
      "        -1.3942e-02, -2.8841e-02,  2.1106e-02, -2.8115e-03, -2.7969e-02,\n",
      "        -1.1159e-02, -1.4443e-02, -8.7004e-03, -5.0777e-03, -2.0218e-02,\n",
      "        -1.5662e-02,  1.0083e-02,  1.0443e-02,  1.6605e-02, -1.9987e-02,\n",
      "         3.2370e-03, -6.3985e-02, -1.8802e-02, -1.9201e-02,  3.3417e-02,\n",
      "        -4.8423e-02, -1.1690e-02, -1.5085e-02,  4.6856e-03, -3.2940e-02,\n",
      "        -1.3441e-02, -2.2589e-03,  2.2053e-02,  1.9735e-02, -1.7714e-02,\n",
      "        -5.1835e-03,  4.4337e-03,  3.8068e-02,  4.9846e-06, -7.1225e-02,\n",
      "         3.4681e-02,  6.9338e-03, -1.8490e-02, -2.1908e-02,  7.6246e-03,\n",
      "         1.0069e-02, -2.1550e-02,  1.0254e-02, -1.8278e-02, -2.6174e-03])\n",
      "tensor([[-0.0770,  0.0347,  0.0074,  ..., -0.0336,  0.0143,  0.0743],\n",
      "        [-0.0192,  0.0327,  0.0788,  ..., -0.0576, -0.0430,  0.0561],\n",
      "        [-0.0069,  0.0412, -0.0471,  ..., -0.0413, -0.0583,  0.0006],\n",
      "        ...,\n",
      "        [-0.0324, -0.0533, -0.0025,  ...,  0.0153,  0.0354, -0.0625],\n",
      "        [-0.0351,  0.0120, -0.0232,  ...,  0.0581, -0.0030,  0.0478],\n",
      "        [ 0.0375, -0.0448, -0.0452,  ...,  0.0433, -0.0350, -0.0641]])\n",
      "tensor([ 3.1220e-02, -6.4367e-02, -5.0851e-02,  2.5917e-02, -4.5012e-02,\n",
      "         2.9362e-02,  1.9246e-02,  6.8199e-02, -4.2455e-03,  5.2977e-02,\n",
      "         2.5955e-02, -1.9022e-02,  8.4306e-02,  7.8209e-03,  1.0526e-02,\n",
      "        -2.5088e-02,  8.5779e-02, -2.4915e-02, -3.8811e-02, -1.5853e-02,\n",
      "         7.4089e-03,  5.5927e-02,  4.8042e-02,  3.3678e-02, -1.2334e-02,\n",
      "        -1.0018e-02,  2.4896e-02,  1.2468e-02,  4.9789e-02,  1.7878e-02,\n",
      "        -1.3859e-02, -3.1982e-03, -4.6200e-02,  4.8855e-02,  9.1821e-02,\n",
      "         4.9626e-02, -4.2201e-02,  2.3957e-02,  4.0595e-03, -3.7398e-02,\n",
      "         3.7986e-02,  4.5612e-02, -1.6361e-03, -3.9586e-02,  1.7108e-02,\n",
      "        -4.2307e-02,  3.8448e-02,  5.3015e-03,  3.5367e-02, -3.5653e-02,\n",
      "         1.0286e-02, -4.9044e-02, -3.8599e-02, -5.5869e-02,  1.1401e-02,\n",
      "         2.6030e-02, -1.6078e-02, -9.1632e-03, -1.1468e-02,  4.9287e-02,\n",
      "        -1.2999e-02,  4.6635e-02,  2.8266e-02, -1.1957e-03, -4.4946e-02,\n",
      "         2.2004e-02, -3.7596e-02,  2.6022e-02,  6.0958e-02,  2.8824e-02,\n",
      "         6.2551e-02, -2.0607e-02, -5.0752e-03,  7.7025e-02,  4.8286e-02,\n",
      "         6.0685e-02,  5.0243e-02, -1.3424e-03, -3.2029e-02,  1.2732e-02,\n",
      "         6.8487e-02,  1.6726e-02,  2.5311e-02,  4.0938e-02, -3.7663e-02,\n",
      "        -6.9774e-03, -2.6668e-02,  4.3938e-02,  1.9049e-02,  2.5011e-02,\n",
      "         5.0649e-03,  2.7766e-02, -1.8677e-02, -6.5871e-03, -1.0926e-02,\n",
      "        -4.5367e-02, -1.4554e-02, -8.9167e-03,  1.2815e-02,  1.9678e-02,\n",
      "        -6.5054e-02,  7.9996e-03, -5.6171e-04,  2.2121e-02, -7.9475e-02,\n",
      "        -1.4613e-02,  1.1928e-02, -7.1760e-03,  2.1885e-02,  7.4417e-02,\n",
      "        -3.4661e-02,  4.3362e-02, -8.3427e-03,  1.1697e-02, -4.1059e-02,\n",
      "        -2.2433e-02,  2.9944e-02, -4.1845e-02, -3.2012e-02,  2.7519e-02,\n",
      "         3.0520e-02, -6.6486e-03,  1.9922e-02,  4.1011e-05,  2.4321e-02,\n",
      "        -3.4112e-02, -8.1252e-03, -1.3817e-03,  4.1487e-02,  5.5383e-03,\n",
      "         7.4264e-02,  5.8538e-02, -1.2504e-02, -5.8637e-02, -3.2831e-02,\n",
      "        -3.8373e-02,  3.7144e-02, -4.6573e-02, -4.4477e-03,  1.4093e-02,\n",
      "         2.4116e-02,  3.7189e-02,  1.8752e-02,  2.8613e-02,  3.1591e-02,\n",
      "        -5.8729e-02, -2.5585e-02,  1.1315e-02,  3.4952e-02,  4.1815e-02,\n",
      "        -4.2700e-02,  9.5975e-02, -5.4345e-02,  4.4778e-02,  3.4699e-02,\n",
      "        -1.4143e-03, -4.2060e-02,  4.5085e-03,  3.6229e-02,  8.6210e-02,\n",
      "         5.6177e-02, -2.5983e-02,  5.1932e-02, -2.1423e-02,  1.7566e-02,\n",
      "         8.7870e-02,  5.8296e-02,  7.4138e-02,  8.4713e-02,  2.6736e-02,\n",
      "         2.2297e-02, -3.9376e-02,  2.6866e-02,  1.8884e-02, -6.2928e-02,\n",
      "        -1.1927e-01,  7.9880e-03,  2.8906e-02,  4.5301e-02, -7.4961e-02,\n",
      "         3.2721e-02,  7.2418e-02,  6.7919e-02, -3.9072e-02, -4.5895e-03,\n",
      "        -1.5853e-03, -3.0371e-02, -3.7048e-02,  5.1904e-02,  4.0411e-02,\n",
      "        -8.3111e-04,  7.5006e-02, -1.7369e-02, -1.9430e-02,  8.6174e-03,\n",
      "        -1.6283e-02, -1.4449e-02, -4.4124e-03,  6.7219e-02,  2.5801e-03,\n",
      "         3.5575e-02,  2.3887e-02,  7.3724e-02, -2.7450e-02,  5.9700e-02,\n",
      "        -4.4520e-02,  1.5025e-02,  4.0414e-02,  1.2516e-02, -6.9264e-02,\n",
      "        -5.9417e-02, -2.6525e-02,  6.7678e-02, -1.2832e-02,  5.0105e-03,\n",
      "         4.0910e-02,  1.0097e-01, -3.5914e-02,  2.8993e-02, -4.8939e-03,\n",
      "         1.0175e-01, -4.1305e-02,  1.1802e-02, -3.7813e-02, -2.6506e-02,\n",
      "         2.8449e-02,  7.3423e-02,  1.0151e-02, -6.7958e-02,  3.9225e-03,\n",
      "        -2.4804e-02, -3.9276e-03,  5.4167e-02, -5.4194e-02,  4.5707e-02,\n",
      "         7.6104e-03, -1.4399e-02,  1.5504e-02, -1.5404e-02,  4.8668e-02,\n",
      "        -6.3860e-02,  7.4544e-02, -7.2998e-02,  2.2758e-02,  1.6362e-02,\n",
      "         4.3762e-02, -2.2281e-03,  4.9973e-02,  4.8246e-02,  5.3134e-02,\n",
      "        -3.1200e-02,  1.8368e-03, -1.1808e-02, -8.7060e-03,  2.1497e-02,\n",
      "        -5.0635e-02, -5.4812e-02,  7.8076e-02,  8.1736e-02,  2.0540e-02,\n",
      "         4.8063e-02, -1.1393e-02, -1.6909e-02,  3.8772e-02,  2.5120e-03,\n",
      "        -1.0431e-02, -5.7217e-02, -2.5625e-02, -1.8534e-03,  3.7000e-02,\n",
      "        -6.7892e-04, -8.4087e-03, -3.5301e-02,  5.0963e-02,  4.7199e-02,\n",
      "        -1.0314e-02, -5.1786e-02, -1.9911e-02,  7.9702e-02, -4.2759e-02,\n",
      "         4.7687e-02, -1.4493e-02, -1.4170e-03,  4.3998e-02, -4.6188e-02,\n",
      "         3.0044e-02, -1.5390e-02, -3.2464e-02, -2.9193e-03, -1.5669e-02,\n",
      "        -8.2792e-03,  3.3157e-02,  2.0329e-02,  1.2330e-02, -1.9292e-02,\n",
      "         3.7264e-02,  4.6310e-02,  2.3605e-02, -1.8521e-03,  5.1313e-02,\n",
      "        -2.3791e-02, -2.9857e-02, -4.8621e-02, -5.4848e-03,  3.3813e-03,\n",
      "         5.3147e-02,  4.2042e-02, -5.6409e-02,  8.6589e-02,  6.4586e-02,\n",
      "         3.0701e-02, -4.3648e-02,  8.4899e-03, -2.5204e-02,  1.7927e-02,\n",
      "         3.1091e-02, -4.4601e-02, -2.4615e-02, -2.9417e-02, -1.9232e-02,\n",
      "        -6.1471e-02, -5.9971e-02, -9.8965e-03,  2.1335e-02, -6.8786e-03,\n",
      "        -1.6642e-02,  1.9179e-02, -1.7309e-02,  1.4843e-03,  4.9123e-02,\n",
      "         7.0532e-02,  4.9847e-02,  7.3972e-02,  7.2877e-02, -6.6248e-03,\n",
      "        -3.3546e-02,  6.9865e-02, -3.2413e-02,  4.0398e-02,  2.2823e-02,\n",
      "         7.1041e-02,  1.2250e-02,  2.2351e-02,  6.0800e-03,  1.6917e-03,\n",
      "        -2.5096e-02, -3.3802e-03, -4.1258e-02,  7.3368e-02,  1.6909e-02])\n",
      "tensor([0.9919, 0.9576, 0.9811, 0.9651, 0.9635, 0.9802, 0.9853, 1.0039, 0.9933,\n",
      "        0.9623, 0.9718, 0.9842, 0.9897, 0.9812, 0.9484, 0.9757, 0.9711, 0.9700,\n",
      "        0.9854, 0.9577, 0.9423, 0.9521, 0.9719, 0.9733, 0.9581, 0.9735, 0.9702,\n",
      "        0.9868, 0.9906, 0.9787, 1.0252, 0.9752, 0.9649, 0.9821, 0.9929, 0.9642,\n",
      "        0.9635, 1.0294, 0.9881, 0.9807, 0.9316, 0.9410, 0.9825, 0.9654, 0.9852,\n",
      "        0.9745, 0.9801, 0.9946, 0.9821, 0.9462, 0.9836, 0.9553, 0.9902, 0.9726,\n",
      "        1.0154, 0.9567, 0.9701, 0.9774, 0.9715, 0.9715, 0.9402, 0.9711, 0.9590,\n",
      "        0.9445, 0.9643, 0.9546, 0.9834, 0.9694, 0.9546, 0.9643, 0.9710, 1.0158,\n",
      "        0.9915, 0.9437, 0.9822, 0.9569, 0.9762, 0.9644, 1.0053, 0.9721, 0.9582,\n",
      "        0.9626, 0.9433, 0.9915, 0.9699, 0.9544, 0.9871, 0.9702, 0.9716, 0.9834,\n",
      "        0.9841, 0.9715, 0.9696, 0.9884, 0.9725, 0.9669, 0.9538, 1.0165, 0.9846,\n",
      "        0.9789, 0.9878, 0.9689, 0.9682, 0.9725, 1.0017, 0.9300, 0.9205, 0.9698,\n",
      "        0.9573, 0.9611, 0.9688, 0.9783, 0.9792, 0.9616, 0.9552, 0.9684, 0.9823,\n",
      "        1.0161, 0.9550, 0.9911, 0.9747, 0.9865, 0.9784, 0.9777, 0.9633, 0.9911,\n",
      "        0.9986, 0.9640, 0.9686, 0.9618, 0.9600, 0.9759, 0.9681, 0.9452, 0.9651,\n",
      "        0.9637, 0.9612, 0.9723, 0.9511, 0.9703, 0.9575, 0.9596, 0.9730, 0.9417,\n",
      "        0.9548, 0.9622, 0.9938, 0.9807, 0.9678, 0.9602, 0.9780, 0.9688, 0.9867,\n",
      "        0.9611, 0.9707, 0.9819, 0.9795, 0.9631, 0.9705, 0.9447, 0.9796, 0.9750,\n",
      "        0.9664, 0.9696, 0.9893, 0.9526, 0.9593, 0.9587, 1.0137, 1.0085, 0.9536,\n",
      "        0.9850, 0.9744, 0.9593, 0.9750, 1.0203, 0.9568, 0.9746, 0.9660, 1.0010,\n",
      "        0.9763, 0.9566, 0.9680, 0.9614, 0.9804, 0.9702, 0.9783, 0.9279, 0.9528,\n",
      "        0.9855, 0.9604, 0.9749, 0.9695, 0.9855, 0.9655, 0.9736, 0.9755, 0.9745,\n",
      "        0.9705, 0.9568, 0.9915, 0.9834, 0.9710, 0.9521, 0.9838, 0.9748, 0.9512,\n",
      "        0.9922, 0.9635, 0.9585, 1.0037, 0.9800, 0.9741, 0.9556, 0.9806, 0.9967,\n",
      "        0.9761, 0.9767, 0.9504, 0.9778, 0.9653, 0.9860, 0.9464, 0.9659, 0.9700,\n",
      "        1.0087, 0.9511, 0.9576, 1.0116, 0.9804, 0.9625, 0.9418, 0.9561, 0.9623,\n",
      "        0.9672, 0.9622, 0.9743, 0.9694, 0.9770, 0.9532, 0.9772, 0.9615, 0.9806,\n",
      "        0.9910, 0.9729, 0.9506, 1.0117, 0.9687, 0.9665, 0.9522, 1.0053, 0.9685,\n",
      "        0.9557, 0.9974, 0.9603, 0.9674, 0.9676, 0.9719, 0.9860, 0.9819, 0.9524,\n",
      "        0.9787, 0.9420, 0.9621, 1.0045, 0.9771, 0.9721, 0.9713, 0.9587, 0.9708,\n",
      "        0.9509, 0.9782, 0.9734, 0.9727, 0.9706, 0.9772, 1.0704, 0.9742, 0.9759,\n",
      "        0.9552, 0.9682, 0.9605, 0.9692, 0.9558, 0.9604, 1.0157, 1.0005, 0.9906,\n",
      "        0.9638, 0.9870, 0.9871, 0.9754, 0.9550, 0.9604, 0.9325, 0.9642, 0.9680,\n",
      "        0.9502, 0.9823, 0.9651, 0.9645, 0.9758, 0.9553, 0.9847, 0.9827, 0.9744,\n",
      "        0.9791, 0.9517, 0.9694, 0.9378, 0.9612, 1.0090, 0.9761, 0.9719, 0.9685,\n",
      "        0.9395, 0.9740, 0.9664, 0.9775, 0.9613, 0.9796, 0.9979, 0.9767, 0.9550,\n",
      "        0.9737, 0.9677, 0.9341, 0.9586, 0.9828, 0.9518, 0.9707, 0.9615, 0.9807,\n",
      "        0.9683, 0.9838, 0.9513, 0.9552, 0.9736, 0.9624, 0.9662, 0.9757, 0.9819,\n",
      "        0.9939, 0.9508, 0.9852, 0.9840, 0.9838, 0.9646, 0.9495, 0.9699])\n",
      "tensor([-0.0226,  0.0538, -0.0304, -0.0098, -0.0627, -0.0219,  0.0288, -0.0105,\n",
      "        -0.0439,  0.0502, -0.0290,  0.0008,  0.0318, -0.0215,  0.0049,  0.0006,\n",
      "        -0.0370, -0.0284, -0.0326, -0.0080,  0.0376, -0.0410, -0.0019, -0.0089,\n",
      "         0.0106,  0.0117,  0.0672,  0.0755, -0.0321, -0.0181, -0.0018, -0.0104,\n",
      "        -0.0053,  0.0507, -0.0463, -0.0272,  0.0375,  0.0580,  0.0277, -0.0274,\n",
      "         0.0584,  0.0443,  0.0370, -0.0239,  0.0466,  0.0283, -0.0166,  0.0271,\n",
      "        -0.0219,  0.0404, -0.0206, -0.0433, -0.0419, -0.0040, -0.0287, -0.0216,\n",
      "        -0.0360,  0.0113, -0.0292, -0.0121,  0.0128,  0.0073,  0.0142, -0.0043,\n",
      "         0.0455, -0.0239,  0.0687, -0.0356, -0.0038, -0.0295, -0.0279,  0.0225,\n",
      "        -0.0423,  0.0161, -0.0070,  0.0353,  0.0574,  0.0290, -0.0213,  0.0148,\n",
      "         0.0412,  0.0224,  0.0521,  0.0329, -0.0406,  0.0632,  0.0655, -0.0137,\n",
      "        -0.0107, -0.0483, -0.0255,  0.0113, -0.0397, -0.0186, -0.0219, -0.0272,\n",
      "         0.0295, -0.0540,  0.0249, -0.0104, -0.0544,  0.0181, -0.0064,  0.0421,\n",
      "         0.0527, -0.0031,  0.0209, -0.0273,  0.0445, -0.0115, -0.0180, -0.0525,\n",
      "         0.0684,  0.0268,  0.0297,  0.0037, -0.0334,  0.0559, -0.0135,  0.0262,\n",
      "        -0.0073, -0.0536,  0.0367,  0.0370,  0.0492, -0.0243, -0.0249,  0.0564,\n",
      "        -0.0323,  0.0475,  0.0380,  0.0363,  0.0292,  0.0296, -0.0212, -0.0284,\n",
      "         0.0024, -0.0171, -0.0420, -0.0194,  0.0465,  0.0109,  0.0269,  0.0133,\n",
      "        -0.0097,  0.0419, -0.0456,  0.0554, -0.0141,  0.0081, -0.0290,  0.0180,\n",
      "        -0.0461,  0.0228, -0.0122, -0.0166, -0.0348, -0.0175,  0.0140,  0.0294,\n",
      "         0.0454,  0.0232, -0.0644, -0.0007, -0.0214, -0.0726, -0.0321,  0.0073,\n",
      "         0.0493, -0.0533,  0.0176, -0.0137,  0.0167,  0.0321, -0.0149,  0.0079,\n",
      "         0.0196,  0.0377, -0.0253,  0.0492, -0.0208, -0.0314, -0.0249, -0.0498,\n",
      "        -0.0063,  0.0046, -0.0281,  0.0252,  0.0158,  0.0198, -0.0183, -0.0288,\n",
      "         0.0306, -0.0366, -0.0438,  0.0158, -0.0165, -0.0193,  0.0122, -0.0367,\n",
      "        -0.0310,  0.0207, -0.0164, -0.0295,  0.0243, -0.0184, -0.0240, -0.0269,\n",
      "         0.0260,  0.0326, -0.0128,  0.0201, -0.0422, -0.0027, -0.0115, -0.0209,\n",
      "         0.0332,  0.0414,  0.0384, -0.0347,  0.0167,  0.0446, -0.0073, -0.0060,\n",
      "        -0.0205,  0.0087,  0.0570, -0.0283,  0.0373,  0.0302, -0.0311,  0.0443,\n",
      "         0.0146, -0.0538,  0.0091, -0.0333, -0.0389, -0.0028, -0.0088, -0.0593,\n",
      "         0.0176, -0.0075, -0.0379, -0.0507, -0.0304, -0.0203, -0.0194, -0.0297,\n",
      "         0.0260,  0.0254,  0.0131,  0.0297, -0.0343, -0.0046,  0.0265,  0.0380,\n",
      "        -0.0683, -0.0740, -0.0397, -0.0493,  0.0575,  0.0233,  0.0267, -0.0381,\n",
      "         0.0405, -0.0256, -0.0002,  0.0137,  0.0168,  0.0080, -0.0579, -0.0265,\n",
      "        -0.0727, -0.0255, -0.0308,  0.0350,  0.0894,  0.0109,  0.0200, -0.0303,\n",
      "        -0.0425,  0.0093, -0.0321,  0.0097, -0.0300, -0.0347,  0.0307, -0.0310,\n",
      "        -0.0165, -0.0126, -0.0199,  0.0453,  0.0281,  0.0370,  0.0057, -0.0380,\n",
      "        -0.0212, -0.0268, -0.0235,  0.0277,  0.0200,  0.0242,  0.0421,  0.0045,\n",
      "        -0.0489, -0.0019,  0.0130,  0.0218,  0.0299, -0.0126,  0.0185, -0.0074,\n",
      "        -0.0175,  0.0084, -0.0208,  0.0642, -0.0090, -0.0129,  0.0289,  0.0152,\n",
      "        -0.0528, -0.0220,  0.0456,  0.0353,  0.0185, -0.0272,  0.0164, -0.0344,\n",
      "         0.0223, -0.0085, -0.0148,  0.0416, -0.0436, -0.0175,  0.0229, -0.0189,\n",
      "         0.0194, -0.0445,  0.0047,  0.0414, -0.0046, -0.0292, -0.0073,  0.0510,\n",
      "         0.0293,  0.0477, -0.0332, -0.0036,  0.0384, -0.0208])\n",
      "tensor([[ 0.0197,  0.0170, -0.0218,  ...,  0.0335,  0.0120,  0.0485],\n",
      "        [ 0.0874,  0.0221, -0.0034,  ..., -0.0036, -0.0230, -0.0055],\n",
      "        [ 0.0230,  0.0156, -0.0149,  ...,  0.0041,  0.0165, -0.0266],\n",
      "        ...,\n",
      "        [ 0.0295,  0.0033,  0.0144,  ...,  0.0327, -0.0194,  0.0147],\n",
      "        [ 0.0323,  0.0046, -0.0191,  ...,  0.0060,  0.0001,  0.0367],\n",
      "        [-0.0169, -0.0113,  0.0556,  ...,  0.0321, -0.0220, -0.0177]])\n",
      "tensor([ 0.0466,  0.0187,  0.0439, -0.0222, -0.0308, -0.0481, -0.0520,  0.0114])\n",
      "tensor([[-0.0739,  0.1198, -0.0276,  ..., -0.0088,  0.0488, -0.0371],\n",
      "        [ 0.0008, -0.1393, -0.1285,  ...,  0.0470, -0.0160,  0.1164],\n",
      "        [ 0.1466,  0.0598,  0.1393,  ..., -0.0500, -0.0836, -0.0630],\n",
      "        ...,\n",
      "        [ 0.0226,  0.0698, -0.0759,  ..., -0.0509, -0.0376,  0.1457],\n",
      "        [-0.0373,  0.0848, -0.1624,  ...,  0.1388, -0.0036, -0.0975],\n",
      "        [-0.0968, -0.1052,  0.0692,  ...,  0.0342,  0.1335,  0.0043]])\n",
      "tensor([-3.0543e-02, -3.5821e-02,  1.2162e-01, -1.0187e-01, -9.1161e-02,\n",
      "         4.5207e-02,  2.0954e-01,  5.8366e-02,  2.3705e-02,  9.1012e-02,\n",
      "        -1.0781e-02,  2.1154e-01,  1.2733e-01,  1.2345e-01, -6.8523e-02,\n",
      "        -8.5949e-02,  2.3186e-01, -4.0249e-02,  8.9672e-02, -1.7098e-01,\n",
      "         1.2167e-01, -1.4403e-01,  1.9679e-01,  1.8696e-01, -3.0602e-02,\n",
      "         1.1708e-01,  2.7519e-01,  9.7253e-02, -7.5421e-02,  1.1782e-01,\n",
      "         1.4252e-01, -1.1547e-01,  1.7381e-01,  2.0104e-01, -5.9491e-02,\n",
      "         1.3054e-01,  7.6997e-02, -5.6495e-03, -9.8097e-02,  1.6119e-02,\n",
      "         1.6657e-01, -2.2475e-02,  6.2619e-02,  9.1626e-02,  6.2916e-02,\n",
      "        -4.4277e-02, -5.7589e-02,  7.4381e-02,  2.6277e-01, -7.9904e-02,\n",
      "        -1.4272e-01,  3.5406e-02, -1.3599e-01,  1.2097e-01,  7.7743e-02,\n",
      "         2.4754e-01,  1.6101e-01,  7.9228e-02,  1.1073e-01, -4.1832e-02,\n",
      "         1.1375e-01,  1.4721e-01,  2.6687e-01, -7.3177e-02,  1.0892e-01,\n",
      "        -1.0496e-01, -6.2271e-02,  8.5179e-02,  1.6241e-01,  3.8793e-02,\n",
      "         9.3146e-04,  1.0429e-01,  1.8227e-01,  5.8587e-02,  1.2392e-01,\n",
      "         6.3384e-02,  3.1772e-02,  2.2902e-01, -7.6355e-02, -4.2899e-02,\n",
      "        -3.7909e-02,  1.9008e-01,  1.8625e-01,  1.5118e-01,  5.3555e-02,\n",
      "         1.0429e-01, -8.5525e-02,  6.0124e-02,  9.5218e-02,  8.3532e-02,\n",
      "         1.0526e-03, -8.8173e-02,  7.5813e-02,  1.1198e-01, -1.2428e-01,\n",
      "        -1.7935e-02,  2.0867e-01,  1.5171e-01,  2.6043e-01,  1.0289e-02,\n",
      "        -1.0227e-01,  1.8176e-01, -6.5464e-02,  1.7260e-01, -4.4620e-02,\n",
      "         1.7386e-01, -9.9804e-02,  2.7085e-02,  4.8649e-02,  9.0978e-02,\n",
      "        -1.4522e-02,  5.0144e-02,  2.7064e-02,  1.1646e-01,  9.5457e-02,\n",
      "         3.4164e-02, -1.0861e-01,  1.5659e-01, -1.5228e-02,  8.3819e-02,\n",
      "         1.7339e-01, -1.5891e-02, -2.1988e-03,  1.2582e-02,  2.0374e-01,\n",
      "         1.5293e-02, -1.2125e-01,  1.8066e-01, -2.8012e-02,  8.5298e-02,\n",
      "        -1.1308e-01,  2.7320e-01,  2.5258e-02, -8.8448e-02,  1.9618e-01,\n",
      "         4.8639e-02, -9.2476e-03,  6.7800e-02,  9.2085e-02, -1.1441e-01,\n",
      "         5.9660e-02, -5.6994e-02,  2.6814e-01,  1.1946e-01,  2.9347e-02,\n",
      "         2.6254e-01,  5.9447e-02,  1.3827e-01,  5.1153e-02, -5.9081e-02,\n",
      "        -5.3996e-02,  1.3619e-01,  1.2323e-01,  1.8285e-01, -8.4436e-03,\n",
      "         1.4477e-01,  4.1536e-02, -1.0732e-01,  3.2855e-02,  2.0624e-01,\n",
      "        -1.0505e-01,  9.9968e-03,  1.6997e-01, -1.0132e-01,  7.3262e-02,\n",
      "         1.1771e-01, -5.6171e-02, -9.7614e-02, -9.3018e-02,  7.7950e-02,\n",
      "         8.8889e-02, -9.8612e-02,  2.5723e-02,  9.8365e-02,  4.7671e-02,\n",
      "         2.1980e-01, -1.5436e-01, -1.0643e-01,  4.0180e-02,  1.1196e-01,\n",
      "         1.6786e-01,  1.0621e-01,  2.3366e-01, -4.0035e-02,  1.2793e-01,\n",
      "         3.3726e-02,  4.1785e-02, -5.1227e-02,  7.1775e-02,  2.0989e-01,\n",
      "         1.3254e-01,  1.6535e-01,  2.1443e-01, -1.2260e-01,  2.1879e-01,\n",
      "         1.2227e-01,  1.1674e-01,  7.1581e-03,  1.8211e-01,  1.1956e-01,\n",
      "         2.4998e-02,  1.9623e-01, -1.5321e-02,  1.0718e-01, -7.8355e-02,\n",
      "        -1.1326e-01,  1.9297e-01,  1.2302e-01,  2.1488e-02,  1.8375e-01,\n",
      "        -7.0644e-02,  2.0471e-01,  3.0901e-03, -1.4418e-02,  1.3463e-01,\n",
      "         1.1034e-01,  6.4821e-02,  2.3115e-01,  2.0275e-01,  1.3634e-01,\n",
      "        -1.3591e-01,  1.6902e-02, -1.9240e-02,  1.1929e-01,  1.3207e-01,\n",
      "         2.0507e-01,  4.3778e-03, -4.0236e-02, -4.2740e-04,  2.6730e-01,\n",
      "        -2.2275e-01, -6.4235e-02, -9.8890e-02,  2.9866e-02,  2.2385e-01,\n",
      "         1.7711e-01,  2.0513e-01, -4.7634e-02, -1.0972e-01, -1.9416e-02,\n",
      "         7.4810e-02,  7.7781e-02,  1.5393e-02,  1.4977e-01,  7.3896e-02,\n",
      "         2.9498e-02,  1.4788e-01, -4.6658e-02,  1.6507e-02, -1.4274e-02,\n",
      "         1.6919e-01,  7.9606e-02,  1.3096e-01,  1.0997e-01,  9.3153e-02,\n",
      "        -2.1021e-03,  1.4450e-01,  6.0856e-02, -1.0615e-02,  2.4149e-02,\n",
      "        -4.2761e-02,  2.6228e-02, -1.1105e-01,  1.4887e-01, -1.2350e-02,\n",
      "         1.2003e-01,  2.3322e-01,  4.1488e-02, -1.1564e-02, -3.2250e-02,\n",
      "         1.1118e-01, -1.0945e-01, -7.6545e-03, -6.3192e-02,  1.7922e-01,\n",
      "         1.8046e-01,  7.7163e-02, -3.8353e-02,  2.1646e-01,  1.4877e-01,\n",
      "         1.8486e-01,  1.6874e-01, -1.0730e-01,  1.0086e-01, -8.8698e-02,\n",
      "         5.3817e-03,  9.7886e-02, -6.4851e-02,  2.6151e-01,  4.5403e-02,\n",
      "        -2.0648e-02,  2.1013e-01, -3.7825e-02, -5.4148e-02,  1.6665e-02,\n",
      "         3.7667e-02, -1.2040e-02,  3.7119e-02,  8.5423e-02,  1.4835e-01,\n",
      "         1.8971e-01, -3.2630e-02,  9.5849e-02,  2.5131e-01,  4.1384e-02,\n",
      "        -8.5663e-02,  1.5133e-01, -3.2198e-03,  5.7627e-02,  2.5137e-01,\n",
      "         8.8389e-02, -9.0132e-02,  6.6890e-02,  3.0833e-02,  1.2972e-01,\n",
      "         2.1958e-01,  1.2693e-01,  1.7336e-01, -1.3838e-01,  1.7328e-01,\n",
      "        -9.3433e-02, -8.9950e-03,  8.1280e-02,  2.0431e-01,  1.2210e-01,\n",
      "        -4.3468e-03, -3.9147e-02,  3.5477e-02,  7.5938e-02,  1.8368e-01,\n",
      "         2.0307e-01,  8.4911e-02, -6.0621e-02,  4.1936e-03, -6.6686e-02,\n",
      "         4.7311e-02,  2.0628e-02,  1.4621e-01,  1.6897e-01,  1.6376e-01,\n",
      "         1.4580e-01,  5.2465e-02, -4.7107e-02,  6.4156e-05,  1.0928e-01,\n",
      "         2.1441e-01,  1.9604e-01,  1.2788e-01, -1.1484e-01,  9.2929e-02])\n",
      "tensor([0.8370, 1.0221, 0.9643, 0.9487, 0.9680, 0.9880, 1.0300, 0.9946, 1.0491,\n",
      "        1.0613, 0.9966, 0.9778, 1.0537, 1.0010, 1.0088, 0.9859, 1.0182, 0.9610,\n",
      "        0.9706, 0.9879, 0.9871, 0.8480, 0.9949, 0.9483, 0.9744, 0.9835, 1.0292,\n",
      "        1.0012, 0.9129, 0.9844, 1.0035, 0.9612, 0.9120, 1.0149, 0.9712, 1.0041,\n",
      "        0.9834, 0.9493, 1.0498, 1.0433, 1.0328, 1.0107, 0.9573, 0.9878, 0.9903,\n",
      "        0.9961, 0.9495, 0.9479, 1.0765, 1.0338, 0.8552, 0.9552, 0.8884, 1.0010,\n",
      "        1.0111, 1.0366, 0.9958, 0.9753, 0.9013, 0.9925, 0.9529, 0.9425, 1.0064,\n",
      "        0.9120, 0.9254, 0.9822, 0.8394, 0.9850, 0.9944, 0.9843, 0.9361, 0.9779,\n",
      "        1.0303, 0.9631, 1.0454, 0.9669, 0.9724, 1.0070, 0.9705, 0.9247, 0.9423,\n",
      "        0.9700, 0.9599, 0.9972, 1.0038, 0.9821, 1.0182, 0.9316, 1.0535, 1.0187,\n",
      "        0.9785, 0.9937, 1.0310, 0.9272, 0.9466, 0.9002, 0.9571, 1.0134, 1.0091,\n",
      "        0.9566, 0.8773, 0.9298, 0.9477, 1.0468, 0.9303, 1.0157, 0.9693, 0.9573,\n",
      "        0.9196, 1.0130, 0.9548, 0.9565, 0.9654, 1.0298, 0.9356, 1.0103, 1.0068,\n",
      "        0.9468, 0.9871, 0.9526, 0.9386, 0.9482, 1.0074, 0.9868, 0.9948, 1.0155,\n",
      "        0.9523, 0.8772, 0.9884, 0.9778, 0.8733, 1.0624, 0.8389, 0.8882, 1.0002,\n",
      "        0.9853, 0.8586, 1.0254, 0.9911, 0.9320, 1.0054, 0.9765, 1.0595, 0.9842,\n",
      "        1.0129, 1.0574, 1.0000, 0.9989, 0.9136, 0.9450, 0.9892, 0.9939, 0.9437,\n",
      "        0.9699, 1.0203, 1.0223, 0.9352, 0.8951, 0.9947, 0.9286, 0.9288, 1.0160,\n",
      "        0.9936, 0.9106, 1.0284, 0.9070, 0.9832, 0.9564, 0.9573, 0.9880, 0.9783,\n",
      "        0.9561, 0.9438, 0.9883, 0.9353, 0.9880, 0.9083, 0.9769, 0.9582, 0.9529,\n",
      "        1.0698, 0.8368, 0.9761, 1.0341, 0.9798, 0.9696, 0.9683, 0.9386, 0.9465,\n",
      "        1.0129, 0.9554, 0.9825, 1.0318, 0.9247, 1.0344, 0.9937, 0.9700, 0.9778,\n",
      "        1.0553, 0.9864, 0.9859, 1.0122, 0.9990, 1.0022, 0.9684, 1.0514, 1.0284,\n",
      "        0.9689, 0.9975, 0.9975, 0.9609, 0.9656, 0.9820, 1.0384, 0.9788, 1.0250,\n",
      "        0.8833, 1.0267, 1.0498, 1.0383, 0.9076, 1.0305, 0.8974, 0.9997, 0.9006,\n",
      "        1.0337, 0.9680, 0.9258, 0.9348, 1.0909, 0.9921, 0.9691, 0.9979, 1.0362,\n",
      "        1.0264, 1.0361, 0.9368, 1.0192, 0.9917, 0.9943, 0.9959, 0.9517, 0.9784,\n",
      "        0.9831, 1.0035, 0.9686, 1.0070, 0.9539, 0.9805, 0.9559, 0.9421, 1.0040,\n",
      "        0.9912, 1.0161, 0.9428, 0.9832, 1.0089, 0.9156, 0.9662, 0.9420, 0.8753,\n",
      "        0.9870, 0.9779, 1.0154, 1.0317, 1.0312, 1.0223, 0.9640, 1.0440, 0.9761,\n",
      "        0.9412, 0.9771, 1.0514, 0.9157, 0.9665, 0.9501, 0.9356, 1.0660, 1.0329,\n",
      "        0.9822, 1.0183, 0.9916, 0.9992, 0.9586, 0.9702, 0.9340, 0.9576, 0.9409,\n",
      "        1.0127, 0.9395, 0.9584, 1.0333, 0.9963, 0.9300, 0.9801, 1.0262, 1.0208,\n",
      "        0.9722, 0.9517, 0.9605, 1.0541, 0.9659, 1.0307, 1.0707, 0.9970, 1.0347,\n",
      "        1.0109, 1.0004, 0.9633, 1.0177, 0.9524, 0.9063, 0.9568, 0.9308, 0.9855,\n",
      "        1.0098, 0.9938, 1.0740, 0.8708, 1.0050, 0.9169, 1.0769, 0.9522, 0.9060,\n",
      "        0.9898, 0.9358, 0.8765, 1.0051, 1.0079, 0.9404, 1.0156, 0.9816, 1.0031,\n",
      "        1.0247, 0.9829, 1.0289, 0.9706, 0.9765, 1.0295, 0.9969, 1.0025, 0.9270,\n",
      "        0.9036, 1.0194, 0.8235, 1.0202, 1.0172, 0.9887, 0.9781, 1.0222])\n",
      "tensor([-8.1144e-03, -1.1477e-01,  6.5374e-02,  7.1216e-02,  1.6664e-01,\n",
      "        -1.9901e-02, -5.4041e-02,  1.2363e-01, -2.1770e-03,  1.3605e-01,\n",
      "        -6.4105e-02, -5.7969e-02, -1.2041e-01, -1.2723e-01, -1.4060e-01,\n",
      "        -1.1969e-01,  2.2015e-01, -1.3179e-03, -7.3500e-02,  8.6607e-02,\n",
      "        -1.4935e-02,  1.0440e-01,  8.7954e-03,  1.3460e-02,  1.0356e-02,\n",
      "        -4.8980e-02,  6.7917e-02, -6.1129e-02, -2.0193e-02,  1.1643e-01,\n",
      "        -4.1386e-02, -8.3423e-02,  2.2341e-03,  1.7840e-01,  5.0502e-02,\n",
      "        -4.3699e-02, -1.3264e-01, -4.2652e-02, -8.1347e-02, -6.8727e-02,\n",
      "        -1.6505e-02,  1.8421e-03,  4.6652e-02,  4.0345e-02,  2.5943e-02,\n",
      "        -5.1128e-02,  1.4282e-01,  3.2128e-02, -2.1327e-02, -1.3249e-01,\n",
      "        -1.7046e-02,  9.7437e-02,  1.1447e-01,  5.9549e-02,  1.6295e-01,\n",
      "         3.8790e-02,  8.5921e-02,  6.9810e-02,  6.6768e-02, -3.2995e-02,\n",
      "         9.4957e-02,  8.5988e-03, -6.4181e-02, -4.5814e-02, -8.1845e-03,\n",
      "        -1.3149e-01, -2.7017e-02,  4.3474e-02, -1.0657e-01, -1.0830e-02,\n",
      "         1.1168e-01, -1.8270e-02,  1.8009e-01, -1.2698e-02, -2.2515e-02,\n",
      "         5.3589e-02, -7.4970e-02,  1.9552e-02,  5.2082e-02,  2.2937e-03,\n",
      "        -3.2120e-02,  1.7009e-01,  6.4640e-02, -1.4250e-01, -4.7977e-02,\n",
      "         9.5050e-02, -4.1076e-02,  4.3931e-02,  4.1493e-02, -7.0248e-04,\n",
      "         1.0054e-01,  3.7532e-02, -6.1912e-03,  3.3785e-02,  3.4195e-02,\n",
      "         1.0011e-01, -9.3150e-04, -1.9278e-01, -9.2620e-02, -6.6880e-02,\n",
      "        -3.4881e-02,  7.4292e-02,  9.4718e-02, -4.3925e-03, -1.1759e-02,\n",
      "        -9.3252e-02,  2.4573e-02,  1.1865e-03, -7.2230e-02, -7.3656e-02,\n",
      "         1.5216e-01,  9.8712e-02,  2.9949e-02,  6.4871e-03, -6.1234e-02,\n",
      "        -6.3716e-04, -1.8832e-01,  1.6440e-01, -3.4098e-02, -7.9614e-02,\n",
      "        -3.2163e-02, -2.8197e-02, -5.4475e-02,  7.7480e-02,  1.3228e-01,\n",
      "        -4.7472e-02,  8.4632e-02,  2.5547e-02, -3.4490e-02, -8.1728e-02,\n",
      "         9.1207e-02,  5.8275e-03,  1.2863e-01, -1.3331e-02, -3.6641e-02,\n",
      "        -5.8843e-02,  5.8967e-02, -1.0698e-01,  1.3907e-01, -1.3259e-02,\n",
      "        -8.7982e-02, -7.4879e-02,  6.6379e-02, -2.1053e-02,  1.4107e-01,\n",
      "        -1.0160e-01, -1.1111e-01, -4.2767e-02, -1.4831e-01,  5.4159e-02,\n",
      "        -1.4913e-01, -2.3089e-02,  1.5194e-01,  1.5491e-02, -1.4185e-01,\n",
      "         5.8096e-02,  2.0699e-03,  3.6854e-02,  5.0500e-02, -4.6747e-02,\n",
      "        -5.9488e-03,  1.0648e-01, -1.2815e-01, -8.4211e-04,  3.8928e-02,\n",
      "        -2.0866e-02, -8.0412e-03,  5.5991e-02,  9.1247e-02, -1.3911e-01,\n",
      "         1.3721e-01, -5.6909e-02, -9.6634e-02, -1.4635e-01,  7.3759e-02,\n",
      "         1.4711e-01, -5.7070e-02, -5.9861e-02,  2.1037e-02, -6.1670e-02,\n",
      "        -4.8901e-02, -1.9954e-03,  1.9451e-01, -1.3784e-01,  1.2642e-01,\n",
      "        -1.4795e-01,  9.9879e-02,  3.6984e-02, -3.1923e-02, -1.3582e-01,\n",
      "         5.4003e-02, -7.6928e-03, -2.9329e-03,  1.9424e-01, -2.5528e-04,\n",
      "         7.3750e-02,  4.3357e-02,  6.3562e-02, -1.4393e-01, -5.7903e-02,\n",
      "         3.2249e-02, -1.8477e-02, -1.0983e-01,  1.7166e-01,  1.0049e-01,\n",
      "        -9.0256e-02,  1.1341e-01,  4.2288e-02, -1.0401e-03, -1.6779e-01,\n",
      "         8.2802e-02, -5.3941e-02,  1.8284e-03, -5.8344e-04,  8.3532e-02,\n",
      "        -1.7525e-02, -2.3756e-02, -8.2716e-02,  6.0895e-02, -1.6640e-01,\n",
      "         3.3623e-02,  6.4700e-02,  5.0722e-02,  8.0817e-02,  2.2480e-01,\n",
      "        -1.4670e-01, -1.3324e-01,  1.6592e-02,  4.8687e-02, -5.3927e-02,\n",
      "        -1.4498e-01, -8.2600e-02, -1.4261e-01, -1.1942e-02, -1.3534e-02,\n",
      "         3.3677e-03,  3.1670e-02,  2.5212e-02, -4.5288e-02, -2.2336e-02,\n",
      "        -1.0292e-01, -4.2005e-02,  5.0229e-02, -6.1796e-02,  1.3958e-01,\n",
      "        -1.1141e-02, -6.2033e-03,  1.4374e-01, -2.1762e-02, -1.0884e-01,\n",
      "        -5.9575e-02, -5.5757e-02, -1.5575e-01, -5.6297e-02,  2.5913e-01,\n",
      "        -7.1701e-02,  2.5687e-03,  5.9178e-02, -1.1603e-01, -3.7085e-02,\n",
      "        -1.0919e-02, -1.8227e-01, -1.3923e-01,  1.5664e-01, -2.7265e-02,\n",
      "         1.3327e-01, -5.2999e-02,  8.1670e-02, -9.2690e-02, -5.9331e-02,\n",
      "         1.6527e-02, -1.0694e-01, -1.0637e-02,  4.3608e-02,  1.3251e-01,\n",
      "         1.0316e-01,  2.2908e-02, -7.3523e-02,  5.8124e-02,  2.8313e-02,\n",
      "        -6.1745e-02,  9.9176e-02, -1.7280e-02,  4.8946e-02, -5.0848e-02,\n",
      "         1.1215e-01,  3.3671e-03, -1.1047e-01,  1.3350e-01,  7.9876e-02,\n",
      "        -3.9340e-02, -4.9876e-02, -6.2516e-02, -1.2915e-02,  5.9618e-02,\n",
      "         1.4237e-02, -4.4857e-02, -9.1821e-02,  2.4345e-02,  7.4726e-02,\n",
      "         1.2336e-01,  1.0910e-01,  2.3629e-01,  6.1586e-02,  6.3471e-02,\n",
      "        -1.6998e-01,  1.8545e-02,  4.5022e-02,  1.8300e-01,  7.2371e-02,\n",
      "         1.3217e-01,  8.1778e-03,  1.0253e-02, -2.4598e-02, -4.7790e-02,\n",
      "        -1.2665e-01, -6.5223e-03, -2.6463e-02,  3.2004e-02, -6.0030e-02,\n",
      "        -5.6529e-02, -6.7958e-02,  6.9051e-02, -5.2029e-02, -2.0657e-02,\n",
      "        -8.7977e-02,  4.0982e-02, -2.0531e-03, -1.2463e-01,  3.6412e-02,\n",
      "        -8.6987e-03,  4.6096e-02,  7.2024e-02, -2.1125e-01,  8.7709e-02,\n",
      "         3.5188e-02,  4.7298e-02,  2.2774e-02, -3.1049e-02,  1.9253e-02,\n",
      "        -3.1815e-02,  9.9070e-03,  3.2314e-02, -9.8031e-02,  4.4015e-02,\n",
      "        -3.9436e-02, -5.3085e-02, -1.6634e-01,  1.6482e-02,  1.0628e-01])\n",
      "tensor([[ 0.0045,  0.0479, -0.0243,  ...,  0.0017,  0.0420,  0.0303],\n",
      "        [-0.0112,  0.0099, -0.0159,  ..., -0.0154, -0.0031, -0.0407],\n",
      "        [ 0.0504, -0.0389, -0.0315,  ...,  0.0269,  0.0071, -0.0259],\n",
      "        ...,\n",
      "        [ 0.0177, -0.0305, -0.0268,  ...,  0.0076,  0.0512,  0.0038],\n",
      "        [ 0.0047,  0.0543, -0.0521,  ...,  0.0069, -0.0438, -0.0285],\n",
      "        [ 0.0054,  0.0565,  0.0435,  ..., -0.0210, -0.0063, -0.0416]])\n",
      "tensor([-0.0304, -0.0259,  0.0006, -0.0254,  0.0673,  0.0076, -0.0397, -0.0381,\n",
      "        -0.0411, -0.0570, -0.0126, -0.0166, -0.0312, -0.0345,  0.0069,  0.0278,\n",
      "        -0.0293,  0.0541,  0.1054,  0.0705,  0.0199, -0.0928,  0.0654, -0.0828,\n",
      "        -0.0079, -0.0337,  0.0607, -0.0004, -0.0502,  0.1061, -0.0520,  0.0047,\n",
      "         0.0089, -0.1322,  0.0213, -0.0380,  0.0107, -0.0041,  0.0212,  0.0564,\n",
      "        -0.0140, -0.0122, -0.0468, -0.0085,  0.0847, -0.0289, -0.0222, -0.0288,\n",
      "         0.0344,  0.0006, -0.0755, -0.1015,  0.0648, -0.0095, -0.0099, -0.0080,\n",
      "         0.0709,  0.0134, -0.0177, -0.0768, -0.0337, -0.0227,  0.0568,  0.0330,\n",
      "        -0.0115, -0.0334, -0.1446,  0.0177,  0.0119, -0.1012,  0.0380, -0.0203,\n",
      "        -0.0245, -0.0384, -0.0106, -0.0129,  0.0492, -0.0038,  0.0255,  0.0551,\n",
      "         0.0090, -0.0741,  0.0491, -0.0539, -0.0135, -0.0647, -0.0437, -0.0484,\n",
      "        -0.1294, -0.0757,  0.0880, -0.0575, -0.0074,  0.0407,  0.0468, -0.1713,\n",
      "        -0.0152,  0.0428,  0.0776, -0.0802,  0.0363,  0.0770, -0.0689, -0.0245,\n",
      "         0.0263, -0.0031,  0.0788,  0.0051,  0.0207, -0.0369, -0.0171, -0.0236,\n",
      "        -0.1416, -0.0010, -0.0200, -0.0266, -0.0494, -0.0361,  0.0471, -0.0189,\n",
      "        -0.0166, -0.0037, -0.0313, -0.0034,  0.0139, -0.0441,  0.0146, -0.0140,\n",
      "         0.0315,  0.0461, -0.1807,  0.0696,  0.0514, -0.0909, -0.0622, -0.0036,\n",
      "         0.0152,  0.0432, -0.0044, -0.0188,  0.0016, -0.0570,  0.1194,  0.0533,\n",
      "        -0.0242, -0.0587,  0.0413, -0.0508,  0.0362,  0.0680,  0.0425,  0.0300,\n",
      "        -0.0651, -0.0587,  0.0446, -0.0082, -0.0307,  0.0430,  0.0897, -0.0247,\n",
      "        -0.0077, -0.0530,  0.0051, -0.0199, -0.0196,  0.0013, -0.0278,  0.0409,\n",
      "         0.0101, -0.0332,  0.0453, -0.0542, -0.0219, -0.0277, -0.0403,  0.0150,\n",
      "        -0.0187, -0.0579, -0.0675,  0.0321,  0.0252,  0.0177, -0.0123,  0.0534,\n",
      "        -0.0099,  0.0033,  0.0732,  0.0472, -0.0346, -0.0547,  0.0227, -0.0831,\n",
      "         0.0696, -0.0329,  0.0606,  0.0225, -0.1579,  0.0851,  0.0176, -0.0045,\n",
      "        -0.0773,  0.1072,  0.0544,  0.0157, -0.1035,  0.0164,  0.0237, -0.0281,\n",
      "        -0.0105,  0.0016, -0.0124,  0.0711,  0.0130, -0.1629,  0.0423, -0.0298,\n",
      "        -0.0368, -0.0352,  0.0448,  0.0215,  0.0625, -0.1029,  0.0150,  0.0519,\n",
      "        -0.0206,  0.0098,  0.0134,  0.0355, -0.0206,  0.1163, -0.0632,  0.0625,\n",
      "         0.0359, -0.1378, -0.0287, -0.0469,  0.0124, -0.1485, -0.0760,  0.0556,\n",
      "         0.0716, -0.1722,  0.0267,  0.0544,  0.0016, -0.0019, -0.1193,  0.0520,\n",
      "        -0.0083, -0.0526,  0.0921, -0.0146,  0.0465, -0.1075, -0.0407, -0.0494,\n",
      "        -0.0080,  0.1083, -0.0139,  0.0290,  0.0757, -0.0482, -0.0794, -0.0409,\n",
      "        -0.0278,  0.1029, -0.0326, -0.0264, -0.0257, -0.0650, -0.0816, -0.0036,\n",
      "        -0.0316,  0.0143, -0.0356,  0.0149, -0.0155,  0.0578,  0.0381,  0.0025,\n",
      "         0.0231, -0.0380,  0.0594,  0.0226, -0.0200,  0.0804,  0.0841, -0.0577,\n",
      "         0.0182, -0.0517, -0.1953, -0.0243, -0.0247, -0.0274,  0.0359, -0.0899,\n",
      "         0.0156,  0.0212, -0.0534, -0.0315,  0.0624, -0.0055, -0.0245,  0.0303,\n",
      "        -0.0021, -0.0571, -0.0627,  0.0491,  0.1401,  0.1133, -0.0540, -0.0852,\n",
      "        -0.0402,  0.0331,  0.0440,  0.0845,  0.0642, -0.0145, -0.0297, -0.2073,\n",
      "        -0.1083,  0.0754, -0.0226,  0.0336, -0.0243, -0.1141, -0.0795,  0.0166,\n",
      "        -0.0037, -0.0360,  0.0333,  0.0233,  0.0533, -0.0403, -0.1543,  0.0134,\n",
      "         0.0880, -0.0506,  0.0095,  0.0206,  0.0316, -0.0759, -0.0027, -0.0014,\n",
      "         0.0044,  0.0700, -0.0142, -0.0792,  0.0037,  0.0897])\n",
      "tensor([0.9773, 1.0180, 0.9657, 0.9664, 0.9859, 0.9889, 1.0059, 0.9501, 1.0717,\n",
      "        1.0873, 1.0021, 0.9841, 0.9734, 0.9902, 0.9558, 1.0329, 1.0050, 1.0458,\n",
      "        0.9666, 0.9482, 0.9712, 1.1008, 0.9727, 1.0203, 0.9933, 0.9694, 0.9899,\n",
      "        1.0129, 1.0232, 1.0106, 0.9734, 0.9549, 1.0045, 1.0593, 0.9628, 0.9459,\n",
      "        0.9828, 0.9327, 1.0186, 1.0187, 0.9979, 0.9824, 1.0711, 1.0706, 0.9905,\n",
      "        0.9849, 0.9892, 0.9855, 0.9690, 0.9776, 1.0665, 0.9744, 1.0045, 0.9832,\n",
      "        0.9740, 0.9855, 0.9908, 1.0356, 0.9873, 1.0128, 0.9729, 1.0878, 0.9827,\n",
      "        0.9961, 0.9564, 0.9539, 1.0524, 1.0281, 1.0108, 1.0142, 0.9549, 0.9774,\n",
      "        1.0707, 0.9920, 0.9589, 0.9830, 1.0264, 0.9766, 0.9733, 0.9637, 1.0279,\n",
      "        1.0677, 0.9805, 1.0235, 0.9811, 1.0182, 0.9820, 1.0378, 1.1031, 1.0191,\n",
      "        0.9674, 1.1976, 0.9978, 0.9481, 1.0194, 1.0284, 0.9988, 0.9349, 0.9895,\n",
      "        1.0412, 0.9863, 0.9674, 1.0434, 0.9988, 0.9690, 0.9823, 1.0015, 0.9412,\n",
      "        1.0693, 1.0450, 1.0368, 1.0766, 1.0858, 0.9559, 0.9082, 0.9808, 1.0013,\n",
      "        0.9810, 0.9779, 0.9855, 0.9828, 1.1215, 0.9613, 1.0408, 1.1043, 0.9883,\n",
      "        1.0107, 0.9640, 0.9663, 0.9802, 1.0189, 0.9814, 0.9523, 1.0413, 0.9916,\n",
      "        0.9903, 1.0234, 0.9914, 0.9556, 0.9892, 1.1197, 1.0289, 0.9727, 0.9704,\n",
      "        0.9506, 0.9940, 1.0009, 0.9988, 0.9642, 1.0177, 0.9516, 0.9872, 1.0066,\n",
      "        0.9606, 0.9954, 0.9653, 0.9158, 1.1389, 0.9857, 0.9738, 0.9574, 0.9989,\n",
      "        1.0022, 1.0289, 1.0337, 1.0204, 0.9695, 0.9774, 0.9992, 0.9685, 1.0588,\n",
      "        1.0037, 0.9411, 1.0685, 1.0302, 1.0028, 0.9778, 1.0043, 0.9845, 0.9650,\n",
      "        0.9603, 1.0447, 1.0437, 1.0170, 0.9762, 0.9627, 0.9966, 0.9692, 1.0182,\n",
      "        0.9715, 0.9788, 0.9902, 1.0042, 0.9594, 0.9882, 1.0183, 0.9993, 0.9718,\n",
      "        0.9785, 0.9869, 1.0187, 0.9740, 1.0059, 0.9994, 1.0186, 0.9648, 0.9907,\n",
      "        0.9893, 0.9657, 0.9990, 0.9323, 0.9639, 1.0256, 1.0827, 1.0057, 1.0061,\n",
      "        0.9818, 0.9609, 0.9498, 1.0053, 0.9558, 1.0565, 0.9810, 0.9907, 0.9615,\n",
      "        0.9580, 1.0378, 0.9673, 0.9524, 0.9887, 1.0312, 0.9869, 0.9338, 1.1124,\n",
      "        0.9954, 0.9752, 1.0069, 1.0399, 0.9764, 0.9713, 1.0072, 1.0681, 1.0491,\n",
      "        1.0006, 0.9871, 0.9931, 0.9556, 1.0069, 0.9974, 1.0354, 1.0055, 1.0101,\n",
      "        0.9754, 1.0314, 1.0016, 0.9906, 0.9716, 0.9668, 1.0110, 0.9092, 0.9737,\n",
      "        0.9893, 1.0221, 0.9898, 1.0187, 0.9942, 1.0067, 0.9789, 0.9605, 0.9899,\n",
      "        1.0691, 0.9645, 1.0108, 0.9953, 0.9998, 0.9396, 0.9452, 0.9772, 0.9839,\n",
      "        0.9492, 1.0158, 0.9458, 0.9996, 0.9598, 0.9885, 0.9549, 1.0054, 1.0739,\n",
      "        0.9731, 1.0789, 1.1329, 1.0086, 1.0475, 1.0289, 1.0332, 1.0420, 0.9626,\n",
      "        0.9424, 1.0043, 0.9748, 0.9579, 1.0684, 1.0307, 0.9588, 0.9748, 1.0065,\n",
      "        0.9850, 0.9859, 1.0342, 0.9817, 0.9908, 0.9846, 0.9733, 0.9588, 0.9485,\n",
      "        1.0645, 1.0141, 1.0070, 1.1046, 1.1741, 1.0385, 0.9683, 1.0114, 0.9335,\n",
      "        0.9741, 1.1568, 1.1207, 1.0231, 1.0103, 0.9802, 0.9571, 0.9520, 0.9869,\n",
      "        0.9984, 1.0452, 1.0117, 0.9749, 1.0114, 0.9913, 0.9860, 0.9722, 0.9980,\n",
      "        0.9742, 0.9794, 0.9306, 0.9517, 0.9469, 1.0021, 0.9977, 0.9707])\n",
      "tensor([-0.0981, -0.1447,  0.0858,  0.0857,  0.0792,  0.0315,  0.0168, -0.0729,\n",
      "         0.0654,  0.0756,  0.0943, -0.0968, -0.1231, -0.1121,  0.0367, -0.1001,\n",
      "        -0.0402, -0.1235, -0.0498, -0.0215,  0.0777,  0.0414,  0.0203,  0.0377,\n",
      "        -0.1197, -0.0743,  0.1209, -0.0150, -0.0553,  0.0143, -0.0307, -0.0098,\n",
      "         0.0031, -0.1200, -0.0469,  0.0359, -0.0814,  0.0627, -0.0880, -0.0736,\n",
      "        -0.0101, -0.0163,  0.0216,  0.0166,  0.0747,  0.0865,  0.1388,  0.0492,\n",
      "         0.0340,  0.1141, -0.0075,  0.0336,  0.0557,  0.0406, -0.1004, -0.0900,\n",
      "         0.0335,  0.0148, -0.0056, -0.0401, -0.0708,  0.0655, -0.0283, -0.0146,\n",
      "        -0.0911,  0.0502, -0.0892,  0.1004,  0.0131, -0.1103,  0.1092, -0.0285,\n",
      "         0.0258,  0.0412, -0.0049,  0.0454, -0.0017,  0.1613,  0.0095,  0.1778,\n",
      "         0.0621, -0.0085, -0.0204, -0.0685, -0.0605, -0.0300, -0.0391,  0.1249,\n",
      "        -0.0720,  0.0903,  0.0437, -0.0250,  0.0491, -0.0487,  0.0571, -0.0659,\n",
      "         0.0130, -0.0342,  0.0984,  0.0754,  0.0509,  0.1435, -0.0024, -0.0186,\n",
      "         0.0188,  0.0220, -0.0937, -0.0261,  0.0018, -0.0124,  0.0376, -0.0957,\n",
      "        -0.1117,  0.0366, -0.0006, -0.0930,  0.0086, -0.0678,  0.1132,  0.0272,\n",
      "         0.0050, -0.0173, -0.1866,  0.0477, -0.0935, -0.0240,  0.0280, -0.1194,\n",
      "        -0.0565, -0.0578, -0.0674, -0.0688, -0.0259,  0.0385, -0.0942,  0.0284,\n",
      "        -0.0206,  0.1064,  0.0614, -0.0139, -0.0218, -0.1134,  0.0802,  0.0083,\n",
      "         0.0271, -0.1470, -0.0353, -0.0268,  0.0242,  0.0214,  0.0421,  0.1293,\n",
      "        -0.0404, -0.0633, -0.0012, -0.0991,  0.0874, -0.0036,  0.0077, -0.0232,\n",
      "        -0.0234, -0.0165,  0.1568, -0.0514,  0.0205, -0.0121, -0.0654, -0.0052,\n",
      "        -0.0491, -0.0748,  0.0240,  0.0500, -0.0049,  0.0648,  0.0817, -0.0026,\n",
      "         0.1139, -0.1146,  0.0170, -0.0393,  0.0365, -0.0167,  0.0531, -0.0074,\n",
      "        -0.0772,  0.0809,  0.0567,  0.0528, -0.0310,  0.0361,  0.1002, -0.1192,\n",
      "        -0.0128,  0.0864,  0.0826, -0.0320,  0.0506, -0.0253, -0.0090, -0.0159,\n",
      "         0.0648, -0.0650, -0.0275, -0.0258, -0.0523, -0.0038, -0.0532,  0.0810,\n",
      "         0.0442, -0.0270,  0.0093,  0.0249,  0.1049,  0.0393, -0.0028, -0.0350,\n",
      "        -0.0285,  0.0526,  0.0231, -0.0878,  0.0239,  0.0573,  0.0138,  0.0188,\n",
      "         0.0466,  0.0260, -0.0127, -0.0833,  0.1594,  0.0759, -0.0604,  0.0546,\n",
      "        -0.0071, -0.0754,  0.0630, -0.0357, -0.0226, -0.2310,  0.0098,  0.0182,\n",
      "        -0.0724,  0.0173,  0.1451, -0.0251,  0.0810,  0.0097, -0.1290, -0.0591,\n",
      "        -0.0549,  0.0751, -0.0862, -0.0156, -0.0534, -0.1199, -0.0837, -0.1032,\n",
      "         0.0306, -0.0107, -0.0436,  0.0070,  0.0044, -0.0763, -0.0839,  0.0097,\n",
      "        -0.0509, -0.0051, -0.0104, -0.0435,  0.1566, -0.1259,  0.1162,  0.0427,\n",
      "        -0.0356,  0.0227,  0.0845, -0.0675, -0.0741,  0.0307,  0.0206,  0.0084,\n",
      "         0.0643,  0.0195,  0.0107, -0.0696,  0.0410,  0.0042,  0.0275, -0.0082,\n",
      "         0.0689,  0.0087,  0.1199,  0.0123,  0.0680,  0.0649,  0.0720, -0.0175,\n",
      "         0.0771, -0.0680, -0.0118, -0.0700,  0.0816, -0.0166,  0.0826,  0.1277,\n",
      "         0.0515, -0.0849,  0.0226,  0.0588,  0.0013, -0.1070, -0.0513, -0.0154,\n",
      "        -0.0392,  0.0598,  0.0029,  0.0473, -0.0660, -0.0715,  0.0610,  0.0209,\n",
      "         0.0140, -0.0459, -0.0520,  0.0216,  0.0028, -0.0537, -0.0282,  0.0597,\n",
      "        -0.0679, -0.1188,  0.0381,  0.0701, -0.0548, -0.0148,  0.0219, -0.0056,\n",
      "         0.0664,  0.0441, -0.0045,  0.0527, -0.0439,  0.0737,  0.1422,  0.0320,\n",
      "         0.0540,  0.0893,  0.0403, -0.0629,  0.0502,  0.0888])\n",
      "tensor([[ 0.0206,  0.0586, -0.0106,  ..., -0.0037,  0.0276, -0.0502],\n",
      "        [ 0.0392, -0.0088,  0.0330,  ...,  0.0184, -0.0493,  0.0568],\n",
      "        [-0.0090, -0.0361, -0.0015,  ...,  0.0018, -0.0065,  0.0037],\n",
      "        ...,\n",
      "        [-0.0088,  0.0149,  0.0138,  ..., -0.0069,  0.0211,  0.0133],\n",
      "        [-0.0255,  0.0366, -0.0025,  ..., -0.0355, -0.0220,  0.0022],\n",
      "        [ 0.0318,  0.0662,  0.0542,  ...,  0.0263,  0.0183, -0.0109]])\n",
      "tensor([-0.0720, -0.0885,  0.0625, -0.0260,  0.0721, -0.0023, -0.0389,  0.0767,\n",
      "        -0.0258, -0.1227,  0.0468,  0.0079,  0.0777, -0.0407,  0.0593,  0.0539,\n",
      "         0.0205,  0.0290,  0.0422,  0.0405,  0.0446, -0.0914,  0.0093, -0.0645,\n",
      "        -0.0303,  0.0303,  0.0347,  0.0379,  0.0383,  0.0005, -0.0035,  0.0052,\n",
      "         0.0606, -0.0305, -0.0146,  0.0429,  0.0107, -0.0187,  0.0583,  0.0173,\n",
      "         0.0677, -0.0616,  0.0253, -0.0033,  0.0542,  0.0097,  0.0112,  0.0363,\n",
      "         0.0560, -0.0436,  0.1094, -0.0219,  0.0424,  0.0415,  0.0027,  0.0186,\n",
      "         0.0473,  0.0023, -0.0708,  0.0295, -0.0069,  0.0356,  0.0562, -0.0656,\n",
      "         0.0390,  0.0706, -0.1649,  0.0204,  0.0958, -0.0100,  0.0053,  0.0141,\n",
      "         0.0155, -0.0022, -0.0050,  0.1083,  0.0767,  0.0387,  0.0132,  0.0727,\n",
      "         0.1003,  0.0316,  0.0484,  0.0101,  0.0230, -0.0057,  0.0682,  0.0148,\n",
      "        -0.0892, -0.0353,  0.0422,  0.0310, -0.0262,  0.0361,  0.0023, -0.1123,\n",
      "        -0.1262, -0.0662, -0.0110, -0.0543,  0.0741,  0.0337,  0.0665,  0.0021,\n",
      "        -0.0411,  0.0445,  0.0647,  0.0333, -0.0271, -0.1226, -0.0169, -0.0555,\n",
      "         0.0055, -0.0108, -0.0274, -0.0330, -0.0263,  0.0460,  0.0926,  0.0037,\n",
      "        -0.0746, -0.0133,  0.0051, -0.0338, -0.0204,  0.0627,  0.0134, -0.0173,\n",
      "         0.0462,  0.0211, -0.1683, -0.0380, -0.0011, -0.1388, -0.0297,  0.0363,\n",
      "         0.0161,  0.0037,  0.0330, -0.0626,  0.0347,  0.0126,  0.0425,  0.0228,\n",
      "         0.0024,  0.0116,  0.0527,  0.0236, -0.0331,  0.0776,  0.0648,  0.0632,\n",
      "         0.0218,  0.0173,  0.0761,  0.0127,  0.0490,  0.0602,  0.0217,  0.0063,\n",
      "        -0.0204,  0.0963, -0.0152, -0.0132,  0.0216,  0.0017,  0.0326,  0.0787,\n",
      "         0.0386,  0.0067, -0.0080, -0.0184,  0.0623,  0.0043, -0.0607, -0.0383,\n",
      "        -0.0195, -0.1301, -0.0003,  0.0522,  0.0050,  0.0864,  0.0999,  0.0511,\n",
      "         0.0364, -0.0083,  0.0826,  0.0396,  0.0424,  0.0276, -0.0024, -0.0824,\n",
      "         0.0698,  0.0045,  0.0694,  0.0929, -0.1128,  0.0569, -0.0332,  0.0006,\n",
      "        -0.0434,  0.0202,  0.0575,  0.0257, -0.0956, -0.0036, -0.0017,  0.0144,\n",
      "        -0.0522, -0.0140,  0.0454,  0.0170,  0.0542, -0.1030,  0.0767, -0.0700,\n",
      "        -0.0798, -0.0221, -0.0162, -0.0140,  0.0166, -0.1165, -0.0437,  0.0488,\n",
      "         0.0213,  0.0554, -0.0431,  0.0579,  0.0465,  0.0321,  0.0322,  0.0257,\n",
      "        -0.0191, -0.0721,  0.0538, -0.0756,  0.0200,  0.0329, -0.0714,  0.0236,\n",
      "         0.0283, -0.1400, -0.0463,  0.0041,  0.0066, -0.0171, -0.0407,  0.0704,\n",
      "         0.0813,  0.0251,  0.0022,  0.0231,  0.0393, -0.0236,  0.0775, -0.0562,\n",
      "         0.0427, -0.0113,  0.0104,  0.0560, -0.0443, -0.0455, -0.0931, -0.0176,\n",
      "        -0.0755,  0.1408,  0.0372, -0.0503, -0.0040, -0.0410,  0.0679, -0.0018,\n",
      "         0.0277, -0.0299, -0.0869,  0.0617,  0.0132,  0.0652,  0.0316,  0.0191,\n",
      "         0.0769, -0.0066,  0.0884, -0.0073, -0.0923, -0.0182,  0.0424,  0.0037,\n",
      "        -0.0120,  0.0510, -0.0211, -0.0378,  0.1332, -0.0468,  0.0524,  0.0493,\n",
      "        -0.0325,  0.0047,  0.0095,  0.0044,  0.0990, -0.0559, -0.0379,  0.0937,\n",
      "         0.0312, -0.0159,  0.0361,  0.0325,  0.1214,  0.0470, -0.0960,  0.0297,\n",
      "         0.0068,  0.0501, -0.0023,  0.0809,  0.0420,  0.0040, -0.0105,  0.0095,\n",
      "        -0.0206, -0.0619, -0.0602,  0.0799, -0.0731, -0.0937, -0.1228, -0.0057,\n",
      "        -0.0017,  0.0359,  0.0469,  0.0217,  0.0471,  0.0480,  0.0280, -0.0177,\n",
      "         0.0769,  0.0271,  0.0708,  0.0151,  0.0058,  0.0381, -0.0405,  0.0244,\n",
      "         0.0114,  0.1012,  0.0775, -0.0390,  0.0427,  0.1021])\n",
      "tensor([0.9961, 0.9584, 0.9426, 0.9500, 0.9654, 0.9326, 0.9693, 0.9571, 1.0326,\n",
      "        1.0008, 0.9414, 0.9597, 0.9726, 0.9570, 0.9361, 0.9946, 0.9409, 0.9289,\n",
      "        0.9575, 0.9475, 0.9542, 1.0505, 0.9575, 0.9725, 0.9631, 0.9650, 0.9311,\n",
      "        0.9729, 0.9574, 0.9538, 0.9619, 0.9862, 0.9742, 0.9685, 0.9466, 0.9448,\n",
      "        0.9627, 1.0173, 0.9651, 0.9649, 0.9549, 0.9689, 1.0485, 1.0501, 0.9455,\n",
      "        0.9671, 0.9883, 0.9359, 0.9414, 1.0024, 0.9736, 0.9211, 0.9498, 0.9460,\n",
      "        0.9549, 0.9440, 0.9352, 0.9972, 0.9706, 0.9764, 0.9477, 0.9768, 0.9382,\n",
      "        1.0597, 0.9656, 0.9437, 1.0058, 0.9800, 0.9392, 0.9844, 0.9473, 0.9609,\n",
      "        0.9557, 0.9668, 0.9589, 0.9656, 0.9699, 0.9720, 0.9476, 0.9293, 0.9494,\n",
      "        0.9957, 0.9568, 0.9883, 0.9945, 0.9470, 0.9572, 1.0071, 1.0400, 0.9659,\n",
      "        0.9459, 1.0267, 0.9769, 0.9457, 0.9499, 1.0073, 1.0144, 0.9699, 0.9541,\n",
      "        1.0645, 0.9565, 0.9775, 0.9354, 0.9681, 1.0138, 0.9370, 0.9666, 0.9526,\n",
      "        1.0948, 1.1175, 1.0283, 1.0012, 1.0328, 0.9434, 0.9504, 0.9669, 0.9906,\n",
      "        0.9699, 0.9294, 0.9728, 0.9584, 1.0057, 0.9492, 0.9797, 1.0070, 0.9496,\n",
      "        0.9728, 0.9578, 0.9453, 0.9598, 1.0352, 0.9408, 0.9566, 0.9612, 0.9955,\n",
      "        0.9683, 0.9528, 0.9340, 0.9494, 1.0311, 1.0197, 0.9610, 0.9530, 0.9629,\n",
      "        0.9547, 1.0039, 0.9956, 0.9528, 0.9487, 0.9574, 0.9480, 0.9440, 0.9479,\n",
      "        0.9605, 0.9781, 0.9411, 0.9374, 1.0678, 0.9337, 0.9386, 0.9323, 0.9647,\n",
      "        0.9652, 0.9995, 0.9330, 0.9550, 0.9483, 0.9587, 0.9702, 0.9618, 0.9714,\n",
      "        1.0259, 0.9614, 1.0171, 0.9871, 0.9653, 0.9343, 0.9761, 0.9656, 0.9384,\n",
      "        0.9531, 1.0183, 0.9639, 0.9621, 0.9725, 0.9151, 0.9494, 0.9403, 0.9404,\n",
      "        0.9963, 0.9673, 0.9601, 0.9609, 0.9369, 0.9661, 0.9636, 1.0350, 0.9416,\n",
      "        0.9626, 0.9570, 0.9681, 0.9482, 0.9673, 0.9595, 0.9817, 1.0039, 0.9683,\n",
      "        0.9901, 0.9777, 0.9663, 0.9413, 0.9686, 0.9010, 1.0026, 0.9611, 0.9661,\n",
      "        0.9653, 0.9400, 0.9407, 0.9717, 0.9264, 0.9965, 0.9436, 0.9374, 0.9206,\n",
      "        0.9180, 0.9930, 0.9620, 0.9190, 0.9621, 0.9978, 0.9543, 0.9716, 1.0587,\n",
      "        0.9357, 1.0004, 0.9297, 0.9591, 0.9870, 0.9477, 0.9598, 1.0270, 1.0004,\n",
      "        1.0188, 0.9497, 0.9463, 0.9714, 1.0221, 0.9971, 0.9334, 0.9290, 0.9624,\n",
      "        0.9527, 0.9739, 0.9642, 0.9659, 0.9688, 0.8866, 0.9741, 0.9200, 0.9545,\n",
      "        0.9719, 1.0034, 0.9612, 0.9598, 0.9589, 0.9446, 0.9985, 0.9667, 0.9904,\n",
      "        0.9732, 0.9476, 0.9652, 0.9209, 0.9693, 0.9834, 0.9082, 0.9768, 0.9288,\n",
      "        0.9520, 0.9773, 0.9791, 0.9521, 0.9873, 0.9785, 0.9132, 0.9402, 0.9927,\n",
      "        0.9502, 1.0130, 0.9530, 0.9596, 0.9704, 0.9623, 0.9619, 0.9532, 0.9428,\n",
      "        0.9142, 0.9603, 0.9439, 0.9632, 0.9913, 0.9740, 0.9680, 0.9555, 0.9783,\n",
      "        0.9540, 0.9325, 0.9536, 0.9366, 0.9504, 0.9495, 0.9548, 0.9590, 1.0076,\n",
      "        0.9690, 0.9622, 0.9678, 0.9899, 0.9737, 0.9648, 0.9095, 0.9605, 0.9262,\n",
      "        1.0008, 1.0936, 1.0966, 0.9631, 1.0013, 0.9533, 0.9690, 0.9210, 0.9362,\n",
      "        0.9558, 0.9406, 1.0009, 0.9723, 0.9568, 0.9492, 0.9617, 0.9689, 0.9516,\n",
      "        0.9673, 0.9513, 0.9387, 0.9292, 0.9580, 0.9734, 0.9472, 0.9582])\n",
      "tensor([ 7.1255e-02, -1.1446e-01, -1.4952e-01, -4.9643e-02,  1.1153e-01,\n",
      "        -1.0721e-01,  1.6778e-01,  1.9671e-02,  1.3573e-01, -6.6729e-04,\n",
      "        -1.8647e-01,  1.6062e-01,  5.7368e-02,  1.0469e-01,  1.5631e-01,\n",
      "        -1.1661e-01, -1.1810e-01,  6.6659e-02, -2.0056e-02, -1.4072e-01,\n",
      "         5.6093e-02,  7.6967e-02,  8.6200e-02, -1.4530e-01, -1.3944e-01,\n",
      "        -7.2337e-02, -7.7419e-02, -6.2910e-02, -5.2494e-02,  1.3175e-02,\n",
      "         1.2075e-01, -1.0314e-01,  3.7735e-02, -2.0240e-02, -1.0003e-01,\n",
      "         1.5289e-01, -1.1382e-01, -4.4267e-03, -4.8755e-02,  3.1001e-02,\n",
      "        -1.0690e-01,  2.3532e-02, -1.5242e-01, -2.3668e-02, -1.1323e-01,\n",
      "        -2.4979e-03,  9.0862e-02, -1.3700e-01, -7.3049e-02,  1.0472e-01,\n",
      "        -3.1433e-02, -1.5827e-02,  1.3542e-01, -1.7294e-01,  1.3904e-01,\n",
      "        -1.4445e-01, -1.9507e-01,  1.5866e-01, -8.8607e-02,  1.6912e-01,\n",
      "         7.4109e-02,  1.4530e-01, -7.7246e-02,  9.5019e-02,  1.2795e-01,\n",
      "         2.7948e-02,  1.0764e-01, -9.3208e-02,  1.0894e-01, -1.0231e-02,\n",
      "        -1.7440e-01,  1.5083e-02, -1.6680e-01, -8.5034e-02, -3.8688e-02,\n",
      "        -7.4587e-02,  3.2319e-02, -4.4447e-02, -1.2976e-01, -1.0644e-01,\n",
      "        -4.1961e-02,  5.1871e-02,  1.1935e-01, -4.3845e-02, -4.0445e-02,\n",
      "        -9.8009e-03, -1.6025e-01,  5.4008e-02,  1.3999e-01,  9.2516e-02,\n",
      "         1.3982e-02, -4.9031e-02,  1.1082e-01,  9.0290e-02, -3.0474e-02,\n",
      "         1.1164e-01, -7.7280e-02,  7.0085e-02,  1.9428e-02,  5.5313e-02,\n",
      "         1.1106e-01, -1.1883e-02,  3.7335e-02, -7.9301e-02,  1.0811e-01,\n",
      "        -1.1222e-01,  1.5074e-01, -3.5795e-02,  2.7488e-02, -3.5600e-03,\n",
      "         8.4681e-02,  3.7762e-02, -9.7633e-03, -1.8905e-01, -7.1461e-02,\n",
      "         1.2992e-01, -1.3973e-01, -4.4679e-02, -1.0386e-01,  9.7433e-02,\n",
      "         7.8750e-02,  8.2856e-02, -1.1721e-01, -4.1345e-02,  9.3564e-02,\n",
      "        -9.2854e-02,  1.1935e-01, -1.1652e-01,  1.6174e-02,  1.2212e-01,\n",
      "         8.6132e-02, -1.3363e-01,  8.2934e-02, -9.2710e-02, -3.3534e-02,\n",
      "         8.3959e-02,  1.3878e-01, -1.5862e-01,  9.1869e-03,  5.9080e-02,\n",
      "         7.9278e-02,  1.7799e-01,  8.4116e-02, -1.3343e-01,  9.9810e-02,\n",
      "        -9.6545e-02,  4.7383e-02, -8.6331e-02,  7.1847e-03,  1.6491e-01,\n",
      "         1.4662e-01,  1.2823e-01, -2.2107e-02,  7.9926e-02, -1.3234e-01,\n",
      "         1.6597e-01,  1.3737e-01, -3.7415e-02,  1.4595e-01,  5.1886e-02,\n",
      "        -5.1679e-02,  8.8925e-02, -1.1383e-01, -2.4260e-02, -1.0519e-01,\n",
      "         1.7529e-01, -3.5633e-02, -3.8340e-02, -1.1919e-01,  3.9863e-02,\n",
      "        -7.1174e-02,  8.5152e-02, -6.4063e-02, -1.0907e-01,  1.3946e-02,\n",
      "        -7.5489e-02, -1.2254e-01,  1.6496e-02, -1.3728e-02, -1.0907e-01,\n",
      "        -1.2434e-01,  1.2942e-02,  3.7925e-02,  1.0458e-01,  1.1242e-02,\n",
      "        -4.5544e-02, -9.1723e-02, -6.7543e-02,  6.1198e-02,  1.5548e-01,\n",
      "        -1.1915e-01,  2.7597e-02, -1.8035e-01,  1.2251e-01,  1.4531e-01,\n",
      "         4.5695e-02,  1.2519e-01,  1.6632e-01, -9.0415e-02, -9.5918e-02,\n",
      "        -8.9977e-02, -1.7291e-02, -9.7382e-02, -1.4566e-02, -7.2189e-02,\n",
      "         1.8011e-03,  1.3028e-01,  4.5420e-02,  2.5145e-02, -9.2173e-02,\n",
      "        -5.8469e-02,  9.2367e-02, -1.1986e-01, -1.3530e-01, -1.3963e-01,\n",
      "         1.2209e-01,  5.5936e-04, -5.0808e-02, -7.4634e-02,  4.9620e-02,\n",
      "        -1.1516e-01,  1.3946e-01, -1.3667e-01,  1.4245e-02,  1.7096e-01,\n",
      "         6.4940e-03, -1.3598e-01, -1.3759e-01,  1.3729e-01,  4.7029e-02,\n",
      "        -6.9926e-02,  1.2544e-01, -5.5896e-02,  1.0131e-01, -1.4268e-01,\n",
      "        -6.6309e-02, -7.4353e-02,  1.7377e-01, -4.7109e-02, -1.3729e-02,\n",
      "        -8.0726e-03,  9.5052e-02, -1.4615e-01, -5.8619e-02, -1.2865e-01,\n",
      "        -1.2545e-01,  7.6196e-02, -1.9686e-02,  1.4760e-01,  8.0157e-02,\n",
      "         9.4014e-02, -4.0117e-02,  1.2042e-01,  1.1177e-01,  1.3027e-01,\n",
      "        -3.0185e-02, -1.2567e-01, -1.3577e-01,  1.5464e-01, -1.4710e-01,\n",
      "         4.3994e-02, -9.9803e-02, -4.5403e-02, -1.1377e-01, -8.6547e-03,\n",
      "        -2.3723e-02,  1.1733e-01, -1.7608e-01, -9.1242e-02,  1.0616e-02,\n",
      "        -8.0084e-02, -1.1333e-01, -5.4316e-02, -1.0779e-01, -1.8977e-02,\n",
      "         4.6597e-02, -3.8798e-02,  4.7470e-05,  2.6071e-02, -2.3994e-02,\n",
      "         9.1402e-02,  1.9966e-02,  7.3941e-02,  5.2774e-02, -1.7478e-02,\n",
      "         1.2389e-01, -1.1391e-01,  2.6951e-02, -1.1606e-01, -1.3344e-01,\n",
      "        -1.4759e-01, -5.7651e-03, -1.4172e-01, -8.3470e-02, -8.9579e-04,\n",
      "         9.7065e-02, -7.3963e-02,  1.5741e-01, -1.3600e-01,  4.3365e-03,\n",
      "        -3.8324e-02,  9.2979e-02,  8.6552e-02,  4.5212e-02, -1.2754e-01,\n",
      "         1.2311e-01,  1.4091e-01, -1.7458e-01,  1.0042e-01,  4.4123e-02,\n",
      "         1.0499e-01, -1.0710e-01, -7.5058e-02,  5.3561e-02, -2.5272e-02,\n",
      "        -5.1344e-02, -9.7772e-02,  1.6128e-01, -9.1485e-02,  1.0397e-01,\n",
      "         1.2419e-02, -2.3559e-02, -8.6567e-02,  1.3798e-01,  8.0930e-02,\n",
      "        -4.8980e-02, -8.7553e-02, -1.5975e-01,  3.0054e-02, -3.2420e-02,\n",
      "         2.0201e-02,  7.8407e-02, -1.3206e-01, -6.7589e-03,  4.9661e-02,\n",
      "         1.3081e-01, -1.4031e-01, -1.8108e-01,  1.5774e-01, -1.2043e-01,\n",
      "        -7.0231e-02,  1.2344e-01,  1.4412e-01,  6.9671e-03, -9.3140e-02,\n",
      "        -1.1692e-01, -1.0187e-01,  1.2905e-01,  7.2723e-03,  1.6543e-01])\n",
      "tensor([[ 4.9389e-04, -3.6792e-02, -2.1432e-02,  ..., -1.9962e-02,\n",
      "         -5.2565e-03,  1.6962e-02],\n",
      "        [ 2.0479e-03,  1.6671e-03, -1.4008e-03,  ..., -3.2802e-02,\n",
      "         -3.2941e-03,  2.8445e-02],\n",
      "        [-6.1091e-02,  2.4868e-02, -1.1177e-02,  ..., -1.0235e-02,\n",
      "         -1.8755e-02, -2.2258e-04],\n",
      "        ...,\n",
      "        [ 2.3627e-02, -1.8557e-02, -3.2102e-02,  ...,  4.4563e-02,\n",
      "          2.9497e-02,  1.1738e-02],\n",
      "        [ 5.0182e-05,  5.9447e-03, -3.5349e-03,  ...,  5.9359e-03,\n",
      "          1.6808e-02,  1.7560e-02],\n",
      "        [ 5.9696e-03, -1.0979e-02, -1.9197e-02,  ...,  6.1531e-02,\n",
      "         -2.2316e-02,  3.5027e-02]])\n",
      "tensor([ 0.0116, -0.0288,  0.0308,  0.0226,  0.2309,  0.1195,  0.1605,  0.1157])\n",
      "tensor([[-0.0784, -0.1295, -0.5098,  0.0275],\n",
      "        [-0.2481,  0.1075, -0.2494,  0.0249],\n",
      "        [ 0.1772,  0.2915,  0.1700, -0.1111],\n",
      "        ...,\n",
      "        [ 0.4124,  0.3857, -0.4344, -0.0914],\n",
      "        [-0.3666, -0.3978,  0.3891, -0.0222],\n",
      "        [ 0.2643,  0.2642,  0.4051, -0.1835]])\n",
      "tensor([-4.3020e-02,  2.9969e-01,  6.5399e-02,  3.0189e-01, -5.0614e-01,\n",
      "         3.7838e-01, -4.2457e-01, -2.2316e-01,  1.9417e-01,  3.0755e-01,\n",
      "        -1.8473e-01,  4.2207e-01, -1.0232e-01, -1.4918e-01, -2.6085e-01,\n",
      "         3.2919e-01,  1.6166e-01, -1.4737e-01, -4.7426e-01, -4.7206e-01,\n",
      "         5.2078e-01,  1.3810e-01, -2.7125e-01,  1.5792e-01, -4.7280e-01,\n",
      "         7.3925e-02,  1.8158e-01, -1.4815e-02, -2.8621e-01, -2.0399e-01,\n",
      "         4.1990e-02, -1.1153e-01,  3.2641e-01, -4.2025e-01, -4.5760e-02,\n",
      "         4.0333e-01,  4.8967e-01, -6.3954e-02, -2.1781e-01,  3.8280e-01,\n",
      "        -2.6189e-01, -2.7963e-02,  3.6660e-01,  1.2413e-02,  1.3932e-01,\n",
      "         4.9994e-01,  3.8468e-01, -1.4283e-01,  1.2342e-01, -4.6008e-01,\n",
      "        -4.6035e-01, -8.6044e-02,  3.2241e-02, -2.3757e-01,  8.4951e-02,\n",
      "        -2.4551e-01,  2.3315e-01,  6.3135e-02,  2.4338e-01, -4.4695e-02,\n",
      "         2.6393e-01,  4.6519e-01, -4.8289e-01,  3.4122e-01,  1.7276e-01,\n",
      "        -3.0560e-01, -4.5741e-01, -4.6753e-01,  2.6667e-01,  1.4752e-01,\n",
      "        -2.4959e-01,  2.2586e-01, -4.6134e-01, -1.4954e-01,  5.1866e-01,\n",
      "        -4.8048e-01, -4.0899e-01, -1.7042e-01,  3.4021e-01,  4.9576e-01,\n",
      "        -4.2467e-01,  3.9106e-01, -2.9685e-01,  5.6869e-02,  1.3299e-01,\n",
      "         9.8934e-02,  1.5276e-01, -7.8478e-02,  4.5528e-01,  3.0151e-02,\n",
      "        -4.9500e-01,  3.0414e-01,  1.7387e-01,  4.5944e-04, -3.8290e-01,\n",
      "        -2.7455e-02, -1.4940e-01,  3.8179e-01, -2.5497e-01,  2.6123e-02,\n",
      "         4.3612e-01, -1.9063e-01,  1.9382e-01, -5.8208e-02,  9.2423e-02,\n",
      "        -2.5205e-01,  8.0242e-02, -5.3147e-01,  3.5583e-01,  1.3991e-01,\n",
      "         3.4884e-01,  8.5409e-02, -3.0843e-01, -1.1506e-01, -3.4391e-01,\n",
      "         7.8779e-02, -4.4042e-01,  9.7868e-02, -2.8637e-01, -3.7152e-01,\n",
      "         8.1405e-03, -3.3772e-01, -5.1432e-01,  4.0029e-01,  7.1581e-03,\n",
      "        -3.9896e-02, -4.4091e-01,  1.9275e-01, -2.0327e-01, -1.8298e-01,\n",
      "         2.1414e-01, -3.7660e-01, -2.8724e-01, -4.7778e-01,  4.4487e-01,\n",
      "        -2.1097e-01, -2.1568e-01,  3.8005e-01, -2.9934e-01, -4.7102e-01,\n",
      "        -6.9711e-02, -3.2850e-01,  4.2776e-01,  3.7263e-02,  1.4695e-01,\n",
      "         3.4278e-01,  1.4984e-01,  2.1059e-01, -4.0187e-01,  2.5999e-01,\n",
      "        -2.4620e-01, -1.1500e-02,  3.8363e-02, -1.5345e-01, -2.0955e-01,\n",
      "         1.5547e-01, -3.0834e-01,  4.8383e-01,  1.8081e-01, -2.6497e-01,\n",
      "         3.8297e-01, -1.1881e-01, -3.5884e-01, -2.1329e-01,  4.5147e-01,\n",
      "        -2.6230e-01, -1.8329e-01, -4.8954e-01,  1.1010e-01,  3.1031e-01,\n",
      "        -1.8755e-01,  2.0964e-02, -5.4057e-03, -2.2887e-01,  2.9438e-01,\n",
      "        -2.7214e-01, -4.5021e-01, -5.2709e-01,  4.9499e-01,  1.9709e-01,\n",
      "        -3.6606e-01, -3.4032e-01, -1.8129e-01,  3.2917e-01, -1.2291e-01,\n",
      "        -4.2586e-01,  3.9982e-01, -2.7707e-01, -2.9440e-01, -8.2126e-02,\n",
      "         6.0296e-02, -3.6475e-02,  4.0823e-01,  2.4409e-01,  2.4589e-01,\n",
      "        -4.0850e-02, -2.7215e-02, -1.2572e-01, -3.5818e-02,  2.9811e-01,\n",
      "        -2.6556e-01, -3.6270e-02,  1.4641e-01,  1.0953e-01, -5.8666e-02,\n",
      "         2.1830e-01,  1.0167e-02, -3.4519e-01, -1.9280e-01,  3.7310e-01,\n",
      "         3.3991e-01,  3.5430e-01, -9.8209e-02, -5.0136e-01, -1.4529e-01,\n",
      "        -3.2692e-01, -4.8623e-01,  5.1324e-01, -3.4279e-01,  4.4396e-01,\n",
      "         2.7882e-02, -3.1055e-01, -5.0080e-01, -3.7689e-01,  4.2322e-01,\n",
      "        -4.6641e-02, -3.9868e-01, -2.9844e-01, -2.3105e-01,  3.1161e-01,\n",
      "         2.6747e-02, -5.8259e-02, -5.8371e-01,  1.6868e-01, -5.0295e-01,\n",
      "        -2.1525e-01, -2.5489e-01, -2.8370e-01,  4.1358e-01, -4.8061e-02,\n",
      "         1.1933e-01, -3.3910e-01, -4.3503e-01, -9.6355e-02,  4.3248e-01,\n",
      "         5.4411e-02, -3.6087e-01,  5.8935e-02,  4.1195e-01, -4.6014e-02,\n",
      "        -4.4776e-02,  1.1352e-01, -1.9175e-01, -4.3912e-01,  2.4847e-01,\n",
      "        -2.0782e-01,  4.2239e-01, -2.4262e-01,  4.0668e-01, -4.0237e-01,\n",
      "         2.9072e-01, -3.3332e-01, -2.9129e-01, -4.9142e-01, -5.4206e-01,\n",
      "         1.7218e-01, -2.5421e-01,  4.9640e-01, -4.4749e-02,  3.0032e-02,\n",
      "        -4.6002e-01, -4.5387e-02,  3.4294e-01, -4.4748e-02,  7.7214e-02,\n",
      "        -4.2339e-01, -1.0649e-01,  1.8151e-01,  1.3043e-01,  7.7580e-02,\n",
      "        -1.0599e-03, -5.9093e-02,  2.3774e-01, -2.8147e-01, -1.4200e-01,\n",
      "        -2.7043e-01,  1.3340e-01,  2.2301e-01, -4.5934e-03, -2.1648e-01,\n",
      "        -4.1928e-01, -3.6548e-01,  2.8747e-01, -4.8238e-01,  1.1651e-02,\n",
      "         3.0597e-01, -2.9933e-01, -4.1778e-01, -2.3436e-01, -3.5503e-01,\n",
      "         1.1685e-01,  3.1811e-01, -3.5806e-01,  1.2905e-01,  1.7959e-02,\n",
      "        -1.4593e-01,  1.3884e-01,  2.4195e-01,  2.4333e-01,  1.8740e-01,\n",
      "        -1.0046e-01,  2.5516e-01, -2.2813e-01, -4.8857e-01, -1.8231e-01,\n",
      "        -5.0858e-01,  4.3496e-01, -2.3021e-01,  4.4912e-01,  4.6785e-01,\n",
      "        -3.0700e-01, -8.5895e-03,  2.1984e-01, -5.3838e-01, -2.6413e-01,\n",
      "        -2.5318e-02,  1.2573e-01,  2.3074e-01, -1.2955e-01, -4.4345e-01,\n",
      "         2.1200e-01, -1.9676e-01, -1.4202e-01,  1.8499e-01, -4.0162e-01,\n",
      "         4.1014e-02,  4.3653e-01, -3.8940e-01,  3.3421e-02, -8.5253e-02,\n",
      "         4.4427e-01,  4.4633e-01, -4.5393e-01,  3.7066e-01, -4.0457e-01,\n",
      "         3.7730e-01, -5.3893e-01,  2.4798e-01,  2.5763e-01, -2.3542e-01])\n",
      "tensor([1.0214, 0.9607, 0.9753, 0.9525, 0.9841, 1.0175, 0.9466, 1.0020, 0.9733,\n",
      "        0.9353, 0.9889, 0.9632, 0.9799, 0.9367, 0.9546, 0.9776, 0.9271, 1.0126,\n",
      "        0.9684, 0.9806, 0.9855, 0.9571, 0.9970, 0.9661, 0.9900, 0.9763, 0.9787,\n",
      "        0.9664, 1.0212, 0.9590, 0.9766, 1.0290, 0.9634, 1.0003, 0.9690, 1.0052,\n",
      "        0.9625, 0.9661, 1.0015, 0.9565, 0.9995, 0.9442, 1.0138, 1.0041, 0.9874,\n",
      "        0.9901, 0.9443, 0.9799, 0.9638, 0.9981, 0.9909, 0.9542, 0.9780, 0.9394,\n",
      "        0.9751, 1.0121, 0.9627, 1.0053, 0.9772, 0.9654, 0.9879, 0.9595, 0.9794,\n",
      "        0.9583, 0.9640, 0.9683, 0.9291, 0.9645, 0.9467, 0.9437, 1.0086, 0.9789,\n",
      "        0.9937, 0.9638, 0.9621, 0.9702, 0.9637, 0.9546, 0.9457, 0.9364, 0.9875,\n",
      "        0.9772, 1.0080, 0.9940, 0.9943, 0.9446, 0.9418, 0.9863, 0.9592, 0.9714,\n",
      "        1.0097, 0.9746, 1.0206, 0.9791, 0.9685, 0.9607, 1.0621, 0.9502, 0.9719,\n",
      "        0.9686, 0.9776, 0.9408, 0.9587, 0.9993, 0.9898, 0.9913, 1.0089, 0.9942,\n",
      "        0.9770, 0.9643, 1.0060, 0.9728, 0.9395, 0.9816, 0.9987, 0.9799, 1.0013,\n",
      "        0.9507, 0.9725, 0.9871, 0.9726, 0.9823, 0.9817, 0.9956, 0.9648, 0.9513,\n",
      "        0.9469, 1.0043, 1.0093, 0.9838, 0.9581, 0.9980, 0.9586, 0.9341, 0.9498,\n",
      "        0.9360, 0.9692, 0.9791, 0.9952, 0.8891, 0.9517, 1.0032, 0.9788, 1.0254,\n",
      "        0.9926, 0.9951, 0.9957, 0.9889, 0.9567, 0.9878, 0.9763, 0.9736, 0.9523,\n",
      "        1.0265, 0.9872, 0.9872, 0.9862, 1.0008, 0.9674, 0.9797, 0.9618, 0.9817,\n",
      "        1.0299, 0.9613, 0.9606, 0.9636, 1.0804, 0.9932, 0.9638, 0.9713, 1.0084,\n",
      "        0.9574, 0.9995, 0.9877, 0.9436, 0.9922, 0.9414, 0.9972, 0.9651, 1.0031,\n",
      "        0.9894, 0.9650, 0.9786, 0.9754, 0.9883, 1.0024, 0.9721, 0.9772, 0.9417,\n",
      "        0.9540, 0.9689, 0.9947, 0.9573, 0.9793, 0.9658, 0.9786, 0.9807, 1.0685,\n",
      "        0.9678, 0.9633, 0.9811, 0.9694, 0.9778, 0.9784, 0.9675, 0.9522, 0.9669,\n",
      "        0.9736, 0.9887, 0.9506, 0.9556, 1.0429, 0.9548, 0.9908, 0.9534, 0.9973,\n",
      "        0.9722, 0.9594, 1.0234, 0.9782, 0.9918, 1.0366, 0.9978, 1.0172, 0.9702,\n",
      "        0.9470, 1.0002, 1.0043, 1.0556, 0.9546, 0.9626, 0.9640, 1.0128, 0.9227,\n",
      "        1.0228, 0.9630, 1.0923, 0.9534, 0.9497, 0.9575, 0.9307, 0.9842, 0.9799,\n",
      "        0.9514, 0.9744, 0.9964, 0.9791, 0.9983, 0.9338, 1.0304, 0.9687, 0.9793,\n",
      "        0.9414, 0.9556, 0.9418, 0.9726, 1.0133, 0.9682, 0.9624, 0.9522, 0.9750,\n",
      "        0.9746, 0.9742, 0.9715, 0.9995, 0.9534, 0.9812, 0.9298, 0.9679, 0.9761,\n",
      "        0.9820, 0.9611, 0.9784, 1.0509, 0.9581, 0.9741, 0.9699, 1.0246, 1.0269,\n",
      "        0.9768, 1.0561, 1.0022, 0.9554, 0.9633, 1.0148, 1.0124, 0.9731, 0.9418,\n",
      "        0.9586, 1.0183, 0.9640, 1.0010, 0.9368, 1.0142, 1.0028, 0.9431, 0.9629,\n",
      "        0.9711, 0.9746, 0.9797, 0.9707, 0.9861, 0.9514, 0.9577, 0.9402, 0.9643,\n",
      "        0.9719, 0.9409, 1.0413, 0.9679, 1.0423, 0.9732, 0.9561, 0.9585, 0.9716,\n",
      "        0.9739, 0.9695, 0.9349, 0.9455, 0.9541, 0.9518, 0.9774, 1.0202, 0.9810,\n",
      "        1.0103, 0.9516, 0.9575, 0.9936, 0.9593, 0.9706, 0.9716, 1.0004, 0.9593,\n",
      "        0.9749, 0.9647, 0.9730, 1.0134, 0.9926, 0.9746, 0.9837, 0.9848, 0.9700,\n",
      "        1.0456, 0.9505, 0.9845, 0.9519, 1.0104, 0.9886, 0.9549, 0.9849])\n",
      "tensor([ 0.0488,  0.0820, -0.0549,  0.0379,  0.0119,  0.0141,  0.0049,  0.0967,\n",
      "        -0.0088, -0.0090,  0.0291, -0.0280, -0.0598,  0.0181,  0.0076,  0.0065,\n",
      "         0.0115,  0.0235, -0.0380,  0.0255,  0.0322, -0.0392,  0.0023, -0.0052,\n",
      "        -0.0989,  0.0308, -0.0584,  0.0343,  0.0648, -0.0091, -0.0136,  0.0784,\n",
      "         0.0263,  0.0371, -0.0014,  0.0035, -0.0264,  0.1138,  0.0549, -0.0071,\n",
      "        -0.0208, -0.0527, -0.0504, -0.0629,  0.0162, -0.0363, -0.0358, -0.0645,\n",
      "        -0.0849, -0.0016,  0.0156, -0.0902, -0.0423,  0.0186, -0.0459, -0.0027,\n",
      "         0.0036,  0.0683,  0.0496,  0.0838, -0.0061, -0.0594, -0.0665,  0.0081,\n",
      "        -0.0146, -0.0643,  0.0135, -0.0214,  0.0075, -0.0309,  0.0130,  0.0389,\n",
      "        -0.0064, -0.0578, -0.0212,  0.0116, -0.0498, -0.0069, -0.0468, -0.0036,\n",
      "         0.0356, -0.0161, -0.0862,  0.0184, -0.0019, -0.0308,  0.0051,  0.0064,\n",
      "         0.0146, -0.0334,  0.0065, -0.0598, -0.0285,  0.0496, -0.0619, -0.0031,\n",
      "         0.0921,  0.0216,  0.0147,  0.0402, -0.0502,  0.0119, -0.1240,  0.0120,\n",
      "         0.0310,  0.0257,  0.0450,  0.0808, -0.0893, -0.0384,  0.0248, -0.0243,\n",
      "         0.0896, -0.0400, -0.0874,  0.0193, -0.0502,  0.0280, -0.0704,  0.0060,\n",
      "        -0.0207, -0.0131,  0.0021,  0.0303, -0.0267,  0.0089, -0.0189,  0.0200,\n",
      "        -0.0105,  0.0243,  0.0221, -0.0040,  0.0567, -0.0263, -0.0798,  0.0232,\n",
      "        -0.0419, -0.0252,  0.0264,  0.0596, -0.0134,  0.0443, -0.0569, -0.0400,\n",
      "         0.0345, -0.0321,  0.0265,  0.0032,  0.0462,  0.0026,  0.0058,  0.0778,\n",
      "        -0.0107,  0.0237, -0.0246,  0.0677,  0.0334,  0.0023,  0.0329, -0.0039,\n",
      "        -0.0657, -0.0281, -0.0512,  0.0586,  0.0116, -0.0163,  0.0583,  0.0248,\n",
      "         0.0117, -0.0200,  0.0103,  0.0567, -0.0736, -0.0940, -0.0574,  0.0160,\n",
      "         0.0685, -0.0544, -0.0161, -0.0147, -0.0137,  0.0357, -0.0145, -0.0185,\n",
      "         0.0126,  0.0170,  0.0355, -0.0348,  0.0333, -0.0147, -0.0524, -0.0258,\n",
      "        -0.0217,  0.0374, -0.0029, -0.0369,  0.0358, -0.0432,  0.0047, -0.0268,\n",
      "         0.0187,  0.0189,  0.0775, -0.0161,  0.0373,  0.0035, -0.0651, -0.0031,\n",
      "        -0.0560,  0.0141, -0.0172, -0.0387, -0.0049,  0.0447, -0.0283,  0.0310,\n",
      "        -0.0122, -0.0771,  0.0084,  0.0383, -0.0182,  0.0588,  0.0323, -0.0089,\n",
      "         0.0117, -0.0462, -0.0441, -0.0307,  0.0336,  0.0416, -0.0540, -0.0496,\n",
      "         0.0024, -0.1044, -0.0312, -0.0071,  0.0705, -0.0173, -0.0912,  0.0395,\n",
      "        -0.0239,  0.0047,  0.0379, -0.0210, -0.0298,  0.0107,  0.0150,  0.0258,\n",
      "        -0.0217,  0.0667, -0.0590, -0.0578, -0.0206, -0.0086, -0.0740,  0.0654,\n",
      "         0.0261, -0.0151, -0.0536,  0.0036, -0.0478,  0.0096, -0.0363, -0.0140,\n",
      "        -0.0173,  0.0389, -0.0813, -0.0603,  0.0012, -0.0014, -0.0011,  0.0354,\n",
      "         0.0588, -0.0531,  0.0156, -0.0762,  0.0138,  0.0003, -0.0225,  0.0383,\n",
      "        -0.0350,  0.0076, -0.0648, -0.0609, -0.0326,  0.0076, -0.0064,  0.0059,\n",
      "        -0.0252,  0.0428, -0.0310,  0.0339, -0.0116, -0.0207,  0.0082, -0.0336,\n",
      "        -0.0006, -0.0714,  0.0014, -0.0133,  0.0509, -0.0076, -0.0505,  0.0008,\n",
      "        -0.0561, -0.0119,  0.0180, -0.0930, -0.0345, -0.0565,  0.0919,  0.0472,\n",
      "         0.1384, -0.0189,  0.0298,  0.0276,  0.0493,  0.0147, -0.0002, -0.0120,\n",
      "         0.0032, -0.0037,  0.0341,  0.0535,  0.0770,  0.0357, -0.0287, -0.0154,\n",
      "         0.0335, -0.0668, -0.0227, -0.0123, -0.0855, -0.0028, -0.0627,  0.0157,\n",
      "        -0.0378, -0.0165, -0.0730, -0.0157, -0.0699,  0.0549,  0.0308,  0.0397,\n",
      "        -0.0565,  0.0091,  0.0140,  0.0361,  0.0060,  0.0498])\n",
      "tensor([[-0.0562, -0.0275,  0.0060,  ..., -0.0115, -0.0477, -0.0365],\n",
      "        [-0.0033,  0.0109, -0.0192,  ..., -0.0086, -0.0638, -0.0599],\n",
      "        [-0.0561,  0.0195, -0.0375,  ..., -0.0382,  0.0477, -0.0713],\n",
      "        ...,\n",
      "        [-0.0341,  0.0589,  0.0352,  ...,  0.0022, -0.0411, -0.0056],\n",
      "        [-0.0303, -0.0055, -0.0358,  ...,  0.0133, -0.0170,  0.0268],\n",
      "        [ 0.0500,  0.0515, -0.0328,  ...,  0.0040, -0.0572,  0.0202]])\n",
      "tensor([-0.0516, -0.0315, -0.0649, -0.0264, -0.0465, -0.0282, -0.0249,  0.0140,\n",
      "        -0.0140, -0.0606,  0.0795, -0.0463, -0.0802, -0.0683,  0.0423, -0.0379,\n",
      "        -0.0756,  0.0067, -0.0238, -0.0600,  0.0647, -0.0417, -0.0580, -0.0487,\n",
      "        -0.0377, -0.0712, -0.0115,  0.0677, -0.1367, -0.0193, -0.0922, -0.0460,\n",
      "         0.0222,  0.0022, -0.0546, -0.0021,  0.0124,  0.0243, -0.1006,  0.0265,\n",
      "        -0.0838, -0.0336,  0.0300, -0.0539, -0.0034,  0.1055, -0.0186, -0.0067,\n",
      "         0.0198, -0.0454, -0.0604,  0.0229, -0.0157, -0.0006, -0.0288,  0.0174,\n",
      "         0.0269, -0.0319,  0.0072, -0.0128,  0.0296,  0.0067, -0.0252, -0.1206,\n",
      "        -0.0445, -0.0155, -0.0021,  0.0393,  0.0393, -0.0284, -0.0207,  0.0328,\n",
      "        -0.0869, -0.0853,  0.1081,  0.0111, -0.0234, -0.0162, -0.0535,  0.0835,\n",
      "         0.0227,  0.0159, -0.0214,  0.0144, -0.1333,  0.0442, -0.0564, -0.0527,\n",
      "        -0.0875,  0.0147, -0.0630, -0.0221, -0.0141,  0.0493, -0.0129, -0.0489,\n",
      "        -0.0315,  0.0431,  0.0095, -0.0986,  0.0444,  0.0235,  0.0093, -0.0177,\n",
      "        -0.0168, -0.0642,  0.0143, -0.0590,  0.0023,  0.0115, -0.0146,  0.0476,\n",
      "         0.0221, -0.1196, -0.0074, -0.0386, -0.0219,  0.0390, -0.0058,  0.0463,\n",
      "         0.0578,  0.0361,  0.0137,  0.0045, -0.0316, -0.0184,  0.0243,  0.0360,\n",
      "        -0.0412,  0.0102, -0.0116,  0.0214,  0.0013, -0.0484,  0.0185, -0.0100,\n",
      "         0.0407, -0.0302, -0.0698, -0.0775,  0.0424, -0.0132, -0.0401, -0.0349,\n",
      "        -0.1585, -0.0277,  0.0492, -0.0412, -0.0259, -0.0109, -0.0444, -0.0546,\n",
      "         0.0507, -0.0110, -0.0712,  0.0713, -0.0502,  0.0775, -0.0290, -0.0380,\n",
      "         0.0292, -0.1119, -0.1075, -0.0159, -0.0094, -0.1256, -0.0179, -0.0658,\n",
      "        -0.0102, -0.0295,  0.0303, -0.0316, -0.0083, -0.0454,  0.0132, -0.0017,\n",
      "        -0.0682, -0.0135,  0.0481, -0.0999, -0.0119, -0.0396,  0.0280,  0.0619,\n",
      "        -0.0382, -0.0210,  0.0075, -0.0248, -0.0703,  0.0410, -0.0458, -0.0060,\n",
      "        -0.0332, -0.0511, -0.0547, -0.0614,  0.0732, -0.0325,  0.0287,  0.0206,\n",
      "         0.0304, -0.0706, -0.0056, -0.0141, -0.0373,  0.0520, -0.0478, -0.0722,\n",
      "         0.0770,  0.0381, -0.0056,  0.0513, -0.0307, -0.0395, -0.0078, -0.0432,\n",
      "         0.0168, -0.0488,  0.0091,  0.0526,  0.0755, -0.0087, -0.0094, -0.1160,\n",
      "         0.0433, -0.0192, -0.0290, -0.0046,  0.0399,  0.0195,  0.0206,  0.0225,\n",
      "        -0.1350, -0.0784, -0.0151, -0.0293, -0.0325, -0.0198, -0.0338, -0.0058,\n",
      "        -0.0611,  0.0136, -0.0412, -0.1146, -0.0347,  0.0399,  0.0269, -0.0095,\n",
      "        -0.0064, -0.0064,  0.0095,  0.0391, -0.0406,  0.0294,  0.0096, -0.0533,\n",
      "        -0.0717, -0.0263,  0.0889,  0.0132, -0.0124, -0.0304, -0.0049,  0.0533,\n",
      "         0.0185, -0.0531,  0.0072,  0.0628, -0.0334, -0.0475, -0.0007,  0.0092,\n",
      "         0.0100, -0.1176, -0.0268,  0.0273, -0.0303, -0.0633, -0.0578, -0.0502,\n",
      "         0.0209,  0.0257,  0.0897, -0.0270, -0.0587,  0.0162, -0.0280, -0.0796,\n",
      "         0.0118, -0.0998,  0.0527,  0.0140, -0.0566, -0.1184,  0.0262,  0.0704,\n",
      "        -0.0224, -0.0155, -0.0184, -0.0867,  0.0138,  0.0345, -0.0886, -0.0243,\n",
      "        -0.0185, -0.0134,  0.0279,  0.0372, -0.0233, -0.0243, -0.0835,  0.0070,\n",
      "        -0.0879, -0.0186, -0.0254, -0.0918, -0.0111,  0.0395, -0.0692, -0.0192,\n",
      "        -0.0111, -0.0070,  0.0169, -0.0404, -0.0346, -0.0487,  0.0467, -0.0058,\n",
      "         0.0258,  0.0350,  0.0144, -0.0068, -0.0341, -0.0500, -0.0047, -0.0787,\n",
      "        -0.0293, -0.0772, -0.0164,  0.0194, -0.0194,  0.0057,  0.0038,  0.0138,\n",
      "        -0.0567, -0.0607, -0.0023, -0.0083, -0.0300,  0.0390])\n",
      "tensor([1.0421, 1.0052, 0.9821, 1.0109, 1.0011, 1.0182, 0.9673, 1.0031, 1.0448,\n",
      "        1.0042, 0.9768, 1.0079, 0.9980, 0.9977, 1.0166, 0.9862, 0.9788, 1.0313,\n",
      "        0.9901, 1.0279, 0.9840, 0.9668, 0.9685, 0.9738, 1.0242, 1.0028, 0.9881,\n",
      "        0.9920, 0.9912, 1.0130, 1.0404, 1.0210, 1.0019, 1.0073, 1.0333, 0.9885,\n",
      "        0.9964, 1.0199, 0.9895, 0.9538, 1.0097, 1.0626, 1.0214, 0.9877, 1.0074,\n",
      "        1.0198, 0.9500, 1.0195, 0.9594, 1.0381, 1.0440, 1.0199, 1.0195, 1.0008,\n",
      "        1.0116, 0.9543, 0.9907, 1.0012, 1.0003, 0.9826, 1.0133, 1.0091, 1.0398,\n",
      "        1.0366, 0.9840, 1.0357, 0.9990, 1.0268, 1.0155, 1.0042, 0.9763, 1.0158,\n",
      "        0.9877, 0.9888, 1.0160, 0.9973, 1.0046, 1.0354, 1.0204, 0.9468, 1.0313,\n",
      "        1.0227, 0.9981, 0.9776, 1.0895, 1.0115, 1.0001, 0.9868, 0.9902, 1.0011,\n",
      "        1.0465, 0.9968, 0.9801, 0.9933, 1.0131, 1.0490, 1.0446, 0.9746, 1.0454,\n",
      "        0.9952, 0.9932, 0.9972, 0.9942, 0.9809, 0.9904, 0.9857, 0.9930, 1.0312,\n",
      "        0.9825, 0.9870, 0.9534, 0.9576, 1.0338, 1.0042, 0.9712, 1.0389, 0.9895,\n",
      "        1.0234, 1.0247, 1.0169, 0.9827, 1.0188, 0.9663, 1.0121, 1.0220, 1.0139,\n",
      "        1.0064, 1.0251, 0.9865, 0.9819, 0.9872, 0.9999, 0.9970, 1.0669, 0.9892,\n",
      "        0.9882, 1.0282, 0.9871, 1.0112, 1.0374, 1.0138, 1.0172, 1.0148, 0.9546,\n",
      "        1.0293, 0.9937, 0.9643, 0.9802, 0.9792, 1.0203, 1.0069, 0.9592, 1.0038,\n",
      "        1.0281, 0.9906, 1.0092, 1.0229, 1.0044, 0.9852, 0.9992, 1.0271, 1.0139,\n",
      "        1.0485, 0.9785, 0.9797, 1.0496, 0.9701, 1.0131, 1.0059, 1.0013, 1.0112,\n",
      "        1.0231, 1.0297, 1.0077, 1.0097, 1.0927, 1.0705, 1.0244, 0.9862, 0.9695,\n",
      "        0.9905, 0.9753, 1.0094, 0.9623, 0.9782, 0.9994, 1.0348, 0.9868, 1.0093,\n",
      "        0.9958, 0.9881, 1.0126, 0.9821, 1.0298, 1.0068, 1.0041, 0.9519, 0.9651,\n",
      "        0.9660, 0.9731, 0.9855, 0.9911, 1.0130, 1.0235, 1.0284, 0.9930, 0.9959,\n",
      "        0.9971, 0.9505, 0.9900, 0.9990, 0.9766, 1.0389, 0.9969, 0.9763, 1.0360,\n",
      "        1.0168, 1.0222, 1.0021, 0.9618, 1.0065, 0.9793, 1.0168, 1.0096, 1.0105,\n",
      "        1.0069, 1.0019, 0.9796, 0.9995, 0.9798, 0.9835, 0.9857, 1.0191, 1.0066,\n",
      "        0.9913, 0.9869, 1.0015, 0.9966, 0.9836, 0.9932, 0.9971, 1.0423, 1.0455,\n",
      "        1.0071, 0.9449, 1.0022, 0.9723, 1.0068, 0.9856, 1.0601, 0.9691, 1.0021,\n",
      "        0.9626, 1.0200, 0.9809, 1.0581, 1.0350, 1.0231, 1.0058, 0.9835, 1.0340,\n",
      "        1.0331, 0.9997, 0.9998, 0.9977, 0.9904, 0.9929, 0.9577, 1.0225, 1.0397,\n",
      "        1.0755, 0.9925, 0.9765, 0.9841, 1.0426, 1.0421, 1.0265, 1.0267, 1.0135,\n",
      "        1.0038, 0.9826, 1.0029, 0.9843, 1.0085, 1.0392, 1.0236, 0.9665, 0.9861,\n",
      "        0.9772, 1.0345, 1.0119, 1.0195, 1.0142, 1.0125, 1.0071, 0.9839, 0.9707,\n",
      "        1.0563, 1.0329, 0.9912, 0.9655, 1.0086, 1.0114, 0.9801, 0.9540, 1.0059,\n",
      "        0.9840, 0.9769, 1.0116, 1.0123, 1.0375, 0.9744, 1.0247, 0.9986, 0.9987,\n",
      "        1.0158, 0.9761, 1.0329, 1.0465, 0.9888, 1.0308, 1.0301, 1.0077, 1.0303,\n",
      "        0.9967, 1.0198, 1.0119, 1.0182, 0.9968, 0.9613, 0.9739, 1.0142, 1.0130,\n",
      "        1.0615, 1.0537, 1.0306, 1.0320, 0.9998, 0.9781, 1.0428, 0.9837, 0.9831,\n",
      "        1.0055, 0.9791, 1.0217, 0.9556, 0.9823, 1.0007, 0.9994, 1.0041])\n",
      "tensor([ 2.6530e-02,  3.4068e-02,  6.7312e-02, -7.8475e-03, -1.3262e-02,\n",
      "         7.5315e-03,  1.1240e-02, -4.1953e-02,  4.3788e-03,  1.3003e-02,\n",
      "        -1.0826e-02, -3.8767e-02,  2.1830e-02,  2.0539e-02,  4.0463e-03,\n",
      "         2.3981e-02, -4.3788e-02, -9.6494e-03,  4.1471e-02, -3.5621e-04,\n",
      "         1.8546e-02, -6.3270e-02, -3.5969e-03,  1.1481e-02, -3.9631e-03,\n",
      "        -6.5629e-03,  9.8632e-02, -1.2366e-02, -4.4768e-03,  1.6067e-02,\n",
      "        -2.1741e-02, -8.0580e-03, -4.1869e-02, -2.9693e-02,  3.8821e-02,\n",
      "        -9.4130e-03, -4.1922e-02,  2.9888e-03,  1.4397e-02, -7.6406e-04,\n",
      "         2.3460e-02, -3.8164e-02,  4.5126e-02,  5.8815e-02,  6.8374e-02,\n",
      "         3.8053e-02,  1.4067e-02,  8.3054e-02, -6.1149e-02, -4.5795e-02,\n",
      "        -2.5191e-02,  1.8380e-02,  7.3665e-02,  1.2341e-02,  1.0799e-02,\n",
      "        -7.4914e-02, -3.9902e-02, -7.8940e-03, -1.2031e-03,  3.0925e-02,\n",
      "         2.3278e-02,  4.8276e-02, -4.7751e-02, -3.0060e-02, -5.1525e-02,\n",
      "        -2.1637e-02,  3.5585e-02,  5.4899e-02,  2.7492e-02,  5.9021e-02,\n",
      "         9.9745e-02,  2.2124e-02,  7.6397e-04, -6.2784e-02,  4.6928e-03,\n",
      "         1.7939e-02, -5.6119e-02,  7.1678e-02, -1.4697e-02,  3.8676e-02,\n",
      "         8.9099e-02, -2.7013e-02,  4.9152e-02, -1.8754e-02, -5.1652e-02,\n",
      "        -5.6167e-02, -1.3397e-03,  1.1871e-02, -5.2695e-02,  1.6686e-02,\n",
      "        -2.6515e-02, -1.9629e-02, -3.1140e-02, -1.8567e-02, -5.4428e-02,\n",
      "        -1.7093e-02,  2.2931e-02,  1.7468e-02,  1.0947e-02, -1.4821e-02,\n",
      "         4.6458e-02, -2.8857e-02, -9.8483e-03, -2.4354e-02,  4.0958e-03,\n",
      "         1.1553e-02,  1.4093e-02,  1.8364e-02, -1.9876e-03,  1.5568e-02,\n",
      "        -5.4005e-02,  3.2357e-03, -2.3236e-02,  5.6477e-03, -1.0799e-02,\n",
      "        -8.8480e-03, -3.9217e-02, -8.9828e-03,  3.4407e-02,  6.3522e-03,\n",
      "        -1.4279e-02,  2.3888e-02,  2.6430e-02,  3.4631e-02,  5.9434e-02,\n",
      "         5.6107e-03, -3.7069e-02, -9.6610e-03,  4.2594e-02,  2.5945e-02,\n",
      "         6.9011e-03, -2.0582e-02, -8.9410e-03, -5.3910e-02,  3.9308e-02,\n",
      "         4.4552e-02, -2.9592e-02, -6.2558e-02,  1.3149e-02,  5.5392e-03,\n",
      "         1.4186e-02, -1.9986e-02,  8.8822e-03, -1.4202e-02,  2.7615e-02,\n",
      "         5.3317e-02,  2.4163e-02, -2.8994e-03,  9.8716e-03,  6.1870e-02,\n",
      "         5.0399e-02, -1.7055e-02,  1.5262e-02, -5.9504e-02,  1.0424e-02,\n",
      "        -5.2957e-02,  2.7312e-02,  2.4682e-02, -9.2089e-03,  1.4456e-02,\n",
      "         5.5120e-03, -7.7923e-03, -3.3280e-02, -3.1693e-02,  3.6217e-02,\n",
      "         1.9619e-02,  4.4898e-02, -3.6008e-02,  1.1219e-02, -1.7697e-02,\n",
      "         2.2847e-02,  1.5782e-02,  6.2070e-02, -2.2819e-02,  8.4239e-02,\n",
      "         7.4501e-03, -9.7937e-03, -6.3666e-02,  4.5607e-02, -3.1805e-02,\n",
      "         3.6856e-02,  3.9972e-02, -4.2071e-02, -4.5021e-03, -3.1917e-02,\n",
      "         2.2301e-02,  2.9831e-02, -2.4223e-02,  4.2356e-02,  4.3691e-03,\n",
      "        -2.2190e-02,  2.5498e-02,  4.9704e-05,  4.6759e-02,  4.6778e-02,\n",
      "        -3.7739e-02, -1.9008e-02, -1.3249e-02, -5.3888e-03, -5.6318e-02,\n",
      "        -1.7213e-02,  2.5543e-02, -3.2097e-02,  5.9015e-02,  3.4878e-02,\n",
      "         4.6962e-02,  7.8576e-02, -9.5014e-03,  1.6976e-02, -9.1342e-02,\n",
      "        -1.4488e-02,  3.2907e-02,  7.1920e-02,  3.9064e-02,  8.3003e-03,\n",
      "         1.2628e-02, -3.6837e-02,  3.9729e-02,  3.3719e-02,  2.0454e-02,\n",
      "         2.8438e-02,  9.0228e-03, -1.4908e-02,  6.6780e-02,  1.2661e-02,\n",
      "         5.1994e-02, -1.4851e-02,  3.0051e-02,  3.6721e-02,  5.2340e-03,\n",
      "         3.0619e-03,  3.0778e-02, -9.4560e-03,  3.2280e-02, -1.1014e-02,\n",
      "         4.3687e-02, -6.7893e-02, -4.6850e-02,  9.7631e-03,  3.2313e-02,\n",
      "         2.8892e-02, -1.3917e-02, -3.4229e-02,  1.6975e-02, -1.3078e-02,\n",
      "        -3.7331e-02,  1.0147e-02, -4.0304e-02, -5.5163e-03, -3.6798e-02,\n",
      "         3.0228e-02,  1.9561e-02, -4.3691e-03, -3.7808e-02, -7.8656e-03,\n",
      "         9.1757e-02,  3.0811e-02, -1.4044e-02,  4.7375e-02,  4.7435e-02,\n",
      "        -4.1955e-02, -2.9351e-02,  1.0494e-02,  3.7287e-02, -2.3301e-02,\n",
      "        -2.1342e-02,  2.9134e-02, -1.6506e-02,  3.6097e-02,  3.2266e-02,\n",
      "         2.9259e-02, -4.7643e-02, -5.4631e-03, -1.9915e-02, -8.0284e-02,\n",
      "         4.2786e-02,  3.2800e-02, -5.5490e-03,  2.5245e-02,  1.3271e-02,\n",
      "        -2.0215e-02,  2.7012e-02,  5.7934e-02,  1.6542e-02, -1.8728e-02,\n",
      "        -2.2956e-02, -3.0397e-02, -4.5016e-03,  5.1769e-02, -3.7549e-02,\n",
      "         6.8749e-03,  4.1289e-03,  3.3011e-02, -5.8532e-02,  5.0159e-02,\n",
      "         3.2656e-02,  2.1113e-03, -8.2517e-03,  2.4149e-02,  3.8592e-02,\n",
      "        -4.5529e-02, -3.7461e-02,  5.1672e-02,  1.5865e-02,  2.1098e-02,\n",
      "         9.4689e-03,  2.8948e-02, -3.2434e-02,  1.9499e-02, -1.7465e-02,\n",
      "         6.4700e-02,  6.2926e-02, -5.3374e-02,  6.8129e-03,  5.6141e-02,\n",
      "         1.6101e-02, -3.6139e-02,  2.8665e-02,  3.5079e-02, -7.9447e-03,\n",
      "        -1.3293e-02,  7.3510e-03, -1.2716e-02,  6.3565e-02, -1.6406e-02,\n",
      "        -5.9021e-02,  7.5282e-03, -1.8560e-02,  8.6926e-02, -9.3193e-03,\n",
      "        -3.4289e-02, -2.4119e-02, -4.7480e-02,  4.7163e-03, -3.2394e-02,\n",
      "         1.8527e-02,  3.9744e-02,  3.7574e-02, -2.8846e-02,  7.7723e-02,\n",
      "         1.2077e-02, -1.2148e-02, -3.3115e-02,  3.1379e-02,  9.3003e-03,\n",
      "        -1.2808e-04, -3.7130e-02,  4.0527e-02, -3.4974e-02,  5.0961e-02])\n",
      "tensor([[-0.0680, -0.0349, -0.0519,  ...,  0.0206,  0.0345, -0.0268],\n",
      "        [ 0.0047, -0.0663, -0.0292,  ..., -0.0101, -0.0019,  0.0266],\n",
      "        [ 0.0143,  0.0309, -0.0437,  ...,  0.0046,  0.0637,  0.0335],\n",
      "        ...,\n",
      "        [ 0.0597, -0.0180,  0.0300,  ..., -0.0700,  0.0120, -0.0122],\n",
      "        [-0.0315, -0.0946,  0.0342,  ...,  0.0589, -0.0048, -0.0078],\n",
      "        [ 0.0026, -0.0551,  0.0204,  ..., -0.0432, -0.0185, -0.0374]])\n",
      "tensor([-0.1055, -0.0142, -0.0393,  0.0444, -0.0327, -0.0736,  0.0404,  0.0006,\n",
      "        -0.0120, -0.0433,  0.0433, -0.0873,  0.0098, -0.0172,  0.0238,  0.0351,\n",
      "        -0.0554,  0.0007,  0.0221, -0.0263,  0.0825,  0.0188, -0.0299, -0.0494,\n",
      "        -0.0481, -0.0058, -0.0290, -0.0067, -0.0858,  0.0847, -0.0294,  0.0431,\n",
      "         0.0653, -0.0078,  0.0702,  0.0192, -0.0056,  0.0226, -0.0197,  0.0203,\n",
      "        -0.0706,  0.0192,  0.0412, -0.0388,  0.0244,  0.0077, -0.0041,  0.0260,\n",
      "         0.0444, -0.0028, -0.0072,  0.0027, -0.0439, -0.0287,  0.0158, -0.0058,\n",
      "         0.0410,  0.0288,  0.0214, -0.0343, -0.0290, -0.0196,  0.0147, -0.0677,\n",
      "        -0.0884, -0.0366,  0.0171, -0.0307,  0.0060, -0.0103, -0.0016, -0.0442,\n",
      "        -0.0925, -0.0045,  0.0720, -0.0186, -0.0189, -0.0284, -0.0600,  0.0052,\n",
      "        -0.0461,  0.0585, -0.0529,  0.0008,  0.0269, -0.0658, -0.0359,  0.0241,\n",
      "         0.0103, -0.0448, -0.0299, -0.0399,  0.0312,  0.0670, -0.0133, -0.0372,\n",
      "         0.0604,  0.0157,  0.0628,  0.0110, -0.0048, -0.0053, -0.0391, -0.0380,\n",
      "        -0.0891,  0.0016, -0.0404,  0.0066,  0.0463, -0.0500, -0.0026,  0.0100,\n",
      "        -0.0095, -0.0106, -0.0084, -0.0128,  0.0305, -0.0434,  0.0403,  0.0264,\n",
      "         0.0672,  0.0486,  0.0070, -0.0218,  0.0331,  0.0556, -0.0398, -0.0167,\n",
      "         0.0311, -0.0303, -0.0097, -0.0624, -0.0065,  0.0167,  0.0568, -0.0315,\n",
      "        -0.0321, -0.1150,  0.0399, -0.0150,  0.0113, -0.0092, -0.0132, -0.0478,\n",
      "        -0.1763,  0.0009,  0.0977,  0.0017,  0.0432, -0.0249, -0.0431, -0.0084,\n",
      "        -0.0065,  0.0039,  0.0545, -0.0258,  0.0111,  0.0406,  0.0111, -0.0100,\n",
      "         0.0041, -0.0345, -0.0063, -0.1012, -0.0153, -0.0805, -0.0614,  0.0287,\n",
      "        -0.0741, -0.0514,  0.0869,  0.0093,  0.0411,  0.0040, -0.0192, -0.0256,\n",
      "         0.0723,  0.0023,  0.0191, -0.0661,  0.0339, -0.0070, -0.0060,  0.0082,\n",
      "         0.0317,  0.0561, -0.0061,  0.0309, -0.0371, -0.0516, -0.0030, -0.0294,\n",
      "        -0.0451, -0.0686, -0.0615, -0.1006,  0.0965, -0.0179,  0.0126,  0.0063,\n",
      "         0.0412, -0.0626, -0.0437, -0.0226, -0.0665,  0.0180, -0.0498,  0.0086,\n",
      "         0.0561,  0.0168, -0.0678,  0.0538,  0.0212,  0.0275,  0.0150, -0.0136,\n",
      "         0.0471,  0.0281, -0.0290,  0.0136,  0.0512,  0.0289, -0.0465, -0.0798,\n",
      "        -0.0129,  0.0411, -0.0499,  0.0238,  0.0515, -0.0375,  0.0591, -0.0040,\n",
      "        -0.0931,  0.0242, -0.0879,  0.0112,  0.0210, -0.0221, -0.0094, -0.0397,\n",
      "        -0.0430, -0.0786, -0.0291, -0.0040, -0.0663,  0.0125, -0.0236, -0.0347,\n",
      "        -0.0112, -0.0275,  0.0040, -0.0528, -0.0341, -0.0186, -0.0188,  0.0525,\n",
      "        -0.0095, -0.0579,  0.0095,  0.0653,  0.0087, -0.0051,  0.0278,  0.0508,\n",
      "        -0.0061,  0.0438, -0.0568,  0.0160, -0.0082, -0.0027,  0.0080,  0.0138,\n",
      "         0.0377, -0.0665, -0.0346, -0.0241,  0.0690,  0.0299, -0.0091, -0.0560,\n",
      "         0.0597,  0.0254,  0.0548, -0.0487, -0.0418, -0.0264, -0.0036, -0.0265,\n",
      "        -0.0823, -0.1047, -0.0287,  0.0706, -0.1070, -0.1340, -0.0259,  0.0291,\n",
      "        -0.0178,  0.0379, -0.0066, -0.0564, -0.0215, -0.0454, -0.0660, -0.0810,\n",
      "        -0.0612, -0.0351,  0.0312, -0.0427,  0.0196,  0.0258, -0.0330, -0.0069,\n",
      "        -0.0731, -0.0620,  0.0282, -0.0913, -0.0217, -0.0255, -0.0352,  0.0288,\n",
      "        -0.0027, -0.0114, -0.0426,  0.0135, -0.0418, -0.0348, -0.0058,  0.0331,\n",
      "         0.0378,  0.0609,  0.0622, -0.0040,  0.0065, -0.0312, -0.0251, -0.0017,\n",
      "        -0.0376,  0.0140, -0.0206,  0.0005, -0.0452,  0.0248, -0.0772, -0.0353,\n",
      "        -0.0640, -0.0321, -0.0169,  0.0729, -0.0877, -0.0501])\n",
      "tensor([1.1148, 1.0141, 1.0548, 1.0368, 1.0481, 1.0464, 1.0362, 1.0098, 1.0570,\n",
      "        1.0138, 1.0650, 1.0548, 1.0176, 1.0360, 0.9849, 1.0148, 1.0273, 1.0450,\n",
      "        0.9993, 0.9943, 1.0344, 1.0194, 1.0069, 1.0280, 1.0707, 1.0314, 0.9807,\n",
      "        1.0301, 1.0656, 1.0639, 1.0158, 1.0774, 0.9911, 1.0202, 1.0284, 1.0395,\n",
      "        1.0048, 1.0222, 0.9598, 1.0272, 1.0131, 1.0464, 1.0313, 1.0447, 1.0433,\n",
      "        1.0491, 1.0265, 1.0357, 1.0277, 1.0640, 1.1031, 1.0382, 1.0608, 1.0049,\n",
      "        1.0169, 1.0651, 1.0599, 1.0296, 1.0190, 1.0565, 1.0975, 1.0964, 1.0636,\n",
      "        1.0858, 1.1103, 1.0148, 1.1150, 1.0316, 1.0196, 1.0764, 1.0368, 1.0305,\n",
      "        1.0579, 1.0245, 1.0122, 1.0240, 1.0243, 1.0361, 1.0408, 1.0258, 1.0534,\n",
      "        1.0300, 1.0150, 1.0148, 1.1104, 1.0179, 1.0375, 1.1150, 1.0061, 1.0883,\n",
      "        1.0451, 1.0324, 1.0150, 1.0291, 1.0310, 0.9939, 1.0330, 1.0334, 1.0532,\n",
      "        1.0646, 1.0297, 1.0111, 1.0311, 1.1973, 1.0490, 1.0353, 1.0394, 1.0500,\n",
      "        1.0635, 1.0529, 1.0230, 1.0270, 0.9999, 1.0281, 1.0255, 1.0577, 0.9895,\n",
      "        1.0642, 1.0232, 1.0373, 1.0520, 1.0139, 1.0136, 1.0102, 1.0114, 1.0849,\n",
      "        1.0295, 1.0333, 1.0479, 1.0028, 1.0530, 1.0175, 1.0257, 1.0313, 1.0125,\n",
      "        1.0514, 1.0123, 1.0544, 1.0061, 1.0032, 1.0078, 1.0259, 1.0279, 1.0048,\n",
      "        1.1384, 1.0247, 1.1074, 1.0146, 1.0376, 1.0450, 1.0196, 1.0425, 1.0546,\n",
      "        1.0394, 1.0445, 1.0280, 1.0564, 1.0031, 1.0220, 1.0195, 1.0217, 1.0132,\n",
      "        1.0571, 1.0498, 1.0699, 1.0769, 1.0432, 1.0593, 0.9973, 1.0127, 1.0188,\n",
      "        1.0346, 1.0429, 1.0588, 1.0302, 1.0309, 1.0700, 1.0541, 1.0049, 1.1042,\n",
      "        1.0160, 1.0060, 1.0411, 0.9783, 1.0621, 0.9909, 1.0407, 1.0125, 1.1026,\n",
      "        1.0127, 1.0452, 1.0576, 1.0514, 1.0759, 1.1319, 1.0630, 1.0454, 0.9996,\n",
      "        1.0311, 1.0308, 1.0389, 1.0083, 1.0693, 1.0669, 1.0640, 1.0365, 1.1174,\n",
      "        1.0213, 1.0392, 1.0146, 1.0710, 1.0647, 1.0396, 1.0135, 1.0627, 1.0278,\n",
      "        1.0304, 1.0123, 1.0725, 1.0405, 1.0650, 1.0202, 1.0154, 1.0154, 1.0354,\n",
      "        1.0536, 1.0213, 1.0656, 1.0390, 1.0398, 1.0160, 0.9966, 1.0843, 1.0373,\n",
      "        1.0001, 1.0423, 1.0084, 1.0398, 1.0356, 1.0320, 1.0907, 1.0578, 1.0450,\n",
      "        1.0264, 1.0476, 1.0479, 1.0325, 1.0899, 1.0582, 1.0416, 1.0448, 1.0274,\n",
      "        1.0658, 0.9916, 1.0130, 1.0361, 1.0178, 1.0288, 1.0943, 1.0254, 1.0932,\n",
      "        1.0225, 0.9969, 1.0483, 1.0138, 1.0475, 1.0560, 1.0308, 1.0149, 1.0659,\n",
      "        1.0496, 1.0327, 1.0585, 1.0088, 1.0533, 1.0587, 1.0017, 1.0372, 1.0104,\n",
      "        1.0476, 1.0089, 1.0262, 1.0588, 1.0031, 1.0393, 1.0153, 1.0527, 1.0350,\n",
      "        1.0230, 1.1048, 1.0438, 1.0562, 1.0787, 1.0798, 1.0069, 1.0465, 1.0197,\n",
      "        1.0574, 1.0887, 1.0507, 1.0307, 1.0410, 1.0302, 1.0299, 1.0315, 1.0135,\n",
      "        1.0214, 1.0707, 1.0321, 1.0357, 1.0057, 1.0579, 1.0603, 1.0529, 1.0457,\n",
      "        1.0758, 1.0392, 1.0133, 1.0939, 1.0209, 0.9829, 1.0161, 1.0406, 1.0188,\n",
      "        1.0195, 1.0696, 1.0549, 1.0186, 1.0516, 1.0240, 1.0055, 1.0768, 1.0526,\n",
      "        1.0950, 1.0771, 1.0412, 1.0531, 1.0068, 1.0706, 1.0415, 1.0456, 1.0440,\n",
      "        1.0662, 1.0423, 1.0919, 1.0246, 1.0229, 1.0328, 1.0970, 1.0104])\n",
      "tensor([-0.0502,  0.1835, -0.1844, -0.0763,  0.0981, -0.0632,  0.1781,  0.1799,\n",
      "        -0.1615, -0.0070,  0.0180, -0.1221,  0.0532, -0.0642,  0.1442, -0.0105,\n",
      "        -0.0110, -0.1797,  0.0989,  0.0290,  0.0524, -0.1219, -0.0814, -0.0964,\n",
      "         0.0850,  0.1046,  0.0888,  0.0730,  0.2475,  0.0987, -0.0807,  0.0227,\n",
      "         0.2099,  0.1751,  0.1787, -0.0726,  0.2360, -0.1719, -0.0662,  0.0993,\n",
      "        -0.1662,  0.1792,  0.0687, -0.0914,  0.2015,  0.1067, -0.1048,  0.2747,\n",
      "         0.0885, -0.2359, -0.1621,  0.0642, -0.0750,  0.0571,  0.1149, -0.1690,\n",
      "         0.1581,  0.0500,  0.1722, -0.1559,  0.1978,  0.1538,  0.2045, -0.2411,\n",
      "         0.1083,  0.1640, -0.1631, -0.0725,  0.1616,  0.0133, -0.0311, -0.2404,\n",
      "         0.0999, -0.1359, -0.1695, -0.1385, -0.0917,  0.1369,  0.0765,  0.1796,\n",
      "         0.1789,  0.1468, -0.2053,  0.0925, -0.0215,  0.1086,  0.2229,  0.0214,\n",
      "        -0.1603,  0.2229,  0.1009, -0.1305, -0.0197, -0.1939, -0.0919, -0.1292,\n",
      "         0.0940,  0.1249, -0.0296,  0.1658,  0.1623,  0.0304,  0.1712, -0.1425,\n",
      "         0.2158,  0.1802,  0.2118,  0.2391, -0.1955,  0.1230,  0.0807,  0.0138,\n",
      "        -0.0447, -0.1215,  0.0992, -0.1799, -0.1511, -0.0713,  0.1787,  0.2354,\n",
      "        -0.0908, -0.0950, -0.0093,  0.1304, -0.1023, -0.1671,  0.1482,  0.2177,\n",
      "        -0.0456, -0.1250,  0.0822, -0.0919,  0.1335,  0.2406, -0.1098, -0.0949,\n",
      "        -0.2020, -0.1000,  0.1127, -0.0588, -0.1748,  0.1471, -0.1005, -0.1136,\n",
      "         0.0628,  0.0628,  0.1542, -0.1733, -0.0977,  0.2574, -0.1552, -0.1288,\n",
      "         0.1105,  0.0325,  0.2301, -0.1985,  0.0388,  0.1362, -0.0735, -0.1593,\n",
      "        -0.1916,  0.1985, -0.1225, -0.1455, -0.1777,  0.2084, -0.0750, -0.2712,\n",
      "        -0.0989, -0.0199, -0.1780,  0.0529,  0.1424,  0.0915, -0.1351, -0.0340,\n",
      "         0.2285, -0.1627,  0.2559,  0.1475, -0.0866, -0.0639,  0.1266, -0.0215,\n",
      "        -0.1581, -0.0855, -0.1540, -0.1085,  0.2112,  0.1174, -0.0925,  0.0609,\n",
      "         0.2068, -0.2392, -0.0464, -0.1811, -0.0253, -0.0691,  0.1281, -0.0803,\n",
      "         0.0856, -0.0878,  0.1700, -0.1894,  0.1141,  0.2125, -0.1014,  0.1147,\n",
      "         0.0724, -0.1748,  0.1498, -0.2617, -0.0550, -0.0813, -0.1159, -0.1020,\n",
      "         0.0562, -0.1539,  0.1956,  0.1433, -0.1581,  0.0814,  0.0280, -0.1372,\n",
      "         0.1417, -0.1184,  0.0162,  0.1014, -0.0249,  0.1090,  0.1286,  0.1625,\n",
      "         0.0682, -0.1908,  0.1283, -0.1346,  0.0377,  0.1623,  0.2513, -0.1629,\n",
      "        -0.0419,  0.1360, -0.2449, -0.1193, -0.1957, -0.2435, -0.1803, -0.1047,\n",
      "        -0.1436, -0.1185, -0.2189, -0.0587, -0.1312, -0.1207, -0.2030,  0.0891,\n",
      "        -0.1623, -0.0724, -0.1750,  0.2263, -0.0869, -0.0677,  0.0577, -0.2483,\n",
      "         0.1628,  0.1870,  0.2058, -0.1071, -0.1557,  0.2590,  0.2499,  0.1801,\n",
      "        -0.1378,  0.1214, -0.2091, -0.1011, -0.1090, -0.1650, -0.0997,  0.2204,\n",
      "        -0.1191,  0.1421,  0.2736, -0.1255, -0.1790,  0.0421, -0.2486, -0.0122,\n",
      "         0.0617,  0.0741, -0.2152, -0.1700,  0.1184, -0.1648, -0.0906,  0.0261,\n",
      "        -0.0988, -0.0908,  0.0077,  0.1079, -0.1607, -0.1200, -0.0441, -0.1713,\n",
      "         0.0992,  0.1517, -0.0167,  0.1583,  0.0858,  0.1436, -0.0345,  0.1662,\n",
      "        -0.1596,  0.0313,  0.1974, -0.1423, -0.1691, -0.1880,  0.1543,  0.1643,\n",
      "         0.1695, -0.2335,  0.1707,  0.1607, -0.1587, -0.0932,  0.1273,  0.0040,\n",
      "        -0.0253, -0.0389, -0.0725,  0.1756, -0.0616, -0.1428, -0.0922, -0.1161,\n",
      "        -0.0915, -0.1616, -0.1734, -0.2097,  0.0257,  0.1436, -0.0120,  0.1366,\n",
      "         0.1910,  0.0631, -0.2381,  0.1449,  0.1477,  0.2067])\n",
      "tensor([[-0.0038,  0.0329, -0.0995,  ...,  0.0798, -0.0322, -0.0546],\n",
      "        [-0.0104,  0.0485,  0.0151,  ..., -0.0214,  0.0907, -0.0470],\n",
      "        [ 0.0438, -0.0056,  0.0693,  ..., -0.0544, -0.0832,  0.0021],\n",
      "        ...,\n",
      "        [-0.0098, -0.0338,  0.0007,  ...,  0.0295, -0.0061,  0.0332],\n",
      "        [-0.1034, -0.1140, -0.0630,  ..., -0.1009,  0.1711, -0.0071],\n",
      "        [ 0.1734,  0.0847,  0.0628,  ...,  0.0281, -0.1722,  0.0327]])\n",
      "tensor([ 0.0382,  0.0118, -0.1258,  0.1912, -0.1298,  0.0823, -0.0080, -0.1503,\n",
      "        -0.1260, -0.2725,  0.2707,  0.0453, -0.0321,  0.0530, -0.0065,  0.0183,\n",
      "         0.0470,  0.0210,  0.0005])\n"
     ]
    }
   ],
   "source": [
    "for p in net.model.parameters():\n",
    "    print(p.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef7bef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
