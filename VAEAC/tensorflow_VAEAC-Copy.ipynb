{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "310825f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import uniform, binomial\n",
    "from sklearn.model_selection import KFold\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# To remove WARNINGS from saving the models without compiling them first\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "#print(tf.__version__)\n",
    "#print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e185160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.config.list_physical_devices())\n",
    "\n",
    "#Working with CPU for now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81781e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import seed\n",
    "#from .UCI_loader import unnormalise_cat_vars\n",
    "\n",
    "\n",
    "# TODO return mean and std for variables + train test split\n",
    "\n",
    "\"\"\"\n",
    "def check_data_file(fname):\n",
    "    files = os.listdir(\".\")  # get the current directory listing\n",
    "    print\n",
    "    \"Looking for file '%s' in the current directory...\" % fname\n",
    "\n",
    "    if fname not in files:\n",
    "        print\n",
    "        \"'%s' not found! Downloading from GitHub...\" % fname\n",
    "        addr = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
    "        try:\n",
    "            response = urllib2.urlopen(addr)\n",
    "        except:\n",
    "            response = urllib3.urlopen(addr)\n",
    "        data = response.read()\n",
    "        fileOut = open(fname, \"w\")\n",
    "        fileOut.write(data)\n",
    "        fileOut.close()\n",
    "        print\n",
    "        \"'%s' download and saved locally..\" % fname\n",
    "    else:\n",
    "        print\n",
    "        \"File found in current directory..\"\n",
    "\"\"\"\n",
    "\n",
    "def check_data_file(fname):\n",
    "    files = os.listdir(\".\")  # get the current directory listing\n",
    "    print\n",
    "    \"Looking for file '%s' in the current directory...\" % fname\n",
    "\n",
    "    if fname not in files:\n",
    "        print\n",
    "        \"'%s' not found! Downloading from GitHub...\" % fname\n",
    "        addr = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
    "        \n",
    "        response = urllib.request.urlopen(addr)\n",
    "\n",
    "        data = response.read()\n",
    "        fileOut = open(fname, \"wb\")\n",
    "        fileOut.write(data)\n",
    "        fileOut.close()\n",
    "        print\n",
    "        \"'%s' download and saved locally..\" % fname\n",
    "    else:\n",
    "        print\n",
    "        \"File found in current directory..\"\n",
    "\n",
    "def get_my_COMPAS(rseed=0, separate_test=True, test_ratio=0.2, save_dir='../data/'):\n",
    "    \"\"\"\n",
    "        The adult dataset can be obtained from: https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\n",
    "        The code will look for the data file in the present directory, if it is not found, it will download them from GitHub.\n",
    "    \"\"\"\n",
    "\n",
    "    SEED = rseed\n",
    "    seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    their_FEATURES_CLASSIFICATION = [\"age_cat\", \"race\", \"sex\", \"priors_count\", \"c_charge_degree\"]\n",
    "    FEATURES_CLASSIFICATION = [\"age_cat\", \"race\", \"sex\", \"c_charge_degree\", \"is_recid\", \"priors_count\",\n",
    "                               \"time_served\"]  # features to be used for classification\n",
    "    CONT_VARIABLES = [\"priors_count\",\n",
    "                      \"time_served\"]  # continuous features, will need to be handled separately from categorical features, categorical features will be encoded using one-hot\n",
    "    CLASS_FEATURE = \"two_year_recid\"  # the decision variable\n",
    "\n",
    "\n",
    "    COMPAS_INPUT_FILE = save_dir + \"compas-scores-two-years.csv\"\n",
    "    check_data_file(COMPAS_INPUT_FILE)\n",
    "\n",
    "    # load the data and get some stats\n",
    "    df = pd.read_csv(COMPAS_INPUT_FILE)\n",
    "    df = df.dropna(subset=[\"days_b_screening_arrest\"])  # dropping missing vals\n",
    "\n",
    "    # convert to np array\n",
    "    data = df.to_dict('list')\n",
    "    for k in data.keys():\n",
    "        data[k] = np.array(data[k])\n",
    "\n",
    "    dates_in = data['c_jail_in']\n",
    "    dates_out = data['c_jail_out']\n",
    "    # this measures time in Jail\n",
    "    time_served = []\n",
    "    for i in range(len(dates_in)):\n",
    "        di = datetime.datetime.strptime(dates_in[i], '%Y-%m-%d %H:%M:%S')\n",
    "        do = datetime.datetime.strptime(dates_out[i], '%Y-%m-%d %H:%M:%S')\n",
    "        time_served.append((do - di).days)\n",
    "    time_served = np.array(time_served)\n",
    "    time_served[time_served < 0] = 0\n",
    "    data[\"time_served\"] = time_served\n",
    "\n",
    "    \"\"\" Filtering the data \"\"\"\n",
    "\n",
    "    # These filters are the same as propublica (refer to https://github.com/propublica/compas-analysis)\n",
    "    # If the charge date of a defendants Compas scored crime was not within 30 days from when the person was arrested, we assume that because of data quality reasons, that we do not have the right offense.\n",
    "    idx = np.logical_and(data[\"days_b_screening_arrest\"] <= 30, data[\"days_b_screening_arrest\"] >= -30)\n",
    "\n",
    "    # We coded the recidivist flag -- is_recid -- to be -1 if we could not find a compas case at all.\n",
    "    idx = np.logical_and(idx, data[\"is_recid\"] != -1)\n",
    "\n",
    "    # In a similar vein, ordinary traffic offenses -- those with a c_charge_degree of 'O' -- will not result in Jail time are removed (only two of them).\n",
    "    idx = np.logical_and(idx, data[\"c_charge_degree\"] != \"O\")  # F: felony, M: misconduct\n",
    "\n",
    "    # We filtered the underlying data from Broward county to include only those rows representing people who had either recidivated in two years, or had at least two years outside of a correctional facility.\n",
    "    idx = np.logical_and(idx, data[\"score_text\"] != \"NA\")\n",
    "\n",
    "    # select the examples that satisfy this criteria\n",
    "    for k in data.keys():\n",
    "        data[k] = data[k][idx]\n",
    "\n",
    "    y = data[CLASS_FEATURE]\n",
    "    \"\"\" Feature normalization and one hot encoding \"\"\"\n",
    "\n",
    "    print\n",
    "    \"\\nNumber of people recidivating within two years\"\n",
    "    print\n",
    "    pd.Series(y).value_counts()\n",
    "    print\n",
    "    \"\\n\"\n",
    "\n",
    "    X = []  # empty array with num rows same as num examples, will hstack the features to it\n",
    "    X_dims = []\n",
    "\n",
    "    feature_names = []\n",
    "    for attr in FEATURES_CLASSIFICATION:\n",
    "        vals = data[attr]\n",
    "        if attr in CONT_VARIABLES:\n",
    "            vals = [float(v) for v in vals]\n",
    "            # vals = preprocessing.scale(vals, axis=0, with_mean=True, with_std=True)  # 0 mean and 1 variance\n",
    "            vals = np.reshape(vals, (len(y), -1))  # convert from 1-d arr to a 2-d arr with one col\n",
    "            X_dims.append(1)\n",
    "\n",
    "        else:  # for binary categorical variables, the label binarizer uses just one var instead of two\n",
    "            enc = preprocessing.OneHotEncoder(categories='auto', handle_unknown='error')\n",
    "            enc.fit(vals.reshape(-1, 1))\n",
    "            vals = enc.transform(vals.reshape(-1, 1)).todense()\n",
    "            X_dims += [vals.shape[1]]*vals.shape[1]\n",
    "\n",
    "        # add to learnable features\n",
    "        X.append(vals)\n",
    "\n",
    "        if attr in CONT_VARIABLES:  # continuous feature, just append the name\n",
    "            feature_names.append(attr)\n",
    "        else:  # categorical features\n",
    "            if vals.shape[1] == 1:  # binary features that passed through lib binarizer\n",
    "                feature_names.append(attr)\n",
    "            else:\n",
    "                for k in enc.categories_:  # non-binary categorical features, need to add the names for each cat\n",
    "                    feature_names.append(attr + \"_\" + str(k))\n",
    "\n",
    "    X = np.array(np.concatenate(list(X), axis=1))\n",
    "    X_dims = np.array(X_dims)\n",
    "\n",
    "    if separate_test:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=rseed, shuffle=True)\n",
    "\n",
    "        x_means, x_stds = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "        x_means[X_dims>1] = 0\n",
    "        x_stds[X_dims>1] = 1\n",
    "        x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "        x_train = ((X_train - x_means) / x_stds).astype(np.float32)\n",
    "        x_test = ((X_test - x_means) / x_stds).astype(np.float32)\n",
    "\n",
    "        return x_train, x_test, x_means, x_stds, y_train, y_test, feature_names, X_dims\n",
    "    else:\n",
    "        x_means, x_stds = X.mean(axis=0), X.std(axis=0)\n",
    "        print(X_dims.shape, x_means.shape)\n",
    "        x_means[:,X_dims>1] = 0\n",
    "        x_stds[:,X_dims>1] = 1\n",
    "        x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "        x_train = ((X - x_means) / x_stds).astype(np.float32)\n",
    "\n",
    "        return x_train, x_means, x_stds, y, feature_names, X_dims\n",
    "\n",
    "\n",
    "def join_compas_targets(x_train, x_test, y_train, y_test, X_dims):\n",
    "    # output from get method is onehot so we need to flatten and append 2\n",
    "    input_dim_vec = X_dims_to_input_dim_vec(X_dims)\n",
    "    input_dim_vec = np.append(input_dim_vec, 2)\n",
    "    enc = preprocessing.OneHotEncoder(categories='auto', handle_unknown='error')\n",
    "    enc.fit(y_train.reshape(-1, 1))\n",
    "\n",
    "    vals_train = np.array(enc.transform(y_train.reshape(-1, 1)).todense()).astype(np.float32)\n",
    "    vals_test = np.array(enc.transform(y_test.reshape(-1, 1)).todense()).astype(np.float32)\n",
    "\n",
    "    x_train = np.concatenate([x_train, vals_train], axis=1)\n",
    "    x_test = np.concatenate([x_test, vals_test], axis=1)\n",
    "    return x_train, x_test, input_dim_vec\n",
    "\n",
    "\n",
    "def X_dims_to_input_dim_vec(X_dims):\n",
    "    \"\"\"This is for our cat_Gauss VAE model\"\"\"\n",
    "    input_dim_vec = []\n",
    "    i = 0\n",
    "    while i < len(X_dims):\n",
    "        input_dim_vec.append(X_dims[i])\n",
    "        i += X_dims[i]\n",
    "    return np.array(input_dim_vec)\n",
    "\n",
    "#\"\"\"\n",
    "def input_dim_vec_to_X_dims(input_dim_vec):\n",
    "    # This is for our cat_Gauss VAE model\n",
    "    X_dims = []\n",
    "    for i in input_dim_vec:\n",
    "        for ii in range(i):\n",
    "            X_dims.append(i)\n",
    "    return np.array(X_dims)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26889334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20a7336c",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a59efb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default_credit\n"
     ]
    }
   ],
   "source": [
    "# For Default credit\n",
    "\n",
    "input_dim_vec = [1, 2, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ,1 ,1 ,1 ,1 ,1, 2]\n",
    "width = 350\n",
    "depth = 3\n",
    "latent_dim = 8\n",
    "\n",
    "# For all tabular data sets\n",
    "names = ['wine', 'default_credit', 'compas', 'lsat']\n",
    "widths = [350, 350, 350, 350] # Bigger than VAE because the task of modelling all conditionals is more complex\n",
    "depths = [3, 3, 3, 3] # We go deeper because we are using residual models\n",
    "latent_dims = [6, 8, 4, 4]\n",
    "under_latent_dims = [6, 8, 4, 4] # following the original paper we set dim(u) = dim(z) with d>r [r is true manifold dim]\n",
    "under_latent_dims2 = [4, 6, 3, 3] # following the original paper we set dim(u) = dim(z) with d>r [r is true manifold dim]\n",
    "\n",
    "dname = 'default_credit'\n",
    "print(dname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33809880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets in UCI\n",
    "def load_UCI(dset_name, splits=10, seed=0, separate_targets=True, save_dir='data/'):\n",
    "    mkdir(save_dir)\n",
    "\n",
    "    if dset_name == 'wine':\n",
    "        if not os.path.isfile(save_dir+'winequality-red.csv'):\n",
    "            urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\",\n",
    "                               filename=save_dir+'winequality-red.csv')\n",
    "        data = pd.read_csv(save_dir+'winequality-red.csv', header=1, delimiter=';').values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'default_credit':\n",
    "        if not os.path.isfile(save_dir + 'default of credit card clients.xls'):\n",
    "            urllib.request.urlretrieve(\n",
    "                \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\",\n",
    "                filename=save_dir + 'default of credit card clients.xls')\n",
    "        data = pd.read_excel(save_dir + 'default of credit card clients.xls', header=[0, 1], index_col=0, # delimiter=\"\\s+\"\n",
    "                             ).values\n",
    "        y_idx = [-1]  # OK\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise Exception('Dataset name doesnt match any known datasets.')\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    data = data[np.random.permutation(np.arange(len(data)))] #Shuffle the data\n",
    "    \n",
    "    kf = KFold(n_splits=splits)\n",
    "    for j, (train_index, test_index) in enumerate(kf.split(data)):\n",
    "\n",
    "        # Not sure what separate targets is\n",
    "        if separate_targets:\n",
    "            x_idx = list(range(data.shape[1]))\n",
    "            for e in y_idx:\n",
    "                x_idx.remove(x_idx[e])\n",
    "\n",
    "            x_idx = np.array(x_idx)\n",
    "            y_idx = np.array(y_idx)\n",
    "            x_train, y_train = data[train_index, :], data[train_index, :]\n",
    "            x_train, y_train = x_train[:, x_idx], y_train[:, y_idx]\n",
    "            x_test, y_test = data[test_index, :], data[test_index, :]\n",
    "            x_test, y_test = x_test[:, x_idx], y_test[:, y_idx]\n",
    "\n",
    "            x_means, x_stds = x_train.mean(axis=0), x_train.std(axis=0)\n",
    "            y_means, y_stds = y_train.mean(axis=0), y_train.std(axis=0)\n",
    "\n",
    "            y_stds[y_stds < 1e-10] = 1\n",
    "            x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "            x_train = ((x_train - x_means) / x_stds).astype(np.float32)\n",
    "            y_train = ((y_train - y_means) / y_stds).astype(np.float32)\n",
    "\n",
    "            x_test = ((x_test - x_means) / x_stds).astype(np.float32)\n",
    "            y_test = ((y_test - y_means) / y_stds).astype(np.float32)\n",
    "\n",
    "            return x_train, x_test, x_means, x_stds, y_train, y_test, y_means, y_stds\n",
    "\n",
    "        else:\n",
    "            x_train, x_test = data[train_index, :], data[test_index, :]\n",
    "            x_means, x_stds = x_train.mean(axis=0), x_train.std(axis=0)\n",
    "\n",
    "            x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "            x_train = ((x_train - x_means) / x_stds).astype(np.float32)\n",
    "            x_test = ((x_test - x_means) / x_stds).astype(np.float32)\n",
    "\n",
    "            return x_train, x_test, x_means, x_stds\n",
    "\n",
    "def mkdir(paths):\n",
    "    if not isinstance(paths, (list, tuple)):\n",
    "        paths = [paths]\n",
    "    for path in paths:\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path, mode=0o777)\n",
    "\n",
    "def unnormalise_cat_vars(x, x_means, x_stds, input_dim_vec):\n",
    "    \"\"\"\n",
    "    Converts a feature vector with continous values into a vector with continous and discrete values for those \n",
    "    which come from a categorical class.\n",
    "    \"\"\"\n",
    "    input_dim_vec = np.array(input_dim_vec)\n",
    "    unnorm_x = np.multiply(x, x_stds) + x_means\n",
    "\n",
    "    fixed_unnorm = unnorm_x.round()\n",
    "    fixed_unnorm -= fixed_unnorm.min(axis=0).reshape([1, fixed_unnorm.shape[1]])  # this sets all mins to 0\n",
    "    for idx, dims in enumerate(input_dim_vec):\n",
    "        if dims > 1:\n",
    "            vec = fixed_unnorm[:, idx]\n",
    "            vec[vec > dims - 1] = dims - 1\n",
    "            fixed_unnorm[:, idx] = vec\n",
    "\n",
    "    x[:, input_dim_vec > 1] = fixed_unnorm[:, input_dim_vec > 1]\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c633c97",
   "metadata": {},
   "source": [
    "## Recognition (Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5836060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The arguments sent to the different keras layers are there to mimic the Torch layers in CLUE.\n",
    "\"\"\"\n",
    "\n",
    "def create_recognition_encoder(width, depth, latent_dim, input_dim_vec):\n",
    "    # Tensorflow network as one big Russian doll\n",
    "    nb_inputs = sum(input_dim_vec)\n",
    "    inputs = keras.Input(shape=(nb_inputs,))\n",
    "    #inputs = keras.Input(shape=(None,nb_inputs))\n",
    "    input = layers.Dense(width, use_bias=True, \\\n",
    "                         kernel_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/nb_inputs), math.sqrt(1/nb_inputs)), \\\n",
    "                         bias_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/nb_inputs), math.sqrt(1/nb_inputs))) \\\n",
    "                         (inputs)\n",
    "\n",
    "    for i in range(depth-1):\n",
    "\n",
    "        x = layers.LeakyReLU(alpha=0.01)(input)\n",
    "        x = layers.BatchNormalization(momentum=0.1, epsilon=1e-5)(x)\n",
    "        x = layers.Dense(width, use_bias=True, \\\n",
    "                         kernel_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width)), \\\n",
    "                         bias_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width))) \\\n",
    "                         (x)\n",
    "\n",
    "        # Skip connection \n",
    "        x = x + input\n",
    "\n",
    "        input = x\n",
    "\n",
    "    # Final layers\n",
    "    x = layers.LeakyReLU(alpha=0.01)(x)\n",
    "    x = layers.BatchNormalization(momentum=0.1, epsilon=1e-5)(x)\n",
    "    outputs = layers.Dense(latent_dim*2, use_bias=True, \\\n",
    "                         kernel_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width)), \\\n",
    "                         bias_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width))) \\\n",
    "                         (x)\n",
    "\n",
    "    recognition_encoder = keras.Model(inputs=inputs, outputs=outputs, name=\"recognition_encoder_model\")\n",
    "    return recognition_encoder\n",
    "#recognition_encoder.summary()\n",
    "\n",
    "#keras.utils.plot_model(encoder, \"encoder.png\")\n",
    "#keras.utils.plot_model(recognition_encoder, \"recognition.png\", show_shapes=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06253187",
   "metadata": {},
   "source": [
    "## Prior network (Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd58a6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The arguments sent to the different keras layers are there to mimic the Torch layers in CLUE.\n",
    "\"\"\"\n",
    "def create_prior_encoder(width, depth, latent_dim, input_dim_vec):\n",
    "    nb_inputs = sum(input_dim_vec)*2\n",
    "    inputs = keras.Input(shape=(nb_inputs,))\n",
    "    #inputs = keras.Input(shape=(None,nb_inputs))\n",
    "    input = layers.Dense(width, use_bias=True, \\\n",
    "                         kernel_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/nb_inputs), math.sqrt(1/nb_inputs)), \\\n",
    "                         bias_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/nb_inputs), math.sqrt(1/nb_inputs))) \\\n",
    "                         (inputs)\n",
    "\n",
    "    for i in range(depth-1):\n",
    "\n",
    "        x = layers.LeakyReLU(alpha=0.01)(input)\n",
    "        x = layers.BatchNormalization(momentum=0.1, epsilon=1e-5)(x)\n",
    "        x = layers.Dense(width, use_bias=True, \\\n",
    "                         kernel_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width)), \\\n",
    "                         bias_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width))) \\\n",
    "                         (x)\n",
    "\n",
    "        x = x + input\n",
    "\n",
    "        input = x\n",
    "\n",
    "    x = layers.LeakyReLU(alpha=0.01)(x)\n",
    "    x = layers.BatchNormalization(momentum=0.1, epsilon=1e-5)(x)\n",
    "    outputs = layers.Dense(latent_dim*2, use_bias=True, \\\n",
    "                         kernel_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width)), \\\n",
    "                         bias_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width))) \\\n",
    "                         (x)\n",
    "\n",
    "    prior_encoder = keras.Model(inputs=inputs, outputs=outputs, name=\"prior_encoder_model\")\n",
    "    return prior_encoder\n",
    "#prior_encoder.summary()\n",
    "\n",
    "#keras.utils.plot_model(encoder, \"encoder.png\")\n",
    "#keras.utils.plot_model(prior_encoder, \"prior.png\", show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8761740b",
   "metadata": {},
   "source": [
    "## Generator (Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57db3240",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The arguments sent to the different keras layers are there to mimic the Torch layers in CLUE.\n",
    "\"\"\"\n",
    "def create_decoder(width, depth, latent_dim, input_dim_vec):\n",
    "    nb_inputs = latent_dim\n",
    "    inputs = keras.Input(shape=(nb_inputs,))\n",
    "    input = layers.Dense(width, use_bias=True, \\\n",
    "                         kernel_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/nb_inputs), math.sqrt(1/nb_inputs)), \\\n",
    "                         bias_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/nb_inputs), math.sqrt(1/nb_inputs))) \\\n",
    "                         (inputs)\n",
    "\n",
    "    for i in range(depth-1):\n",
    "\n",
    "        x = layers.LeakyReLU(alpha=0.01)(input)\n",
    "        x = layers.BatchNormalization(momentum=0.1, epsilon=1e-5)(x)\n",
    "        x = layers.Dense(width, use_bias=True, \\\n",
    "                         kernel_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width)), \\\n",
    "                         bias_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width))) \\\n",
    "                         (x)\n",
    "        x = x + input\n",
    "\n",
    "        input = x\n",
    "\n",
    "    x = layers.LeakyReLU(alpha=0.01)(x)\n",
    "    x = layers.BatchNormalization(momentum=0.1, epsilon=1e-5)(x)\n",
    "    outputs = layers.Dense(sum(input_dim_vec), use_bias=True, \\\n",
    "                         kernel_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width)), \\\n",
    "                         bias_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width))) \\\n",
    "                         (x)\n",
    "\n",
    "    decoder = keras.Model(inputs=inputs, outputs=outputs, name=\"decoder_model\")\n",
    "    return decoder\n",
    "\n",
    "#decoder.summary()\n",
    "\n",
    "#keras.utils.plot_model(model, \"decoder_model.png\")\n",
    "#keras.utils.plot_model(decoder, \"generator.png\", show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f25722",
   "metadata": {},
   "source": [
    "## Masker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ddccccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class top_masker_tensorflow:\n",
    "    \"\"\"\n",
    "    Returned mask is sampled from component-wise independent Bernoulli\n",
    "    distribution with probability of component to be unobserved p.\n",
    "    Such mask induces the type of missingness which is called\n",
    "    in literature \"missing completely at random\" (MCAR).\n",
    "    If some value in batch is missed, it automatically becomes unobserved.\n",
    "    \"\"\"\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            - batch is a numpy array with as many rows as batch_size and as many columns as features\n",
    "        \n",
    "        Returned:\n",
    "       \n",
    "            - mask is a float32 tensor\n",
    "        \n",
    "        The mask seems to be random\n",
    "        \"\"\"\n",
    "        # Generate one uniform number for each row (1xrow numpy matrix)\n",
    "        pp = uniform(low=0.0, high=self.p, size=batch.shape[0]) \n",
    "        pp = np.expand_dims(pp, axis=1) # Put the number in 1x1 matrices in a 1x#row matrix\n",
    "        pp = np.repeat(pp, batch.shape[1], axis=1) # Repeat the number across each row\n",
    "        nan_mask = tf.math.is_nan(batch) # If nan => should be unobserved i.e. boolean True\n",
    "        \n",
    "        # Generate Bernoulli samples (0 or 1) from pp i.e. for each sample in batch determine if a feature is\n",
    "        # observed or hidden.\n",
    "        bernoulli_mask_numpy = binomial(1, pp, size=None) \n",
    "        bernoulli_mask = tf.convert_to_tensor(tf.cast(bernoulli_mask_numpy, tf.bool))\n",
    "        mask = tf.math.logical_or(bernoulli_mask, nan_mask) # Logical or between bernoulli and nan mask\n",
    "        \n",
    "        # Logical not to invert the mask (This is done in CLUE)\n",
    "        # Mask is converted to a boolean tensor with floats for element wise multiplication with the batch\n",
    "        # which is done in apply mask\n",
    "        # (True => 0, False => 1)\n",
    "        \n",
    "        #TODO: The logical_not might be unnecessary as the probability of getting a true or false is equal.\n",
    "        #      No mirror the Torch code however I did this but it can perhaps be removed later...\n",
    "        return tf.cast(tf.math.logical_not(mask), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510e8c65",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02f82868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_cat_to_flat_mask(mask, input_dim_vec):\n",
    "    output = []\n",
    "    for idx, dim in enumerate(input_dim_vec):\n",
    "        if dim == 1:\n",
    "            \"\"\"\n",
    "            tf.expand_dims (axis=1) takes mask[:, idx] (batch_size,) and \n",
    "            converts it into (64,1) e.g. [1,2,3] => [[1];[2];[3]] i.e. same as torch unsqueeze(1)\n",
    "            \"\"\"\n",
    "            output.append(tf.expand_dims(mask[:, idx], axis=1))\n",
    "\n",
    "        elif dim > 1: \n",
    "            \"\"\" COMMENT\n",
    "            tf.expand_dims (read comment above)\n",
    "            tf.ones([mask.shape[0], dim]) creates an array of batch_size x dim with ones\n",
    "            oh_vec will be mask.shape[0] x dim and contain 0 or 1 on rows depending on if mask is 0 or 1.\n",
    "            \"\"\"\n",
    "            oh_vec = tf.ones([mask.shape[0], dim]) * tf.expand_dims(mask[:, idx], axis=1)\n",
    "\n",
    "            output.append(oh_vec)\n",
    "        else:\n",
    "            raise ValueError('Error, invalid dimension value')\n",
    "    return tf.concat(output, axis=1)\n",
    "\n",
    "\n",
    "def gauss_cat_to_flat(x, input_dim_vec):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - x: numpy array\n",
    "        - input_dim_vec: list e.g. [1, 2, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ,1 ,1 ,1 ,1 ,1, 2] credit\n",
    "    Returns:\n",
    "        - numpy array \n",
    "        \n",
    "    Example:\n",
    "        \n",
    "        x:\n",
    "             [-0.52121574  0.          2.          1.          1.2496392   0.01383046\n",
    "              0.1105278   1.8173771   0.18815508  0.2341654   1.9953084   0.2038664\n",
    "              0.31341553  0.31455126  0.32473356  0.4501966   0.45570025  0.0774322\n",
    "             -0.2517514  -0.1535475   0.03951775 -0.31174627 -0.12532774  0.        ]\n",
    "        \n",
    "        input_dim_vec:\n",
    "            [1, 2, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ,1 ,1 ,1 ,1 ,1, 2]\n",
    "        \n",
    "        return:\n",
    "             [-0.52121574  1.          0.          0.          0.          1.\n",
    "              0.          0.          1.          0.          1.2496392   0.01383046\n",
    "              0.1105278   1.8173771   0.18815508  0.2341654   1.9953084   0.2038664\n",
    "              0.31341553  0.31455126  0.32473356  0.4501966   0.45570025  0.0774322\n",
    "             -0.2517514  -0.1535475   0.03951775 -0.31174627 -0.12532774  1.\n",
    "              0.        ]\n",
    "    \n",
    "    \"\"\"\n",
    "    output = []\n",
    "    for idx, dim in enumerate(input_dim_vec):\n",
    "        if dim == 1:\n",
    "            output.append(tf.expand_dims(x[:, idx], axis=1))\n",
    "        elif dim > 1:\n",
    "            oh_vec = tf.one_hot(x[:, idx], dim) # Returns one hot encoding 0 with dim 2 -> 1 0, 1 -> 0 1\n",
    "            output.append(oh_vec)\n",
    "        else:\n",
    "            raise ValueError('Error, invalid dimension value')\n",
    "    return tf.concat(output, axis=1).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abe98e6",
   "metadata": {},
   "source": [
    "## CLASS VAEAC and loss and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fc4120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEAC_gauss_cat(tf.keras.Model):\n",
    "    def __init__(self, width, depth, latent_dim, input_dim_vec, batch_size, lr, optimizer, save_model):\n",
    "        super(VAEAC_gauss_cat, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.input_dim_vec = input_dim_vec\n",
    "        self.recognition_encoder = create_recognition_encoder(width, depth, latent_dim, input_dim_vec)\n",
    "        self.prior_encoder = create_prior_encoder(width, depth, latent_dim, input_dim_vec)\n",
    "        self.decoder = create_decoder(width, depth, latent_dim, input_dim_vec)\n",
    "        self.sigma_mu = 1e4\n",
    "        self.sigma_sigma = 1e-4\n",
    "        self.vlb_scale = 1 / len(self.input_dim_vec)\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer\n",
    "        self.save_model = save_model\n",
    "        self.lr = lr\n",
    "        \n",
    "\n",
    "    # Inspiration taken from \n",
    "    # https://github.com/joocxi/tf2-VAEAC/blob/d2b1bbc258ec77ee0975ea7eb68e63c4efcda6f0/model/vaeac.py\n",
    "    def prior_regularizer(self, prior):\n",
    "\n",
    "        mu = tf.reshape(prior.mean(), (self.batch_size, -1))\n",
    "        sigma = tf.reshape(prior.scale, (self.batch_size, -1))\n",
    "\n",
    "        mu_regularizer = -tf.reduce_sum(tf.square(mu), -1) / (2 * self.sigma_mu ** 2)\n",
    "        sigma_regularizer = tf.reduce_sum((tf.math.log(sigma) - sigma), -1) * self.sigma_sigma\n",
    "        return mu_regularizer + sigma_regularizer\n",
    "\n",
    "    def apply_mask(self, x, mask):\n",
    "        return x * mask\n",
    "\n",
    "    def reconstruction_loss(self, x, y):\n",
    "        log_prob_vec = []\n",
    "        cum_dims = 0\n",
    "        reshape_dim = self.batch_size\n",
    "        for idx, dims in enumerate(self.input_dim_vec):\n",
    "            if dims == 1:\n",
    "                # Gaussian_case\n",
    "                log_prob_vec.append(tf.expand_dims(-(x[:, cum_dims] - y[:, cum_dims])**2, 1))\n",
    "                \n",
    "                cum_dims += 1\n",
    "\n",
    "            elif dims > 1:\n",
    "                # if x.shape[1] == y.shape[1]:\n",
    "                #    raise Exception('Input and target seem to be in flat format. Need integer cat targets.'\n",
    "\n",
    "                cce = tf.keras.losses.CategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE, from_logits = True)\n",
    "                cat_cross_entropy = -cce(tf.cast(y[:, cum_dims:cum_dims + dims], dtype=tf.int64), x[:, cum_dims:cum_dims + dims])\n",
    "                \n",
    "                log_prob_vec.append(tf.expand_dims(cat_cross_entropy, 1))\n",
    "                cum_dims += dims\n",
    "\n",
    "            else:\n",
    "                raise ValueError('Error, invalid dimension value')\n",
    "\n",
    "        log_prob_vec = tf.reshape(log_prob_vec, [reshape_dim, len(self.input_dim_vec)])\n",
    "        log_prob_vec = tf.reduce_sum(log_prob_vec, axis= -1) # Do I want this? \n",
    "                                                                 # Yes vlb in original code does this when return\n",
    "        return log_prob_vec\n",
    "\n",
    "    \"\"\"\n",
    "    def generate_samples_params(self, inputs, masks, sample=1):\n",
    "\n",
    "        #Takes a model and \n",
    "\n",
    "        # (batch_size, width, height, channels)\n",
    "        observed_inputs = self.make_observed_inputs(inputs, masks)\n",
    "        # (batch_size, width, height, 2*channels)\n",
    "        observed_inputs_with_masks = tf.concat([observed_inputs, masks], axis=-1)\n",
    "\n",
    "        prior_params = self.prior_net(observed_inputs_with_masks)\n",
    "\n",
    "        prior_distribution = tfd.Normal(\n",
    "          loc=prior_params[..., :256],\n",
    "          scale=tf.clip_by_value(\n",
    "            tf.nn.softplus(prior_params[..., 256:]),\n",
    "            1e-3,\n",
    "            tf.float32.max),\n",
    "          name=\"priors\")\n",
    "\n",
    "        samples_params = []\n",
    "        for i in range(sample):\n",
    "          latent = prior_distribution.sample()\n",
    "          sample_params = self.generative_net(latent)\n",
    "          samples_params.append(sample_params)\n",
    "        return tf.stack(samples_params, axis=1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO Implement correctly\n",
    "    \"\"\"\n",
    "    def generate_sample(self, sample=1):\n",
    "        if(self.proposal_distribution == None):\n",
    "            raise Exception('Network has no proposal distribution. Train it first')\n",
    "        else:\n",
    "            latent = proposal_distribution.sample()\n",
    "\n",
    "            generative_params = self.decoder(latent)\n",
    "        \n",
    "            return generative_params\n",
    "    \"\"\"\n",
    "\n",
    "def eval(model, x_batch, x_flat, x_masked, mask):\n",
    "\n",
    "    x_flat = tf.convert_to_tensor(x_flat)\n",
    "\n",
    "    prior_params = model.prior_encoder(x_masked)\n",
    "    #prior_params = model.prior_encoder(x_masked, training = True)\n",
    "    #prior_params = model.prior_encoder(x_masked, training = False)\n",
    "\n",
    "    proposal_params = model.recognition_encoder(x_flat)\n",
    "    #proposal_params = model.recognition_encoder(x_flat, training = True)\n",
    "    #proposal_params = model.recognition_encoder(x_flat, training = False)\n",
    "\n",
    "    # Essentially CLUEs normal_parse_params\n",
    "    prior_distribution = tfd.Normal(\n",
    "      loc=prior_params[..., :model.latent_dim],\n",
    "      scale=tf.clip_by_value(\n",
    "        tf.nn.softplus(prior_params[..., model.latent_dim:]),\n",
    "        1e-3,\n",
    "        tf.float32.max),\n",
    "      name=\"priors\")\n",
    "\n",
    "    proposal_distribution = tfd.Normal(\n",
    "      loc=proposal_params[..., :model.latent_dim],\n",
    "      scale=tf.clip_by_value(\n",
    "        tf.nn.softplus(proposal_params[..., model.latent_dim:]),\n",
    "        1e-3,\n",
    "        tf.float32.max),\n",
    "      name=\"proposal\")\n",
    "\n",
    "    z_sample = proposal_distribution.loc\n",
    "\n",
    "    rec_params = model.decoder(z_sample)\n",
    "    #rec_params = model.decoder(z_sample, training = True)\n",
    "    #rec_params = model.decoder(z_sample, training = False)\n",
    "    \n",
    "    regularizer = model.prior_regularizer(prior_distribution)\n",
    "\n",
    "    rec_loss = model.reconstruction_loss(rec_params, x_flat)\n",
    "\n",
    "    # True distribution, Estimated distribution in this order\n",
    "    kl_divergence = tf.reduce_sum(\n",
    "      tf.reshape(\n",
    "        tfd.kl_divergence(proposal_distribution, prior_distribution),\n",
    "        (model.batch_size, -1)), -1)\n",
    "\n",
    "    vlb = tf.reduce_mean(-kl_divergence + rec_loss + regularizer) # For comparing\n",
    "    return vlb, kl_divergence, rec_loss, regularizer\n",
    "\n",
    "def compute_loss_VAEAC(model, x_batch, x_flat, x_masked, mask):\n",
    "    \n",
    "    prior_params = model.prior_encoder(x_masked) \n",
    "    proposal_params = model.recognition_encoder(x_flat)\n",
    "    #prior_params = model.prior_encoder(x_masked, training = True) \n",
    "    #proposal_params = model.recognition_encoder(x_flat, training = True)\n",
    "    #prior_params = model.prior_encoder(x_masked, training = False) \n",
    "    #proposal_params = model.recognition_encoder(x_flat, training = False)\n",
    "\n",
    "    # Essentially CLUEs normal_parse_params\n",
    "    proposal_distribution = tfd.Normal(\n",
    "      loc=proposal_params[..., :model.latent_dim],\n",
    "      scale=tf.clip_by_value(\n",
    "        tf.nn.softplus(proposal_params[..., model.latent_dim:]),\n",
    "        1e-3,\n",
    "        tf.float32.max),\n",
    "      name=\"proposal\")\n",
    "\n",
    "    prior_distribution = tfd.Normal(\n",
    "      loc=prior_params[..., :model.latent_dim],\n",
    "      scale=tf.clip_by_value(\n",
    "        tf.nn.softplus(prior_params[..., model.latent_dim:]),\n",
    "        1e-3,\n",
    "        tf.float32.max),\n",
    "      name=\"priors\")\n",
    "\n",
    "    regularizer = model.prior_regularizer(prior_distribution)\n",
    "\n",
    "    latent = proposal_distribution.sample()\n",
    "\n",
    "    generative_params = model.decoder(latent)\n",
    "    #generative_params = model.decoder(latent, training = True)\n",
    "    #generative_params = model.decoder(latent, training = False)\n",
    "    \n",
    "    rec_loss = model.reconstruction_loss(generative_params, x_flat)\n",
    "    \n",
    "    # True distribution, Estimated distribution in this order\n",
    "    kl_divergence = tf.reduce_sum(\n",
    "      tf.reshape(\n",
    "        tfd.kl_divergence(proposal_distribution, prior_distribution),\n",
    "        (model.batch_size, -1)), -1)\n",
    "\n",
    "    vlb = tf.reduce_mean(-kl_divergence + rec_loss + regularizer) # For comparing\n",
    "    loss = tf.reduce_mean((kl_divergence - rec_loss - regularizer) * model.vlb_scale) \n",
    "    return loss, vlb, kl_divergence, rec_loss, regularizer\n",
    "\n",
    "@tf.function # Converts all numpy arrays to tensors\n",
    "def train_step_VAEAC(model, x_batch, x_flat, x_masked, mask):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, vlb, kl_divergence, rec_loss, regularizer = compute_loss_VAEAC(model, x_batch, x_flat, x_masked, mask)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss, vlb, kl_divergence, rec_loss, regularizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6d5c7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "def train_VAEAC(model, x_train, x_test, masker, nb_epochs, early_stop = None):\n",
    "    \n",
    "    vlb_train = np.zeros(nb_epochs)\n",
    "    vlb_val = np.zeros(nb_epochs)\n",
    "    best_vlb = -np.inf\n",
    "    best_epoch = 0\n",
    "    \n",
    "    overall_batch_size = model.batch_size\n",
    "    \n",
    "    test_data = []\n",
    "    for x in batch(x_test, n = overall_batch_size):\n",
    "        test_data.append(x)\n",
    "    \n",
    "    epoch = 0\n",
    "    for epoch in range(0, nb_epochs):\n",
    "        \n",
    "        # Shuffle the training data and sort it into batches every epoch\n",
    "        train_data = []\n",
    "        np.random.shuffle(x_train)\n",
    "        for x in batch(x_train, n = overall_batch_size):\n",
    "            train_data.append(x)\n",
    "        \n",
    "        tic = time.time()\n",
    "\n",
    "        ## Training\n",
    "        nb_samples = 0\n",
    "        for x_batch in train_data:\n",
    "\n",
    "            model.batch_size = x_batch.shape[0] # TODO: FIX THIS: Very ugly solution now to make sure batches \n",
    "                                                # that do not have the full size\n",
    "                \n",
    "            mask = masker(x_batch) #tensor with floats\n",
    "            \n",
    "            # Flatten the batch\n",
    "            x_batch_flat = gauss_cat_to_flat(x_batch, model.input_dim_vec) # numpy\n",
    "            \n",
    "            # Flatten the mask\n",
    "            mask_flat = gauss_cat_to_flat_mask(mask, model.input_dim_vec)\n",
    "            \n",
    "            # Mask flattened batch\n",
    "            x_batch_flat_masked = model.apply_mask(tf.convert_to_tensor(x_batch_flat), mask_flat)\n",
    "            \n",
    "            # Concat the mask flattened batch with the flattened mask\n",
    "            x_batch_flat_masked_concat = tf.concat([x_batch_flat_masked, mask_flat], axis=1)\n",
    "            \n",
    "            loss, vlb, kl_divergence, rec_loss, regularizer = train_step_VAEAC(model, x_batch, x_batch_flat, x_batch_flat_masked_concat, mask_flat)\n",
    "\n",
    "            vlb_train[epoch] += vlb.numpy() * x_batch.shape[0]\n",
    "            nb_samples += x_batch.shape[0]\n",
    "\n",
    "        vlb_train[epoch] /= nb_samples\n",
    "        toc = time.time()\n",
    "        print(\"Epoch_\" + str(epoch) + \", vlb: \" + str(vlb_train[epoch]) + \", took: \" + str(toc-tic))\n",
    "        \n",
    "        ## Validation\n",
    "        nb_samples = 0\n",
    "        for x_batch in test_data:\n",
    "            model.batch_size = x_batch.shape[0] # TODO: FIX THIS: Very ugly solution now to make sure batches \n",
    "                                                # that do not have the full size\n",
    "\n",
    "            mask = masker(x_batch) #tensor with floats\n",
    "            \n",
    "            # Flatten the batch\n",
    "            x_batch_flat = gauss_cat_to_flat(x_batch, model.input_dim_vec) # numpy\n",
    "            \n",
    "            # Flatten the mask\n",
    "            mask_flat = gauss_cat_to_flat_mask(mask, model.input_dim_vec)\n",
    "            \n",
    "            # Mask flattened batch\n",
    "            x_batch_flat_masked = model.apply_mask(tf.convert_to_tensor(x_batch_flat), mask_flat)\n",
    "            \n",
    "            # Concat the mask flattened batch with the flattened mask\n",
    "            x_batch_flat_masked_concat = tf.concat([x_batch_flat_masked, mask_flat], axis=1)\n",
    "            \n",
    "            vlb, kl_divergence, rec_loss, regularizer = eval(model, x_batch, x_batch_flat, x_batch_flat_masked_concat, mask_flat)\n",
    "\n",
    "            vlb_val[epoch] += vlb.numpy() * x_batch.shape[0]\n",
    "            nb_samples += x_batch.shape[0]\n",
    "\n",
    "        vlb_val[epoch] /= nb_samples\n",
    "        \n",
    "        if vlb_val[epoch] > best_vlb:\n",
    "            best_vlb = vlb_val[epoch]\n",
    "            best_epoch = epoch\n",
    "            if(model.save_model):\n",
    "                \n",
    "                #open text file\n",
    "                text_file = open(str(dname) + \"_best_epoch_VAEAC_lr_\" + str(model.lr) + \".txt\", \"w\")\n",
    "\n",
    "                #write string to file\n",
    "                text_file.write(str(epoch))\n",
    "\n",
    "                #close file\n",
    "                text_file.close()\n",
    "\n",
    "                model.recognition_encoder.save(str(dname) + \"_recog_encoder_lr_\" + str(model.lr))\n",
    "                model.prior_encoder.save(str(dname) + \"_prior_encoder_lr_\" + str(model.lr))\n",
    "                model.decoder.save(str(dname) + \"_decoder_lr_\" + str(model.lr))\n",
    "\n",
    "        print(\"Validation vlb: \" + str(vlb_val[epoch]) + \", Best vlb: \" + str(best_vlb) + \"\\n\")\n",
    "\n",
    "        if early_stop is not None and (epoch - best_epoch) > early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    \n",
    "    np.save(str(dname) + \"_vlb_train_lr_\" + str(model.lr), vlb_train)\n",
    "    np.save(str(dname) + \"_vlb_val_lr_\" + str(model.lr), vlb_val)\n",
    "    return vlb_train, vlb_val, best_epoch, best_vlb, epoch\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130d71c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86726769",
   "metadata": {},
   "source": [
    "## Train VAEAC (COMPAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7e46c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compas (5554, 19) (618, 19)\n",
      "[3 6 2 2 2 1 1 2]\n",
      "compas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow_addons/optimizers/rectified_adam.py:120: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "2021-12-08 17:58:14.816755: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_0, vlb: -6.739775293973182, took: 9.378506422042847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 17:58:25.475826: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation vlb: -6.108806411039482, Best vlb: -6.108806411039482\n",
      "\n",
      "Epoch_1, vlb: -5.614978189815237, took: 1.2691938877105713\n",
      "Validation vlb: -5.320394500559588, Best vlb: -5.320394500559588\n",
      "\n",
      "Epoch_2, vlb: -4.625620614808371, took: 1.262967824935913\n",
      "Validation vlb: -4.458042714202288, Best vlb: -4.458042714202288\n",
      "\n",
      "Epoch_3, vlb: -3.6744451618984444, took: 1.375373125076294\n",
      "Validation vlb: -3.5053702342085855, Best vlb: -3.5053702342085855\n",
      "\n",
      "Epoch_4, vlb: -3.1181316210891343, took: 1.25313401222229\n",
      "Validation vlb: -2.974641837734235, Best vlb: -2.974641837734235\n",
      "\n",
      "Epoch_5, vlb: -2.8642919397543007, took: 1.2558913230895996\n",
      "Validation vlb: -2.7030546649760026, Best vlb: -2.7030546649760026\n",
      "\n",
      "Epoch_6, vlb: -2.7022584546672643, took: 1.482248067855835\n",
      "Validation vlb: -2.4460959858878915, Best vlb: -2.4460959858878915\n",
      "\n",
      "Epoch_7, vlb: -2.5204285815503042, took: 1.2463889122009277\n",
      "Validation vlb: -2.103960581196165, Best vlb: -2.103960581196165\n",
      "\n",
      "Epoch_8, vlb: -2.251483758748501, took: 1.2703211307525635\n",
      "Validation vlb: -1.703152217139704, Best vlb: -1.703152217139704\n",
      "\n",
      "Epoch_9, vlb: -1.9671471360178787, took: 1.2637608051300049\n",
      "Validation vlb: -1.3521412919640154, Best vlb: -1.3521412919640154\n",
      "\n",
      "Epoch_10, vlb: -1.7523069727279146, took: 1.2569308280944824\n",
      "Validation vlb: -1.2044012399938886, Best vlb: -1.2044012399938886\n",
      "\n",
      "Epoch_11, vlb: -1.5867704458703744, took: 1.2752220630645752\n",
      "Validation vlb: -1.1161525349786752, Best vlb: -1.1161525349786752\n",
      "\n",
      "Epoch_12, vlb: -1.521923347948093, took: 1.260166883468628\n",
      "Validation vlb: -1.1286864500601315, Best vlb: -1.1161525349786752\n",
      "\n",
      "Epoch_13, vlb: -1.4723973712948617, took: 1.2346770763397217\n",
      "Validation vlb: -1.0257398430197755, Best vlb: -1.0257398430197755\n",
      "\n",
      "Epoch_14, vlb: -1.4314528314338384, took: 2.1150190830230713\n",
      "Validation vlb: -1.0157855612174593, Best vlb: -1.0157855612174593\n",
      "\n",
      "Epoch_15, vlb: -1.3909867107331602, took: 1.3797988891601562\n",
      "Validation vlb: -1.025976022084554, Best vlb: -1.0157855612174593\n",
      "\n",
      "Epoch_16, vlb: -1.3962120672846854, took: 1.4596049785614014\n",
      "Validation vlb: -1.028278489521792, Best vlb: -1.0157855612174593\n",
      "\n",
      "Epoch_17, vlb: -1.3598742804425974, took: 1.3388087749481201\n",
      "Validation vlb: -1.008604869487602, Best vlb: -1.008604869487602\n",
      "\n",
      "Epoch_18, vlb: -1.3546001171095576, took: 1.3967862129211426\n",
      "Validation vlb: -1.026001647259425, Best vlb: -1.008604869487602\n",
      "\n",
      "Epoch_19, vlb: -1.3517443744407354, took: 1.3624272346496582\n",
      "Validation vlb: -1.0216852659932232, Best vlb: -1.008604869487602\n",
      "\n",
      "Epoch_20, vlb: -1.3217011255929485, took: 1.363097906112671\n",
      "Validation vlb: -0.9889252457803893, Best vlb: -0.9889252457803893\n",
      "\n",
      "Epoch_21, vlb: -1.3201705467198708, took: 1.395432949066162\n",
      "Validation vlb: -0.9755605393628858, Best vlb: -0.9755605393628858\n",
      "\n",
      "Epoch_22, vlb: -1.3233712445843937, took: 1.495185136795044\n",
      "Validation vlb: -1.0262856496962143, Best vlb: -0.9755605393628858\n",
      "\n",
      "Epoch_23, vlb: -1.299801639657163, took: 1.5274407863616943\n",
      "Validation vlb: -1.0062527899603242, Best vlb: -0.9755605393628858\n",
      "\n",
      "Epoch_24, vlb: -1.2989883744806792, took: 1.5328929424285889\n",
      "Validation vlb: -0.95615195668631, Best vlb: -0.95615195668631\n",
      "\n",
      "Epoch_25, vlb: -1.2459250788573615, took: 1.4694838523864746\n",
      "Validation vlb: -1.0497829620892176, Best vlb: -0.95615195668631\n",
      "\n",
      "Epoch_26, vlb: -1.2780762044568692, took: 1.599261999130249\n",
      "Validation vlb: -1.0706210879060443, Best vlb: -0.95615195668631\n",
      "\n",
      "Epoch_27, vlb: -1.2695270350909207, took: 1.549086093902588\n",
      "Validation vlb: -1.0133885237777118, Best vlb: -0.95615195668631\n",
      "\n",
      "Epoch_28, vlb: -1.2479515012151094, took: 1.4896109104156494\n",
      "Validation vlb: -0.9824840480455689, Best vlb: -0.95615195668631\n",
      "\n",
      "Epoch_29, vlb: -1.275450817062823, took: 1.5299761295318604\n",
      "Validation vlb: -1.0239938323937574, Best vlb: -0.95615195668631\n",
      "\n",
      "Epoch_30, vlb: -1.2546972546768052, took: 1.4793891906738281\n",
      "Validation vlb: -1.0689129482195214, Best vlb: -0.95615195668631\n",
      "\n",
      "Epoch_31, vlb: -1.2370506807072454, took: 1.4682278633117676\n",
      "Validation vlb: -1.0582562360948728, Best vlb: -0.95615195668631\n",
      "\n",
      "Epoch_32, vlb: -1.2207947577939087, took: 1.4463181495666504\n",
      "Validation vlb: -0.9638338198939573, Best vlb: -0.95615195668631\n",
      "\n",
      "Epoch_33, vlb: -1.2388109023681766, took: 1.5134780406951904\n",
      "Validation vlb: -0.9229791723794536, Best vlb: -0.9229791723794536\n",
      "\n",
      "Epoch_34, vlb: -1.2343968286074707, took: 1.546734094619751\n",
      "Validation vlb: -0.8389901758011876, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_35, vlb: -1.1956272770871694, took: 1.3084716796875\n",
      "Validation vlb: -0.9529624482960377, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_36, vlb: -1.2260532947937963, took: 1.6084527969360352\n",
      "Validation vlb: -0.9784558691253168, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_37, vlb: -1.2139553478565155, took: 1.6992478370666504\n",
      "Validation vlb: -0.9699602370123261, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_38, vlb: -1.202610284165165, took: 1.5144388675689697\n",
      "Validation vlb: -1.012749943339709, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_39, vlb: -1.2296769982153535, took: 1.7417230606079102\n",
      "Validation vlb: -0.9583109133066097, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_40, vlb: -1.2394974395273226, took: 1.5662438869476318\n",
      "Validation vlb: -0.9687511351887848, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_41, vlb: -1.2201439531578364, took: 1.3474528789520264\n",
      "Validation vlb: -0.9178383170979695, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_42, vlb: -1.202882232836008, took: 1.6197900772094727\n",
      "Validation vlb: -0.9929942432730715, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_43, vlb: -1.2052222732421, took: 1.9208791255950928\n",
      "Validation vlb: -0.8869792776586168, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_44, vlb: -1.1941259870178889, took: 2.1747777462005615\n",
      "Validation vlb: -0.9886217778940417, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_45, vlb: -1.2119139775896741, took: 1.806974172592163\n",
      "Validation vlb: -0.991829215708674, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_46, vlb: -1.2118636922609647, took: 1.486219882965088\n",
      "Validation vlb: -0.8910419977598592, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_47, vlb: -1.2022110876695347, took: 1.534221887588501\n",
      "Validation vlb: -1.00495984504138, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_48, vlb: -1.1777255205443578, took: 1.8121211528778076\n",
      "Validation vlb: -0.9754390257461943, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_49, vlb: -1.1946735304810823, took: 2.5440869331359863\n",
      "Validation vlb: -0.9045718586174801, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_50, vlb: -1.1535164778035967, took: 1.979099988937378\n",
      "Validation vlb: -0.9001706801957683, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_51, vlb: -1.1589809656143188, took: 1.9275169372558594\n",
      "Validation vlb: -0.9763544363882936, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_52, vlb: -1.1827624595375597, took: 1.5626327991485596\n",
      "Validation vlb: -0.9852540986823418, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_53, vlb: -1.2081250767077651, took: 1.4492392539978027\n",
      "Validation vlb: -0.9817579328820929, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_54, vlb: -1.1727679017897685, took: 1.460075855255127\n",
      "Validation vlb: -0.9522487565537486, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_55, vlb: -1.1645678245939708, took: 1.4674479961395264\n",
      "Validation vlb: -0.9332181242291595, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_56, vlb: -1.1653392272842071, took: 1.4440679550170898\n",
      "Validation vlb: -0.9688559317280173, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_57, vlb: -1.1529043331651065, took: 1.4595818519592285\n",
      "Validation vlb: -0.9670968379789185, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_58, vlb: -1.1661381047966242, took: 1.5101120471954346\n",
      "Validation vlb: -0.8946150919380311, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_59, vlb: -1.1526772054915038, took: 1.4265687465667725\n",
      "Validation vlb: -0.9503899220509823, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_60, vlb: -1.1627351951805172, took: 1.4730207920074463\n",
      "Validation vlb: -0.9509232389502541, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_61, vlb: -1.1727593428031722, took: 1.464210033416748\n",
      "Validation vlb: -0.9657706068557443, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_62, vlb: -1.1710839951514855, took: 1.6436171531677246\n",
      "Validation vlb: -0.9664963535892154, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_63, vlb: -1.159157835368334, took: 1.4709758758544922\n",
      "Validation vlb: -0.9262227821118624, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_64, vlb: -1.1315029492943158, took: 1.5674538612365723\n",
      "Validation vlb: -0.8897528243296354, Best vlb: -0.8389901758011876\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_65, vlb: -1.153685171440174, took: 1.478036880493164\n",
      "Validation vlb: -0.9836066361384098, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_66, vlb: -1.167438203411906, took: 1.4729628562927246\n",
      "Validation vlb: -0.9901650231632986, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_67, vlb: -1.1362993822365646, took: 1.4499890804290771\n",
      "Validation vlb: -0.9667647652641469, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_68, vlb: -1.1328687552801344, took: 1.5161209106445312\n",
      "Validation vlb: -0.9577951504574624, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_69, vlb: -1.130715997304082, took: 1.5004327297210693\n",
      "Validation vlb: -0.8901954736910206, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_70, vlb: -1.1565889213254308, took: 1.5256187915802002\n",
      "Validation vlb: -0.9282615678595879, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_71, vlb: -1.1757599211958274, took: 1.4579660892486572\n",
      "Validation vlb: -0.8884238207224503, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_72, vlb: -1.1331367519720021, took: 1.4447150230407715\n",
      "Validation vlb: -0.9045625361809838, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_73, vlb: -1.1463626198602057, took: 1.441584825515747\n",
      "Validation vlb: -0.925396444342283, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_74, vlb: -1.1397607428674694, took: 1.944237232208252\n",
      "Validation vlb: -0.9637761883751088, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_75, vlb: -1.145097699962076, took: 1.6826670169830322\n",
      "Validation vlb: -0.9106763882930224, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_76, vlb: -1.1570473910451236, took: 1.4433438777923584\n",
      "Validation vlb: -0.9647755763677331, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_77, vlb: -1.1315062791699337, took: 2.1326022148132324\n",
      "Validation vlb: -1.0099598512680399, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_78, vlb: -1.1293949505215632, took: 1.2862191200256348\n",
      "Validation vlb: -0.9361288470357753, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_79, vlb: -1.1394544557396955, took: 1.2582859992980957\n",
      "Validation vlb: -0.9865170207995813, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_80, vlb: -1.1205825142865564, took: 1.2764050960540771\n",
      "Validation vlb: -0.9340285626044166, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_81, vlb: -1.150046053961279, took: 2.147968053817749\n",
      "Validation vlb: -0.9387673915011211, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_82, vlb: -1.1322205799880283, took: 1.348323106765747\n",
      "Validation vlb: -0.8792285386798451, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_83, vlb: -1.143046099675144, took: 1.2619960308074951\n",
      "Validation vlb: -0.9343251921598194, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_84, vlb: -1.1562062637128545, took: 1.280775785446167\n",
      "Validation vlb: -0.9150757326663119, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_85, vlb: -1.166505401925213, took: 1.252004861831665\n",
      "Validation vlb: -0.8583016627043196, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_86, vlb: -1.1378021800436473, took: 1.242828607559204\n",
      "Validation vlb: -0.8884808635248721, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_87, vlb: -1.147066762561544, took: 1.3509202003479004\n",
      "Validation vlb: -0.9394686584719563, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_88, vlb: -1.1492456081834765, took: 1.234100103378296\n",
      "Validation vlb: -0.8713076774356434, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_89, vlb: -1.1521917854576262, took: 1.2361278533935547\n",
      "Validation vlb: -0.9428510339900513, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_90, vlb: -1.1221369248652227, took: 1.2408146858215332\n",
      "Validation vlb: -0.9762643714552944, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_91, vlb: -1.1143500620742732, took: 1.2595720291137695\n",
      "Validation vlb: -0.8742750580256811, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_92, vlb: -1.118319058057684, took: 1.2861320972442627\n",
      "Validation vlb: -0.8710095425253933, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_93, vlb: -1.160157187233136, took: 1.2886662483215332\n",
      "Validation vlb: -0.9222141566014213, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_94, vlb: -1.0967536099660211, took: 1.3696708679199219\n",
      "Validation vlb: -0.8924248052646427, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_95, vlb: -1.1540496124824604, took: 1.28025221824646\n",
      "Validation vlb: -0.9348368648572262, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_96, vlb: -1.1246372648053002, took: 1.2635388374328613\n",
      "Validation vlb: -0.9169283219911519, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_97, vlb: -1.1497350629388234, took: 1.2939527034759521\n",
      "Validation vlb: -0.8922455827780912, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_98, vlb: -1.1516949448346834, took: 1.3210971355438232\n",
      "Validation vlb: -0.8915112859608672, Best vlb: -0.8389901758011876\n",
      "\n",
      "Epoch_99, vlb: -1.1384414257572244, took: 1.2726097106933594\n",
      "Validation vlb: -0.8046140477881076, Best vlb: -0.8046140477881076\n",
      "\n",
      "Epoch_100, vlb: -1.114518420877412, took: 1.314723014831543\n",
      "Validation vlb: -0.8419981355805999, Best vlb: -0.8046140477881076\n",
      "\n",
      "Epoch_101, vlb: -1.1278349618324497, took: 1.2852051258087158\n",
      "Validation vlb: -0.9555234855046936, Best vlb: -0.8046140477881076\n",
      "\n",
      "Epoch_102, vlb: -1.1583004350665667, took: 1.2698791027069092\n",
      "Validation vlb: -1.0126258358600455, Best vlb: -0.8046140477881076\n",
      "\n",
      "Epoch_103, vlb: -1.1223703633979107, took: 1.3135240077972412\n",
      "Validation vlb: -0.8863305603802011, Best vlb: -0.8046140477881076\n",
      "\n",
      "Epoch_104, vlb: -1.1127127431732826, took: 1.479358196258545\n",
      "Validation vlb: -0.9163070969211246, Best vlb: -0.8046140477881076\n",
      "\n",
      "Epoch_105, vlb: -1.1164818025124688, took: 1.2612848281860352\n",
      "Validation vlb: -0.8964121594398153, Best vlb: -0.8046140477881076\n",
      "\n",
      "Epoch_106, vlb: -1.1206165837176978, took: 1.25404691696167\n",
      "Validation vlb: -0.9398367509101201, Best vlb: -0.8046140477881076\n",
      "\n",
      "Epoch_107, vlb: -1.1204669150595274, took: 1.2176859378814697\n",
      "Validation vlb: -0.8912897937506148, Best vlb: -0.8046140477881076\n",
      "\n",
      "Epoch_108, vlb: -1.1103012745956486, took: 1.2270698547363281\n",
      "Validation vlb: -0.9461563887333793, Best vlb: -0.8046140477881076\n",
      "\n",
      "Epoch_109, vlb: -1.1469655016750409, took: 1.247870922088623\n",
      "Validation vlb: -1.024304124915484, Best vlb: -0.8046140477881076\n",
      "\n",
      "Epoch_110, vlb: -1.1340134984132495, took: 1.2612578868865967\n",
      "Validation vlb: -0.9285151917957565, Best vlb: -0.8046140477881076\n",
      "\n",
      "Epoch_111, vlb: -1.1338999286846674, took: 1.3479020595550537\n",
      "Validation vlb: -0.8570374838742624, Best vlb: -0.8046140477881076\n",
      "\n",
      "Epoch_112, vlb: -1.1272167420275656, took: 1.2389147281646729\n",
      "Validation vlb: -0.9328723196844453, Best vlb: -0.8046140477881076\n",
      "\n",
      "Epoch_113, vlb: -1.09777110761389, took: 1.2239947319030762\n",
      "Validation vlb: -0.8694351103313532, Best vlb: -0.8046140477881076\n",
      "\n",
      "Epoch_114, vlb: -1.1042321562809956, took: 1.2412259578704834\n",
      "Validation vlb: -0.8477077758042172, Best vlb: -0.8046140477881076\n",
      "\n",
      "Epoch_115, vlb: -1.1076420266979412, took: 1.226534128189087\n",
      "Validation vlb: -0.9677600044648624, Best vlb: -0.8046140477881076\n",
      "\n",
      "Epoch_116, vlb: -1.138698310121427, took: 1.2329480648040771\n",
      "Validation vlb: -0.8729489762034617, Best vlb: -0.8046140477881076\n",
      "\n",
      "Epoch_117, vlb: -1.123228181168292, took: 1.2633130550384521\n",
      "Validation vlb: -0.957982108430955, Best vlb: -0.8046140477881076\n",
      "\n",
      "Epoch_118, vlb: -1.1169762133178327, took: 1.3864467144012451\n",
      "Validation vlb: -0.9432739871991105, Best vlb: -0.8046140477881076\n",
      "\n",
      "Epoch_119, vlb: -1.1482142785662663, took: 1.2428910732269287\n",
      "Validation vlb: -0.8976881424971769, Best vlb: -0.8046140477881076\n",
      "\n",
      "Epoch_120, vlb: -1.1066861023008288, took: 1.2884178161621094\n",
      "Validation vlb: -0.8630766758640993, Best vlb: -0.8046140477881076\n",
      "\n",
      "Epoch_121, vlb: -1.121216691480423, took: 1.2414381504058838\n",
      "Validation vlb: -0.9328106016788668, Best vlb: -0.8046140477881076\n",
      "\n",
      "Epoch_122, vlb: -1.130128074387745, took: 1.2272260189056396\n",
      "Validation vlb: -0.7976614611048529, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_123, vlb: -1.1345237902183518, took: 1.2543859481811523\n",
      "Validation vlb: -0.8901116944439589, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_124, vlb: -1.1230615544413594, took: 1.3483538627624512\n",
      "Validation vlb: -0.9421456072322759, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_125, vlb: -1.1357781411848524, took: 1.488133192062378\n",
      "Validation vlb: -0.9599248431261304, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_126, vlb: -1.1373109539335564, took: 1.6225850582122803\n",
      "Validation vlb: -0.9653906548293277, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_127, vlb: -1.140269897462311, took: 1.8877041339874268\n",
      "Validation vlb: -0.9274088746135675, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_128, vlb: -1.1026206863068144, took: 1.232133150100708\n",
      "Validation vlb: -0.8657456430030872, Best vlb: -0.7976614611048529\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_129, vlb: -1.1106962116665233, took: 1.2316029071807861\n",
      "Validation vlb: -0.8787905500544699, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_130, vlb: -1.1431896496171097, took: 1.2411298751831055\n",
      "Validation vlb: -0.9367334090390251, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_131, vlb: -1.1461068541187154, took: 1.246838092803955\n",
      "Validation vlb: -0.9994551313733592, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_132, vlb: -1.1454947813835596, took: 1.240243911743164\n",
      "Validation vlb: -0.900289638528546, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_133, vlb: -1.1204712358274862, took: 1.2330009937286377\n",
      "Validation vlb: -0.8977257076976368, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_134, vlb: -1.1691681737087594, took: 1.3013689517974854\n",
      "Validation vlb: -0.975983972688323, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_135, vlb: -1.12572693328957, took: 1.2557599544525146\n",
      "Validation vlb: -0.9268679861883515, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_136, vlb: -1.1085745496289696, took: 1.24076509475708\n",
      "Validation vlb: -0.959121384474066, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_137, vlb: -1.1199369937943433, took: 1.2636611461639404\n",
      "Validation vlb: -0.9411391885921021, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_138, vlb: -1.1193914180785836, took: 1.2459571361541748\n",
      "Validation vlb: -0.9458633057507883, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_139, vlb: -1.1477514404507048, took: 1.2396652698516846\n",
      "Validation vlb: -1.008614583501538, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_140, vlb: -1.1237341253114426, took: 1.35595703125\n",
      "Validation vlb: -0.9121853460385961, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_141, vlb: -1.1242856367225909, took: 1.3010432720184326\n",
      "Validation vlb: -0.9326302036884148, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_142, vlb: -1.1536160740957098, took: 1.2411878108978271\n",
      "Validation vlb: -0.9132754125641388, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_143, vlb: -1.132195444728644, took: 1.2265307903289795\n",
      "Validation vlb: -0.944293996276979, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_144, vlb: -1.1298905692600199, took: 1.2691919803619385\n",
      "Validation vlb: -0.9409992422872376, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_145, vlb: -1.1195327653101892, took: 1.2738633155822754\n",
      "Validation vlb: -0.8687096222704668, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_146, vlb: -1.1341619643540017, took: 1.2358760833740234\n",
      "Validation vlb: -0.9174834366369402, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_147, vlb: -1.1370378149727673, took: 1.3569061756134033\n",
      "Validation vlb: -0.9783747990926107, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_148, vlb: -1.128814283853647, took: 1.2446088790893555\n",
      "Validation vlb: -1.0077634808700833, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_149, vlb: -1.097461029699392, took: 1.2475016117095947\n",
      "Validation vlb: -0.886073427871593, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_150, vlb: -1.1349807703155899, took: 1.2390050888061523\n",
      "Validation vlb: -0.9813937668275678, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_151, vlb: -1.11709979332133, took: 1.2398021221160889\n",
      "Validation vlb: -1.0156254405728435, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_152, vlb: -1.088745711259289, took: 1.2538599967956543\n",
      "Validation vlb: -0.9142367060516259, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_153, vlb: -1.1257817917776742, took: 1.2626538276672363\n",
      "Validation vlb: -0.921237267915485, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_154, vlb: -1.0895151387112665, took: 1.427211046218872\n",
      "Validation vlb: -0.977065610847041, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_155, vlb: -1.1036122751442012, took: 1.2399649620056152\n",
      "Validation vlb: -0.9089804490407308, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_156, vlb: -1.1190566608791606, took: 1.2510480880737305\n",
      "Validation vlb: -0.9366674733779191, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_157, vlb: -1.1379922727994194, took: 1.235564947128296\n",
      "Validation vlb: -0.9572659744413925, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_158, vlb: -1.1378295668861957, took: 1.2443749904632568\n",
      "Validation vlb: -0.9530598953318056, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_159, vlb: -1.121065457475928, took: 1.2447562217712402\n",
      "Validation vlb: -0.9625267878319453, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_160, vlb: -1.0961030816319266, took: 1.245147943496704\n",
      "Validation vlb: -0.8817246278512825, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_161, vlb: -1.0805979893711517, took: 1.3117289543151855\n",
      "Validation vlb: -0.950848920445612, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_162, vlb: -1.1207499158524077, took: 1.373676061630249\n",
      "Validation vlb: -0.9586942172744899, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_163, vlb: -1.1340464645038202, took: 1.2589170932769775\n",
      "Validation vlb: -0.8922368511798698, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_164, vlb: -1.1352510299878367, took: 1.23630690574646\n",
      "Validation vlb: -0.8693083378103559, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_165, vlb: -1.1294623281529022, took: 1.2474110126495361\n",
      "Validation vlb: -0.8994645011463598, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_166, vlb: -1.1264318496493242, took: 1.2486588954925537\n",
      "Validation vlb: -0.810380351196215, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_167, vlb: -1.1166283331159468, took: 1.3297581672668457\n",
      "Validation vlb: -0.8945868067371036, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_168, vlb: -1.1340959859400892, took: 1.2956891059875488\n",
      "Validation vlb: -0.8734179827773455, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_169, vlb: -1.1364892621584664, took: 1.239022970199585\n",
      "Validation vlb: -0.9663498467226245, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_170, vlb: -1.0828718325196303, took: 1.2607879638671875\n",
      "Validation vlb: -0.8427507252369112, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_171, vlb: -1.1504735726008624, took: 1.2464261054992676\n",
      "Validation vlb: -0.8154194987707539, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_172, vlb: -1.1338621347466518, took: 1.2442500591278076\n",
      "Validation vlb: -0.8991746968050219, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_173, vlb: -1.1432561319338834, took: 1.2442407608032227\n",
      "Validation vlb: -0.911125886401698, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_174, vlb: -1.1366776601419115, took: 1.2628748416900635\n",
      "Validation vlb: -0.9503072355557414, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_175, vlb: -1.1001564718039722, took: 1.2857887744903564\n",
      "Validation vlb: -0.8882751383827728, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_176, vlb: -1.1281613944286832, took: 1.269880771636963\n",
      "Validation vlb: -0.9227260823774492, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_177, vlb: -1.1494150788053403, took: 1.2566368579864502\n",
      "Validation vlb: -0.8540202377297732, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_178, vlb: -1.1042053888373808, took: 1.2497880458831787\n",
      "Validation vlb: -0.9004463964295619, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_179, vlb: -1.1246226308669467, took: 1.2513751983642578\n",
      "Validation vlb: -0.916503584886446, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_180, vlb: -1.119216686493637, took: 1.254857063293457\n",
      "Validation vlb: -0.874705571958548, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_181, vlb: -1.114739987547122, took: 1.2628600597381592\n",
      "Validation vlb: -0.9065187581148734, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_182, vlb: -1.1430091187814007, took: 1.266981840133667\n",
      "Validation vlb: -0.884804077904587, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_183, vlb: -1.1253661589953896, took: 1.2454969882965088\n",
      "Validation vlb: -0.8771435930890944, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_184, vlb: -1.0932286251007675, took: 1.2743010520935059\n",
      "Validation vlb: -0.9023387032805137, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_185, vlb: -1.1202300142632313, took: 1.265578031539917\n",
      "Validation vlb: -0.9098470756922725, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_186, vlb: -1.1133370656824644, took: 1.2697241306304932\n",
      "Validation vlb: -0.9352169542250895, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_187, vlb: -1.1036414378059396, took: 1.2444300651550293\n",
      "Validation vlb: -0.9509385453844533, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_188, vlb: -1.09745185036946, took: 1.3069350719451904\n",
      "Validation vlb: -0.9405173136578409, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_189, vlb: -1.1147261647642368, took: 1.2445030212402344\n",
      "Validation vlb: -0.8782734101258435, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_190, vlb: -1.1304275497584881, took: 1.2522428035736084\n",
      "Validation vlb: -0.9332340569943672, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_191, vlb: -1.1208087955827888, took: 1.2543911933898926\n",
      "Validation vlb: -0.9851379371383815, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_192, vlb: -1.1140362211041284, took: 1.286264181137085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation vlb: -0.9193037293103906, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_193, vlb: -1.0926989377897165, took: 1.2965681552886963\n",
      "Validation vlb: -0.9293261880241938, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_194, vlb: -1.1486591675919424, took: 1.2466719150543213\n",
      "Validation vlb: -0.9854782538120801, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_195, vlb: -1.103167103441834, took: 1.3479318618774414\n",
      "Validation vlb: -0.9041964562193862, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_196, vlb: -1.1330587589401626, took: 1.234921932220459\n",
      "Validation vlb: -0.8407914073336086, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_197, vlb: -1.1519731910785926, took: 1.2432382106781006\n",
      "Validation vlb: -0.8913454205087088, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_198, vlb: -1.1199838017704764, took: 1.2433159351348877\n",
      "Validation vlb: -0.8819153264502492, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_199, vlb: -1.1504082235879591, took: 1.241164207458496\n",
      "Validation vlb: -0.9484187842958568, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_200, vlb: -1.1236458136788967, took: 1.2412991523742676\n",
      "Validation vlb: -0.8964046280360917, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_201, vlb: -1.1038871011074645, took: 1.2621471881866455\n",
      "Validation vlb: -0.9307934695848755, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_202, vlb: -1.1121802325080443, took: 1.3187000751495361\n",
      "Validation vlb: -0.9261667311770244, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_203, vlb: -1.1015082705480228, took: 1.235260009765625\n",
      "Validation vlb: -0.9079759500944884, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_204, vlb: -1.1254195512779162, took: 1.2255423069000244\n",
      "Validation vlb: -0.8722282312448743, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_205, vlb: -1.1450342595426994, took: 1.2440941333770752\n",
      "Validation vlb: -0.9565223769848401, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_206, vlb: -1.1162503737101763, took: 1.2452731132507324\n",
      "Validation vlb: -0.9024615025443167, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_207, vlb: -1.1518430860513915, took: 1.240185260772705\n",
      "Validation vlb: -0.9121949763359761, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_208, vlb: -1.1133438024453908, took: 1.2487480640411377\n",
      "Validation vlb: -0.9568271035129584, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_209, vlb: -1.1328900206161996, took: 1.3211431503295898\n",
      "Validation vlb: -0.9095746022212081, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_210, vlb: -1.0981559599308732, took: 1.251796007156372\n",
      "Validation vlb: -0.8426102955364486, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_211, vlb: -1.12186264279166, took: 1.2511942386627197\n",
      "Validation vlb: -0.9486364330674453, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_212, vlb: -1.1151029925832312, took: 1.2582972049713135\n",
      "Validation vlb: -0.8589260418052427, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_213, vlb: -1.1339444396819491, took: 1.2832300662994385\n",
      "Validation vlb: -0.95037876653054, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_214, vlb: -1.102123085912458, took: 1.2475552558898926\n",
      "Validation vlb: -0.8749097149734744, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_215, vlb: -1.0927699976015486, took: 1.230597734451294\n",
      "Validation vlb: -0.9847872801197385, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_216, vlb: -1.080128276738753, took: 1.5995118618011475\n",
      "Validation vlb: -0.9519621562031866, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_217, vlb: -1.1548894095601132, took: 1.2330939769744873\n",
      "Validation vlb: -0.8977368381416914, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_218, vlb: -1.1113946259257859, took: 1.3651549816131592\n",
      "Validation vlb: -0.922846290864605, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_219, vlb: -1.1257644667228779, took: 1.312324047088623\n",
      "Validation vlb: -0.8708765927256118, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_220, vlb: -1.0945484184471874, took: 1.2783317565917969\n",
      "Validation vlb: -0.9568160222572031, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_221, vlb: -1.138112409620522, took: 1.25205397605896\n",
      "Validation vlb: -0.8987915033661432, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_222, vlb: -1.1075180591032752, took: 1.224395751953125\n",
      "Validation vlb: -0.841800973446238, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_223, vlb: -1.1274143889476638, took: 1.335299015045166\n",
      "Validation vlb: -0.9037614640294541, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_224, vlb: -1.1187012565621035, took: 1.3310799598693848\n",
      "Validation vlb: -0.9290665962935266, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_225, vlb: -1.1160685140793813, took: 1.273320198059082\n",
      "Validation vlb: -0.8749849856092706, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_226, vlb: -1.1232570940786273, took: 1.257781982421875\n",
      "Validation vlb: -0.9403631708382788, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_227, vlb: -1.1257761226612155, took: 1.2414090633392334\n",
      "Validation vlb: -0.9357786419707981, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_228, vlb: -1.1038360537751184, took: 1.2695338726043701\n",
      "Validation vlb: -0.9456451701114864, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_229, vlb: -1.1415587371744438, took: 1.2525379657745361\n",
      "Validation vlb: -0.87285106853374, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_230, vlb: -1.1084202971439547, took: 1.3071990013122559\n",
      "Validation vlb: -0.9087220019507176, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_231, vlb: -1.1431582891387309, took: 1.2653257846832275\n",
      "Validation vlb: -0.9498184203715772, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_232, vlb: -1.1163837154606155, took: 1.2850391864776611\n",
      "Validation vlb: -0.9001461186455292, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_233, vlb: -1.1080139729673586, took: 1.2707929611206055\n",
      "Validation vlb: -0.9203278707840682, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_234, vlb: -1.1312334325444926, took: 1.266204833984375\n",
      "Validation vlb: -0.8545621988456997, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_235, vlb: -1.1179470308916493, took: 1.248610019683838\n",
      "Validation vlb: -0.8334738876441535, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_236, vlb: -1.101819902154066, took: 1.280777931213379\n",
      "Validation vlb: -0.8323761170736023, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_237, vlb: -1.116596715459731, took: 1.254809856414795\n",
      "Validation vlb: -0.8742920862432436, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_238, vlb: -1.1226272220700861, took: 1.284649133682251\n",
      "Validation vlb: -0.8581745175096209, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_239, vlb: -1.126900915774159, took: 1.2537221908569336\n",
      "Validation vlb: -0.8284621504903997, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_240, vlb: -1.10759339068148, took: 1.2346820831298828\n",
      "Validation vlb: -0.925345350237726, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_241, vlb: -1.1150484590338081, took: 1.268604040145874\n",
      "Validation vlb: -0.8976654499094078, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_242, vlb: -1.093744337923629, took: 1.249324083328247\n",
      "Validation vlb: -0.9101473644713368, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_243, vlb: -1.1317638975276678, took: 1.3480010032653809\n",
      "Validation vlb: -0.9540533406063191, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_244, vlb: -1.1207832356859206, took: 1.2444353103637695\n",
      "Validation vlb: -0.8297180977068287, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_245, vlb: -1.0971782943693305, took: 1.240898847579956\n",
      "Validation vlb: -0.9735880496046689, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_246, vlb: -1.1019782665010232, took: 1.2484769821166992\n",
      "Validation vlb: -0.9431875323014738, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_247, vlb: -1.1359201170018478, took: 1.241940975189209\n",
      "Validation vlb: -0.9182559735952458, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_248, vlb: -1.0953695070928149, took: 1.2385940551757812\n",
      "Validation vlb: -0.9391852950972647, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_249, vlb: -1.1353846556169334, took: 1.25657320022583\n",
      "Validation vlb: -0.9176038399483394, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_250, vlb: -1.1207671646252784, took: 1.3396389484405518\n",
      "Validation vlb: -0.9209729662605088, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_251, vlb: -1.116050963012014, took: 1.2632319927215576\n",
      "Validation vlb: -0.9149885825740481, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_252, vlb: -1.1011273295578015, took: 1.2368361949920654\n",
      "Validation vlb: -0.9518634420382552, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_253, vlb: -1.1425519418407353, took: 1.2461156845092773\n",
      "Validation vlb: -0.8710964200566116, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_254, vlb: -1.126049011441328, took: 1.2545251846313477\n",
      "Validation vlb: -0.9664638154329219, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_255, vlb: -1.119685870089203, took: 1.259294033050537\n",
      "Validation vlb: -1.0060271369986549, Best vlb: -0.7976614611048529\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_256, vlb: -1.0992938856620087, took: 1.2828481197357178\n",
      "Validation vlb: -0.8422707201593517, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_257, vlb: -1.1076407587523935, took: 1.3302741050720215\n",
      "Validation vlb: -0.9239061062004188, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_258, vlb: -1.094573388908422, took: 1.266303300857544\n",
      "Validation vlb: -0.8877057344396523, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_259, vlb: -1.0945241324627704, took: 1.255838394165039\n",
      "Validation vlb: -0.9053858422152818, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_260, vlb: -1.1150308988658395, took: 1.2532129287719727\n",
      "Validation vlb: -0.933823422899524, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_261, vlb: -1.120872989506184, took: 1.2419090270996094\n",
      "Validation vlb: -0.9479041103406246, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_262, vlb: -1.1024363552188976, took: 1.2505090236663818\n",
      "Validation vlb: -0.9186795587292766, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_263, vlb: -1.1292582434589673, took: 1.2847378253936768\n",
      "Validation vlb: -0.8712503514629352, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_264, vlb: -1.1187890649356986, took: 1.3712599277496338\n",
      "Validation vlb: -0.909298191178578, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_265, vlb: -1.1234535144136928, took: 1.258929967880249\n",
      "Validation vlb: -0.8799142825950697, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_266, vlb: -1.1292391296423807, took: 1.2356979846954346\n",
      "Validation vlb: -0.9323223250583538, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_267, vlb: -1.1133603559881395, took: 1.2730367183685303\n",
      "Validation vlb: -0.9622348141901701, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_268, vlb: -1.1264578338617208, took: 1.2599220275878906\n",
      "Validation vlb: -0.8654602337809443, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_269, vlb: -1.1126244389671707, took: 1.2475628852844238\n",
      "Validation vlb: -0.9419099536142689, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_270, vlb: -1.1129164116649608, took: 1.2453639507293701\n",
      "Validation vlb: -0.9231675982861072, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_271, vlb: -1.117491534471426, took: 1.3859920501708984\n",
      "Validation vlb: -0.9079040466388838, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_272, vlb: -1.1451484458370091, took: 1.2446560859680176\n",
      "Validation vlb: -0.9290536649790396, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_273, vlb: -1.1280835353087555, took: 1.2485461235046387\n",
      "Validation vlb: -0.9418128391299818, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_274, vlb: -1.115008286275922, took: 1.242643117904663\n",
      "Validation vlb: -0.9301207125379816, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_275, vlb: -1.1126009633655294, took: 1.2551140785217285\n",
      "Validation vlb: -0.924436888455573, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_276, vlb: -1.1251201484973457, took: 1.2624001502990723\n",
      "Validation vlb: -0.9919942879753977, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_277, vlb: -1.1331870698825808, took: 1.270141839981079\n",
      "Validation vlb: -0.9447917147361731, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_278, vlb: -1.1210064213611601, took: 1.2878150939941406\n",
      "Validation vlb: -0.8709411125352853, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_279, vlb: -1.081598781620722, took: 1.2358448505401611\n",
      "Validation vlb: -0.9006574038162972, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_280, vlb: -1.1041238311816344, took: 1.2683639526367188\n",
      "Validation vlb: -0.8410039248975735, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_281, vlb: -1.1184546282534382, took: 1.2533981800079346\n",
      "Validation vlb: -0.8922138559393898, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_282, vlb: -1.1121987821732144, took: 1.2527410984039307\n",
      "Validation vlb: -0.87694118420283, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_283, vlb: -1.1383212923273835, took: 1.2400569915771484\n",
      "Validation vlb: -0.9263320640452857, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_284, vlb: -1.1502597013908966, took: 1.3054699897766113\n",
      "Validation vlb: -0.8900141764227241, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_285, vlb: -1.0841471616595952, took: 1.2497918605804443\n",
      "Validation vlb: -0.9159096236367827, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_286, vlb: -1.1026368002518931, took: 1.260101079940796\n",
      "Validation vlb: -0.9675085482859689, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_287, vlb: -1.1157428291061176, took: 1.2479190826416016\n",
      "Validation vlb: -0.8696619928076044, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_288, vlb: -1.152104395497047, took: 1.2457020282745361\n",
      "Validation vlb: -0.9359289214834812, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_289, vlb: -1.1162599900836003, took: 1.2480456829071045\n",
      "Validation vlb: -1.007066871163143, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_290, vlb: -1.1199901878383034, took: 1.2608909606933594\n",
      "Validation vlb: -0.9615686842538778, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_291, vlb: -1.118451729025135, took: 1.3705551624298096\n",
      "Validation vlb: -0.9603384735129026, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_292, vlb: -1.0903440321097067, took: 1.2332851886749268\n",
      "Validation vlb: -0.9243655546197613, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_293, vlb: -1.1139741394431375, took: 1.2397480010986328\n",
      "Validation vlb: -0.9256639411148516, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_294, vlb: -1.1033631799931058, took: 1.2462069988250732\n",
      "Validation vlb: -0.8539034552558726, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_295, vlb: -1.096399055696624, took: 1.2298989295959473\n",
      "Validation vlb: -0.9616932664488511, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_296, vlb: -1.106287801227082, took: 1.3175041675567627\n",
      "Validation vlb: -0.893742972978882, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_297, vlb: -1.1038785484741178, took: 1.30674409866333\n",
      "Validation vlb: -0.9564541454839861, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_298, vlb: -1.11227620115888, took: 1.3412179946899414\n",
      "Validation vlb: -0.8955241713709045, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_299, vlb: -1.1042279795558065, took: 1.2464890480041504\n",
      "Validation vlb: -0.8895879339631707, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_300, vlb: -1.1139841163205637, took: 1.231705904006958\n",
      "Validation vlb: -0.9005814289197953, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_301, vlb: -1.1063814392654767, took: 1.253870964050293\n",
      "Validation vlb: -0.9844335606954631, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_302, vlb: -1.1146668980651335, took: 1.241830825805664\n",
      "Validation vlb: -0.9128224407970712, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_303, vlb: -1.1096183962645099, took: 1.2770280838012695\n",
      "Validation vlb: -0.9168029111180105, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_304, vlb: -1.1200144007174198, took: 1.2467012405395508\n",
      "Validation vlb: -0.8879269060579319, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_305, vlb: -1.0904756855269238, took: 1.356154203414917\n",
      "Validation vlb: -0.9214867270494356, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_306, vlb: -1.1133980919312865, took: 1.251370906829834\n",
      "Validation vlb: -0.9143470516096812, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_307, vlb: -1.1039201321686005, took: 1.2351868152618408\n",
      "Validation vlb: -0.8920901793878055, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_308, vlb: -1.1385359989467783, took: 1.2431600093841553\n",
      "Validation vlb: -0.928551623350594, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_309, vlb: -1.1560007578012195, took: 1.2384099960327148\n",
      "Validation vlb: -0.9852414667413458, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_310, vlb: -1.1211303754551702, took: 1.2975449562072754\n",
      "Validation vlb: -0.9147761159344399, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_311, vlb: -1.1214846088210584, took: 1.2533929347991943\n",
      "Validation vlb: -0.9487280369190723, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_312, vlb: -1.1262606340730452, took: 1.3550090789794922\n",
      "Validation vlb: -0.9102511386655295, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_313, vlb: -1.1068940435247452, took: 1.2566378116607666\n",
      "Validation vlb: -0.9618617886478461, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_314, vlb: -1.1207023640523766, took: 1.2594738006591797\n",
      "Validation vlb: -0.9596010373248252, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_315, vlb: -1.1473511167468895, took: 1.2647230625152588\n",
      "Validation vlb: -0.914211566394201, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_316, vlb: -1.13662484978273, took: 1.2682931423187256\n",
      "Validation vlb: -0.9491339945098729, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_317, vlb: -1.1176286296188553, took: 1.2373969554901123\n",
      "Validation vlb: -0.9660574766810273, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_318, vlb: -1.109636312514962, took: 1.242082118988037\n",
      "Validation vlb: -0.8786880705734673, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_319, vlb: -1.1281934206246214, took: 1.3557960987091064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation vlb: -0.8839590098094015, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_320, vlb: -1.0902715945870576, took: 1.2599260807037354\n",
      "Validation vlb: -0.9489115110107225, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_321, vlb: -1.1088102776067568, took: 1.2330329418182373\n",
      "Validation vlb: -0.9191771408115004, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_322, vlb: -1.1241654694230598, took: 1.2450339794158936\n",
      "Validation vlb: -0.9707088314213799, Best vlb: -0.7976614611048529\n",
      "\n",
      "Epoch_323, vlb: -1.1024656808329092, took: 1.3453679084777832\n",
      "Validation vlb: -0.9226082152533299, Best vlb: -0.7976614611048529\n",
      "\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "masker = top_masker_tensorflow(p=1)\n",
    "\n",
    "x_train, x_test, x_means, x_stds, y_train, y_test, feature_names, X_dims = \\\n",
    "    get_my_COMPAS(rseed=42, separate_test=True, test_ratio=0.1, save_dir='../data/')\n",
    "\n",
    "x_train, x_test, input_dim_vec = join_compas_targets(x_train, x_test, y_train, y_test, X_dims)\n",
    "\n",
    "print('Compas', x_train.shape, x_test.shape)\n",
    "print(input_dim_vec)\n",
    "\n",
    "\n",
    "dname = 'compas'\n",
    "print(dname)\n",
    "\n",
    "#trainset = Datafeed(x_train, x_train, transform=None)\n",
    "#valset = Datafeed(x_test, x_test, transform=None)\n",
    "\n",
    "save_dir = '../saves/fc_preact_VAEAC_NEW_' + dname\n",
    "\n",
    "width = widths[names.index(dname)] # 350\n",
    "depth = depths[names.index(dname)] # number of hidden layers # 3\n",
    "latent_dim = latent_dims[names.index(dname)] # 4\n",
    "\n",
    "batch_size = 128\n",
    "nb_epochs = 2000\n",
    "early_stop = 200\n",
    "lr = 1e-4\n",
    "\n",
    "optimizer = tfa.optimizers.RectifiedAdam(lr = lr, epsilon = 1e-8)\n",
    "\n",
    "\n",
    "model = VAEAC_gauss_cat(width, depth, latent_dim, input_dim_vec, batch_size, lr, optimizer, save_model = True)\n",
    "\n",
    "vlb_train, vlb_val, best_epoch, best_vlb, curr_epoch = train_VAEAC(model, x_train, x_test, masker, nb_epochs, early_stop=early_stop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3ac60b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f8e2b58",
   "metadata": {},
   "source": [
    "## Train VAEAC (default credit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa76c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 14:16:07.409349: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_0, vlb: -13.534921236673991, took: 20.936994075775146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 14:16:32.162300: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation vlb: -8.918633893330892, Best vlb: -8.918633893330892\n",
      "\n",
      "Epoch_1, vlb: -9.405318282939769, took: 16.20621609687805\n",
      "Validation vlb: -7.606949746449788, Best vlb: -7.606949746449788\n",
      "\n",
      "Epoch_2, vlb: -9.12646210889463, took: 13.30259919166565\n",
      "Validation vlb: -6.763457542419434, Best vlb: -6.763457542419434\n",
      "\n",
      "Epoch_3, vlb: -8.916708846480757, took: 13.35439682006836\n",
      "Validation vlb: -6.410967491149902, Best vlb: -6.410967491149902\n",
      "\n",
      "Epoch_4, vlb: -8.506200861612955, took: 14.877076864242554\n",
      "Validation vlb: -6.665459166208903, Best vlb: -6.410967491149902\n",
      "\n",
      "Epoch_5, vlb: -8.752956290774875, took: 15.877367973327637\n",
      "Validation vlb: -5.999795426686605, Best vlb: -5.999795426686605\n",
      "\n",
      "Epoch_6, vlb: -8.065714134781448, took: 17.56549906730652\n",
      "Validation vlb: -6.41845418548584, Best vlb: -5.999795426686605\n",
      "\n",
      "Epoch_7, vlb: -8.424937565697563, took: 13.260844230651855\n",
      "Validation vlb: -5.907668764750163, Best vlb: -5.907668764750163\n",
      "\n",
      "Epoch_8, vlb: -8.198094186712195, took: 14.929549932479858\n",
      "Validation vlb: -6.015542894999186, Best vlb: -5.907668764750163\n",
      "\n",
      "Epoch_9, vlb: -7.974819850921631, took: 16.701974868774414\n",
      "Validation vlb: -5.744850457509359, Best vlb: -5.744850457509359\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_3707/860040963.py\", line 25, in <module>\n",
      "    vlb_train, vlb_val = train_VAEAC(model, x_train, x_test, masker, nb_epochs, early_stop=early_stop)\n",
      "  File \"/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_3707/2537167201.py\", line 40, in train_VAEAC\n",
      "    mask_flat = gauss_cat_to_flat_mask(mask, model.input_dim_vec)\n",
      "  File \"/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_3707/2386555261.py\", line 9, in gauss_cat_to_flat_mask\n",
      "    output.append(tf.expand_dims(mask[:, idx], axis=1))\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 1096, in op_dispatch_handler\n",
      "    return dispatch_target(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\", line 1032, in _slice_helper\n",
      "    packed_strides.dtype == dtypes.int64):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1123, in dtype\n",
      "    return dtypes._INTERN_TABLE[self._datatype_enum()]  # pylint: disable=protected-access\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 1515, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 1473, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/posixpath.py\", line 391, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/posixpath.py\", line 425, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/posixpath.py\", line 167, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_3707/860040963.py\", line 25, in <module>\n",
      "    vlb_train, vlb_val = train_VAEAC(model, x_train, x_test, masker, nb_epochs, early_stop=early_stop)\n",
      "  File \"/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_3707/2537167201.py\", line 40, in train_VAEAC\n",
      "    mask_flat = gauss_cat_to_flat_mask(mask, model.input_dim_vec)\n",
      "  File \"/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_3707/2386555261.py\", line 9, in gauss_cat_to_flat_mask\n",
      "    output.append(tf.expand_dims(mask[:, idx], axis=1))\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 1096, in op_dispatch_handler\n",
      "    return dispatch_target(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\", line 1032, in _slice_helper\n",
      "    packed_strides.dtype == dtypes.int64):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1123, in dtype\n",
      "    return dtypes._INTERN_TABLE[self._datatype_enum()]  # pylint: disable=protected-access\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3364, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3461, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2066, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 1515, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 1473, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 751, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 720, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 705, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_3707/860040963.py\", line 25, in <module>\n",
      "    vlb_train, vlb_val = train_VAEAC(model, x_train, x_test, masker, nb_epochs, early_stop=early_stop)\n",
      "  File \"/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_3707/2537167201.py\", line 40, in train_VAEAC\n",
      "    mask_flat = gauss_cat_to_flat_mask(mask, model.input_dim_vec)\n",
      "  File \"/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_3707/2386555261.py\", line 9, in gauss_cat_to_flat_mask\n",
      "    output.append(tf.expand_dims(mask[:, idx], axis=1))\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 1096, in op_dispatch_handler\n",
      "    return dispatch_target(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\", line 1032, in _slice_helper\n",
      "    packed_strides.dtype == dtypes.int64):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1123, in dtype\n",
      "    return dtypes._INTERN_TABLE[self._datatype_enum()]  # pylint: disable=protected-access\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3364, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3461, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2066, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2947, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3172, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3383, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2066, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n",
      "    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 1515, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 1473, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/posixpath.py\", line 391, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/posixpath.py\", line 425, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/posixpath.py\", line 167, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "nb_epochs = 2000 # 2000\n",
    "early_stop = 200\n",
    "lr = 7e-4        # Maybe this should be 1e-4, but it makes the performance terrible...\n",
    "\n",
    "optimizer = tfa.optimizers.RectifiedAdam(lr = lr, epsilon = 1e-8)\n",
    "\n",
    "# set the dimensionality of the latent space to a plane for visualization later\n",
    "\n",
    "width = 350\n",
    "depth = 3\n",
    "latent_dim = 8\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "masker = top_masker_tensorflow(p=1)\n",
    "\n",
    "x_train, x_test, x_means, x_stds = \\\n",
    "load_UCI(dset_name=dname, splits=10, seed=42, separate_targets=False, save_dir='../data/') # np.arrays\n",
    "\n",
    "model = VAEAC_gauss_cat(width, depth, latent_dim, input_dim_vec, batch_size, lr, optimizer, save_model = True)\n",
    "\n",
    "x_train = unnormalise_cat_vars(x_train, x_means, x_stds, input_dim_vec) # np.array\n",
    "x_test = unnormalise_cat_vars(x_test, x_means, x_stds, input_dim_vec) \n",
    "\n",
    "vlb_train, vlb_val, best_epoch, best_vlb, curr_epoch = train_VAEAC(model, x_train, x_test, masker, nb_epochs, early_stop=early_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6e5738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdec9a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e7b4b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.clip(vlb_train[:curr_epoch], -1000, 1000), 'r')\n",
    "plt.plot(np.clip(vlb_val[:curr_epoch], -1000, 1000), 'b')\n",
    "plt.legend(['cost_train', 'cost_dev'])\n",
    "plt.ylabel('vlb')\n",
    "plt.xlabel('it')\n",
    "plt.grid(True)\n",
    "#plt.savefig( str(dname) + '_vlb_lr_' + str(model.lr) + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad555fe9",
   "metadata": {},
   "source": [
    "## Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9b8ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e427ab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the total number of trainable variables \n",
    "\"\"\"\n",
    "total_parameters = 0\n",
    "for variable in model.trainable_variables:\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    #print(shape)\n",
    "    #print(len(shape))\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        #print(dim)\n",
    "        variable_parameters *= dim\n",
    "    #print(variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)\n",
    "\n",
    "time.sleep(10)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b0e8fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469f0d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20896d98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae708749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b7c650e",
   "metadata": {},
   "source": [
    "## Generate sample parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e71b276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55cd3a92",
   "metadata": {},
   "source": [
    "## UNDER VAEAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b107d2c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af920dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class under_VAEAC(tf.keras.Model):\n",
    "    def __init__(self, base_VAE, width, depth, latent_dim, batch_size, lr, optimizer, save_model = True):\n",
    "        super(under_VAEAC, self).__init__()\n",
    "        \n",
    "        self.base_VAEAC = base_VAE\n",
    "        self.input_dim = self.base_VAEAC.latent_dim # 8 for default credit\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.latent_dim = latent_dim # 6 for default credit\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.prior_encoder = tfd.Normal(loc=tf.zeros(latent_dim), scale=tf.ones(latent_dim))\n",
    "        \n",
    "        # self.input_dim is put in a list to make sum(input_dim_vec in recognition_encoder work)\n",
    "        self.recognition_encoder = create_recognition_encoder(width, depth, latent_dim, [self.input_dim])\n",
    "        self.decoder = create_decoder(width, depth, latent_dim, [self.input_dim])\n",
    "        \n",
    "        self.vlb_scale = 1 / self.input_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer\n",
    "        self.save_model = save_model\n",
    "\n",
    "    # Inspiration taken from \n",
    "    # https://github.com/joocxi/tf2-VAEAC/blob/d2b1bbc258ec77ee0975ea7eb68e63c4efcda6f0/model/vaeac.py\n",
    "\n",
    "    def reconstruction_loss(self, x, y):\n",
    "        log_prob_vec = []\n",
    "        reshape_dim = self.batch_size\n",
    "        for idx in range(self.input_dim):\n",
    "            # Gaussian_case\n",
    "            log_prob_vec.append(tf.expand_dims(-(x[:, idx] - y[:, idx])**2, 1))\n",
    "\n",
    "        log_prob_vec = tf.reshape(log_prob_vec, [reshape_dim, self.input_dim])\n",
    "        log_prob_vec = tf.math.reduce_sum(log_prob_vec, axis= -1)\n",
    "        \n",
    "        return log_prob_vec\n",
    "    \n",
    "def compute_loss_under_VAEAC(model, x_flat, proposal_params_VAEAC):\n",
    "    \n",
    "    # Essentially CLUEs normal_parse_params\n",
    "    proposal_distribution_VAEAC = tfd.Normal(\n",
    "      loc=proposal_params_VAEAC[..., :model.input_dim],\n",
    "      scale=tf.clip_by_value(\n",
    "        tf.nn.softplus(proposal_params_VAEAC[..., model.input_dim:]),\n",
    "        1e-3,\n",
    "        tf.float32.max),\n",
    "      name=\"proposal\")\n",
    "    \n",
    "    z_sample = proposal_distribution_VAEAC.sample() # tensor with dim (base_VAEAC.latent_dim,)\n",
    "    \n",
    "    proposal_params_VAE = model.recognition_encoder(z_sample) \n",
    "\n",
    "    proposal_distribution_VAE = tfd.Normal(\n",
    "      loc=proposal_params_VAE[..., :model.latent_dim],\n",
    "      scale=tf.clip_by_value(\n",
    "        tf.nn.softplus(proposal_params_VAE[..., model.latent_dim:]),\n",
    "        1e-3,\n",
    "        tf.float32.max),\n",
    "      name=\"priors\")\n",
    "\n",
    "    u_sample = proposal_distribution_VAE.sample() \n",
    "    \n",
    "    rec_params = model.decoder(u_sample) \n",
    "    \n",
    "    # True distribution, Estimated distribution in this order\n",
    "    kl_divergence = tf.reduce_sum(\n",
    "      tf.reshape(\n",
    "        tfd.kl_divergence(proposal_distribution_VAE, model.prior_encoder),\n",
    "        (model.batch_size, -1)), -1)\n",
    "    \n",
    "    rec_loss = model.reconstruction_loss(rec_params, z_sample)\n",
    "    \n",
    "    vlb = tf.reduce_mean(-kl_divergence + rec_loss) # For comparing\n",
    "    loss = tf.reduce_mean((kl_divergence - rec_loss) * model.vlb_scale) \n",
    "    return loss, vlb, kl_divergence, rec_loss\n",
    "\n",
    "@tf.function # Converts all numpy arrays to tensors\n",
    "def train_step_under_VAEAC(model, x_flat, proposal_params_VAEAC):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, vlb, kl_divergence, rec_loss = compute_loss_under_VAEAC(model, x_flat, proposal_params_VAEAC)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss, vlb, kl_divergence, rec_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e843dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "def train_VAE(model, x_train, x_test, nb_epochs, early_stop = None):\n",
    "    \n",
    "    vlb_train = np.zeros(nb_epochs)\n",
    "    vlb_val = np.zeros(nb_epochs)\n",
    "    best_vlb = -np.inf\n",
    "    best_epoch = 0\n",
    "    \n",
    "    overall_batch_size = model.batch_size\n",
    "    \n",
    "    test_data = []\n",
    "    for x in batch(x_test, n = overall_batch_size):\n",
    "        test_data.append(x)\n",
    "    \n",
    "    epoch = 0\n",
    "    for epoch in range(0, nb_epochs):\n",
    "        tic = time.time()\n",
    "        \n",
    "        train_data = []\n",
    "        np.random.shuffle(x_train)\n",
    "        for x in batch(x_train, n = overall_batch_size):\n",
    "            train_data.append(x)\n",
    "        \n",
    "        ## Training\n",
    "        nb_samples = 0\n",
    "        for x_batch in train_data:\n",
    "\n",
    "            model.batch_size = x_batch.shape[0] # TODO: FIX THIS: Very ugly solution now to make sure batches \n",
    "                                                # that do not have the full size\n",
    "            \n",
    "            # Flatten the batch\n",
    "            x_batch_flat = gauss_cat_to_flat(x_batch, model.base_VAEAC.input_dim_vec) # numpy\n",
    "            \n",
    "            #\"\"\"\n",
    "            x_batch_flat = tf.convert_to_tensor(x_batch_flat)\n",
    "    \n",
    "            proposal_params_VAEAC = model.base_VAEAC.recognition_encoder(x_batch_flat) # tensor with dim (16,)\n",
    "            #\"\"\"\n",
    "            \n",
    "            loss, vlb, kl_divergence, rec_loss = train_step_under_VAEAC(model, x_batch_flat, proposal_params_VAEAC)\n",
    "\n",
    "            vlb_train[epoch] += vlb.numpy() * x_batch.shape[0]\n",
    "            nb_samples += x_batch.shape[0]\n",
    "\n",
    "        vlb_train[epoch] /= nb_samples\n",
    "        toc = time.time()\n",
    "        print(\"Epoch\" + str(epoch) + \", vlb: \" + str(vlb_train[epoch]) + \", took: \" + str(toc-tic))\n",
    "        \n",
    "        ## Validation\n",
    "        nb_samples = 0\n",
    "        for x_batch in test_data:\n",
    "            model.batch_size = x_batch.shape[0] # TODO: FIX THIS: Very ugly solution now to make sure batches \n",
    "                                                # that do not have the full size\n",
    "            \n",
    "            # Flatten the batch\n",
    "            x_batch_flat = gauss_cat_to_flat(x_batch, model.base_VAEAC.input_dim_vec) # numpy\n",
    "            \n",
    "            #\"\"\"\n",
    "            x_batch_flat = tf.convert_to_tensor(x_batch_flat)\n",
    "    \n",
    "            proposal_params_VAEAC = model.base_VAEAC.recognition_encoder(x_batch_flat) # tensor with dim (16,)\n",
    "            #\"\"\"\n",
    "            \n",
    "            # In CLUE there is actually no difference between eval and fitother than that we should not update the weights.\n",
    "            # Therefore ok to just call compute_loss_under_VAEAC directly instead of a special eval func\n",
    "            loss, vlb, kl_divergence, rec_loss = compute_loss_under_VAEAC(model, x_batch_flat, proposal_params_VAEAC)\n",
    "\n",
    "            vlb_val[epoch] += vlb.numpy() * x_batch.shape[0]\n",
    "            nb_samples += x_batch.shape[0]\n",
    "\n",
    "        vlb_val[epoch] /= nb_samples\n",
    "        \n",
    "        if vlb_val[epoch] > best_vlb:\n",
    "            best_vlb = vlb_val[epoch]\n",
    "            best_epoch = epoch\n",
    "            if(model.save_model):\n",
    "                #open text file\n",
    "                text_file = open(\"./COMPAS_under_VAEAC/\" + str(dname) + \"_best_epoch_under_VAEAC_lr_\" + str(model.lr) + \".txt\", \"w\")\n",
    "\n",
    "                #write string to file\n",
    "                text_file.write(str(epoch))\n",
    "\n",
    "                #close file\n",
    "                text_file.close()\n",
    "\n",
    "                model.recognition_encoder.save(\"./COMPAS_under_VAEAC/\" + str(dname) + \"_under_recog_encoder_lr_\" + str(model.lr))\n",
    "                model.decoder.save(\"./COMPAS_under_VAEAC/\" + str(dname) + \"_under_decoder_lr_\" + str(model.lr))\n",
    "\n",
    "        print(\"Validation vlb: \" + str(vlb_val[epoch]) + \", Best vlb: \" + str(best_vlb) + \"\\n\")\n",
    "\n",
    "        if early_stop is not None and (epoch - best_epoch) > early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    \n",
    "    np.save(\"./COMPAS_under_VAEAC/\" + str(dname) + \"_under_vlb_train_lr_\" + str(model.lr), vlb_train)\n",
    "    np.save(\"./COMPAS_under_VAEAC/\" + str(dname) + \"_under_vlb_val_lr_\" + str(model.lr), vlb_val)\n",
    "    return vlb_train, vlb_val, best_epoch, best_vlb, epoch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbf0a1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compas\n",
      "Compas (5554, 19) (618, 19)\n",
      "[3 6 2 2 2 1 1 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow_addons/optimizers/rectified_adam.py:120: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "2021-12-09 08:14:35.983787: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-09 08:14:39.583222: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0, vlb: -11.87457807449544, took: 5.396807909011841\n",
      "Validation vlb: -11.647859098070262, Best vlb: -11.647859098070262\n",
      "\n",
      "Epoch1, vlb: -11.46009496462873, took: 0.7050871849060059\n",
      "Validation vlb: -11.280903269943682, Best vlb: -11.280903269943682\n",
      "\n",
      "Epoch2, vlb: -10.796587539483284, took: 0.6516151428222656\n",
      "Validation vlb: -10.587609889823643, Best vlb: -10.587609889823643\n",
      "\n",
      "Epoch3, vlb: -10.329865588873979, took: 0.8168058395385742\n",
      "Validation vlb: -10.162653509467166, Best vlb: -10.162653509467166\n",
      "\n",
      "Epoch4, vlb: -9.68864701675948, took: 0.7665059566497803\n",
      "Validation vlb: -9.435937813570584, Best vlb: -9.435937813570584\n",
      "\n",
      "Epoch5, vlb: -9.103045261975112, took: 0.9943850040435791\n",
      "Validation vlb: -8.907617038121886, Best vlb: -8.907617038121886\n",
      "\n",
      "Epoch6, vlb: -8.547457183656565, took: 0.8868169784545898\n",
      "Validation vlb: -8.651977341537723, Best vlb: -8.651977341537723\n",
      "\n",
      "Epoch7, vlb: -7.828872829192742, took: 0.7899839878082275\n",
      "Validation vlb: -7.683699603219634, Best vlb: -7.683699603219634\n",
      "\n",
      "Epoch8, vlb: -7.30775254932633, took: 0.7532758712768555\n",
      "Validation vlb: -7.040338392782366, Best vlb: -7.040338392782366\n",
      "\n",
      "Epoch9, vlb: -6.469925056741871, took: 0.7620596885681152\n",
      "Validation vlb: -6.2859091326642575, Best vlb: -6.2859091326642575\n",
      "\n",
      "Epoch10, vlb: -5.88724210496682, took: 0.7563791275024414\n",
      "Validation vlb: -5.5259800975762525, Best vlb: -5.5259800975762525\n",
      "\n",
      "Epoch11, vlb: -5.46353882579096, took: 0.7133939266204834\n",
      "Validation vlb: -5.359928195916333, Best vlb: -5.359928195916333\n",
      "\n",
      "Epoch12, vlb: -5.207221478835516, took: 0.8115053176879883\n",
      "Validation vlb: -5.056000166340553, Best vlb: -5.056000166340553\n",
      "\n",
      "Epoch13, vlb: -4.9915754959057, took: 0.7817783355712891\n",
      "Validation vlb: -4.8056574515926025, Best vlb: -4.8056574515926025\n",
      "\n",
      "Epoch14, vlb: -4.877062432492084, took: 0.753270149230957\n",
      "Validation vlb: -4.907146341206572, Best vlb: -4.8056574515926025\n",
      "\n",
      "Epoch15, vlb: -4.782282115488461, took: 0.7785830497741699\n",
      "Validation vlb: -4.710175472555808, Best vlb: -4.710175472555808\n",
      "\n",
      "Epoch16, vlb: -4.715159651440684, took: 0.769517183303833\n",
      "Validation vlb: -4.600961182109747, Best vlb: -4.600961182109747\n",
      "\n",
      "Epoch17, vlb: -4.67563009571162, took: 0.7690460681915283\n",
      "Validation vlb: -4.669738996376112, Best vlb: -4.600961182109747\n",
      "\n",
      "Epoch18, vlb: -4.601551443284735, took: 0.7175302505493164\n",
      "Validation vlb: -4.637865373617623, Best vlb: -4.600961182109747\n",
      "\n",
      "Epoch19, vlb: -4.624052945083107, took: 0.802454948425293\n",
      "Validation vlb: -4.604756333681372, Best vlb: -4.600961182109747\n",
      "\n",
      "Epoch20, vlb: -4.561334102240491, took: 0.8223588466644287\n",
      "Validation vlb: -4.450104576098495, Best vlb: -4.450104576098495\n",
      "\n",
      "Epoch21, vlb: -4.564784923645503, took: 0.7042081356048584\n",
      "Validation vlb: -4.517324466149784, Best vlb: -4.450104576098495\n",
      "\n",
      "Epoch22, vlb: -4.548756373628335, took: 0.7071590423583984\n",
      "Validation vlb: -4.523070022126232, Best vlb: -4.450104576098495\n",
      "\n",
      "Epoch23, vlb: -4.5807599063223545, took: 0.7242231369018555\n",
      "Validation vlb: -4.53021394241975, Best vlb: -4.450104576098495\n",
      "\n",
      "Epoch24, vlb: -4.53762719765987, took: 0.7802698612213135\n",
      "Validation vlb: -4.455667136170717, Best vlb: -4.450104576098495\n",
      "\n",
      "Epoch25, vlb: -4.518852985459006, took: 0.6949522495269775\n",
      "Validation vlb: -4.579377927440656, Best vlb: -4.450104576098495\n",
      "\n",
      "Epoch26, vlb: -4.480878880782053, took: 0.70967698097229\n",
      "Validation vlb: -4.539816916567608, Best vlb: -4.450104576098495\n",
      "\n",
      "Epoch27, vlb: -4.483726316190341, took: 0.728600025177002\n",
      "Validation vlb: -4.489614749031931, Best vlb: -4.450104576098495\n",
      "\n",
      "Epoch28, vlb: -4.49643701179705, took: 0.7151830196380615\n",
      "Validation vlb: -4.480212398331528, Best vlb: -4.450104576098495\n",
      "\n",
      "Epoch29, vlb: -4.535462322619737, took: 0.714608907699585\n",
      "Validation vlb: -4.416255063609398, Best vlb: -4.416255063609398\n",
      "\n",
      "Epoch30, vlb: -4.501750535831032, took: 0.7062170505523682\n",
      "Validation vlb: -4.359211788979935, Best vlb: -4.359211788979935\n",
      "\n",
      "Epoch31, vlb: -4.440427655833721, took: 0.8802471160888672\n",
      "Validation vlb: -4.487477652849117, Best vlb: -4.359211788979935\n",
      "\n",
      "Epoch32, vlb: -4.505860086220565, took: 0.8090510368347168\n",
      "Validation vlb: -4.492411236932749, Best vlb: -4.359211788979935\n",
      "\n",
      "Epoch33, vlb: -4.452592518160486, took: 0.8412861824035645\n",
      "Validation vlb: -4.413972363888639, Best vlb: -4.359211788979935\n",
      "\n",
      "Epoch34, vlb: -4.4851080051720205, took: 0.76407790184021\n",
      "Validation vlb: -4.518869162377416, Best vlb: -4.359211788979935\n",
      "\n",
      "Epoch35, vlb: -4.481477153614609, took: 0.7408120632171631\n",
      "Validation vlb: -4.57843886687146, Best vlb: -4.359211788979935\n",
      "\n",
      "Epoch36, vlb: -4.455358861546144, took: 0.6997511386871338\n",
      "Validation vlb: -4.4422571574214205, Best vlb: -4.359211788979935\n",
      "\n",
      "Epoch37, vlb: -4.435338232557594, took: 0.7593197822570801\n",
      "Validation vlb: -4.4312424767750365, Best vlb: -4.359211788979935\n",
      "\n",
      "Epoch38, vlb: -4.437952082149408, took: 0.7170968055725098\n",
      "Validation vlb: -4.493479819745308, Best vlb: -4.359211788979935\n",
      "\n",
      "Epoch39, vlb: -4.464075495452386, took: 0.6993677616119385\n",
      "Validation vlb: -4.425670850624159, Best vlb: -4.359211788979935\n",
      "\n",
      "Epoch40, vlb: -4.459320012038673, took: 0.6877090930938721\n",
      "Validation vlb: -4.4104727834559565, Best vlb: -4.359211788979935\n",
      "\n",
      "Epoch41, vlb: -4.433346837812677, took: 0.9146950244903564\n",
      "Validation vlb: -4.407452331777529, Best vlb: -4.359211788979935\n",
      "\n",
      "Epoch42, vlb: -4.4072710170477984, took: 0.7171337604522705\n",
      "Validation vlb: -4.526640203778412, Best vlb: -4.359211788979935\n",
      "\n",
      "Epoch43, vlb: -4.436312252212266, took: 0.677649974822998\n",
      "Validation vlb: -4.5028029630099295, Best vlb: -4.359211788979935\n",
      "\n",
      "Epoch44, vlb: -4.411076069908431, took: 0.6969320774078369\n",
      "Validation vlb: -4.4291689172146, Best vlb: -4.359211788979935\n",
      "\n",
      "Epoch45, vlb: -4.423537362167035, took: 0.7312159538269043\n",
      "Validation vlb: -4.544985748031764, Best vlb: -4.359211788979935\n",
      "\n",
      "Epoch46, vlb: -4.405971549593548, took: 0.6951420307159424\n",
      "Validation vlb: -4.318450410774997, Best vlb: -4.318450410774997\n",
      "\n",
      "Epoch47, vlb: -4.397591649777691, took: 0.7324919700622559\n",
      "Validation vlb: -4.449242145883998, Best vlb: -4.318450410774997\n",
      "\n",
      "Epoch48, vlb: -4.463446679972126, took: 0.7209010124206543\n",
      "Validation vlb: -4.353676951818867, Best vlb: -4.318450410774997\n",
      "\n",
      "Epoch49, vlb: -4.424281576360607, took: 0.8303732872009277\n",
      "Validation vlb: -4.49752979216838, Best vlb: -4.318450410774997\n",
      "\n",
      "Epoch50, vlb: -4.401304459288518, took: 0.8689169883728027\n",
      "Validation vlb: -4.382928232544834, Best vlb: -4.318450410774997\n",
      "\n",
      "Epoch51, vlb: -4.4738978436407875, took: 0.9637339115142822\n",
      "Validation vlb: -4.584545925597157, Best vlb: -4.318450410774997\n",
      "\n",
      "Epoch52, vlb: -4.425999471254387, took: 0.6742880344390869\n",
      "Validation vlb: -4.381575368368896, Best vlb: -4.318450410774997\n",
      "\n",
      "Epoch53, vlb: -4.420584079555612, took: 0.657783031463623\n",
      "Validation vlb: -4.538450239545705, Best vlb: -4.318450410774997\n",
      "\n",
      "Epoch54, vlb: -4.420710701885779, took: 0.6527419090270996\n",
      "Validation vlb: -4.349611583265286, Best vlb: -4.318450410774997\n",
      "\n",
      "Epoch55, vlb: -4.411947778372787, took: 0.6562259197235107\n",
      "Validation vlb: -4.540837044083185, Best vlb: -4.318450410774997\n",
      "\n",
      "Epoch56, vlb: -4.419735418792933, took: 0.6782879829406738\n",
      "Validation vlb: -4.4308711008732375, Best vlb: -4.318450410774997\n",
      "\n",
      "Epoch57, vlb: -4.417625078675899, took: 0.6495509147644043\n",
      "Validation vlb: -4.357657108492064, Best vlb: -4.318450410774997\n",
      "\n",
      "Epoch58, vlb: -4.39789928289296, took: 0.6678247451782227\n",
      "Validation vlb: -4.421682607780382, Best vlb: -4.318450410774997\n",
      "\n",
      "Epoch59, vlb: -4.393668439691342, took: 0.6598188877105713\n",
      "Validation vlb: -4.530678738282337, Best vlb: -4.318450410774997\n",
      "\n",
      "Epoch60, vlb: -4.396614455386894, took: 0.6757659912109375\n",
      "Validation vlb: -4.368700795960658, Best vlb: -4.318450410774997\n",
      "\n",
      "Epoch61, vlb: -4.436117422250178, took: 0.7038998603820801\n",
      "Validation vlb: -4.342455979689811, Best vlb: -4.318450410774997\n",
      "\n",
      "Epoch62, vlb: -4.405556477185072, took: 0.650367021560669\n",
      "Validation vlb: -4.434667323399516, Best vlb: -4.318450410774997\n",
      "\n",
      "Epoch63, vlb: -4.374168258628529, took: 0.6553020477294922\n",
      "Validation vlb: -4.258453893815815, Best vlb: -4.258453893815815\n",
      "\n",
      "Epoch64, vlb: -4.433244389769925, took: 0.6665201187133789\n",
      "Validation vlb: -4.340784244166994, Best vlb: -4.258453893815815\n",
      "\n",
      "Epoch65, vlb: -4.390571850428789, took: 0.6572191715240479\n",
      "Validation vlb: -4.561984940254186, Best vlb: -4.258453893815815\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch66, vlb: -4.4116613836566625, took: 0.6807680130004883\n",
      "Validation vlb: -4.4113454618114485, Best vlb: -4.258453893815815\n",
      "\n",
      "Epoch67, vlb: -4.366206982516109, took: 0.6651320457458496\n",
      "Validation vlb: -4.393482131093837, Best vlb: -4.258453893815815\n",
      "\n",
      "Epoch68, vlb: -4.445198648560592, took: 0.6770241260528564\n",
      "Validation vlb: -4.2117198107697815, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch69, vlb: -4.375985882374121, took: 0.6772909164428711\n",
      "Validation vlb: -4.417117487651245, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch70, vlb: -4.398749639286512, took: 0.6631197929382324\n",
      "Validation vlb: -4.372707519716429, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch71, vlb: -4.3935899787727, took: 0.6481359004974365\n",
      "Validation vlb: -4.402967252391828, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch72, vlb: -4.414402947307363, took: 0.7378270626068115\n",
      "Validation vlb: -4.379942097710174, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch73, vlb: -4.399346081524562, took: 0.6562709808349609\n",
      "Validation vlb: -4.332456701395967, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch74, vlb: -4.388704375478918, took: 0.6679971218109131\n",
      "Validation vlb: -4.441460484439887, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch75, vlb: -4.399290855986032, took: 0.6983499526977539\n",
      "Validation vlb: -4.418146386501473, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch76, vlb: -4.391026822963463, took: 0.6699550151824951\n",
      "Validation vlb: -4.369953320635947, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch77, vlb: -4.397826228761656, took: 0.6794350147247314\n",
      "Validation vlb: -4.354628069115303, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch78, vlb: -4.374468914158361, took: 0.6606171131134033\n",
      "Validation vlb: -4.452041286480851, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch79, vlb: -4.338536957592083, took: 0.6604740619659424\n",
      "Validation vlb: -4.497940157609464, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch80, vlb: -4.3675414536297215, took: 0.6718080043792725\n",
      "Validation vlb: -4.388855125526009, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch81, vlb: -4.38643799954539, took: 0.6742680072784424\n",
      "Validation vlb: -4.310388395315621, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch82, vlb: -4.387531327565969, took: 0.6626889705657959\n",
      "Validation vlb: -4.363395406976101, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch83, vlb: -4.35433257377358, took: 0.6732699871063232\n",
      "Validation vlb: -4.449480547488315, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch84, vlb: -4.398714156557769, took: 0.661837100982666\n",
      "Validation vlb: -4.366109824875026, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch85, vlb: -4.407790108468836, took: 0.6583409309387207\n",
      "Validation vlb: -4.3945664186693705, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch86, vlb: -4.369627696556885, took: 0.6558980941772461\n",
      "Validation vlb: -4.343899142009155, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch87, vlb: -4.365741938706735, took: 0.6603939533233643\n",
      "Validation vlb: -4.276997918064154, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch88, vlb: -4.360043803006572, took: 0.6575307846069336\n",
      "Validation vlb: -4.381862168173188, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch89, vlb: -4.366336414871518, took: 0.6601672172546387\n",
      "Validation vlb: -4.402632676281975, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch90, vlb: -4.360156232519977, took: 0.6620192527770996\n",
      "Validation vlb: -4.378131142711948, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch91, vlb: -4.346960458044616, took: 0.6756460666656494\n",
      "Validation vlb: -4.261786135269214, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch92, vlb: -4.411871437722502, took: 0.662912130355835\n",
      "Validation vlb: -4.37589007905386, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch93, vlb: -4.354358538818909, took: 0.7076270580291748\n",
      "Validation vlb: -4.276605223374845, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch94, vlb: -4.363468297574432, took: 0.6536939144134521\n",
      "Validation vlb: -4.355812032631686, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch95, vlb: -4.329559531364675, took: 0.6540379524230957\n",
      "Validation vlb: -4.44920992619783, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch96, vlb: -4.405311090807971, took: 0.6635568141937256\n",
      "Validation vlb: -4.346391092997925, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch97, vlb: -4.348652756724977, took: 0.6562409400939941\n",
      "Validation vlb: -4.415414996903305, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch98, vlb: -4.374046920828567, took: 0.659743070602417\n",
      "Validation vlb: -4.395604119717496, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch99, vlb: -4.365463547711755, took: 0.6951351165771484\n",
      "Validation vlb: -4.354800406397353, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch100, vlb: -4.343500318997705, took: 0.6686980724334717\n",
      "Validation vlb: -4.406097685249106, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch101, vlb: -4.346324129588835, took: 0.6779778003692627\n",
      "Validation vlb: -4.304341164993237, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch102, vlb: -4.395229432494425, took: 0.6741881370544434\n",
      "Validation vlb: -4.331013094645874, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch103, vlb: -4.4011371761592635, took: 0.6626911163330078\n",
      "Validation vlb: -4.366988384222136, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch104, vlb: -4.3487469697539485, took: 0.6690981388092041\n",
      "Validation vlb: -4.447342446706827, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch105, vlb: -4.406209370623744, took: 0.6588809490203857\n",
      "Validation vlb: -4.354017791624594, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch106, vlb: -4.359991171225224, took: 1.4732379913330078\n",
      "Validation vlb: -4.37926495190963, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch107, vlb: -4.378488124425694, took: 1.3648731708526611\n",
      "Validation vlb: -4.4056196320789915, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch108, vlb: -4.305872234973626, took: 0.6710062026977539\n",
      "Validation vlb: -4.30620967377351, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch109, vlb: -4.371874311136006, took: 0.6836879253387451\n",
      "Validation vlb: -4.479844696699223, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch110, vlb: -4.329124894380827, took: 0.673792839050293\n",
      "Validation vlb: -4.373480573055428, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch111, vlb: -4.387792229094349, took: 0.6769819259643555\n",
      "Validation vlb: -4.270343265101362, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch112, vlb: -4.379773005161691, took: 0.6784970760345459\n",
      "Validation vlb: -4.367201538147664, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch113, vlb: -4.342891289598014, took: 0.6508331298828125\n",
      "Validation vlb: -4.381126098262453, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch114, vlb: -4.389767310754146, took: 0.6581010818481445\n",
      "Validation vlb: -4.329925353473058, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch115, vlb: -4.38141795204061, took: 0.6651408672332764\n",
      "Validation vlb: -4.3908179554738656, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch116, vlb: -4.377164800861649, took: 0.660876989364624\n",
      "Validation vlb: -4.274738770086788, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch117, vlb: -4.36838548699428, took: 0.847498893737793\n",
      "Validation vlb: -4.295499783117794, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch118, vlb: -4.356412205199866, took: 0.676706075668335\n",
      "Validation vlb: -4.271518941836064, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch119, vlb: -4.427324916117046, took: 0.692706823348999\n",
      "Validation vlb: -4.419309819786294, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch120, vlb: -4.337907640205084, took: 0.7246153354644775\n",
      "Validation vlb: -4.303235504619512, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch121, vlb: -4.330500765331022, took: 0.6998636722564697\n",
      "Validation vlb: -4.326603196585449, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch122, vlb: -4.362124132603504, took: 0.7019009590148926\n",
      "Validation vlb: -4.262944739999123, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch123, vlb: -4.336437079045341, took: 0.725275993347168\n",
      "Validation vlb: -4.281071406737886, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch124, vlb: -4.3439171387216025, took: 0.8403360843658447\n",
      "Validation vlb: -4.307898039956695, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch125, vlb: -4.379545818441499, took: 0.6943061351776123\n",
      "Validation vlb: -4.3918828269810355, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch126, vlb: -4.371127720826987, took: 0.6852121353149414\n",
      "Validation vlb: -4.317975217084669, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch127, vlb: -4.33336312117489, took: 0.6781067848205566\n",
      "Validation vlb: -4.41613040541368, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch128, vlb: -4.3548672881794355, took: 0.6795499324798584\n",
      "Validation vlb: -4.336043939621318, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch129, vlb: -4.368345894056299, took: 0.7704651355743408\n",
      "Validation vlb: -4.269727740858751, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch130, vlb: -4.316861133760885, took: 0.6511459350585938\n",
      "Validation vlb: -4.272051852883645, Best vlb: -4.2117198107697815\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch131, vlb: -4.3527545192321515, took: 0.6603426933288574\n",
      "Validation vlb: -4.284350361253066, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch132, vlb: -4.322032071636613, took: 0.671781063079834\n",
      "Validation vlb: -4.307039959916791, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch133, vlb: -4.331656520728117, took: 0.653810977935791\n",
      "Validation vlb: -4.347257143471233, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch134, vlb: -4.326041698112392, took: 0.6653330326080322\n",
      "Validation vlb: -4.436149364150458, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch135, vlb: -4.371186492680902, took: 0.6680779457092285\n",
      "Validation vlb: -4.397863341766653, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch136, vlb: -4.343921754100059, took: 0.6617050170898438\n",
      "Validation vlb: -4.41014367477022, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch137, vlb: -4.3736996563123975, took: 0.6838998794555664\n",
      "Validation vlb: -4.4062405789940104, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch138, vlb: -4.316483376907882, took: 0.6536650657653809\n",
      "Validation vlb: -4.427131566414941, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch139, vlb: -4.334461690712456, took: 0.6559350490570068\n",
      "Validation vlb: -4.4062778155008955, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch140, vlb: -4.3420952346027, took: 0.651298999786377\n",
      "Validation vlb: -4.427043201853928, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch141, vlb: -4.3365753513469425, took: 0.6756939888000488\n",
      "Validation vlb: -4.427021376909176, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch142, vlb: -4.379973289474636, took: 0.6770267486572266\n",
      "Validation vlb: -4.403541811847378, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch143, vlb: -4.310451464125867, took: 0.6558701992034912\n",
      "Validation vlb: -4.2193615598586, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch144, vlb: -4.333459307918198, took: 0.653252124786377\n",
      "Validation vlb: -4.441777636703936, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch145, vlb: -4.353770571301041, took: 0.6783678531646729\n",
      "Validation vlb: -4.237242797431822, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch146, vlb: -4.367597647094452, took: 0.6558480262756348\n",
      "Validation vlb: -4.339064792521949, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch147, vlb: -4.366932436678965, took: 0.6521899700164795\n",
      "Validation vlb: -4.261164622013623, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch148, vlb: -4.3427272462827675, took: 0.6546258926391602\n",
      "Validation vlb: -4.348196949387831, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch149, vlb: -4.345284141393544, took: 0.6623191833496094\n",
      "Validation vlb: -4.441910746799704, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch150, vlb: -4.3109080811219975, took: 0.655803918838501\n",
      "Validation vlb: -4.331669512690078, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch151, vlb: -4.348243958580009, took: 0.6652359962463379\n",
      "Validation vlb: -4.300709364869448, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch152, vlb: -4.363154230162443, took: 0.6589419841766357\n",
      "Validation vlb: -4.285066905530911, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch153, vlb: -4.367021204028406, took: 0.660736083984375\n",
      "Validation vlb: -4.277102010535576, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch154, vlb: -4.363170073450491, took: 0.6890428066253662\n",
      "Validation vlb: -4.341923218328976, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch155, vlb: -4.320920203287566, took: 0.6566369533538818\n",
      "Validation vlb: -4.398250332928012, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch156, vlb: -4.335289858466736, took: 0.6521809101104736\n",
      "Validation vlb: -4.3046752858702035, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch157, vlb: -4.327492265251415, took: 0.6595759391784668\n",
      "Validation vlb: -4.231997354130915, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch158, vlb: -4.335926808167328, took: 0.6613340377807617\n",
      "Validation vlb: -4.3524575711839795, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch159, vlb: -4.32689638312961, took: 0.6702389717102051\n",
      "Validation vlb: -4.468921198428256, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch160, vlb: -4.336587513699392, took: 0.6596369743347168\n",
      "Validation vlb: -4.464914738553242, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch161, vlb: -4.303025303017462, took: 0.6548609733581543\n",
      "Validation vlb: -4.290350816782238, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch162, vlb: -4.352398959088763, took: 0.6640088558197021\n",
      "Validation vlb: -4.388685493407512, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch163, vlb: -4.325270488960175, took: 0.6598308086395264\n",
      "Validation vlb: -4.229309014132108, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch164, vlb: -4.324145054705588, took: 0.6704959869384766\n",
      "Validation vlb: -4.434446470637152, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch165, vlb: -4.329480131023601, took: 0.6679949760437012\n",
      "Validation vlb: -4.3465148638753055, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch166, vlb: -4.358906061991883, took: 0.6579599380493164\n",
      "Validation vlb: -4.482996917465358, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch167, vlb: -4.370831666252018, took: 0.7189290523529053\n",
      "Validation vlb: -4.354008798074568, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch168, vlb: -4.342939834265044, took: 0.6603097915649414\n",
      "Validation vlb: -4.382131667584663, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch169, vlb: -4.321963760635601, took: 0.6698000431060791\n",
      "Validation vlb: -4.380718252805444, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch170, vlb: -4.358108246631231, took: 0.655703067779541\n",
      "Validation vlb: -4.396920327615583, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch171, vlb: -4.3339954624512576, took: 0.6551072597503662\n",
      "Validation vlb: -4.309489841214275, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch172, vlb: -4.37889598314823, took: 0.6548101902008057\n",
      "Validation vlb: -4.337336663675154, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch173, vlb: -4.29344624720935, took: 0.6605291366577148\n",
      "Validation vlb: -4.354629795913943, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch174, vlb: -4.336066063162145, took: 0.6554808616638184\n",
      "Validation vlb: -4.280667102838411, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch175, vlb: -4.346399475869662, took: 0.6667158603668213\n",
      "Validation vlb: -4.255424649198464, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch176, vlb: -4.324588083302241, took: 0.661304235458374\n",
      "Validation vlb: -4.342406075363406, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch177, vlb: -4.367450090290876, took: 0.66025710105896\n",
      "Validation vlb: -4.446024268190452, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch178, vlb: -4.388516640723245, took: 0.6572191715240479\n",
      "Validation vlb: -4.404194149770397, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch179, vlb: -4.310235036720163, took: 0.6702351570129395\n",
      "Validation vlb: -4.539847819936314, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch180, vlb: -4.34711548111874, took: 0.7188849449157715\n",
      "Validation vlb: -4.3291620621789235, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch181, vlb: -4.313368946002635, took: 0.6527562141418457\n",
      "Validation vlb: -4.228787240087022, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch182, vlb: -4.340505966575273, took: 0.6670598983764648\n",
      "Validation vlb: -4.389577481353167, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch183, vlb: -4.370702166243084, took: 0.6674330234527588\n",
      "Validation vlb: -4.307774103960945, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch184, vlb: -4.336819714850816, took: 0.6772911548614502\n",
      "Validation vlb: -4.328588393128034, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch185, vlb: -4.32033948001992, took: 0.6562318801879883\n",
      "Validation vlb: -4.365918784465604, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch186, vlb: -4.33040499764464, took: 0.6592676639556885\n",
      "Validation vlb: -4.255844855385691, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch187, vlb: -4.329329635756111, took: 0.6731131076812744\n",
      "Validation vlb: -4.272033188335333, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch188, vlb: -4.332007201232196, took: 0.6563088893890381\n",
      "Validation vlb: -4.413420354664133, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch189, vlb: -4.31147371627978, took: 0.6577770709991455\n",
      "Validation vlb: -4.451624621851159, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch190, vlb: -4.33520931711633, took: 0.655846118927002\n",
      "Validation vlb: -4.364488004480751, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch191, vlb: -4.325589848972715, took: 0.6560320854187012\n",
      "Validation vlb: -4.377702992325076, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch192, vlb: -4.375164123160142, took: 0.6575028896331787\n",
      "Validation vlb: -4.323801673345967, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch193, vlb: -4.343009947528502, took: 0.7013628482818604\n",
      "Validation vlb: -4.387605856923224, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch194, vlb: -4.355362124781322, took: 0.6585259437561035\n",
      "Validation vlb: -4.466921895838864, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch195, vlb: -4.334428002519234, took: 0.6591012477874756\n",
      "Validation vlb: -4.448406023500807, Best vlb: -4.2117198107697815\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch196, vlb: -4.319963469967209, took: 0.6646640300750732\n",
      "Validation vlb: -4.396641478183586, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch197, vlb: -4.3265764241944895, took: 0.6492860317230225\n",
      "Validation vlb: -4.372824437409928, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch198, vlb: -4.300621580811426, took: 0.6487069129943848\n",
      "Validation vlb: -4.304592744432221, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch199, vlb: -4.339559602703085, took: 0.6502058506011963\n",
      "Validation vlb: -4.3023420799897325, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch200, vlb: -4.365684978215867, took: 0.6549088954925537\n",
      "Validation vlb: -4.310039307307271, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch201, vlb: -4.309328633845174, took: 0.6578559875488281\n",
      "Validation vlb: -4.429527929299858, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch202, vlb: -4.36026374272919, took: 0.6560900211334229\n",
      "Validation vlb: -4.3517907244487875, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch203, vlb: -4.328513423616095, took: 0.6594529151916504\n",
      "Validation vlb: -4.273093234373913, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch204, vlb: -4.357062938705296, took: 0.6497499942779541\n",
      "Validation vlb: -4.347470052033952, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch205, vlb: -4.363412156481085, took: 0.6612069606781006\n",
      "Validation vlb: -4.3318443591540685, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch206, vlb: -4.368419871125927, took: 0.7123920917510986\n",
      "Validation vlb: -4.305934460031947, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch207, vlb: -4.328480425179756, took: 0.6581377983093262\n",
      "Validation vlb: -4.338276605390036, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch208, vlb: -4.337519527898231, took: 0.6661009788513184\n",
      "Validation vlb: -4.245714828034435, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch209, vlb: -4.337469647503346, took: 0.6453299522399902\n",
      "Validation vlb: -4.313247100435029, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch210, vlb: -4.337663320143703, took: 0.6471197605133057\n",
      "Validation vlb: -4.303737739143248, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch211, vlb: -4.385313215039384, took: 0.6658599376678467\n",
      "Validation vlb: -4.254231269305578, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch212, vlb: -4.337612905045572, took: 0.6534950733184814\n",
      "Validation vlb: -4.359219237824474, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch213, vlb: -4.295674058401117, took: 0.6531169414520264\n",
      "Validation vlb: -4.329438445637527, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch214, vlb: -4.343949270883738, took: 0.6611559391021729\n",
      "Validation vlb: -4.23052508468381, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch215, vlb: -4.346380299185951, took: 0.7501769065856934\n",
      "Validation vlb: -4.372938807342431, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch216, vlb: -4.348559785155371, took: 0.6616799831390381\n",
      "Validation vlb: -4.2857844636664035, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch217, vlb: -4.320003153911202, took: 0.6502647399902344\n",
      "Validation vlb: -4.278904061487192, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch218, vlb: -4.3333590191218505, took: 0.6915009021759033\n",
      "Validation vlb: -4.279341523315528, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch219, vlb: -4.330350897318175, took: 0.6690099239349365\n",
      "Validation vlb: -4.330570382979309, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch220, vlb: -4.350455189335033, took: 0.6855020523071289\n",
      "Validation vlb: -4.294762492565662, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch221, vlb: -4.347167336515785, took: 0.6637039184570312\n",
      "Validation vlb: -4.308974158030884, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch222, vlb: -4.304089319032004, took: 0.6954159736633301\n",
      "Validation vlb: -4.416027107670855, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch223, vlb: -4.3541518614195125, took: 0.6613550186157227\n",
      "Validation vlb: -4.426811915771089, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch224, vlb: -4.312862160139733, took: 0.6616866588592529\n",
      "Validation vlb: -4.227198537499388, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch225, vlb: -4.346876877436846, took: 0.6638150215148926\n",
      "Validation vlb: -4.337266727558617, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch226, vlb: -4.340517157910752, took: 0.6658802032470703\n",
      "Validation vlb: -4.231918159040433, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch227, vlb: -4.361203010278852, took: 0.6641199588775635\n",
      "Validation vlb: -4.510109804208996, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch228, vlb: -4.342467447472512, took: 0.6540870666503906\n",
      "Validation vlb: -4.395477824226552, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch229, vlb: -4.382503832850389, took: 0.6659150123596191\n",
      "Validation vlb: -4.329849957648219, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch230, vlb: -4.337499990281245, took: 0.6991081237792969\n",
      "Validation vlb: -4.296925581774666, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch231, vlb: -4.30603239284731, took: 0.7258250713348389\n",
      "Validation vlb: -4.4196007876720245, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch232, vlb: -4.305912926680414, took: 0.6756901741027832\n",
      "Validation vlb: -4.270828500148934, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch233, vlb: -4.333851447155204, took: 0.6622231006622314\n",
      "Validation vlb: -4.404882691824707, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch234, vlb: -4.342506621267712, took: 0.6529378890991211\n",
      "Validation vlb: -4.348710695902507, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch235, vlb: -4.322287639525884, took: 0.668602705001831\n",
      "Validation vlb: -4.2855928939523045, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch236, vlb: -4.328516792212195, took: 0.7059478759765625\n",
      "Validation vlb: -4.375051072500284, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch237, vlb: -4.328892915936567, took: 0.721393346786499\n",
      "Validation vlb: -4.4816542829124675, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch238, vlb: -4.364029090010438, took: 0.6737337112426758\n",
      "Validation vlb: -4.41887729916372, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch239, vlb: -4.315307550564231, took: 0.8107531070709229\n",
      "Validation vlb: -4.372221989924854, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch240, vlb: -4.349706049312307, took: 0.6669120788574219\n",
      "Validation vlb: -4.335940760701991, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch241, vlb: -4.338400840587568, took: 0.847567081451416\n",
      "Validation vlb: -4.403212590510791, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch242, vlb: -4.284678767901082, took: 0.7026791572570801\n",
      "Validation vlb: -4.334169724226769, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch243, vlb: -4.357099243321349, took: 0.730212926864624\n",
      "Validation vlb: -4.330411207328722, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch244, vlb: -4.35970264894309, took: 0.6806881427764893\n",
      "Validation vlb: -4.323597455101877, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch245, vlb: -4.315487978224021, took: 0.6586151123046875\n",
      "Validation vlb: -4.332841377813839, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch246, vlb: -4.3430951921726075, took: 0.6619231700897217\n",
      "Validation vlb: -4.309424847846664, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch247, vlb: -4.315067420556986, took: 0.6633081436157227\n",
      "Validation vlb: -4.256253767167866, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch248, vlb: -4.329515450113233, took: 0.699199914932251\n",
      "Validation vlb: -4.423215810535024, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch249, vlb: -4.321008418848671, took: 0.6566720008850098\n",
      "Validation vlb: -4.461190415045976, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch250, vlb: -4.334619147081275, took: 0.6506528854370117\n",
      "Validation vlb: -4.458408206026145, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch251, vlb: -4.318861044092834, took: 0.6808559894561768\n",
      "Validation vlb: -4.468232247435931, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch252, vlb: -4.3201305916895745, took: 0.6606359481811523\n",
      "Validation vlb: -4.3455960943475125, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch253, vlb: -4.312917939956384, took: 0.6655778884887695\n",
      "Validation vlb: -4.369753376179914, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch254, vlb: -4.330599213402006, took: 0.6588079929351807\n",
      "Validation vlb: -4.415775771279937, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch255, vlb: -4.322071330771414, took: 0.6630959510803223\n",
      "Validation vlb: -4.4017593050466, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch256, vlb: -4.3288538111685675, took: 0.7540180683135986\n",
      "Validation vlb: -4.405455206590177, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch257, vlb: -4.324532264850961, took: 0.6666707992553711\n",
      "Validation vlb: -4.398401956342185, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch258, vlb: -4.333439264445156, took: 0.659271240234375\n",
      "Validation vlb: -4.249087936284087, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch259, vlb: -4.349873169660998, took: 0.6952168941497803\n",
      "Validation vlb: -4.327050466753518, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch260, vlb: -4.332799089869279, took: 0.66190505027771\n",
      "Validation vlb: -4.358989789647963, Best vlb: -4.2117198107697815\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch261, vlb: -4.316840296303321, took: 0.6580290794372559\n",
      "Validation vlb: -4.352828073655903, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch262, vlb: -4.364917336499301, took: 0.6608402729034424\n",
      "Validation vlb: -4.395222998745619, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch263, vlb: -4.325177434798321, took: 0.6520540714263916\n",
      "Validation vlb: -4.242438415107603, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch264, vlb: -4.343286157298269, took: 0.6584420204162598\n",
      "Validation vlb: -4.331316225737044, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch265, vlb: -4.337517446264501, took: 0.6577877998352051\n",
      "Validation vlb: -4.268645820494223, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch266, vlb: -4.248979257343912, took: 0.664417028427124\n",
      "Validation vlb: -4.372772047049019, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch267, vlb: -4.317908653648027, took: 0.673408031463623\n",
      "Validation vlb: -4.313725712229904, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch268, vlb: -4.326522042092806, took: 0.6762478351593018\n",
      "Validation vlb: -4.557080629959847, Best vlb: -4.2117198107697815\n",
      "\n",
      "Epoch269, vlb: -4.364740314696562, took: 0.7180531024932861\n",
      "Validation vlb: -4.369006127601303, Best vlb: -4.2117198107697815\n",
      "\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "# For Default credit\n",
    "\"\"\"\n",
    "input_dim_vec = [1, 2, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ,1 ,1 ,1 ,1 ,1, 2]\n",
    "width = 350\n",
    "depth = 3\n",
    "latent_dim = 8\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "batch_size = 128\n",
    "nb_epochs = 2000 # 2000\n",
    "early_stop = 200\n",
    "lr = 1e-4        # Maybe this should be 1e-4, but it makes the performance terrible...\n",
    "\n",
    "x_train, x_test, x_means, x_stds = \\\n",
    "load_UCI(dset_name=dname, splits=10, seed=42, separate_targets=False, save_dir='../data/') # np.arrays\n",
    "\n",
    "x_train = unnormalise_cat_vars(x_train, x_means, x_stds, input_dim_vec) # np.array\n",
    "x_test = unnormalise_cat_vars(x_test, x_means, x_stds, input_dim_vec) \n",
    "\"\"\"\n",
    "\n",
    "# For COMPAS\n",
    "#\"\"\"\n",
    "dname = 'compas'\n",
    "print(dname)\n",
    "\n",
    "input_dim_vec = [3, 6, 2, 2, 2, 1, 1, 2]\n",
    "batch_size = 128\n",
    "nb_epochs = 2000\n",
    "early_stop = 200\n",
    "lr = 1e-4\n",
    "width = widths[names.index(dname)] # 350\n",
    "depth = depths[names.index(dname)] # number of hidden layers # 3\n",
    "latent_dim = latent_dims[names.index(dname)] # 4\n",
    "\n",
    "x_train, x_test, x_means, x_stds, y_train, y_test, feature_names, X_dims = \\\n",
    "    get_my_COMPAS(rseed=42, separate_test=True, test_ratio=0.1, save_dir='../data/')\n",
    "\n",
    "x_train, x_test, input_dim_vec = join_compas_targets(x_train, x_test, y_train, y_test, X_dims)\n",
    "\n",
    "print('Compas', x_train.shape, x_test.shape)\n",
    "print(input_dim_vec)\n",
    "#\"\"\"\n",
    "\n",
    "optimizer_VAEAC = tfa.optimizers.RectifiedAdam(lr= lr , epsilon=1e-8)\n",
    "\n",
    "# Create new model to load in weightsinto that can then continued to be trained\n",
    "model2 = VAEAC_gauss_cat(width, depth, latent_dim, input_dim_vec, batch_size, lr, optimizer_VAEAC, save_model = True)\n",
    "\n",
    "model2.recognition_encoder = keras.models.load_model(\"./COMPAS_VAEAC/compas_recog_encoder_lr_0.0001\")\n",
    "model2.prior_encoder = keras.models.load_model(\"./COMPAS_VAEAC/compas_prior_encoder_lr_0.0001\")\n",
    "model2.decoder = keras.models.load_model(\"./COMPAS_VAEAC/compas_decoder_lr_0.0001\")\n",
    "\n",
    "\n",
    "# No mask is used to train the 2nd lvl VAE\n",
    "#masker = top_masker_tensorflow(p=1)\n",
    "\n",
    "base_network = model2\n",
    "width = 150\n",
    "depth = 2\n",
    "latent_dim = under_latent_dims[names.index(dname)]\n",
    "\n",
    "batch_size = 128\n",
    "nb_epochs = 2000\n",
    "early_stop = 200\n",
    "lr = 1e-4\n",
    "\n",
    "optimizer_under_VAEAC = tfa.optimizers.RectifiedAdam(lr = lr, epsilon = 1e-8)\n",
    "\n",
    "under_VAEAC_net = under_VAEAC(base_network, width, depth, latent_dim, batch_size, lr, \\\n",
    "                              optimizer_under_VAEAC, save_model = True)\n",
    "\n",
    "vlb_train, vlb_val, best_epoch, best_vlb, curr_epoch = train_VAE(under_VAEAC_net, x_train, x_test, nb_epochs, early_stop=early_stop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de3fcb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 6 2 2 2 1 1]\n",
      "[3 6 2 2 2 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "input_dim_vec_X_dims = X_dims_to_input_dim_vec(X_dims)\n",
    "print(input_dim_vec_X_dims)\n",
    "print(input_dim_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0042d8b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.clip(vlb_train[:curr_epoch], -1000, 1000), 'r')\n",
    "plt.plot(np.clip(vlb_val[:curr_epoch], -1000, 1000), 'b')\n",
    "plt.legend(['cost_train', 'cost_dev'])\n",
    "plt.ylabel('vlb')\n",
    "plt.xlabel('it')\n",
    "plt.grid(True)\n",
    "#plt.savefig( str(dname) + '_vlb_lr_' + str(model.lr) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38286498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "827897\n"
     ]
    }
   ],
   "source": [
    "#\"\"\"\n",
    "total_parameters = 0\n",
    "for variable in under_VAEAC_net.trainable_variables:\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    #print(shape)\n",
    "    #print(len(shape))\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        #print(dim)\n",
    "        variable_parameters *= dim\n",
    "    #print(variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)\n",
    "\n",
    "time.sleep(10)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce88d893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeebb797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50abd922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ec54fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "813be207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(under_VAEAC_net.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9337631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model2.decoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7eb025d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO check the that the parameters in the VAEAC are not trained during the training of under_VAEAC\n",
    "\n",
    "# Encoder\n",
    "#print(model2.recognition_encoder.trainable_variables[0])\n",
    "\n",
    "# Decoder\n",
    "#print(model2.decoder.trainable_variables[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0dbe5d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel2 = VAEAC_gauss_cat(width, depth, latent_dim, input_dim_vec, batch_size, lr, optimizer_VAEAC, save_model = True)\\n\\nmodel2.decoder = keras.models.load_model(\"./COMPAS_VAEAC/compas_decoder_lr_0.0001\")\\n\\nprint(model2.decoder.trainable_variables[0])\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoder\n",
    "\"\"\"\n",
    "model2 = VAEAC_gauss_cat(width, depth, latent_dim, input_dim_vec, batch_size, lr, optimizer_VAEAC, save_model = True)\n",
    "model2.recognition_encoder = keras.models.load_model(\"./COMPAS_VAEAC/compas_recog_encoder_lr_0.0001\")\n",
    "print(model2.recognition_encoder.trainable_variables[0])\n",
    "\"\"\"\n",
    "\n",
    "# Decoder\n",
    "\"\"\"\n",
    "model2 = VAEAC_gauss_cat(width, depth, latent_dim, input_dim_vec, batch_size, lr, optimizer_VAEAC, save_model = True)\n",
    "\n",
    "model2.decoder = keras.models.load_model(\"./COMPAS_VAEAC/compas_decoder_lr_0.0001\")\n",
    "\n",
    "print(model2.decoder.trainable_variables[0])\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56b7b615",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model2.prior_encoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb8cdcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79e0896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(under_VAEAC_net.decoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78fb90c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(under_VAEAC_net.recognition_encoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88ef766",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Currently Working on ###\n",
    "\n",
    "### TODO ###\n",
    "\n",
    "# Make code work for only continous data (GaussianLogLike?)\n",
    "\n",
    "# Train VAEAC (default credit) using 7e-4 lr and save the model\n",
    "\n",
    "# Clean up the code\n",
    "\n",
    "## eval/training mode things ##\n",
    "\n",
    "# Verify if BatchNorm does what it should when training = True/False is not sent into the model\n",
    "\n",
    "# Do I need to set the models to eval mode and not training during validation data or does tf handle this?\n",
    "    # Apparently batchnorm layers do different things during training/evaluation\n",
    "    # Adding training = True affects a lot..., it is not enabled by default during train_step gradient.\n",
    "    # Setting training = True in eval affects the training_VAE still. \n",
    "    # Not using training = True/False makes the model work well, \n",
    "    # Question is if BatchNorm does the correct thing during eval\n",
    "\n",
    "##---------------------------##\n",
    "\n",
    "\n",
    "## Ask Ali if \n",
    "    # His network has the correct structure (The decoder was wrong for VAEAC)\n",
    "    # He has thought of initlialisations\n",
    "    # hyperparameters, bias, epsilon, momentum etc.\n",
    "\n",
    "# Skip connections from prior to decoder? I don't think CLUE got this to work properly...\n",
    "    # Memory layer is used in Tf2 github\n",
    "\n",
    "# Not sure what the TF equivalence of affine and track_running_stats is in Torch BatchNorm1D \n",
    "    \n",
    "# Why does train_step only print things inside it twice for the first batch and then never for any other batch?\n",
    "    \n",
    "### TO IMPLEMENT ###\n",
    "\n",
    "\n",
    "### DONE ###\n",
    "\"\"\"\n",
    "\n",
    " under_VAEAC updates the VAEAC recognition_encoder parameters. Need to freeze them somehow...\n",
    "    # Maybe calculate proposal_params_VAEAC before calling train_step_under_VAEAC to make it work?\n",
    "    # Yup, that did the trick\n",
    "\n",
    " Something wrong with the number of trainable parameters in my under_VAEAC? Seems to be way more than in Torch?\n",
    "    # The VAEAC recognition_encoder was being trained in under_VAEAC\n",
    "\n",
    " Make VAEAC work for COMPAS\n",
    "    # How to load COMPAS?\n",
    "\n",
    " Train the VAEAC for COMPAS\n",
    "\n",
    " Fix the under_VAEAC code  (i.e. get eval to work there just as in VAEAC)\n",
    "\n",
    " update_train VAE with the train_VAEAC code (Add Shuffle among many things)\n",
    "\n",
    " Plot the loss graph over train and validation set\n",
    "\n",
    " Save the vlb_train & vlb_val after training\n",
    "\n",
    " rec_los: Should the target not be flattened but instead just x_batch?\n",
    "    # Don't think so, the final values look fairly similar.\n",
    "    # I think it is fine since the program seems to be doing what it should\n",
    "\n",
    " How do the batches work in the network? How can we send a 64x31 batch to encoder? It should only take 31 as input\n",
    "    # the keras.input((31, )) means it expect features with dimension 31 and unspecified batch_size. \n",
    "    # When a batch with 64,31 size comes it in will treat each row as a sample\n",
    "\n",
    " Should I have 7e-4 or 1e-4 learning rate?\n",
    "    # 7e-4 for comparing and making sure the model works as intended but 1e-4 for the real training\n",
    "\n",
    " Remove reparametrize?\n",
    "\n",
    " Add lr to print epoch in VAEAC training\n",
    "\n",
    " Is something wrong with the trainable variables? Should I before training use tf.Variable to make them trainable?\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/Variable\n",
    "    # Seems right, it is the exact same amount of trainable parameters in torch and tf\n",
    "\n",
    " Have I forgotten to do a tf.reduce_sum over regularisation? \n",
    "    # Don't think so... their sum(-1) on reg_cost does nothing\n",
    "\n",
    " Something might be wrong with how the training is done with batches... \n",
    "    # Probably not, input(shape,) makes it so that it expects one dimension to be of shape \n",
    "    # without specifying the batch size\n",
    "\n",
    " vlb_val is calculated using eval not fit\n",
    "\n",
    " Add shuffling of the training data \n",
    "\n",
    " Verify the number of trainable parameters in CLUE vs TF \n",
    "    # Exact same amount for under_VAEAC and VAEAC\n",
    "\n",
    " implement under_VAEAC vlb (MSELoss and KL-divergence)\n",
    "\n",
    " rsample instead of sample? (rsample for derivatives in torch, tf does not care)\n",
    "\n",
    " Save the model during training (best vlb)\n",
    "\n",
    " Fix so that training with 2nd lvl VAE is done without the mask\n",
    "\n",
    " 1e-4 can at times get very poor vlb at the start but then recover. Why is this?   \n",
    "    # Seems to have been solved by initialising the dense weights and bias weights the same way as Torch\n",
    "    # I get like -19 or -20, -22 every single first epoch now\n",
    "\n",
    " Validation vlb not low enough? \n",
    "    # Not terrible but indeed not as low, seems to go towards the right values at least but it happens slowly\n",
    "\n",
    " Need to initialise the weights in keras dense for the neurons and bias the same way as nn.linear\n",
    "\n",
    " Need to make the layers in a skip connection sequential? nn.sequential in torch\n",
    "    # The russian doll effect does exactly this!\n",
    "    \n",
    " What activation is used in dense/nn.linear?\n",
    "    # None\n",
    "    \n",
    " Is keras dense and torch nn.linear the same thing?\n",
    "    # How are the weights inited in each? (Different ways by default but I made them init the same way)\n",
    "    # They are basically the same, input to neural network nodes (bias = True add a bias node)\n",
    "    # Google images for keras dense and nn.linear and you see that it is just a normal feed forward process.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
