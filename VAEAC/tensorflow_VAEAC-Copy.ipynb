{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "310825f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import uniform, binomial\n",
    "from sklearn.model_selection import KFold\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# To remove WARNINGS from saving the models without compiling them first\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "#print(tf.__version__)\n",
    "#print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e185160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.config.list_physical_devices())\n",
    "\n",
    "#Working with CPU for now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeb23f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import seed\n",
    "#from .UCI_loader import unnormalise_cat_vars\n",
    "\n",
    "\n",
    "# TODO return mean and std for variables + train test split\n",
    "\n",
    "\"\"\"\n",
    "def check_data_file(fname):\n",
    "    files = os.listdir(\".\")  # get the current directory listing\n",
    "    print\n",
    "    \"Looking for file '%s' in the current directory...\" % fname\n",
    "\n",
    "    if fname not in files:\n",
    "        print\n",
    "        \"'%s' not found! Downloading from GitHub...\" % fname\n",
    "        addr = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
    "        try:\n",
    "            response = urllib2.urlopen(addr)\n",
    "        except:\n",
    "            response = urllib3.urlopen(addr)\n",
    "        data = response.read()\n",
    "        fileOut = open(fname, \"w\")\n",
    "        fileOut.write(data)\n",
    "        fileOut.close()\n",
    "        print\n",
    "        \"'%s' download and saved locally..\" % fname\n",
    "    else:\n",
    "        print\n",
    "        \"File found in current directory..\"\n",
    "\"\"\"\n",
    "\n",
    "def check_data_file(fname):\n",
    "    files = os.listdir(\".\")  # get the current directory listing\n",
    "    print\n",
    "    \"Looking for file '%s' in the current directory...\" % fname\n",
    "\n",
    "    if fname not in files:\n",
    "        print\n",
    "        \"'%s' not found! Downloading from GitHub...\" % fname\n",
    "        addr = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
    "        \n",
    "        response = urllib.request.urlopen(addr)\n",
    "\n",
    "        data = response.read()\n",
    "        fileOut = open(fname, \"wb\")\n",
    "        fileOut.write(data)\n",
    "        fileOut.close()\n",
    "        print\n",
    "        \"'%s' download and saved locally..\" % fname\n",
    "    else:\n",
    "        print\n",
    "        \"File found in current directory..\"\n",
    "\n",
    "def get_my_COMPAS(rseed=0, separate_test=True, test_ratio=0.2, save_dir='../data/'):\n",
    "    \"\"\"\n",
    "        The adult dataset can be obtained from: https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\n",
    "        The code will look for the data file in the present directory, if it is not found, it will download them from GitHub.\n",
    "    \"\"\"\n",
    "\n",
    "    SEED = rseed\n",
    "    seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    their_FEATURES_CLASSIFICATION = [\"age_cat\", \"race\", \"sex\", \"priors_count\", \"c_charge_degree\"]\n",
    "    FEATURES_CLASSIFICATION = [\"age_cat\", \"race\", \"sex\", \"c_charge_degree\", \"is_recid\", \"priors_count\",\n",
    "                               \"time_served\"]  # features to be used for classification\n",
    "    CONT_VARIABLES = [\"priors_count\",\n",
    "                      \"time_served\"]  # continuous features, will need to be handled separately from categorical features, categorical features will be encoded using one-hot\n",
    "    CLASS_FEATURE = \"two_year_recid\"  # the decision variable\n",
    "\n",
    "\n",
    "    COMPAS_INPUT_FILE = save_dir + \"compas-scores-two-years.csv\"\n",
    "    check_data_file(COMPAS_INPUT_FILE)\n",
    "\n",
    "    # load the data and get some stats\n",
    "    df = pd.read_csv(COMPAS_INPUT_FILE)\n",
    "    df = df.dropna(subset=[\"days_b_screening_arrest\"])  # dropping missing vals\n",
    "\n",
    "    # convert to np array\n",
    "    data = df.to_dict('list')\n",
    "    for k in data.keys():\n",
    "        data[k] = np.array(data[k])\n",
    "\n",
    "    dates_in = data['c_jail_in']\n",
    "    dates_out = data['c_jail_out']\n",
    "    # this measures time in Jail\n",
    "    time_served = []\n",
    "    for i in range(len(dates_in)):\n",
    "        di = datetime.datetime.strptime(dates_in[i], '%Y-%m-%d %H:%M:%S')\n",
    "        do = datetime.datetime.strptime(dates_out[i], '%Y-%m-%d %H:%M:%S')\n",
    "        time_served.append((do - di).days)\n",
    "    time_served = np.array(time_served)\n",
    "    time_served[time_served < 0] = 0\n",
    "    data[\"time_served\"] = time_served\n",
    "\n",
    "    \"\"\" Filtering the data \"\"\"\n",
    "\n",
    "    # These filters are the same as propublica (refer to https://github.com/propublica/compas-analysis)\n",
    "    # If the charge date of a defendants Compas scored crime was not within 30 days from when the person was arrested, we assume that because of data quality reasons, that we do not have the right offense.\n",
    "    idx = np.logical_and(data[\"days_b_screening_arrest\"] <= 30, data[\"days_b_screening_arrest\"] >= -30)\n",
    "\n",
    "    # We coded the recidivist flag -- is_recid -- to be -1 if we could not find a compas case at all.\n",
    "    idx = np.logical_and(idx, data[\"is_recid\"] != -1)\n",
    "\n",
    "    # In a similar vein, ordinary traffic offenses -- those with a c_charge_degree of 'O' -- will not result in Jail time are removed (only two of them).\n",
    "    idx = np.logical_and(idx, data[\"c_charge_degree\"] != \"O\")  # F: felony, M: misconduct\n",
    "\n",
    "    # We filtered the underlying data from Broward county to include only those rows representing people who had either recidivated in two years, or had at least two years outside of a correctional facility.\n",
    "    idx = np.logical_and(idx, data[\"score_text\"] != \"NA\")\n",
    "\n",
    "    # select the examples that satisfy this criteria\n",
    "    for k in data.keys():\n",
    "        data[k] = data[k][idx]\n",
    "\n",
    "    y = data[CLASS_FEATURE]\n",
    "    \"\"\" Feature normalization and one hot encoding \"\"\"\n",
    "\n",
    "    print\n",
    "    \"\\nNumber of people recidivating within two years\"\n",
    "    print\n",
    "    pd.Series(y).value_counts()\n",
    "    print\n",
    "    \"\\n\"\n",
    "\n",
    "    X = []  # empty array with num rows same as num examples, will hstack the features to it\n",
    "    X_dims = []\n",
    "\n",
    "    feature_names = []\n",
    "    for attr in FEATURES_CLASSIFICATION:\n",
    "        vals = data[attr]\n",
    "        if attr in CONT_VARIABLES:\n",
    "            vals = [float(v) for v in vals]\n",
    "            # vals = preprocessing.scale(vals, axis=0, with_mean=True, with_std=True)  # 0 mean and 1 variance\n",
    "            vals = np.reshape(vals, (len(y), -1))  # convert from 1-d arr to a 2-d arr with one col\n",
    "            X_dims.append(1)\n",
    "\n",
    "        else:  # for binary categorical variables, the label binarizer uses just one var instead of two\n",
    "            enc = preprocessing.OneHotEncoder(categories='auto', handle_unknown='error')\n",
    "            enc.fit(vals.reshape(-1, 1))\n",
    "            vals = enc.transform(vals.reshape(-1, 1)).todense()\n",
    "            X_dims += [vals.shape[1]]*vals.shape[1]\n",
    "\n",
    "        # add to learnable features\n",
    "        X.append(vals)\n",
    "\n",
    "        if attr in CONT_VARIABLES:  # continuous feature, just append the name\n",
    "            feature_names.append(attr)\n",
    "        else:  # categorical features\n",
    "            if vals.shape[1] == 1:  # binary features that passed through lib binarizer\n",
    "                feature_names.append(attr)\n",
    "            else:\n",
    "                for k in enc.categories_:  # non-binary categorical features, need to add the names for each cat\n",
    "                    feature_names.append(attr + \"_\" + str(k))\n",
    "\n",
    "    X = np.array(np.concatenate(list(X), axis=1))\n",
    "    X_dims = np.array(X_dims)\n",
    "\n",
    "    if separate_test:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=rseed, shuffle=True)\n",
    "\n",
    "        x_means, x_stds = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "        x_means[X_dims>1] = 0\n",
    "        x_stds[X_dims>1] = 1\n",
    "        x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "        x_train = ((X_train - x_means) / x_stds).astype(np.float32)\n",
    "        x_test = ((X_test - x_means) / x_stds).astype(np.float32)\n",
    "\n",
    "        return x_train, x_test, x_means, x_stds, y_train, y_test, feature_names, X_dims\n",
    "    else:\n",
    "        x_means, x_stds = X.mean(axis=0), X.std(axis=0)\n",
    "        x_means[:,X_dims>1] = 0\n",
    "        x_stds[:,X_dims>1] = 1\n",
    "        x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "        x_train = ((X - x_means) / x_stds).astype(np.float32)\n",
    "\n",
    "        return x_train, x_means, x_stds, y, feature_names, X_dims\n",
    "\n",
    "def join_compas_targets(x_train, x_test, y_train, y_test, X_dims):\n",
    "    # output from get method is onehot so we need to flatten and append 2\n",
    "    input_dim_vec = X_dims_to_input_dim_vec(X_dims)\n",
    "    input_dim_vec = np.append(input_dim_vec, 2)\n",
    "    enc = preprocessing.OneHotEncoder(categories='auto', handle_unknown='error')\n",
    "    enc.fit(y_train.reshape(-1, 1))\n",
    "\n",
    "    vals_train = np.array(enc.transform(y_train.reshape(-1, 1)).todense()).astype(np.float32)\n",
    "    vals_test = np.array(enc.transform(y_test.reshape(-1, 1)).todense()).astype(np.float32)\n",
    "\n",
    "    x_train = np.concatenate([x_train, vals_train], axis=1)\n",
    "    x_test = np.concatenate([x_test, vals_test], axis=1)\n",
    "    return x_train, x_test, input_dim_vec\n",
    "\n",
    "def X_dims_to_input_dim_vec(X_dims):\n",
    "    \"\"\"This is for our cat_Gauss VAE model\"\"\"\n",
    "    input_dim_vec = []\n",
    "    i = 0\n",
    "    while i < len(X_dims):\n",
    "        input_dim_vec.append(X_dims[i])\n",
    "        i += X_dims[i]\n",
    "    return np.array(input_dim_vec)\n",
    "\n",
    "#\"\"\"\n",
    "def input_dim_vec_to_X_dims(input_dim_vec):\n",
    "    # This is for our cat_Gauss VAE model\n",
    "    X_dims = []\n",
    "    for i in input_dim_vec:\n",
    "        for ii in range(i):\n",
    "            X_dims.append(i)\n",
    "    return np.array(X_dims)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6079933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20a7336c",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a59efb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default_credit\n"
     ]
    }
   ],
   "source": [
    "# For Default credit\n",
    "\n",
    "input_dim_vec = [1, 2, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ,1 ,1 ,1 ,1 ,1, 2]\n",
    "width = 350\n",
    "depth = 3\n",
    "latent_dim = 8\n",
    "\n",
    "# For all tabular data sets\n",
    "names = ['wine', 'default_credit', 'compas', 'lsat']\n",
    "widths = [350, 350, 350, 350] # Bigger than VAE because the task of modelling all conditionals is more complex\n",
    "depths = [3, 3, 3, 3] # We go deeper because we are using residual models\n",
    "latent_dims = [6, 8, 4, 4]\n",
    "under_latent_dims = [6, 8, 4, 4] # following the original paper we set dim(u) = dim(z) with d>r [r is true manifold dim]\n",
    "under_latent_dims2 = [4, 6, 3, 3] # following the original paper we set dim(u) = dim(z) with d>r [r is true manifold dim]\n",
    "\n",
    "dname = 'default_credit'\n",
    "print(dname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33809880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets in UCI\n",
    "def load_UCI(dset_name, splits=10, seed=0, separate_targets=True, save_dir='data/'):\n",
    "    mkdir(save_dir)\n",
    "\n",
    "    if dset_name == 'wine':\n",
    "        if not os.path.isfile(save_dir+'winequality-red.csv'):\n",
    "            urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\",\n",
    "                               filename=save_dir+'winequality-red.csv')\n",
    "        data = pd.read_csv(save_dir+'winequality-red.csv', header=1, delimiter=';').values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'default_credit':\n",
    "        if not os.path.isfile(save_dir + 'default of credit card clients.xls'):\n",
    "            urllib.request.urlretrieve(\n",
    "                \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\",\n",
    "                filename=save_dir + 'default of credit card clients.xls')\n",
    "        data = pd.read_excel(save_dir + 'default of credit card clients.xls', header=[0, 1], index_col=0, # delimiter=\"\\s+\"\n",
    "                             ).values\n",
    "        y_idx = [-1]  # OK\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise Exception('Dataset name doesnt match any known datasets.')\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    data = data[np.random.permutation(np.arange(len(data)))] #Shuffle the data\n",
    "    \n",
    "    kf = KFold(n_splits=splits)\n",
    "    for j, (train_index, test_index) in enumerate(kf.split(data)):\n",
    "\n",
    "        # Not sure what separate targets is\n",
    "        if separate_targets:\n",
    "            x_idx = list(range(data.shape[1]))\n",
    "            for e in y_idx:\n",
    "                x_idx.remove(x_idx[e])\n",
    "\n",
    "            x_idx = np.array(x_idx)\n",
    "            y_idx = np.array(y_idx)\n",
    "            x_train, y_train = data[train_index, :], data[train_index, :]\n",
    "            x_train, y_train = x_train[:, x_idx], y_train[:, y_idx]\n",
    "            x_test, y_test = data[test_index, :], data[test_index, :]\n",
    "            x_test, y_test = x_test[:, x_idx], y_test[:, y_idx]\n",
    "\n",
    "            x_means, x_stds = x_train.mean(axis=0), x_train.std(axis=0)\n",
    "            y_means, y_stds = y_train.mean(axis=0), y_train.std(axis=0)\n",
    "\n",
    "            y_stds[y_stds < 1e-10] = 1\n",
    "            x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "            x_train = ((x_train - x_means) / x_stds).astype(np.float32)\n",
    "            y_train = ((y_train - y_means) / y_stds).astype(np.float32)\n",
    "\n",
    "            x_test = ((x_test - x_means) / x_stds).astype(np.float32)\n",
    "            y_test = ((y_test - y_means) / y_stds).astype(np.float32)\n",
    "\n",
    "            return x_train, x_test, x_means, x_stds, y_train, y_test, y_means, y_stds\n",
    "\n",
    "        else:\n",
    "            x_train, x_test = data[train_index, :], data[test_index, :]\n",
    "            x_means, x_stds = x_train.mean(axis=0), x_train.std(axis=0)\n",
    "\n",
    "            x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "            x_train = ((x_train - x_means) / x_stds).astype(np.float32)\n",
    "            x_test = ((x_test - x_means) / x_stds).astype(np.float32)\n",
    "\n",
    "            return x_train, x_test, x_means, x_stds\n",
    "\n",
    "def mkdir(paths):\n",
    "    if not isinstance(paths, (list, tuple)):\n",
    "        paths = [paths]\n",
    "    for path in paths:\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path, mode=0o777)\n",
    "\n",
    "def unnormalise_cat_vars(x, x_means, x_stds, input_dim_vec):\n",
    "    \"\"\"\n",
    "    Converts a feature vector with continous values into a vector with continous and discrete values for those \n",
    "    which come from a categorical class.\n",
    "    \"\"\"\n",
    "    input_dim_vec = np.array(input_dim_vec)\n",
    "    unnorm_x = np.multiply(x, x_stds) + x_means\n",
    "\n",
    "    fixed_unnorm = unnorm_x.round()\n",
    "    fixed_unnorm -= fixed_unnorm.min(axis=0).reshape([1, fixed_unnorm.shape[1]])  # this sets all mins to 0\n",
    "    for idx, dims in enumerate(input_dim_vec):\n",
    "        if dims > 1:\n",
    "            vec = fixed_unnorm[:, idx]\n",
    "            vec[vec > dims - 1] = dims - 1\n",
    "            fixed_unnorm[:, idx] = vec\n",
    "\n",
    "    x[:, input_dim_vec > 1] = fixed_unnorm[:, input_dim_vec > 1]\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c633c97",
   "metadata": {},
   "source": [
    "## Recognition (Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5836060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The arguments sent to the different keras layers are there to mimic the Torch layers in CLUE.\n",
    "\"\"\"\n",
    "\n",
    "def create_recognition_encoder(width, depth, latent_dim, input_dim_vec):\n",
    "    # Tensorflow network as one big Russian doll\n",
    "    nb_inputs = sum(input_dim_vec)\n",
    "    inputs = keras.Input(shape=(nb_inputs,))\n",
    "    #inputs = keras.Input(shape=(None,nb_inputs))\n",
    "    input = layers.Dense(width, use_bias=True, \\\n",
    "                         kernel_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/nb_inputs), math.sqrt(1/nb_inputs)), \\\n",
    "                         bias_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/nb_inputs), math.sqrt(1/nb_inputs))) \\\n",
    "                         (inputs)\n",
    "\n",
    "    for i in range(depth-1):\n",
    "\n",
    "        x = layers.LeakyReLU(alpha=0.01)(input)\n",
    "        x = layers.BatchNormalization(momentum=0.1, epsilon=1e-5)(x)\n",
    "        x = layers.Dense(width, use_bias=True, \\\n",
    "                         kernel_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width)), \\\n",
    "                         bias_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width))) \\\n",
    "                         (x)\n",
    "\n",
    "        # Skip connection \n",
    "        x = x + input\n",
    "\n",
    "        input = x\n",
    "\n",
    "    # Final layers\n",
    "    x = layers.LeakyReLU(alpha=0.01)(x)\n",
    "    x = layers.BatchNormalization(momentum=0.1, epsilon=1e-5)(x)\n",
    "    outputs = layers.Dense(latent_dim*2, use_bias=True, \\\n",
    "                         kernel_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width)), \\\n",
    "                         bias_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width))) \\\n",
    "                         (x)\n",
    "\n",
    "    recognition_encoder = keras.Model(inputs=inputs, outputs=outputs, name=\"recognition_encoder_model\")\n",
    "    return recognition_encoder\n",
    "#recognition_encoder.summary()\n",
    "\n",
    "#keras.utils.plot_model(encoder, \"encoder.png\")\n",
    "#keras.utils.plot_model(recognition_encoder, \"recognition.png\", show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06253187",
   "metadata": {},
   "source": [
    "## Prior network (Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8644fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The arguments sent to the different keras layers are there to mimic the Torch layers in CLUE.\n",
    "\"\"\"\n",
    "\n",
    "def create_prior_encoder(width, depth, latent_dim, input_dim_vec):\n",
    "    nb_inputs = sum(input_dim_vec)*2\n",
    "    inputs = keras.Input(shape=(nb_inputs,))\n",
    "    #inputs = keras.Input(shape=(None,nb_inputs))\n",
    "    input = layers.Dense(width, use_bias=True, \\\n",
    "                         kernel_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/nb_inputs), math.sqrt(1/nb_inputs)), \\\n",
    "                         bias_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/nb_inputs), math.sqrt(1/nb_inputs))) \\\n",
    "                         (inputs)\n",
    "\n",
    "    for i in range(depth-1):\n",
    "\n",
    "        x = layers.LeakyReLU(alpha=0.01)(input)\n",
    "        x = layers.BatchNormalization(momentum=0.1, epsilon=1e-5)(x)\n",
    "        x = layers.Dense(width, use_bias=True, \\\n",
    "                         kernel_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width)), \\\n",
    "                         bias_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width))) \\\n",
    "                         (x)\n",
    "\n",
    "        x = x + input\n",
    "\n",
    "        input = x\n",
    "\n",
    "    x = layers.LeakyReLU(alpha=0.01)(x)\n",
    "    x = layers.BatchNormalization(momentum=0.1, epsilon=1e-5)(x)\n",
    "    outputs = layers.Dense(latent_dim*2, use_bias=True, \\\n",
    "                         kernel_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width)), \\\n",
    "                         bias_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width))) \\\n",
    "                         (x)\n",
    "\n",
    "    prior_encoder = keras.Model(inputs=inputs, outputs=outputs, name=\"prior_encoder_model\")\n",
    "    return prior_encoder\n",
    "#prior_encoder.summary()\n",
    "\n",
    "#keras.utils.plot_model(encoder, \"encoder.png\")\n",
    "#keras.utils.plot_model(prior_encoder, \"prior.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8761740b",
   "metadata": {},
   "source": [
    "## Generator (Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15e47705",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The arguments sent to the different keras layers are there to mimic the Torch layers in CLUE.\n",
    "\"\"\"\n",
    "\n",
    "def create_decoder(width, depth, latent_dim, input_dim_vec):\n",
    "    nb_inputs = latent_dim\n",
    "    inputs = keras.Input(shape=(nb_inputs,))\n",
    "    input = layers.Dense(width, use_bias=True, \\\n",
    "                         kernel_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/nb_inputs), math.sqrt(1/nb_inputs)), \\\n",
    "                         bias_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/nb_inputs), math.sqrt(1/nb_inputs))) \\\n",
    "                         (inputs)\n",
    "\n",
    "    for i in range(depth-1):\n",
    "\n",
    "        x = layers.LeakyReLU(alpha=0.01)(input)\n",
    "        x = layers.BatchNormalization(momentum=0.1, epsilon=1e-5)(x)\n",
    "        x = layers.Dense(width, use_bias=True, \\\n",
    "                         kernel_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width)), \\\n",
    "                         bias_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width))) \\\n",
    "                         (x)\n",
    "        x = x + input\n",
    "\n",
    "        input = x\n",
    "\n",
    "    x = layers.LeakyReLU(alpha=0.01)(x)\n",
    "    x = layers.BatchNormalization(momentum=0.1, epsilon=1e-5)(x)\n",
    "    outputs = layers.Dense(sum(input_dim_vec), use_bias=True, \\\n",
    "                         kernel_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width)), \\\n",
    "                         bias_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width))) \\\n",
    "                         (x)\n",
    "\n",
    "    decoder = keras.Model(inputs=inputs, outputs=outputs, name=\"decoder_model\")\n",
    "    return decoder\n",
    "\n",
    "#decoder.summary()\n",
    "\n",
    "#keras.utils.plot_model(model, \"decoder_model.png\")\n",
    "#keras.utils.plot_model(decoder, \"generator.png\", show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f25722",
   "metadata": {},
   "source": [
    "## Masker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ddccccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class top_masker_tensorflow:\n",
    "    \"\"\"\n",
    "    Returned mask is sampled from component-wise independent Bernoulli\n",
    "    distribution with probability of component to be unobserved p.\n",
    "    Such mask induces the type of missingness which is called\n",
    "    in literature \"missing completely at random\" (MCAR).\n",
    "    If some value in batch is missed, it automatically becomes unobserved.\n",
    "    \"\"\"\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            - batch is a numpy array with as many rows as batch_size and as many columns as features\n",
    "        \n",
    "        Returned:\n",
    "       \n",
    "            - mask is a float32 tensor\n",
    "        \n",
    "        The mask seems to be random\n",
    "        \"\"\"\n",
    "        # Generate one uniform number for each row (1xrow numpy matrix)\n",
    "        pp = uniform(low=0.0, high=self.p, size=batch.shape[0]) \n",
    "        pp = np.expand_dims(pp, axis=1) # Put the number in 1x1 matrices in a 1x#row matrix\n",
    "        pp = np.repeat(pp, batch.shape[1], axis=1) # Repeat the number across each row\n",
    "        nan_mask = tf.math.is_nan(batch) # If nan => should be unobserved i.e. boolean True\n",
    "        \n",
    "        # Generate Bernoulli samples (0 or 1) from pp i.e. for each sample in batch determine if a feature is\n",
    "        # observed or hidden.\n",
    "        bernoulli_mask_numpy = binomial(1, pp, size=None) \n",
    "        bernoulli_mask = tf.convert_to_tensor(tf.cast(bernoulli_mask_numpy, tf.bool))\n",
    "        mask = tf.math.logical_or(bernoulli_mask, nan_mask) # Logical or between bernoulli and nan mask\n",
    "        \n",
    "        # Logical not to invert the mask (This is done in CLUE)\n",
    "        # Mask is converted to a boolean tensor with floats for element wise multiplication with the batch\n",
    "        # which is done in apply mask\n",
    "        # (True => 0, False => 1)\n",
    "        \n",
    "        # TODO: The logical_not might be unnecessary as the probability of getting a true or false is equal.\n",
    "        #      To mirror the Torch code however I did this but it can perhaps be removed later...\n",
    "        \n",
    "        return tf.cast(tf.math.logical_not(mask), dtype=tf.float32)\n",
    "        #return tf.cast(mask, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510e8c65",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02f82868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_cat_to_flat_mask(mask, input_dim_vec):\n",
    "    output = []\n",
    "    for idx, dim in enumerate(input_dim_vec):\n",
    "        if dim == 1:\n",
    "            \"\"\"\n",
    "            tf.expand_dims (axis=1) takes mask[:, idx] (batch_size,) and \n",
    "            converts it into (64,1) e.g. [1,2,3] => [[1];[2];[3]] i.e. same as torch unsqueeze(1)\n",
    "            \"\"\"\n",
    "            output.append(tf.expand_dims(mask[:, idx], axis=1))\n",
    "\n",
    "        elif dim > 1: \n",
    "            \"\"\"\n",
    "            tf.expand_dims (read comment above)\n",
    "            tf.ones([mask.shape[0], dim]) creates an array of batch_size x dim with ones\n",
    "            oh_vec will be mask.shape[0] x dim and contain 0 or 1 on rows depending on if mask is 0 or 1.\n",
    "            \"\"\"\n",
    "            oh_vec = tf.ones([mask.shape[0], dim]) * tf.expand_dims(mask[:, idx], axis=1)\n",
    "\n",
    "            output.append(oh_vec)\n",
    "        else:\n",
    "            raise ValueError('Error, invalid dimension value')\n",
    "    return tf.concat(output, axis=1)\n",
    "\n",
    "def gauss_cat_to_flat(x, input_dim_vec):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - x: numpy array\n",
    "        - input_dim_vec: list e.g. [1, 2, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ,1 ,1 ,1 ,1 ,1, 2] credit\n",
    "    Returns:\n",
    "        - numpy array \n",
    "        \n",
    "    Example:\n",
    "        \n",
    "        x:\n",
    "             [-0.52121574  0.          2.          1.          1.2496392   0.01383046\n",
    "              0.1105278   1.8173771   0.18815508  0.2341654   1.9953084   0.2038664\n",
    "              0.31341553  0.31455126  0.32473356  0.4501966   0.45570025  0.0774322\n",
    "             -0.2517514  -0.1535475   0.03951775 -0.31174627 -0.12532774  0.        ]\n",
    "        \n",
    "        input_dim_vec:\n",
    "            [1, 2, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ,1 ,1 ,1 ,1 ,1, 2]\n",
    "        \n",
    "        return:\n",
    "             [-0.52121574  1.          0.          0.          0.          1.\n",
    "              0.          0.          1.          0.          1.2496392   0.01383046\n",
    "              0.1105278   1.8173771   0.18815508  0.2341654   1.9953084   0.2038664\n",
    "              0.31341553  0.31455126  0.32473356  0.4501966   0.45570025  0.0774322\n",
    "             -0.2517514  -0.1535475   0.03951775 -0.31174627 -0.12532774  1.\n",
    "              0.        ]\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    for idx, dim in enumerate(input_dim_vec):\n",
    "        if dim == 1:\n",
    "            output.append(tf.expand_dims(x[:, idx], axis=1))\n",
    "        elif dim > 1:\n",
    "            oh_vec = tf.one_hot(x[:, idx], dim) # Returns one hot encoding 0 with dim 2 -> 1 0, 1 -> 0 1\n",
    "            output.append(oh_vec)\n",
    "        else:\n",
    "            raise ValueError('Error, invalid dimension value')\n",
    "    return tf.concat(output, axis=1).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abe98e6",
   "metadata": {},
   "source": [
    "## CLASS VAEAC and loss and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fc4120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEAC_gauss_cat(tf.keras.Model):\n",
    "    def __init__(self, width, depth, latent_dim, input_dim_vec, batch_size, lr, optimizer, save_model):\n",
    "        super(VAEAC_gauss_cat, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.input_dim_vec = input_dim_vec\n",
    "        self.recognition_encoder = create_recognition_encoder(width, depth, latent_dim, input_dim_vec)\n",
    "        self.prior_encoder = create_prior_encoder(width, depth, latent_dim, input_dim_vec)\n",
    "        self.decoder = create_decoder(width, depth, latent_dim, input_dim_vec)\n",
    "        self.sigma_mu = 1e4\n",
    "        self.sigma_sigma = 1e-4\n",
    "        self.vlb_scale = 1 / len(self.input_dim_vec)\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer\n",
    "        self.save_model = save_model\n",
    "        self.lr = lr\n",
    "\n",
    "    # Inspiration taken from \n",
    "    # https://github.com/joocxi/tf2-VAEAC/blob/d2b1bbc258ec77ee0975ea7eb68e63c4efcda6f0/model/vaeac.py\n",
    "    def prior_regularizer(self, prior):\n",
    "\n",
    "        mu = tf.reshape(prior.mean(), (self.batch_size, -1))\n",
    "        sigma = tf.reshape(prior.scale, (self.batch_size, -1))\n",
    "\n",
    "        mu_regularizer = -tf.reduce_sum(tf.square(mu), -1) / (2 * self.sigma_mu ** 2)\n",
    "        sigma_regularizer = tf.reduce_sum((tf.math.log(sigma) - sigma), -1) * self.sigma_sigma\n",
    "        return mu_regularizer + sigma_regularizer\n",
    "\n",
    "    def apply_mask(self, x, mask):\n",
    "        return x * mask\n",
    "\n",
    "    def reconstruction_loss(self, x, y):\n",
    "        log_prob_vec = []\n",
    "        \n",
    "        cum_dims = 0\n",
    "        reshape_dim = self.batch_size\n",
    "        for idx, dims in enumerate(self.input_dim_vec):\n",
    "            if dims == 1:\n",
    "                # Gaussian_case\n",
    "                log_prob_vec.append(tf.expand_dims(-(x[:, cum_dims] - y[:, cum_dims])**2, 1))\n",
    "                \n",
    "                cum_dims += 1\n",
    "\n",
    "            elif dims > 1:\n",
    "                # if x.shape[1] == y.shape[1]:\n",
    "                #    raise Exception('Input and target seem to be in flat format. Need integer cat targets.'\n",
    "\n",
    "                cce = tf.keras.losses.CategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE, from_logits = True)\n",
    "                cat_cross_entropy = -cce(tf.cast(y[:, cum_dims:cum_dims + dims], dtype=tf.int64), x[:, cum_dims:cum_dims + dims])\n",
    "                #cat_cross_entropy = -tf.nn.softmax_cross_entropy_with_logits(labels=tf.cast(y[:, cum_dims:cum_dims + dims], dtype=tf.int64), logits=x[:, cum_dims:cum_dims + dims])\n",
    "                log_prob_vec.append(tf.expand_dims(cat_cross_entropy, 1))\n",
    "                cum_dims += dims\n",
    "                \n",
    "            else:\n",
    "                raise ValueError('Error, invalid dimension value')\n",
    "        \n",
    "        log_prob_vec = tf.reshape(log_prob_vec, [reshape_dim, len(self.input_dim_vec)])\n",
    "        log_prob_vec = tf.reduce_sum(log_prob_vec, axis= -1)     # Do I want this? \n",
    "                                                                 # Yes vlb in original code does this when return\n",
    "        return log_prob_vec\n",
    "\n",
    "    \"\"\"\n",
    "    def generate_samples_params(self, inputs, masks, sample=1):\n",
    "\n",
    "        #Takes a model and \n",
    "\n",
    "        # (batch_size, width, height, channels)\n",
    "        observed_inputs = self.make_observed_inputs(inputs, masks)\n",
    "        # (batch_size, width, height, 2*channels)\n",
    "        observed_inputs_with_masks = tf.concat([observed_inputs, masks], axis=-1)\n",
    "\n",
    "        prior_params = self.prior_net(observed_inputs_with_masks)\n",
    "\n",
    "        prior_distribution = tfd.Normal(\n",
    "          loc=prior_params[..., :256],\n",
    "          scale=tf.clip_by_value(\n",
    "            tf.nn.softplus(prior_params[..., 256:]),\n",
    "            1e-3,\n",
    "            tf.float32.max),\n",
    "          name=\"priors\")\n",
    "\n",
    "        samples_params = []\n",
    "        for i in range(sample):\n",
    "          latent = prior_distribution.sample()\n",
    "          sample_params = self.generative_net(latent)\n",
    "          samples_params.append(sample_params)\n",
    "        return tf.stack(samples_params, axis=1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO Implement correctly\n",
    "    \"\"\"\n",
    "    def generate_sample(self, sample=1):\n",
    "        if(self.proposal_distribution == None):\n",
    "            raise Exception('Network has no proposal distribution. Train it first')\n",
    "        else:\n",
    "            latent = proposal_distribution.sample()\n",
    "\n",
    "            generative_params = self.decoder(latent)\n",
    "        \n",
    "            return generative_params\n",
    "    \"\"\"\n",
    "def eval(model, x_batch, x_flat, x_masked, mask):\n",
    "\n",
    "    x_flat = tf.convert_to_tensor(x_flat)\n",
    "\n",
    "    prior_params = model.prior_encoder(x_masked)\n",
    "    #prior_params = model.prior_encoder(x_masked, training = True)\n",
    "    #prior_params = model.prior_encoder(x_masked, training = False)\n",
    "\n",
    "    proposal_params = model.recognition_encoder(x_flat)\n",
    "    #proposal_params = model.recognition_encoder(x_flat, training = True)\n",
    "    #proposal_params = model.recognition_encoder(x_flat, training = False)\n",
    "\n",
    "    # Essentially CLUEs normal_parse_params\n",
    "    prior_distribution = tfd.Normal(\n",
    "      loc=prior_params[..., :model.latent_dim],\n",
    "      scale=tf.clip_by_value(\n",
    "        tf.nn.softplus(prior_params[..., model.latent_dim:]),\n",
    "        1e-3,\n",
    "        tf.float32.max),\n",
    "      name=\"priors\")\n",
    "\n",
    "    proposal_distribution = tfd.Normal(\n",
    "      loc=proposal_params[..., :model.latent_dim],\n",
    "      scale=tf.clip_by_value(\n",
    "        tf.nn.softplus(proposal_params[..., model.latent_dim:]),\n",
    "        1e-3,\n",
    "        tf.float32.max),\n",
    "      name=\"proposal\")\n",
    "\n",
    "    z_sample = proposal_distribution.loc\n",
    "\n",
    "    rec_params = model.decoder(z_sample)\n",
    "    #rec_params = model.decoder(z_sample, training = True)\n",
    "    #rec_params = model.decoder(z_sample, training = False)\n",
    "    \n",
    "    regularizer = model.prior_regularizer(prior_distribution)\n",
    "\n",
    "    rec_loss = model.reconstruction_loss(rec_params, x_flat)\n",
    "\n",
    "    # True distribution, Estimated distribution in this order\n",
    "    kl_divergence = tf.reduce_sum(\n",
    "      tf.reshape(\n",
    "        tfd.kl_divergence(proposal_distribution, prior_distribution),\n",
    "        (model.batch_size, -1)), -1)\n",
    "\n",
    "    vlb = tf.reduce_mean(-kl_divergence + rec_loss + regularizer) # For comparing\n",
    "    return vlb, kl_divergence, rec_loss, regularizer\n",
    "\n",
    "def compute_loss_VAEAC(model, x_batch, x_flat, x_masked, mask):\n",
    "    \n",
    "    prior_params = model.prior_encoder(x_masked) \n",
    "    proposal_params = model.recognition_encoder(x_flat)\n",
    "    #prior_params = model.prior_encoder(x_masked, training = True) \n",
    "    #proposal_params = model.recognition_encoder(x_flat, training = True)\n",
    "    #prior_params = model.prior_encoder(x_masked, training = False) \n",
    "    #proposal_params = model.recognition_encoder(x_flat, training = False)\n",
    "\n",
    "    # Essentially CLUEs normal_parse_params\n",
    "    proposal_distribution = tfd.Normal(\n",
    "      loc=proposal_params[..., :model.latent_dim],\n",
    "      scale=tf.clip_by_value(\n",
    "        tf.nn.softplus(proposal_params[..., model.latent_dim:]),\n",
    "        1e-3,\n",
    "        tf.float32.max),\n",
    "      name=\"proposal\")\n",
    "\n",
    "    prior_distribution = tfd.Normal(\n",
    "      loc=prior_params[..., :model.latent_dim],\n",
    "      scale=tf.clip_by_value(\n",
    "        tf.nn.softplus(prior_params[..., model.latent_dim:]),\n",
    "        1e-3,\n",
    "        tf.float32.max),\n",
    "      name=\"priors\")\n",
    "\n",
    "    regularizer = model.prior_regularizer(prior_distribution)\n",
    "\n",
    "    latent = proposal_distribution.sample()\n",
    "\n",
    "    generative_params = model.decoder(latent)\n",
    "    #generative_params = model.decoder(latent, training = True)\n",
    "    #generative_params = model.decoder(latent, training = False)\n",
    "    \n",
    "    rec_loss = model.reconstruction_loss(generative_params, x_flat)\n",
    "    \n",
    "    # True distribution, Estimated distribution in this order\n",
    "    kl_divergence = tf.reduce_sum(\n",
    "      tf.reshape(\n",
    "        tfd.kl_divergence(proposal_distribution, prior_distribution),\n",
    "        (model.batch_size, -1)), -1)\n",
    "\n",
    "    vlb = tf.reduce_mean(-kl_divergence + rec_loss + regularizer) # For comparing\n",
    "    loss = tf.reduce_mean((kl_divergence - rec_loss - regularizer) * model.vlb_scale) \n",
    "    return loss, vlb, kl_divergence, rec_loss, regularizer\n",
    "\n",
    "@tf.function # Converts all numpy arrays to tensors\n",
    "def train_step_VAEAC(model, x_batch, x_flat, x_masked, mask):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, vlb, kl_divergence, rec_loss, regularizer = compute_loss_VAEAC(model, x_batch, x_flat, x_masked, mask)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, vlb, kl_divergence, rec_loss, regularizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6d5c7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "def train_VAEAC(model, x_train, x_test, masker, nb_epochs, early_stop = None, flatten=False):\n",
    "    \n",
    "    vlb_train = np.zeros(nb_epochs)\n",
    "    vlb_val = np.zeros(nb_epochs)\n",
    "    best_vlb = -np.inf\n",
    "    best_epoch = 0\n",
    "    \n",
    "    overall_batch_size = model.batch_size\n",
    "    \n",
    "    test_data = []\n",
    "    for x in batch(x_test, n = overall_batch_size):\n",
    "        test_data.append(x)\n",
    "    \n",
    "    epoch = 0\n",
    "    for epoch in range(0, nb_epochs):\n",
    "        \n",
    "        # Shuffle the training data and sort it into batches every epoch\n",
    "        train_data = []\n",
    "        np.random.shuffle(x_train)\n",
    "        for x in batch(x_train, n = overall_batch_size):\n",
    "            train_data.append(x)\n",
    "        \n",
    "        tic = time.time()\n",
    "\n",
    "        ## Training\n",
    "        nb_samples = 0\n",
    "        \n",
    "        for x_batch in train_data:\n",
    "\n",
    "            model.batch_size = x_batch.shape[0] # TODO: FIX THIS: Very ugly solution now to make sure batches \n",
    "                                                # that do not have the full size\n",
    "                \n",
    "            mask = masker(x_batch) #tensor with floats\n",
    "            \n",
    "            # If data is not already flattened (default credit)\n",
    "            if flatten:\n",
    "                x_batch_flat = gauss_cat_to_flat(x_batch, model.input_dim_vec) # numpy\n",
    "                mask_flat = gauss_cat_to_flat_mask(mask, model.input_dim_vec)\n",
    "            \n",
    "            # If data is already flattened (COMPAS from join_compas_targets)\n",
    "            else:\n",
    "                x_batch_flat = x_batch\n",
    "                mask_flat = mask\n",
    "\n",
    "            # Mask flattened batch\n",
    "            x_batch_flat_masked = model.apply_mask(tf.convert_to_tensor(x_batch_flat), mask_flat)\n",
    "            \n",
    "            # Concat the mask flattened batch with the flattened mask\n",
    "            x_batch_flat_masked_concat = tf.concat([x_batch_flat_masked, mask_flat], axis=1)\n",
    "            \n",
    "            loss, vlb, kl_divergence, rec_loss, regularizer = train_step_VAEAC(model, x_batch, x_batch_flat, x_batch_flat_masked_concat, mask_flat)\n",
    "\n",
    "            vlb_train[epoch] += vlb.numpy() * x_batch.shape[0]\n",
    "            nb_samples += x_batch.shape[0]\n",
    "\n",
    "        vlb_train[epoch] /= nb_samples\n",
    "        toc = time.time()\n",
    "        print(\"Epoch_\" + str(epoch) + \", vlb: \" + str(vlb_train[epoch]) + \", took: \" + str(toc-tic))\n",
    "        \n",
    "        ## Validation\n",
    "        nb_samples = 0\n",
    "        for x_batch in test_data:\n",
    "            model.batch_size = x_batch.shape[0] # TODO: FIX THIS: Very ugly solution now to make sure batches \n",
    "                                                # that do not have the full size\n",
    "\n",
    "            mask = masker(x_batch) #tensor with floats\n",
    "            \n",
    "            # If data is not already flattened (default credit)\n",
    "            if flatten:\n",
    "                x_batch_flat = gauss_cat_to_flat(x_batch, model.input_dim_vec) # numpy\n",
    "                mask_flat = gauss_cat_to_flat_mask(mask, model.input_dim_vec)\n",
    "            \n",
    "            # If data is already flattened (COMPAS from join_compas_targets)\n",
    "            else:\n",
    "                x_batch_flat = x_batch\n",
    "                mask_flat = mask\n",
    "            \n",
    "            # Mask flattened batch\n",
    "            x_batch_flat_masked = model.apply_mask(tf.convert_to_tensor(x_batch_flat), mask_flat)\n",
    "            \n",
    "            # Concat the mask flattened batch with the flattened mask\n",
    "            x_batch_flat_masked_concat = tf.concat([x_batch_flat_masked, mask_flat], axis=1)\n",
    "            \n",
    "            vlb, kl_divergence, rec_loss, regularizer = eval(model, x_batch, x_batch_flat, x_batch_flat_masked_concat, mask_flat)\n",
    "\n",
    "            vlb_val[epoch] += vlb.numpy() * x_batch.shape[0]\n",
    "            nb_samples += x_batch.shape[0]\n",
    "\n",
    "        vlb_val[epoch] /= nb_samples\n",
    "    \n",
    "        \n",
    "        if vlb_val[epoch] > best_vlb:\n",
    "            best_vlb = vlb_val[epoch]\n",
    "            best_epoch = epoch\n",
    "            if(model.save_model):\n",
    "                \n",
    "                #open text file\n",
    "                text_file = open(str(dname) + \"_best_epoch_VAEAC_lr_\" + str(model.lr) + \".txt\", \"w\")\n",
    "\n",
    "                #write string to file\n",
    "                text_file.write(str(epoch))\n",
    "\n",
    "                #close file\n",
    "                text_file.close()\n",
    "\n",
    "                model.recognition_encoder.save(str(dname) + \"_recog_encoder_lr_\" + str(model.lr))\n",
    "                model.prior_encoder.save(str(dname) + \"_prior_encoder_lr_\" + str(model.lr))\n",
    "                model.decoder.save(str(dname) + \"_decoder_lr_\" + str(model.lr))\n",
    "\n",
    "        print(\"Validation vlb: \" + str(vlb_val[epoch]) + \", Best vlb: \" + str(best_vlb) + \"\\n\")\n",
    "\n",
    "        if early_stop is not None and (epoch - best_epoch) > early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    \n",
    "    np.save(str(dname) + \"_vlb_train_lr_\" + str(model.lr), vlb_train)\n",
    "    np.save(str(dname) + \"_vlb_val_lr_\" + str(model.lr), vlb_val)\n",
    "    return vlb_train, vlb_val, best_epoch, best_vlb, epoch\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62a7519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compas (5554, 19) (618, 19)\n",
      "[3 6 2 2 2 1 1 2]\n",
      "compas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow_addons/optimizers/rectified_adam.py:120: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "2021-12-11 11:38:20.661557: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob vec:  [<tf.Tensor 'ExpandDims:0' shape=(128, 1) dtype=float32>, <tf.Tensor 'ExpandDims_1:0' shape=(128, 1) dtype=float32>, <tf.Tensor 'ExpandDims_2:0' shape=(128, 1) dtype=float32>, <tf.Tensor 'ExpandDims_3:0' shape=(128, 1) dtype=float32>, <tf.Tensor 'ExpandDims_4:0' shape=(128, 1) dtype=float32>, <tf.Tensor 'ExpandDims_5:0' shape=(128, 1) dtype=float32>, <tf.Tensor 'ExpandDims_6:0' shape=(128, 1) dtype=float32>, <tf.Tensor 'ExpandDims_7:0' shape=(128, 1) dtype=float32>]\n",
      "log prob vec:  [<tf.Tensor 'ExpandDims:0' shape=(128, 1) dtype=float32>, <tf.Tensor 'ExpandDims_1:0' shape=(128, 1) dtype=float32>, <tf.Tensor 'ExpandDims_2:0' shape=(128, 1) dtype=float32>, <tf.Tensor 'ExpandDims_3:0' shape=(128, 1) dtype=float32>, <tf.Tensor 'ExpandDims_4:0' shape=(128, 1) dtype=float32>, <tf.Tensor 'ExpandDims_5:0' shape=(128, 1) dtype=float32>, <tf.Tensor 'ExpandDims_6:0' shape=(128, 1) dtype=float32>, <tf.Tensor 'ExpandDims_7:0' shape=(128, 1) dtype=float32>]\n",
      "log prob vec:  [<tf.Tensor 'ExpandDims:0' shape=(50, 1) dtype=float32>, <tf.Tensor 'ExpandDims_1:0' shape=(50, 1) dtype=float32>, <tf.Tensor 'ExpandDims_2:0' shape=(50, 1) dtype=float32>, <tf.Tensor 'ExpandDims_3:0' shape=(50, 1) dtype=float32>, <tf.Tensor 'ExpandDims_4:0' shape=(50, 1) dtype=float32>, <tf.Tensor 'ExpandDims_5:0' shape=(50, 1) dtype=float32>, <tf.Tensor 'ExpandDims_6:0' shape=(50, 1) dtype=float32>, <tf.Tensor 'ExpandDims_7:0' shape=(50, 1) dtype=float32>]\n",
      "Epoch_0, vlb: -8.515166342752119, took: 15.804718017578125\n",
      "log prob vec:  [<tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-0.8754162 ],\n",
      "       [-0.90110743],\n",
      "       [-0.9142881 ],\n",
      "       [-1.0764605 ],\n",
      "       [-1.0796194 ],\n",
      "       [-0.8943868 ],\n",
      "       [-0.875224  ],\n",
      "       [-0.88010293],\n",
      "       [-0.90269685],\n",
      "       [-0.88516206],\n",
      "       [-1.0469288 ],\n",
      "       [-0.9045379 ],\n",
      "       [-1.071179  ],\n",
      "       [-0.91555655],\n",
      "       [-1.0489302 ],\n",
      "       [-1.3824198 ],\n",
      "       [-1.3184426 ],\n",
      "       [-1.4227124 ],\n",
      "       [-1.0577443 ],\n",
      "       [-0.8914508 ],\n",
      "       [-0.9141191 ],\n",
      "       [-0.9800455 ],\n",
      "       [-1.057982  ],\n",
      "       [-1.3800731 ],\n",
      "       [-1.4400542 ],\n",
      "       [-1.3762097 ],\n",
      "       [-1.0788398 ],\n",
      "       [-0.885551  ],\n",
      "       [-0.9175621 ],\n",
      "       [-0.90273935],\n",
      "       [-1.0809406 ],\n",
      "       [-1.0965478 ],\n",
      "       [-0.89978194],\n",
      "       [-0.9169968 ],\n",
      "       [-0.9156388 ],\n",
      "       [-0.9066468 ],\n",
      "       [-1.3682269 ],\n",
      "       [-0.91697496],\n",
      "       [-1.0765593 ],\n",
      "       [-0.9010502 ],\n",
      "       [-1.366183  ],\n",
      "       [-0.9156388 ],\n",
      "       [-0.9175621 ],\n",
      "       [-1.0691996 ],\n",
      "       [-1.393031  ],\n",
      "       [-0.87643987],\n",
      "       [-0.8730927 ],\n",
      "       [-0.87810063],\n",
      "       [-0.9096391 ],\n",
      "       [-0.8929627 ],\n",
      "       [-1.3645945 ],\n",
      "       [-0.9087399 ],\n",
      "       [-0.90093005],\n",
      "       [-0.8973405 ],\n",
      "       [-0.9291935 ],\n",
      "       [-1.0706294 ],\n",
      "       [-0.90411633],\n",
      "       [-0.8622986 ],\n",
      "       [-0.8934767 ],\n",
      "       [-1.06383   ],\n",
      "       [-1.0569501 ],\n",
      "       [-1.0788205 ],\n",
      "       [-0.8562343 ],\n",
      "       [-1.0571668 ],\n",
      "       [-1.388701  ],\n",
      "       [-0.89853835],\n",
      "       [-0.9151869 ],\n",
      "       [-1.0739244 ],\n",
      "       [-0.88255703],\n",
      "       [-0.9011329 ],\n",
      "       [-0.88993204],\n",
      "       [-0.8948302 ],\n",
      "       [-0.8957012 ],\n",
      "       [-0.90193725],\n",
      "       [-0.8888046 ],\n",
      "       [-0.87169856],\n",
      "       [-1.0445766 ],\n",
      "       [-0.8917474 ],\n",
      "       [-0.90298355],\n",
      "       [-0.9167702 ],\n",
      "       [-0.94425046],\n",
      "       [-0.9000486 ],\n",
      "       [-1.3826226 ],\n",
      "       [-1.074302  ],\n",
      "       [-0.88825786],\n",
      "       [-0.90160346],\n",
      "       [-1.0625994 ],\n",
      "       [-1.0336035 ],\n",
      "       [-1.3894517 ],\n",
      "       [-1.3514109 ],\n",
      "       [-0.8804252 ],\n",
      "       [-0.8928332 ],\n",
      "       [-0.92862904],\n",
      "       [-0.85263205],\n",
      "       [-0.8943868 ],\n",
      "       [-0.89212155],\n",
      "       [-0.897593  ],\n",
      "       [-0.87733567],\n",
      "       [-0.90377754],\n",
      "       [-0.9000486 ],\n",
      "       [-0.91819227],\n",
      "       [-0.8917303 ],\n",
      "       [-0.9224504 ],\n",
      "       [-0.9167702 ],\n",
      "       [-1.0563684 ],\n",
      "       [-0.89889306],\n",
      "       [-1.0572578 ],\n",
      "       [-0.89889306],\n",
      "       [-1.0529647 ],\n",
      "       [-1.3681674 ],\n",
      "       [-1.0824041 ],\n",
      "       [-0.8994897 ],\n",
      "       [-0.90010875],\n",
      "       [-0.92393804],\n",
      "       [-0.904619  ],\n",
      "       [-0.91605675],\n",
      "       [-0.904619  ],\n",
      "       [-1.3563052 ],\n",
      "       [-1.4074094 ],\n",
      "       [-0.9139194 ],\n",
      "       [-1.3769977 ],\n",
      "       [-0.9024277 ],\n",
      "       [-0.90003234],\n",
      "       [-1.075411  ],\n",
      "       [-0.91697496],\n",
      "       [-0.87874335],\n",
      "       [-0.8917332 ],\n",
      "       [-0.90152967]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-1.6789439],\n",
      "       [-1.8024325],\n",
      "       [-1.6539469],\n",
      "       [-1.8000913],\n",
      "       [-1.8000666],\n",
      "       [-1.6581612],\n",
      "       [-1.7283888],\n",
      "       [-1.7548969],\n",
      "       [-1.693095 ],\n",
      "       [-1.7633432],\n",
      "       [-1.6742197],\n",
      "       [-1.6844357],\n",
      "       [-1.6909668],\n",
      "       [-1.6479504],\n",
      "       [-1.7714802],\n",
      "       [-1.6891947],\n",
      "       [-1.6845999],\n",
      "       [-1.7127212],\n",
      "       [-1.709035 ],\n",
      "       [-1.8528006],\n",
      "       [-1.8075508],\n",
      "       [-1.6405149],\n",
      "       [-1.7090523],\n",
      "       [-1.7675693],\n",
      "       [-1.6787875],\n",
      "       [-1.678456 ],\n",
      "       [-1.7035797],\n",
      "       [-1.7433442],\n",
      "       [-1.7727526],\n",
      "       [-1.6904839],\n",
      "       [-1.7433501],\n",
      "       [-1.7841461],\n",
      "       [-1.7860692],\n",
      "       [-1.6623619],\n",
      "       [-1.8114784],\n",
      "       [-1.6606362],\n",
      "       [-1.8082247],\n",
      "       [-1.8151855],\n",
      "       [-1.6622602],\n",
      "       [-1.804633 ],\n",
      "       [-1.7590653],\n",
      "       [-1.8114784],\n",
      "       [-1.7727526],\n",
      "       [-1.8109887],\n",
      "       [-1.7492102],\n",
      "       [-1.7649329],\n",
      "       [-1.7047225],\n",
      "       [-1.7088524],\n",
      "       [-1.6563568],\n",
      "       [-1.8603766],\n",
      "       [-1.8060356],\n",
      "       [-1.782515 ],\n",
      "       [-1.6842674],\n",
      "       [-1.6844833],\n",
      "       [-1.6590459],\n",
      "       [-1.6855973],\n",
      "       [-1.651437 ],\n",
      "       [-1.6796291],\n",
      "       [-1.6823766],\n",
      "       [-1.6894506],\n",
      "       [-1.7079548],\n",
      "       [-1.6809801],\n",
      "       [-1.7035832],\n",
      "       [-1.7092047],\n",
      "       [-1.6925095],\n",
      "       [-1.6836016],\n",
      "       [-1.65328  ],\n",
      "       [-1.7718692],\n",
      "       [-1.6779735],\n",
      "       [-1.8191248],\n",
      "       [-1.7633802],\n",
      "       [-1.6579082],\n",
      "       [-1.6542702],\n",
      "       [-1.7921268],\n",
      "       [-1.7659686],\n",
      "       [-1.8923128],\n",
      "       [-1.6712708],\n",
      "       [-1.7573261],\n",
      "       [-1.6892809],\n",
      "       [-1.6625855],\n",
      "       [-1.6745871],\n",
      "       [-1.6641973],\n",
      "       [-1.7129086],\n",
      "       [-1.688729 ],\n",
      "       [-1.6905966],\n",
      "       [-1.6620685],\n",
      "       [-1.6898023],\n",
      "       [-1.7198707],\n",
      "       [-1.7499379],\n",
      "       [-1.6813673],\n",
      "       [-1.6435888],\n",
      "       [-1.6877425],\n",
      "       [-1.8502198],\n",
      "       [-1.7204874],\n",
      "       [-1.6581612],\n",
      "       [-1.6770501],\n",
      "       [-1.7749059],\n",
      "       [-1.7107289],\n",
      "       [-1.6760465],\n",
      "       [-1.6641973],\n",
      "       [-1.8182975],\n",
      "       [-1.757288 ],\n",
      "       [-1.6557069],\n",
      "       [-1.6625855],\n",
      "       [-1.7044448],\n",
      "       [-1.7752054],\n",
      "       [-1.7054673],\n",
      "       [-1.7752054],\n",
      "       [-1.7907754],\n",
      "       [-1.691181 ],\n",
      "       [-1.6573128],\n",
      "       [-1.6830695],\n",
      "       [-1.7912949],\n",
      "       [-1.8359027],\n",
      "       [-1.8076491],\n",
      "       [-1.6469604],\n",
      "       [-1.8076491],\n",
      "       [-1.7638944],\n",
      "       [-1.7100494],\n",
      "       [-1.8075826],\n",
      "       [-1.7853822],\n",
      "       [-1.800086 ],\n",
      "       [-1.6641669],\n",
      "       [-1.7541891],\n",
      "       [-1.8151855],\n",
      "       [-1.7089143],\n",
      "       [-1.6569867],\n",
      "       [-1.8059812]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-0.64005965],\n",
      "       [-0.6152295 ],\n",
      "       [-0.61071455],\n",
      "       [-0.6114253 ],\n",
      "       [-0.78667414],\n",
      "       [-0.75678605],\n",
      "       [-0.6385834 ],\n",
      "       [-0.6231018 ],\n",
      "       [-0.61555344],\n",
      "       [-0.7566347 ],\n",
      "       [-0.58568794],\n",
      "       [-0.62709564],\n",
      "       [-0.61535585],\n",
      "       [-0.616717  ],\n",
      "       [-0.58307636],\n",
      "       [-0.5858499 ],\n",
      "       [-0.5310066 ],\n",
      "       [-0.62160313],\n",
      "       [-0.5923268 ],\n",
      "       [-0.62432986],\n",
      "       [-0.5937394 ],\n",
      "       [-0.4558252 ],\n",
      "       [-0.59219915],\n",
      "       [-0.7689847 ],\n",
      "       [-0.6359928 ],\n",
      "       [-0.7839393 ],\n",
      "       [-0.7786791 ],\n",
      "       [-0.75410795],\n",
      "       [-0.5891049 ],\n",
      "       [-0.6173477 ],\n",
      "       [-0.63150996],\n",
      "       [-0.6189301 ],\n",
      "       [-0.62236935],\n",
      "       [-0.6021258 ],\n",
      "       [-0.59353757],\n",
      "       [-0.61182404],\n",
      "       [-0.5849404 ],\n",
      "       [-0.5935644 ],\n",
      "       [-0.7921682 ],\n",
      "       [-0.6231376 ],\n",
      "       [-0.7700828 ],\n",
      "       [-0.59353757],\n",
      "       [-0.5891049 ],\n",
      "       [-0.59111446],\n",
      "       [-0.60553825],\n",
      "       [-0.6298134 ],\n",
      "       [-0.6478663 ],\n",
      "       [-0.63868064],\n",
      "       [-0.6339812 ],\n",
      "       [-0.62347734],\n",
      "       [-0.5846477 ],\n",
      "       [-0.61472553],\n",
      "       [-0.6227954 ],\n",
      "       [-0.62537926],\n",
      "       [-0.61005014],\n",
      "       [-0.6134765 ],\n",
      "       [-0.6197675 ],\n",
      "       [-0.7342687 ],\n",
      "       [-0.62727016],\n",
      "       [-0.57781506],\n",
      "       [-0.5933881 ],\n",
      "       [-0.7544452 ],\n",
      "       [-0.73216826],\n",
      "       [-0.6028526 ],\n",
      "       [-0.61632764],\n",
      "       [-0.6248011 ],\n",
      "       [-0.6103741 ],\n",
      "       [-0.61298686],\n",
      "       [-0.74585885],\n",
      "       [-0.61596715],\n",
      "       [-0.62114644],\n",
      "       [-0.7564876 ],\n",
      "       [-0.7508321 ],\n",
      "       [-0.77563775],\n",
      "       [-0.6213333 ],\n",
      "       [-0.74297243],\n",
      "       [-0.5879264 ],\n",
      "       [-0.62243307],\n",
      "       [-0.6181923 ],\n",
      "       [-0.60219437],\n",
      "       [-0.5896936 ],\n",
      "       [-0.6169703 ],\n",
      "       [-0.7773097 ],\n",
      "       [-0.6165577 ],\n",
      "       [-0.6335775 ],\n",
      "       [-0.61458015],\n",
      "       [-0.57748246],\n",
      "       [-0.59244657],\n",
      "       [-0.6016969 ],\n",
      "       [-0.5526967 ],\n",
      "       [-0.72957516],\n",
      "       [-0.62965435],\n",
      "       [-0.5956653 ],\n",
      "       [-0.7343588 ],\n",
      "       [-0.75678605],\n",
      "       [-0.6273552 ],\n",
      "       [-0.6252583 ],\n",
      "       [-0.74517846],\n",
      "       [-0.62370855],\n",
      "       [-0.6169703 ],\n",
      "       [-0.5946578 ],\n",
      "       [-0.6225591 ],\n",
      "       [-0.60439265],\n",
      "       [-0.60219437],\n",
      "       [-0.5960996 ],\n",
      "       [-0.62373716],\n",
      "       [-0.593748  ],\n",
      "       [-0.62373716],\n",
      "       [-0.60466707],\n",
      "       [-0.5765118 ],\n",
      "       [-0.7763956 ],\n",
      "       [-0.62498707],\n",
      "       [-0.6215504 ],\n",
      "       [-0.8129274 ],\n",
      "       [-0.61335796],\n",
      "       [-0.62016916],\n",
      "       [-0.61335796],\n",
      "       [-0.6121611 ],\n",
      "       [-0.6234734 ],\n",
      "       [-0.59431714],\n",
      "       [-0.57943225],\n",
      "       [-0.7795368 ],\n",
      "       [-0.61693007],\n",
      "       [-0.6241683 ],\n",
      "       [-0.5935644 ],\n",
      "       [-0.6383833 ],\n",
      "       [-0.7516082 ],\n",
      "       [-0.6174732 ]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-0.65690345],\n",
      "       [-0.6844347 ],\n",
      "       [-0.702866  ],\n",
      "       [-0.68095315],\n",
      "       [-0.7307026 ],\n",
      "       [-0.7341621 ],\n",
      "       [-0.625635  ],\n",
      "       [-0.7320742 ],\n",
      "       [-0.70247173],\n",
      "       [-0.6509099 ],\n",
      "       [-0.702316  ],\n",
      "       [-0.72162336],\n",
      "       [-0.6780578 ],\n",
      "       [-0.7129297 ],\n",
      "       [-0.6856124 ],\n",
      "       [-0.6898481 ],\n",
      "       [-0.7723815 ],\n",
      "       [-0.6602601 ],\n",
      "       [-0.70037746],\n",
      "       [-0.6427935 ],\n",
      "       [-0.7008977 ],\n",
      "       [-0.8672371 ],\n",
      "       [-0.70053124],\n",
      "       [-0.6566663 ],\n",
      "       [-0.67730916],\n",
      "       [-0.72108406],\n",
      "       [-0.71984977],\n",
      "       [-0.64540416],\n",
      "       [-0.6782932 ],\n",
      "       [-0.7033845 ],\n",
      "       [-0.6453104 ],\n",
      "       [-0.66298836],\n",
      "       [-0.67390305],\n",
      "       [-0.7007055 ],\n",
      "       [-0.6999776 ],\n",
      "       [-0.674265  ],\n",
      "       [-0.697705  ],\n",
      "       [-0.7003197 ],\n",
      "       [-0.72827995],\n",
      "       [-0.6816245 ],\n",
      "       [-0.65516424],\n",
      "       [-0.6999776 ],\n",
      "       [-0.6782932 ],\n",
      "       [-0.70610535],\n",
      "       [-0.7381411 ],\n",
      "       [-0.7348287 ],\n",
      "       [-0.7240634 ],\n",
      "       [-0.616817  ],\n",
      "       [-0.71453923],\n",
      "       [-0.6410799 ],\n",
      "       [-0.7017616 ],\n",
      "       [-0.69605166],\n",
      "       [-0.7093226 ],\n",
      "       [-0.7157825 ],\n",
      "       [-0.7308712 ],\n",
      "       [-0.6768041 ],\n",
      "       [-0.6868769 ],\n",
      "       [-0.59392834],\n",
      "       [-0.6570378 ],\n",
      "       [-0.7043116 ],\n",
      "       [-0.70128083],\n",
      "       [-0.6600204 ],\n",
      "       [-0.59958905],\n",
      "       [-0.7115384 ],\n",
      "       [-0.62370443],\n",
      "       [-0.7142019 ],\n",
      "       [-0.7045701 ],\n",
      "       [-0.7270201 ],\n",
      "       [-0.7429907 ],\n",
      "       [-0.7124824 ],\n",
      "       [-0.7315951 ],\n",
      "       [-0.73484915],\n",
      "       [-0.65680647],\n",
      "       [-0.71100473],\n",
      "       [-0.72974616],\n",
      "       [-0.60589904],\n",
      "       [-0.70516646],\n",
      "       [-0.7341897 ],\n",
      "       [-0.70605713],\n",
      "       [-0.70005465],\n",
      "       [-0.74482214],\n",
      "       [-0.6785685 ],\n",
      "       [-0.73517126],\n",
      "       [-0.6758566 ],\n",
      "       [-0.7210052 ],\n",
      "       [-0.6788567 ],\n",
      "       [-0.7034084 ],\n",
      "       [-0.77196735],\n",
      "       [-0.73711807],\n",
      "       [-0.7895786 ],\n",
      "       [-0.64572716],\n",
      "       [-0.64814156],\n",
      "       [-0.77233756],\n",
      "       [-0.6116606 ],\n",
      "       [-0.7341621 ],\n",
      "       [-0.66466343],\n",
      "       [-0.66736287],\n",
      "       [-0.76599514],\n",
      "       [-0.7181375 ],\n",
      "       [-0.6785685 ],\n",
      "       [-0.70153564],\n",
      "       [-0.7342363 ],\n",
      "       [-0.71531713],\n",
      "       [-0.70005465],\n",
      "       [-0.70719486],\n",
      "       [-0.682675  ],\n",
      "       [-0.70611274],\n",
      "       [-0.682675  ],\n",
      "       [-0.7114471 ],\n",
      "       [-0.6939858 ],\n",
      "       [-0.65363353],\n",
      "       [-0.7148418 ],\n",
      "       [-0.67637265],\n",
      "       [-0.77753264],\n",
      "       [-0.691102  ],\n",
      "       [-0.717416  ],\n",
      "       [-0.691102  ],\n",
      "       [-0.6693401 ],\n",
      "       [-0.66374075],\n",
      "       [-0.7017672 ],\n",
      "       [-0.681231  ],\n",
      "       [-0.7402354 ],\n",
      "       [-0.6781276 ],\n",
      "       [-0.6514908 ],\n",
      "       [-0.7003197 ],\n",
      "       [-0.61758727],\n",
      "       [-0.6543871 ],\n",
      "       [-0.6804767 ]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-0.6889955 ],\n",
      "       [-0.6082301 ],\n",
      "       [-0.62338257],\n",
      "       [-0.59612846],\n",
      "       [-0.61557716],\n",
      "       [-0.62810576],\n",
      "       [-0.7051406 ],\n",
      "       [-0.70556754],\n",
      "       [-0.76849073],\n",
      "       [-0.6356169 ],\n",
      "       [-0.6028352 ],\n",
      "       [-0.728265  ],\n",
      "       [-0.7651297 ],\n",
      "       [-0.6360955 ],\n",
      "       [-0.6042683 ],\n",
      "       [-0.61153746],\n",
      "       [-0.57445776],\n",
      "       [-0.7145251 ],\n",
      "       [-0.77315724],\n",
      "       [-0.7391938 ],\n",
      "       [-0.60670054],\n",
      "       [-0.8166456 ],\n",
      "       [-0.7735272 ],\n",
      "       [-0.64441967],\n",
      "       [-0.67830163],\n",
      "       [-0.6291132 ],\n",
      "       [-0.7625638 ],\n",
      "       [-0.62017375],\n",
      "       [-0.62039316],\n",
      "       [-0.7654805 ],\n",
      "       [-0.73307914],\n",
      "       [-0.5949596 ],\n",
      "       [-0.61198246],\n",
      "       [-0.61096877],\n",
      "       [-0.6097851 ],\n",
      "       [-0.61220866],\n",
      "       [-0.6136232 ],\n",
      "       [-0.6141525 ],\n",
      "       [-0.6068331 ],\n",
      "       [-0.63330203],\n",
      "       [-0.6191902 ],\n",
      "       [-0.6097851 ],\n",
      "       [-0.62039316],\n",
      "       [-0.605948  ],\n",
      "       [-0.72561806],\n",
      "       [-0.70596385],\n",
      "       [-0.681085  ],\n",
      "       [-0.7110661 ],\n",
      "       [-0.6698427 ],\n",
      "       [-0.74356467],\n",
      "       [-0.61315995],\n",
      "       [-0.6362452 ],\n",
      "       [-0.7493015 ],\n",
      "       [-0.7286193 ],\n",
      "       [-0.65025914],\n",
      "       [-0.7676462 ],\n",
      "       [-0.6238841 ],\n",
      "       [-0.7055559 ],\n",
      "       [-0.7468885 ],\n",
      "       [-0.80402446],\n",
      "       [-0.77154803],\n",
      "       [-0.7467034 ],\n",
      "       [-0.69253737],\n",
      "       [-0.75550944],\n",
      "       [-0.6996129 ],\n",
      "       [-0.7348007 ],\n",
      "       [-0.6231966 ],\n",
      "       [-0.7402839 ],\n",
      "       [-0.75815624],\n",
      "       [-0.75279254],\n",
      "       [-0.7287898 ],\n",
      "       [-0.630701  ],\n",
      "       [-0.63891643],\n",
      "       [-0.62267196],\n",
      "       [-0.72444797],\n",
      "       [-0.72263104],\n",
      "       [-0.60700595],\n",
      "       [-0.7347137 ],\n",
      "       [-0.76455164],\n",
      "       [-0.6111074 ],\n",
      "       [-0.6500319 ],\n",
      "       [-0.60429883],\n",
      "       [-0.7431984 ],\n",
      "       [-0.76620626],\n",
      "       [-0.709459  ],\n",
      "       [-0.6076243 ],\n",
      "       [-0.80386883],\n",
      "       [-0.7434683 ],\n",
      "       [-0.72983724],\n",
      "       [-0.776052  ],\n",
      "       [-0.772612  ],\n",
      "       [-0.7556443 ],\n",
      "       [-0.7289314 ],\n",
      "       [-0.6826205 ],\n",
      "       [-0.62810576],\n",
      "       [-0.736164  ],\n",
      "       [-0.60680306],\n",
      "       [-0.7264459 ],\n",
      "       [-0.7455194 ],\n",
      "       [-0.60429883],\n",
      "       [-0.6191334 ],\n",
      "       [-0.7347717 ],\n",
      "       [-0.62046105],\n",
      "       [-0.6111074 ],\n",
      "       [-0.7676537 ],\n",
      "       [-0.6285652 ],\n",
      "       [-0.7713549 ],\n",
      "       [-0.6285652 ],\n",
      "       [-0.7461518 ],\n",
      "       [-0.60021913],\n",
      "       [-0.61359644],\n",
      "       [-0.73534346],\n",
      "       [-0.616021  ],\n",
      "       [-0.62587535],\n",
      "       [-0.62453973],\n",
      "       [-0.6464285 ],\n",
      "       [-0.62453973],\n",
      "       [-0.6057229 ],\n",
      "       [-0.7205774 ],\n",
      "       [-0.6072291 ],\n",
      "       [-0.605205  ],\n",
      "       [-0.62836057],\n",
      "       [-0.6042564 ],\n",
      "       [-0.73018545],\n",
      "       [-0.6141525 ],\n",
      "       [-0.7123456 ],\n",
      "       [-0.62911713],\n",
      "       [-0.6041255 ]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-4.3257642e+00],\n",
      "       [-1.5971629e-01],\n",
      "       [-4.3028869e-02],\n",
      "       [-3.5886723e-01],\n",
      "       [-1.4509019e-01],\n",
      "       [-1.6733538e-01],\n",
      "       [-2.1255446e-04],\n",
      "       [-1.1342480e+00],\n",
      "       [-4.0706493e-02],\n",
      "       [-5.9119798e-04],\n",
      "       [-3.6341542e-01],\n",
      "       [-1.6086117e+00],\n",
      "       [-3.6976106e-02],\n",
      "       [-1.7560463e-01],\n",
      "       [-3.5812208e-01],\n",
      "       [-1.5221940e-01],\n",
      "       [-2.8563228e-01],\n",
      "       [-1.1313363e+00],\n",
      "       [-3.5444379e-01],\n",
      "       [-4.4015042e-02],\n",
      "       [-3.5512885e-01],\n",
      "       [-8.3634853e-01],\n",
      "       [-3.5408342e-01],\n",
      "       [-3.6482978e-01],\n",
      "       [-1.5882382e+01],\n",
      "       [-1.5839289e-01],\n",
      "       [-3.6597258e-01],\n",
      "       [-3.6104414e-01],\n",
      "       [-4.8939441e-04],\n",
      "       [-5.1760890e-05],\n",
      "       [-3.7715042e-01],\n",
      "       [-3.5521254e-01],\n",
      "       [-3.6067165e-02],\n",
      "       [-3.8338441e-01],\n",
      "       [-1.4952493e-01],\n",
      "       [-3.7135351e-02],\n",
      "       [-1.4070463e-01],\n",
      "       [-3.1162564e-02],\n",
      "       [-3.6184970e-01],\n",
      "       [-4.0048313e-01],\n",
      "       [-1.4772373e-01],\n",
      "       [-1.4952493e-01],\n",
      "       [-4.8939441e-04],\n",
      "       [-3.5052365e-01],\n",
      "       [-3.6869319e-03],\n",
      "       [-1.6586167e+00],\n",
      "       [-1.2776521e+01],\n",
      "       [-3.7425965e-01],\n",
      "       [-1.2699787e+01],\n",
      "       [-3.8660219e-01],\n",
      "       [-1.3705668e-01],\n",
      "       [-7.1311522e-01],\n",
      "       [-4.0373817e-01],\n",
      "       [-1.5870280e+00],\n",
      "       [-1.6167252e+00],\n",
      "       [-3.5204262e-02],\n",
      "       [-1.8114088e-01],\n",
      "       [-3.6907372e-01],\n",
      "       [-1.4995151e-05],\n",
      "       [-2.6378380e-02],\n",
      "       [-1.4812218e-01],\n",
      "       [-3.9726186e-01],\n",
      "       [-5.4124650e-02],\n",
      "       [-4.4594261e-01],\n",
      "       [-3.3081961e-01],\n",
      "       [-1.1066053e+00],\n",
      "       [-4.2822871e-02],\n",
      "       [-3.6009696e-01],\n",
      "       [-3.7974098e-01],\n",
      "       [-4.3889002e-05],\n",
      "       [-6.2374084e-04],\n",
      "       [-3.9857518e-02],\n",
      "       [-1.7767881e-01],\n",
      "       [-1.6335595e-01],\n",
      "       [-1.9964275e-01],\n",
      "       [-3.8226110e-01],\n",
      "       [-1.5559316e-01],\n",
      "       [-3.7151116e-01],\n",
      "       [-1.0077968e-04],\n",
      "       [-3.8371944e-01],\n",
      "       [-2.9077621e+00],\n",
      "       [-3.6987528e-01],\n",
      "       [-3.6130828e-01],\n",
      "       [-1.6071063e-01],\n",
      "       [-4.4091263e+00],\n",
      "       [-1.6019282e-01],\n",
      "       [-3.4199306e-01],\n",
      "       [-4.4032124e-01],\n",
      "       [-1.3150308e-01],\n",
      "       [-2.4326494e-01],\n",
      "       [-3.9665821e-01],\n",
      "       [-3.8763434e-01],\n",
      "       [-4.6251082e-01],\n",
      "       [-4.2846459e-01],\n",
      "       [-1.6733538e-01],\n",
      "       [-3.7617436e-01],\n",
      "       [-3.6861080e-01],\n",
      "       [-3.7448737e-01],\n",
      "       [-1.8518682e-01],\n",
      "       [-3.6987528e-01],\n",
      "       [-1.0938029e-03],\n",
      "       [-3.7173158e-01],\n",
      "       [-3.9423496e-02],\n",
      "       [-3.8371944e-01],\n",
      "       [-1.3854591e-03],\n",
      "       [-4.0862477e-01],\n",
      "       [-2.9550355e-02],\n",
      "       [-4.0862477e-01],\n",
      "       [-1.5313134e-01],\n",
      "       [-3.4741041e-01],\n",
      "       [-1.5816520e-01],\n",
      "       [-1.1109557e+00],\n",
      "       [-2.9599600e-04],\n",
      "       [-2.4999212e-02],\n",
      "       [-3.9140144e-01],\n",
      "       [-1.5841484e+00],\n",
      "       [-3.9140144e-01],\n",
      "       [-3.5424560e-01],\n",
      "       [-1.6495333e+00],\n",
      "       [-3.5536772e-01],\n",
      "       [-3.5250041e-01],\n",
      "       [-1.6356832e-03],\n",
      "       [-3.6950642e-01],\n",
      "       [-4.3711870e-04],\n",
      "       [-3.1162564e-02],\n",
      "       [-3.7422875e-01],\n",
      "       [-8.2313592e-05],\n",
      "       [-3.6726585e-01]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-8.08074251e-02],\n",
      "       [-9.36183780e-02],\n",
      "       [-1.50209650e-01],\n",
      "       [-1.11089207e-01],\n",
      "       [-1.31126493e-01],\n",
      "       [-1.41613156e-01],\n",
      "       [-1.16324797e-01],\n",
      "       [-1.79792732e-01],\n",
      "       [-1.07775882e-01],\n",
      "       [-2.12023165e-02],\n",
      "       [-1.39270857e-01],\n",
      "       [-5.26515543e-01],\n",
      "       [-1.03128217e-01],\n",
      "       [-2.77146399e-02],\n",
      "       [-1.31978959e-01],\n",
      "       [-1.26060665e-01],\n",
      "       [-1.85771675e+01],\n",
      "       [-1.67588726e-01],\n",
      "       [-1.43184707e-01],\n",
      "       [-4.22869287e-02],\n",
      "       [-9.78566036e-02],\n",
      "       [-1.56148407e+02],\n",
      "       [-1.27100453e-01],\n",
      "       [-1.23467520e-01],\n",
      "       [-2.05808431e-01],\n",
      "       [-1.53782800e-01],\n",
      "       [-1.27115384e-01],\n",
      "       [-1.13317683e-01],\n",
      "       [-1.45353034e-01],\n",
      "       [-1.25962034e-01],\n",
      "       [-7.74859712e-02],\n",
      "       [-9.75688025e-02],\n",
      "       [-1.12995431e-01],\n",
      "       [-1.23503439e-01],\n",
      "       [-1.16675615e-01],\n",
      "       [-5.85334226e-02],\n",
      "       [-9.63334367e-02],\n",
      "       [-1.38510108e-01],\n",
      "       [-1.07563555e-01],\n",
      "       [-1.33832246e-01],\n",
      "       [-7.68107548e-02],\n",
      "       [-1.16675615e-01],\n",
      "       [-1.45353034e-01],\n",
      "       [-1.02435395e-01],\n",
      "       [-1.24918900e-01],\n",
      "       [-1.90610558e-01],\n",
      "       [-1.77869454e-01],\n",
      "       [-9.92148593e-02],\n",
      "       [-1.84401944e-01],\n",
      "       [-8.90223682e-02],\n",
      "       [-2.79642991e-03],\n",
      "       [-1.74184546e-01],\n",
      "       [-1.26237333e-01],\n",
      "       [-1.43513113e-01],\n",
      "       [-6.28883123e-01],\n",
      "       [-1.18830524e-01],\n",
      "       [-1.28131419e-01],\n",
      "       [-7.54177794e-02],\n",
      "       [-8.52354020e-02],\n",
      "       [-5.91088273e-03],\n",
      "       [-1.45590335e-01],\n",
      "       [-1.25299647e-01],\n",
      "       [-1.17415637e-01],\n",
      "       [-1.39924616e-01],\n",
      "       [-7.01857954e-02],\n",
      "       [-1.07378609e-01],\n",
      "       [-1.04473971e-01],\n",
      "       [-1.48300037e-01],\n",
      "       [-7.08484352e-02],\n",
      "       [-1.26986474e-01],\n",
      "       [-1.34139806e-01],\n",
      "       [-1.28199831e-01],\n",
      "       [-1.15352005e-01],\n",
      "       [-1.21735364e-01],\n",
      "       [-1.31097227e-01],\n",
      "       [-6.96964115e-02],\n",
      "       [-8.61095116e-02],\n",
      "       [-1.15656003e-01],\n",
      "       [-5.84403053e-03],\n",
      "       [-1.39097407e-01],\n",
      "       [-4.26752758e+00],\n",
      "       [-1.20027579e-01],\n",
      "       [-1.12176768e-01],\n",
      "       [-1.32365093e-01],\n",
      "       [-1.84394438e-02],\n",
      "       [-3.32743563e-02],\n",
      "       [-1.29281908e-01],\n",
      "       [-1.83224220e+01],\n",
      "       [-8.75059068e-02],\n",
      "       [-2.29111443e+01],\n",
      "       [-9.59210023e-02],\n",
      "       [-9.32993367e-02],\n",
      "       [-2.88622131e+01],\n",
      "       [-7.21496865e-02],\n",
      "       [-1.41613156e-01],\n",
      "       [-8.70840326e-02],\n",
      "       [-1.06030844e-01],\n",
      "       [-1.07658096e-01],\n",
      "       [-6.04919493e-01],\n",
      "       [-1.20027579e-01],\n",
      "       [-1.30057096e-01],\n",
      "       [-1.30687550e-01],\n",
      "       [-1.89121545e-03],\n",
      "       [-1.39097407e-01],\n",
      "       [-6.81601316e-02],\n",
      "       [-1.04023658e-01],\n",
      "       [-2.90585477e-02],\n",
      "       [-1.04023658e-01],\n",
      "       [-1.11829579e-01],\n",
      "       [-7.67485006e-03],\n",
      "       [-1.37525722e-01],\n",
      "       [-3.16794552e-02],\n",
      "       [-1.17503315e-01],\n",
      "       [-1.22036295e+01],\n",
      "       [-1.18449420e-01],\n",
      "       [-2.53964439e-02],\n",
      "       [-1.18449420e-01],\n",
      "       [-1.00804657e-01],\n",
      "       [-1.50368989e-01],\n",
      "       [-6.20409064e-02],\n",
      "       [-9.77400690e-02],\n",
      "       [-2.00532731e-02],\n",
      "       [-1.05736934e-01],\n",
      "       [-1.60777211e-01],\n",
      "       [-1.38510108e-01],\n",
      "       [-5.15716746e-02],\n",
      "       [-1.22566864e-01],\n",
      "       [-1.04256019e-01]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-0.693305  ],\n",
      "       [-0.708969  ],\n",
      "       [-0.7189589 ],\n",
      "       [-0.691264  ],\n",
      "       [-0.65838987],\n",
      "       [-0.6543292 ],\n",
      "       [-0.7235457 ],\n",
      "       [-0.6958211 ],\n",
      "       [-0.7007676 ],\n",
      "       [-0.6312815 ],\n",
      "       [-0.7231922 ],\n",
      "       [-0.72949314],\n",
      "       [-0.7121069 ],\n",
      "       [-0.71778345],\n",
      "       [-0.73096347],\n",
      "       [-0.7237523 ],\n",
      "       [-0.7158651 ],\n",
      "       [-0.7053694 ],\n",
      "       [-0.6906934 ],\n",
      "       [-0.69100183],\n",
      "       [-0.69061506],\n",
      "       [-0.66351134],\n",
      "       [-0.6908982 ],\n",
      "       [-0.64229065],\n",
      "       [-0.68612534],\n",
      "       [-0.6675667 ],\n",
      "       [-0.72266024],\n",
      "       [-0.6300092 ],\n",
      "       [-0.7443847 ],\n",
      "       [-0.70300937],\n",
      "       [-0.74089825],\n",
      "       [-0.66365755],\n",
      "       [-0.67328227],\n",
      "       [-0.71558535],\n",
      "       [-0.6921457 ],\n",
      "       [-0.68693143],\n",
      "       [-0.70263016],\n",
      "       [-0.69332206],\n",
      "       [-0.6793422 ],\n",
      "       [-0.67937165],\n",
      "       [-0.64850396],\n",
      "       [-0.6921457 ],\n",
      "       [-0.7443847 ],\n",
      "       [-0.6887079 ],\n",
      "       [-0.6961831 ],\n",
      "       [-0.73106277],\n",
      "       [-0.7317576 ],\n",
      "       [-0.728177  ],\n",
      "       [-0.69609815],\n",
      "       [-0.69254225],\n",
      "       [-0.69835377],\n",
      "       [-0.7062669 ],\n",
      "       [-0.7154682 ],\n",
      "       [-0.72250223],\n",
      "       [-0.70552194],\n",
      "       [-0.7135437 ],\n",
      "       [-0.7124466 ],\n",
      "       [-0.7585659 ],\n",
      "       [-0.700463  ],\n",
      "       [-0.706106  ],\n",
      "       [-0.6925835 ],\n",
      "       [-0.7364829 ],\n",
      "       [-0.7505119 ],\n",
      "       [-0.70901006],\n",
      "       [-0.71583617],\n",
      "       [-0.72146374],\n",
      "       [-0.7181512 ],\n",
      "       [-0.72994053],\n",
      "       [-0.7344178 ],\n",
      "       [-0.70031667],\n",
      "       [-0.73884696],\n",
      "       [-0.6548022 ],\n",
      "       [-0.66043276],\n",
      "       [-0.67380375],\n",
      "       [-0.7360788 ],\n",
      "       [-0.713184  ],\n",
      "       [-0.72196156],\n",
      "       [-0.7355352 ],\n",
      "       [-0.70798224],\n",
      "       [-0.71610314],\n",
      "       [-0.7168512 ],\n",
      "       [-0.68474853],\n",
      "       [-0.71982574],\n",
      "       [-0.715638  ],\n",
      "       [-0.73164415],\n",
      "       [-0.6863071 ],\n",
      "       [-0.7070793 ],\n",
      "       [-0.7051467 ],\n",
      "       [-0.6918438 ],\n",
      "       [-0.71293414],\n",
      "       [-0.64768887],\n",
      "       [-0.7055478 ],\n",
      "       [-0.72798383],\n",
      "       [-0.7615998 ],\n",
      "       [-0.6543292 ],\n",
      "       [-0.6913121 ],\n",
      "       [-0.6682819 ],\n",
      "       [-0.76231605],\n",
      "       [-0.7244585 ],\n",
      "       [-0.68474853],\n",
      "       [-0.69257194],\n",
      "       [-0.735556  ],\n",
      "       [-0.7147639 ],\n",
      "       [-0.71610314],\n",
      "       [-0.70087695],\n",
      "       [-0.6940971 ],\n",
      "       [-0.698493  ],\n",
      "       [-0.6940971 ],\n",
      "       [-0.6950018 ],\n",
      "       [-0.72063184],\n",
      "       [-0.6660085 ],\n",
      "       [-0.72436625],\n",
      "       [-0.6753373 ],\n",
      "       [-0.6572738 ],\n",
      "       [-0.7033262 ],\n",
      "       [-0.7127826 ],\n",
      "       [-0.7033262 ],\n",
      "       [-0.67702913],\n",
      "       [-0.7270475 ],\n",
      "       [-0.6894426 ],\n",
      "       [-0.73226094],\n",
      "       [-0.64728326],\n",
      "       [-0.6840092 ],\n",
      "       [-0.73458815],\n",
      "       [-0.69332206],\n",
      "       [-0.7286337 ],\n",
      "       [-0.65277165],\n",
      "       [-0.70277643]], dtype=float32)>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob vec:  [<tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-1.0545901 ],\n",
      "       [-1.0807317 ],\n",
      "       [-0.9142616 ],\n",
      "       [-0.9156388 ],\n",
      "       [-1.4191878 ],\n",
      "       [-0.9018734 ],\n",
      "       [-1.0469288 ],\n",
      "       [-1.3683088 ],\n",
      "       [-0.8895385 ],\n",
      "       [-1.3560337 ],\n",
      "       [-0.89624405],\n",
      "       [-0.89875   ],\n",
      "       [-1.076362  ],\n",
      "       [-0.8988509 ],\n",
      "       [-0.88848484],\n",
      "       [-1.0565735 ],\n",
      "       [-1.3627641 ],\n",
      "       [-1.3831102 ],\n",
      "       [-0.87942326],\n",
      "       [-0.9376354 ],\n",
      "       [-1.3563052 ],\n",
      "       [-1.0446215 ],\n",
      "       [-0.8996727 ],\n",
      "       [-0.91221714],\n",
      "       [-1.3598466 ],\n",
      "       [-1.4320891 ],\n",
      "       [-1.0781715 ],\n",
      "       [-0.8782779 ],\n",
      "       [-0.91805756],\n",
      "       [-0.89889306],\n",
      "       [-1.0559549 ],\n",
      "       [-1.3586613 ],\n",
      "       [-0.9118594 ],\n",
      "       [-0.881337  ],\n",
      "       [-1.3781722 ],\n",
      "       [-0.88847893],\n",
      "       [-1.3560337 ],\n",
      "       [-0.89826375],\n",
      "       [-1.394613  ],\n",
      "       [-0.8850975 ],\n",
      "       [-0.87854195],\n",
      "       [-1.0915202 ],\n",
      "       [-1.0761997 ],\n",
      "       [-1.0565548 ],\n",
      "       [-1.0569501 ],\n",
      "       [-0.8761312 ],\n",
      "       [-0.8647535 ],\n",
      "       [-1.364255  ],\n",
      "       [-0.9190818 ],\n",
      "       [-1.0776218 ],\n",
      "       [-1.0149356 ],\n",
      "       [-0.88993204],\n",
      "       [-0.87685186],\n",
      "       [-0.8902893 ],\n",
      "       [-1.072966  ],\n",
      "       [-0.86025196],\n",
      "       [-0.9024658 ],\n",
      "       [-1.4648583 ],\n",
      "       [-0.90325546],\n",
      "       [-1.3648157 ],\n",
      "       [-1.076362  ],\n",
      "       [-1.4228337 ],\n",
      "       [-0.91746116],\n",
      "       [-1.0558388 ],\n",
      "       [-0.88925093],\n",
      "       [-1.053005  ],\n",
      "       [-0.8766947 ],\n",
      "       [-0.89286655],\n",
      "       [-0.9205269 ],\n",
      "       [-0.8941611 ],\n",
      "       [-1.3893645 ],\n",
      "       [-0.89333653],\n",
      "       [-0.9058908 ],\n",
      "       [-0.89575434],\n",
      "       [-0.9507283 ],\n",
      "       [-0.9011592 ],\n",
      "       [-1.364255  ],\n",
      "       [-0.89575434],\n",
      "       [-0.8973616 ],\n",
      "       [-0.8914971 ],\n",
      "       [-1.0824041 ],\n",
      "       [-1.0469288 ],\n",
      "       [-0.90241814],\n",
      "       [-0.89767396],\n",
      "       [-0.89319587],\n",
      "       [-0.9007469 ],\n",
      "       [-1.4135823 ],\n",
      "       [-0.91676784],\n",
      "       [-0.90353924],\n",
      "       [-0.87861514],\n",
      "       [-1.0490282 ],\n",
      "       [-1.3527608 ],\n",
      "       [-0.87169856],\n",
      "       [-1.0676072 ],\n",
      "       [-0.8927238 ],\n",
      "       [-1.3678775 ],\n",
      "       [-0.86605453],\n",
      "       [-1.0710776 ],\n",
      "       [-0.9127423 ],\n",
      "       [-0.8848576 ],\n",
      "       [-0.88993204],\n",
      "       [-1.3653429 ],\n",
      "       [-0.8904749 ],\n",
      "       [-0.88847893],\n",
      "       [-0.8980403 ],\n",
      "       [-0.95050645],\n",
      "       [-1.0388455 ],\n",
      "       [-0.9169968 ],\n",
      "       [-0.89286655],\n",
      "       [-1.0775695 ],\n",
      "       [-0.90170795],\n",
      "       [-0.9142616 ],\n",
      "       [-1.0653604 ],\n",
      "       [-0.9142616 ],\n",
      "       [-1.0776218 ],\n",
      "       [-0.8903775 ],\n",
      "       [-0.9130226 ],\n",
      "       [-0.91350186],\n",
      "       [-1.0446279 ],\n",
      "       [-0.9168295 ],\n",
      "       [-0.89889306],\n",
      "       [-1.3784208 ],\n",
      "       [-0.8954548 ],\n",
      "       [-1.0718926 ],\n",
      "       [-1.0975621 ],\n",
      "       [-0.88848484],\n",
      "       [-0.8988496 ],\n",
      "       [-0.91892123]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-1.8046398],\n",
      "       [-1.6984464],\n",
      "       [-1.8076438],\n",
      "       [-1.8114784],\n",
      "       [-1.6627753],\n",
      "       [-1.8214295],\n",
      "       [-1.6742197],\n",
      "       [-1.7995325],\n",
      "       [-1.764504 ],\n",
      "       [-1.7639397],\n",
      "       [-1.7804108],\n",
      "       [-1.6838793],\n",
      "       [-1.6623769],\n",
      "       [-1.7752395],\n",
      "       [-1.6612782],\n",
      "       [-1.7060685],\n",
      "       [-1.8040458],\n",
      "       [-1.8142781],\n",
      "       [-1.6779628],\n",
      "       [-1.6706389],\n",
      "       [-1.7638944],\n",
      "       [-1.6714755],\n",
      "       [-1.7860299],\n",
      "       [-1.6462582],\n",
      "       [-1.790644 ],\n",
      "       [-1.764144 ],\n",
      "       [-1.6636368],\n",
      "       [-1.7088423],\n",
      "       [-1.6475365],\n",
      "       [-1.7752054],\n",
      "       [-1.7055933],\n",
      "       [-1.8054231],\n",
      "       [-1.6465856],\n",
      "       [-1.7106813],\n",
      "       [-1.8031535],\n",
      "       [-1.6613548],\n",
      "       [-1.7639397],\n",
      "       [-1.7865571],\n",
      "       [-1.696626 ],\n",
      "       [-1.8506081],\n",
      "       [-1.7598535],\n",
      "       [-1.6492721],\n",
      "       [-1.8021829],\n",
      "       [-1.7061712],\n",
      "       [-1.7079548],\n",
      "       [-1.7227823],\n",
      "       [-1.6944467],\n",
      "       [-1.7516363],\n",
      "       [-1.8232317],\n",
      "       [-1.6639606],\n",
      "       [-1.6568067],\n",
      "       [-1.7633802],\n",
      "       [-1.6753008],\n",
      "       [-1.762191 ],\n",
      "       [-1.7720473],\n",
      "       [-1.6616826],\n",
      "       [-1.8670866],\n",
      "       [-1.7013012],\n",
      "       [-1.8150437],\n",
      "       [-1.6983248],\n",
      "       [-1.6623769],\n",
      "       [-1.6722727],\n",
      "       [-1.6563995],\n",
      "       [-1.7078292],\n",
      "       [-1.6597149],\n",
      "       [-1.6682789],\n",
      "       [-1.6934762],\n",
      "       [-1.6877341],\n",
      "       [-1.6570985],\n",
      "       [-1.6587355],\n",
      "       [-1.692077 ],\n",
      "       [-1.6819448],\n",
      "       [-1.8134111],\n",
      "       [-1.6853524],\n",
      "       [-1.6791222],\n",
      "       [-1.802205 ],\n",
      "       [-1.7516363],\n",
      "       [-1.6853524],\n",
      "       [-1.774688 ],\n",
      "       [-1.8159801],\n",
      "       [-1.6573128],\n",
      "       [-1.6742197],\n",
      "       [-1.6957182],\n",
      "       [-1.8155214],\n",
      "       [-1.7671509],\n",
      "       [-1.798456 ],\n",
      "       [-1.7142271],\n",
      "       [-1.8150476],\n",
      "       [-1.6552864],\n",
      "       [-1.6931726],\n",
      "       [-1.7714661],\n",
      "       [-1.689771 ],\n",
      "       [-1.8923128],\n",
      "       [-1.6922331],\n",
      "       [-1.679606 ],\n",
      "       [-1.8097597],\n",
      "       [-1.6683601],\n",
      "       [-1.6738881],\n",
      "       [-1.6515883],\n",
      "       [-1.7031405],\n",
      "       [-1.7633802],\n",
      "       [-1.8051267],\n",
      "       [-1.7030103],\n",
      "       [-1.6613548],\n",
      "       [-1.6838617],\n",
      "       [-1.6827781],\n",
      "       [-1.6662657],\n",
      "       [-1.6623619],\n",
      "       [-1.6877341],\n",
      "       [-1.6875784],\n",
      "       [-1.8209531],\n",
      "       [-1.8076438],\n",
      "       [-1.7577469],\n",
      "       [-1.8076438],\n",
      "       [-1.6639606],\n",
      "       [-1.7624456],\n",
      "       [-1.6513098],\n",
      "       [-1.6660666],\n",
      "       [-1.6716694],\n",
      "       [-1.8150976],\n",
      "       [-1.7752054],\n",
      "       [-1.7853326],\n",
      "       [-1.6853946],\n",
      "       [-1.7735065],\n",
      "       [-1.784431 ],\n",
      "       [-1.6612782],\n",
      "       [-1.6837356],\n",
      "       [-1.7711117]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-0.6047282 ],\n",
      "       [-0.77671593],\n",
      "       [-0.5933367 ],\n",
      "       [-0.59353757],\n",
      "       [-0.622506  ],\n",
      "       [-0.6156802 ],\n",
      "       [-0.58568794],\n",
      "       [-0.80118775],\n",
      "       [-0.62096924],\n",
      "       [-0.6118878 ],\n",
      "       [-0.6253181 ],\n",
      "       [-0.62399256],\n",
      "       [-0.79180074],\n",
      "       [-0.6237612 ],\n",
      "       [-0.75435257],\n",
      "       [-0.59528375],\n",
      "       [-0.5822758 ],\n",
      "       [-0.5984666 ],\n",
      "       [-0.7424647 ],\n",
      "       [-0.60514873],\n",
      "       [-0.6121611 ],\n",
      "       [-0.588309  ],\n",
      "       [-0.6224663 ],\n",
      "       [-0.6228502 ],\n",
      "       [-0.8012912 ],\n",
      "       [-0.6385933 ],\n",
      "       [-0.59764224],\n",
      "       [-0.6385917 ],\n",
      "       [-0.61671174],\n",
      "       [-0.62373716],\n",
      "       [-0.5965507 ],\n",
      "       [-0.6062264 ],\n",
      "       [-0.6255001 ],\n",
      "       [-0.6371057 ],\n",
      "       [-0.79715985],\n",
      "       [-0.75464773],\n",
      "       [-0.6118878 ],\n",
      "       [-0.6237213 ],\n",
      "       [-0.61903054],\n",
      "       [-0.75286734],\n",
      "       [-0.63169265],\n",
      "       [-0.74887246],\n",
      "       [-0.78544825],\n",
      "       [-0.59544677],\n",
      "       [-0.5933881 ],\n",
      "       [-0.6392979 ],\n",
      "       [-0.6534193 ],\n",
      "       [-0.7678137 ],\n",
      "       [-0.5970663 ],\n",
      "       [-0.59811366],\n",
      "       [-0.55791587],\n",
      "       [-0.62114644],\n",
      "       [-0.7420368 ],\n",
      "       [-0.6210809 ],\n",
      "       [-0.6124504 ],\n",
      "       [-0.7343717 ],\n",
      "       [-0.6377816 ],\n",
      "       [-0.6526473 ],\n",
      "       [-0.6218017 ],\n",
      "       [-0.5997288 ],\n",
      "       [-0.79180074],\n",
      "       [-0.6303644 ],\n",
      "       [-0.605846  ],\n",
      "       [-0.6005826 ],\n",
      "       [-0.7533529 ],\n",
      "       [-0.5792236 ],\n",
      "       [-0.641569  ],\n",
      "       [-0.62976766],\n",
      "       [-0.60311675],\n",
      "       [-0.7584652 ],\n",
      "       [-0.6167781 ],\n",
      "       [-0.6281429 ],\n",
      "       [-0.6137954 ],\n",
      "       [-0.62646645],\n",
      "       [-0.57506424],\n",
      "       [-0.61499923],\n",
      "       [-0.7678137 ],\n",
      "       [-0.62646645],\n",
      "       [-0.625371  ],\n",
      "       [-0.7537721 ],\n",
      "       [-0.7763956 ],\n",
      "       [-0.58568794],\n",
      "       [-0.61453605],\n",
      "       [-0.6171814 ],\n",
      "       [-0.61647075],\n",
      "       [-0.61260915],\n",
      "       [-0.6333872 ],\n",
      "       [-0.5940996 ],\n",
      "       [-0.6174481 ],\n",
      "       [-0.6397583 ],\n",
      "       [-0.5829965 ],\n",
      "       [-0.5694959 ],\n",
      "       [-0.74297243],\n",
      "       [-0.6153629 ],\n",
      "       [-0.62849575],\n",
      "       [-0.76433086],\n",
      "       [-0.7333545 ],\n",
      "       [-0.6070835 ],\n",
      "       [-0.6141853 ],\n",
      "       [-0.6504172 ],\n",
      "       [-0.62114644],\n",
      "       [-0.5843629 ],\n",
      "       [-0.6186755 ],\n",
      "       [-0.75464773],\n",
      "       [-0.62484604],\n",
      "       [-0.56831825],\n",
      "       [-0.5947379 ],\n",
      "       [-0.6021258 ],\n",
      "       [-0.62976766],\n",
      "       [-0.617796  ],\n",
      "       [-0.6161376 ],\n",
      "       [-0.5933367 ],\n",
      "       [-0.6198394 ],\n",
      "       [-0.5933367 ],\n",
      "       [-0.59811366],\n",
      "       [-0.6208812 ],\n",
      "       [-0.61415625],\n",
      "       [-0.6096975 ],\n",
      "       [-0.588731  ],\n",
      "       [-0.5939257 ],\n",
      "       [-0.62373716],\n",
      "       [-0.5798044 ],\n",
      "       [-0.6263404 ],\n",
      "       [-0.6102825 ],\n",
      "       [-0.61578715],\n",
      "       [-0.75435257],\n",
      "       [-0.62398934],\n",
      "       [-0.5919181 ]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-0.7046567 ],\n",
      "       [-0.723605  ],\n",
      "       [-0.7003665 ],\n",
      "       [-0.6999776 ],\n",
      "       [-0.66712636],\n",
      "       [-0.7115307 ],\n",
      "       [-0.702316  ],\n",
      "       [-0.7166001 ],\n",
      "       [-0.73063093],\n",
      "       [-0.66928405],\n",
      "       [-0.6666595 ],\n",
      "       [-0.7119393 ],\n",
      "       [-0.72788304],\n",
      "       [-0.68265253],\n",
      "       [-0.64949703],\n",
      "       [-0.70360136],\n",
      "       [-0.7087539 ],\n",
      "       [-0.70716417],\n",
      "       [-0.7455268 ],\n",
      "       [-0.74123085],\n",
      "       [-0.6693401 ],\n",
      "       [-0.7045341 ],\n",
      "       [-0.67371845],\n",
      "       [-0.71238244],\n",
      "       [-0.71888775],\n",
      "       [-0.64014304],\n",
      "       [-0.6753579 ],\n",
      "       [-0.6170089 ],\n",
      "       [-0.7186784 ],\n",
      "       [-0.682675  ],\n",
      "       [-0.70559484],\n",
      "       [-0.6799871 ],\n",
      "       [-0.71685624],\n",
      "       [-0.6225588 ],\n",
      "       [-0.7167632 ],\n",
      "       [-0.64907956],\n",
      "       [-0.66928405],\n",
      "       [-0.6711764 ],\n",
      "       [-0.62329775],\n",
      "       [-0.74944526],\n",
      "       [-0.6496468 ],\n",
      "       [-0.6506548 ],\n",
      "       [-0.7300666 ],\n",
      "       [-0.7033949 ],\n",
      "       [-0.70128083],\n",
      "       [-0.62305874],\n",
      "       [-0.7305496 ],\n",
      "       [-0.6525913 ],\n",
      "       [-0.7039429 ],\n",
      "       [-0.6760637 ],\n",
      "       [-0.7862363 ],\n",
      "       [-0.7315951 ],\n",
      "       [-0.7475732 ],\n",
      "       [-0.73171204],\n",
      "       [-0.72692263],\n",
      "       [-0.6173638 ],\n",
      "       [-0.72748876],\n",
      "       [-0.6320834 ],\n",
      "       [-0.68034256],\n",
      "       [-0.68667924],\n",
      "       [-0.72788304],\n",
      "       [-0.719658  ],\n",
      "       [-0.70470655],\n",
      "       [-0.7095019 ],\n",
      "       [-0.65065914],\n",
      "       [-0.774232  ],\n",
      "       [-0.7229505 ],\n",
      "       [-0.6482803 ],\n",
      "       [-0.70966876],\n",
      "       [-0.734656  ],\n",
      "       [-0.62273693],\n",
      "       [-0.657782  ],\n",
      "       [-0.689924  ],\n",
      "       [-0.7175858 ],\n",
      "       [-0.75023454],\n",
      "       [-0.6845861 ],\n",
      "       [-0.6525913 ],\n",
      "       [-0.7175858 ],\n",
      "       [-0.66704905],\n",
      "       [-0.6606232 ],\n",
      "       [-0.65363353],\n",
      "       [-0.702316  ],\n",
      "       [-0.7016406 ],\n",
      "       [-0.71632445],\n",
      "       [-0.7289272 ],\n",
      "       [-0.68857783],\n",
      "       [-0.73475945],\n",
      "       [-0.7011891 ],\n",
      "       [-0.68799895],\n",
      "       [-0.7228574 ],\n",
      "       [-0.6857637 ],\n",
      "       [-0.7136063 ],\n",
      "       [-0.60589904],\n",
      "       [-0.6799712 ],\n",
      "       [-0.6671557 ],\n",
      "       [-0.6623926 ],\n",
      "       [-0.6209487 ],\n",
      "       [-0.66722864],\n",
      "       [-0.703829  ],\n",
      "       [-0.72123134],\n",
      "       [-0.7315951 ],\n",
      "       [-0.6967606 ],\n",
      "       [-0.6374342 ],\n",
      "       [-0.64907956],\n",
      "       [-0.71386933],\n",
      "       [-0.7574419 ],\n",
      "       [-0.7064753 ],\n",
      "       [-0.7007055 ],\n",
      "       [-0.6482803 ],\n",
      "       [-0.67366385],\n",
      "       [-0.7114408 ],\n",
      "       [-0.7003665 ],\n",
      "       [-0.733286  ],\n",
      "       [-0.7003665 ],\n",
      "       [-0.6760637 ],\n",
      "       [-0.7315816 ],\n",
      "       [-0.70447063],\n",
      "       [-0.62716097],\n",
      "       [-0.7038622 ],\n",
      "       [-0.7009038 ],\n",
      "       [-0.682675  ],\n",
      "       [-0.68082327],\n",
      "       [-0.7173852 ],\n",
      "       [-0.7261565 ],\n",
      "       [-0.66098964],\n",
      "       [-0.64949703],\n",
      "       [-0.7120313 ],\n",
      "       [-0.68267655]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-0.75231427],\n",
      "       [-0.7558503 ],\n",
      "       [-0.6063186 ],\n",
      "       [-0.6097851 ],\n",
      "       [-0.6595734 ],\n",
      "       [-0.763624  ],\n",
      "       [-0.6028352 ],\n",
      "       [-0.62479293],\n",
      "       [-0.7268472 ],\n",
      "       [-0.60535645],\n",
      "       [-0.6126424 ],\n",
      "       [-0.74146384],\n",
      "       [-0.6071214 ],\n",
      "       [-0.6285019 ],\n",
      "       [-0.61530876],\n",
      "       [-0.76938623],\n",
      "       [-0.61778843],\n",
      "       [-0.6293369 ],\n",
      "       [-0.74910194],\n",
      "       [-0.6650403 ],\n",
      "       [-0.6057229 ],\n",
      "       [-0.60723233],\n",
      "       [-0.61224484],\n",
      "       [-0.6446709 ],\n",
      "       [-0.61300224],\n",
      "       [-0.67629   ],\n",
      "       [-0.5901345 ],\n",
      "       [-0.71138054],\n",
      "       [-0.6422576 ],\n",
      "       [-0.6285652 ],\n",
      "       [-0.76699483],\n",
      "       [-0.597646  ],\n",
      "       [-0.651644  ],\n",
      "       [-0.7184654 ],\n",
      "       [-0.6361571 ],\n",
      "       [-0.6152085 ],\n",
      "       [-0.60535645],\n",
      "       [-0.6159282 ],\n",
      "       [-0.6919773 ],\n",
      "       [-0.74280024],\n",
      "       [-0.7061943 ],\n",
      "       [-0.7831901 ],\n",
      "       [-0.6212326 ],\n",
      "       [-0.76918113],\n",
      "       [-0.77154803],\n",
      "       [-0.70887256],\n",
      "       [-0.6634801 ],\n",
      "       [-0.6169443 ],\n",
      "       [-0.62676483],\n",
      "       [-0.5907327 ],\n",
      "       [-0.75671816],\n",
      "       [-0.7287898 ],\n",
      "       [-0.7401327 ],\n",
      "       [-0.730508  ],\n",
      "       [-0.73952466],\n",
      "       [-0.71931374],\n",
      "       [-0.6787322 ],\n",
      "       [-0.645934  ],\n",
      "       [-0.65238506],\n",
      "       [-0.6065764 ],\n",
      "       [-0.6071214 ],\n",
      "       [-0.6678668 ],\n",
      "       [-0.61682653],\n",
      "       [-0.7588975 ],\n",
      "       [-0.61889637],\n",
      "       [-0.7684816 ],\n",
      "       [-0.6905219 ],\n",
      "       [-0.75562197],\n",
      "       [-0.6158118 ],\n",
      "       [-0.6250033 ],\n",
      "       [-0.6983504 ],\n",
      "       [-0.7457222 ],\n",
      "       [-0.6294165 ],\n",
      "       [-0.7234334 ],\n",
      "       [-0.63126904],\n",
      "       [-0.6082329 ],\n",
      "       [-0.6169443 ],\n",
      "       [-0.7234334 ],\n",
      "       [-0.60704625],\n",
      "       [-0.6137871 ],\n",
      "       [-0.61359644],\n",
      "       [-0.6028352 ],\n",
      "       [-0.77128494],\n",
      "       [-0.73224705],\n",
      "       [-0.7349437 ],\n",
      "       [-0.61144525],\n",
      "       [-0.7031165 ],\n",
      "       [-0.6148949 ],\n",
      "       [-0.6191855 ],\n",
      "       [-0.694223  ],\n",
      "       [-0.6041644 ],\n",
      "       [-0.59392744],\n",
      "       [-0.72263104],\n",
      "       [-0.762575  ],\n",
      "       [-0.73101497],\n",
      "       [-0.6181904 ],\n",
      "       [-0.73302484],\n",
      "       [-0.7750144 ],\n",
      "       [-0.62869287],\n",
      "       [-0.679137  ],\n",
      "       [-0.7287898 ],\n",
      "       [-0.61029345],\n",
      "       [-0.7195752 ],\n",
      "       [-0.6152085 ],\n",
      "       [-0.73442453],\n",
      "       [-0.618344  ],\n",
      "       [-0.61852247],\n",
      "       [-0.61096877],\n",
      "       [-0.75562197],\n",
      "       [-0.76727825],\n",
      "       [-0.7632705 ],\n",
      "       [-0.6063186 ],\n",
      "       [-0.70835894],\n",
      "       [-0.6063186 ],\n",
      "       [-0.5907327 ],\n",
      "       [-0.73057836],\n",
      "       [-0.6286297 ],\n",
      "       [-0.7567771 ],\n",
      "       [-0.60752994],\n",
      "       [-0.6146493 ],\n",
      "       [-0.6285652 ],\n",
      "       [-0.6060633 ],\n",
      "       [-0.7232931 ],\n",
      "       [-0.741715  ],\n",
      "       [-0.5946354 ],\n",
      "       [-0.61530876],\n",
      "       [-0.74162674],\n",
      "       [-0.6212169 ]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-3.7115753e-01],\n",
      "       [-3.2410525e-02],\n",
      "       [-3.5498437e-01],\n",
      "       [-1.4952493e-01],\n",
      "       [-5.3698044e+00],\n",
      "       [-1.7027408e-01],\n",
      "       [-3.6341542e-01],\n",
      "       [-2.6803911e-02],\n",
      "       [-5.6034021e-02],\n",
      "       [-3.5376540e-01],\n",
      "       [-1.5779260e-01],\n",
      "       [-7.1090317e-01],\n",
      "       [-3.6228028e-01],\n",
      "       [-4.0857789e-01],\n",
      "       [-3.7530968e-01],\n",
      "       [-3.0453220e-02],\n",
      "       [-2.2220900e-02],\n",
      "       [-3.6604124e-01],\n",
      "       [-1.6662818e-01],\n",
      "       [-6.4609981e+00],\n",
      "       [-3.5424560e-01],\n",
      "       [-1.5605649e-01],\n",
      "       [-3.6056507e-02],\n",
      "       [-1.0902594e+00],\n",
      "       [-3.3274278e-01],\n",
      "       [-4.5994477e+00],\n",
      "       [-3.5467356e-01],\n",
      "       [-3.7423310e-01],\n",
      "       [-7.0677966e-01],\n",
      "       [-4.0862477e-01],\n",
      "       [-1.3180103e-03],\n",
      "       [-3.5323575e-01],\n",
      "       [-2.8051436e+00],\n",
      "       [-3.7405401e-01],\n",
      "       [-2.1845809e-01],\n",
      "       [-3.7495410e-01],\n",
      "       [-3.5376540e-01],\n",
      "       [-3.5602514e-02],\n",
      "       [-2.3778634e-02],\n",
      "       [-4.0929396e-02],\n",
      "       [-4.9941286e-02],\n",
      "       [-3.8195911e-01],\n",
      "       [-2.9609760e-02],\n",
      "       [-3.0539485e-02],\n",
      "       [-1.4812218e-01],\n",
      "       [-3.8526945e-02],\n",
      "       [-2.3309332e+01],\n",
      "       [-3.5436267e-01],\n",
      "       [-2.0417030e-01],\n",
      "       [-3.5554925e-01],\n",
      "       [-1.1994120e+00],\n",
      "       [-6.2374084e-04],\n",
      "       [-1.8160382e-01],\n",
      "       [-3.4936938e-02],\n",
      "       [-1.5099184e-01],\n",
      "       [-1.2139863e-04],\n",
      "       [-1.0030667e+01],\n",
      "       [-1.7821316e+01],\n",
      "       [-1.5948585e+00],\n",
      "       [-3.6149561e-01],\n",
      "       [-3.6228028e-01],\n",
      "       [-7.4429369e+00],\n",
      "       [-1.7027192e-01],\n",
      "       [-2.0911561e-01],\n",
      "       [-1.6354539e-01],\n",
      "       [-3.4732899e-01],\n",
      "       [-8.6014214e+00],\n",
      "       [-3.8761356e-01],\n",
      "       [-1.6662292e-01],\n",
      "       [-3.8119158e-01],\n",
      "       [-3.3079588e-01],\n",
      "       [-1.9389712e-05],\n",
      "       [-1.7504431e-01],\n",
      "       [-2.1589036e+00],\n",
      "       [-4.4452116e-01],\n",
      "       [-1.5982784e-01],\n",
      "       [-3.5436267e-01],\n",
      "       [-2.1589036e+00],\n",
      "       [-3.6851317e-01],\n",
      "       [-3.7764004e-01],\n",
      "       [-1.5816520e-01],\n",
      "       [-3.6341542e-01],\n",
      "       [-3.8867977e-01],\n",
      "       [-4.0217334e-01],\n",
      "       [-1.5452266e-01],\n",
      "       [-3.6791943e-02],\n",
      "       [-3.6647439e+00],\n",
      "       [-3.1219766e-02],\n",
      "       [-4.7817089e-02],\n",
      "       [-7.4176521e+00],\n",
      "       [-3.5794246e-01],\n",
      "       [-1.3385902e-01],\n",
      "       [-3.8226110e-01],\n",
      "       [-2.2746711e-04],\n",
      "       [-1.6404153e-01],\n",
      "       [-3.6712959e-01],\n",
      "       [-3.9807847e-01],\n",
      "       [-3.3457488e-02],\n",
      "       [-2.0399732e-08],\n",
      "       [-1.5991834e+01],\n",
      "       [-6.2374084e-04],\n",
      "       [-3.4269741e-01],\n",
      "       [-1.6756691e-01],\n",
      "       [-3.7495410e-01],\n",
      "       [-1.1036288e+00],\n",
      "       [-2.6260374e-02],\n",
      "       [-5.2570436e-02],\n",
      "       [-3.8338441e-01],\n",
      "       [-3.8761356e-01],\n",
      "       [-3.7161574e-01],\n",
      "       [-1.7067607e-01],\n",
      "       [-3.5498437e-01],\n",
      "       [-1.6828212e+00],\n",
      "       [-3.5498437e-01],\n",
      "       [-3.5554925e-01],\n",
      "       [-3.4822796e-02],\n",
      "       [-5.5800200e-08],\n",
      "       [-1.3735566e-03],\n",
      "       [-1.5658264e-01],\n",
      "       [-3.1200090e-02],\n",
      "       [-4.0862477e-01],\n",
      "       [-3.5379922e-01],\n",
      "       [-2.1572363e+00],\n",
      "       [-3.0575717e-02],\n",
      "       [-1.4539115e-01],\n",
      "       [-3.7530968e-01],\n",
      "       [-7.1160764e-01],\n",
      "       [-4.3553126e-04]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-1.49331525e-01],\n",
      "       [-1.72848143e-02],\n",
      "       [-1.26321852e-01],\n",
      "       [-1.16675615e-01],\n",
      "       [-5.43563738e-02],\n",
      "       [-1.15379170e-01],\n",
      "       [-1.39270857e-01],\n",
      "       [-1.21215135e-01],\n",
      "       [-1.24734759e-01],\n",
      "       [-8.74881223e-02],\n",
      "       [-1.10597454e-03],\n",
      "       [-1.47036880e-01],\n",
      "       [-1.22225508e-01],\n",
      "       [-1.18327148e-01],\n",
      "       [-1.29408255e-01],\n",
      "       [-1.32529020e-01],\n",
      "       [-2.30035752e-01],\n",
      "       [-1.61982715e-01],\n",
      "       [-6.95291609e-02],\n",
      "       [-3.14334989e+00],\n",
      "       [-1.00804657e-01],\n",
      "       [-1.13273166e-01],\n",
      "       [-9.91160199e-02],\n",
      "       [-1.53539836e-01],\n",
      "       [-9.47481208e-03],\n",
      "       [-8.51322114e-01],\n",
      "       [-8.33495408e-02],\n",
      "       [-8.58378634e-02],\n",
      "       [-1.08612276e-05],\n",
      "       [-1.04023658e-01],\n",
      "       [-1.52077377e-01],\n",
      "       [-8.69220719e-02],\n",
      "       [-1.60216302e-01],\n",
      "       [-2.09681764e-02],\n",
      "       [-1.52824491e-01],\n",
      "       [-1.14427932e-01],\n",
      "       [-8.74881223e-02],\n",
      "       [-3.66636529e-03],\n",
      "       [-1.19106174e-01],\n",
      "       [-8.50797147e-02],\n",
      "       [-1.90626371e+00],\n",
      "       [-1.24957450e-01],\n",
      "       [-1.51553199e-01],\n",
      "       [-1.48690328e-01],\n",
      "       [-1.45590335e-01],\n",
      "       [-1.09023176e-01],\n",
      "       [-2.52603590e-01],\n",
      "       [-1.12781473e-01],\n",
      "       [-1.40370086e-01],\n",
      "       [-1.10193580e-01],\n",
      "       [-2.30297680e+01],\n",
      "       [-1.34139806e-01],\n",
      "       [-1.09262131e-01],\n",
      "       [-1.44449785e-01],\n",
      "       [-1.33960575e-01],\n",
      "       [-1.08862892e-01],\n",
      "       [-1.08031712e+01],\n",
      "       [-2.40811974e-01],\n",
      "       [-8.35576132e-02],\n",
      "       [-1.03687450e-01],\n",
      "       [-1.22225508e-01],\n",
      "       [-2.01329052e-01],\n",
      "       [-7.48282149e-02],\n",
      "       [-1.39801696e-01],\n",
      "       [-1.16556905e-01],\n",
      "       [-1.24257345e+01],\n",
      "       [-1.98035344e-01],\n",
      "       [-8.07216465e-02],\n",
      "       [-1.02864513e-02],\n",
      "       [-1.23925544e-01],\n",
      "       [-1.09154463e-01],\n",
      "       [-6.30799308e-03],\n",
      "       [-1.23415165e-01],\n",
      "       [-1.33339882e-01],\n",
      "       [-5.91903830e+00],\n",
      "       [-1.07249722e-01],\n",
      "       [-1.12781473e-01],\n",
      "       [-1.33339882e-01],\n",
      "       [-9.25715119e-02],\n",
      "       [-1.02362484e-01],\n",
      "       [-1.37525722e-01],\n",
      "       [-1.39270857e-01],\n",
      "       [-1.17834747e-01],\n",
      "       [-1.31772593e-01],\n",
      "       [-7.40119442e-02],\n",
      "       [-1.10576615e-01],\n",
      "       [-1.78601012e-01],\n",
      "       [-9.52628851e-02],\n",
      "       [-1.38089597e-01],\n",
      "       [-1.92339852e-01],\n",
      "       [-1.16652876e-01],\n",
      "       [-4.63623226e-01],\n",
      "       [-6.96964115e-02],\n",
      "       [-1.19108588e-01],\n",
      "       [-1.84799939e-01],\n",
      "       [-1.05912507e-01],\n",
      "       [-6.92364424e-02],\n",
      "       [-1.85545707e+00],\n",
      "       [-1.54699162e-01],\n",
      "       [-1.47745788e+00],\n",
      "       [-1.34139806e-01],\n",
      "       [-1.18472703e-01],\n",
      "       [-9.17996988e-02],\n",
      "       [-1.14427932e-01],\n",
      "       [-1.53439015e-01],\n",
      "       [-9.24745846e+00],\n",
      "       [-9.45551544e-02],\n",
      "       [-1.23503439e-01],\n",
      "       [-8.07216465e-02],\n",
      "       [-1.31260544e-01],\n",
      "       [-8.84537250e-02],\n",
      "       [-1.26321852e-01],\n",
      "       [-1.25777438e-01],\n",
      "       [-1.26321852e-01],\n",
      "       [-1.10193580e-01],\n",
      "       [-1.28659844e-01],\n",
      "       [-1.38312072e-01],\n",
      "       [-1.62822590e+01],\n",
      "       [-1.44227698e-01],\n",
      "       [-1.08770795e-01],\n",
      "       [-1.04023658e-01],\n",
      "       [-1.27137348e-01],\n",
      "       [-1.66447550e-01],\n",
      "       [-3.00837476e-02],\n",
      "       [-3.31270099e-02],\n",
      "       [-1.29408255e-01],\n",
      "       [-1.30983561e-01],\n",
      "       [-9.89111606e-03]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-0.695035  ],\n",
      "       [-0.7333458 ],\n",
      "       [-0.6913806 ],\n",
      "       [-0.6921457 ],\n",
      "       [-0.69246876],\n",
      "       [-0.6968401 ],\n",
      "       [-0.7231922 ],\n",
      "       [-0.6743078 ],\n",
      "       [-0.7375859 ],\n",
      "       [-0.6770264 ],\n",
      "       [-0.66477674],\n",
      "       [-0.7182657 ],\n",
      "       [-0.67941374],\n",
      "       [-0.6946612 ],\n",
      "       [-0.64402   ],\n",
      "       [-0.69656175],\n",
      "       [-0.69378704],\n",
      "       [-0.6883027 ],\n",
      "       [-0.7468252 ],\n",
      "       [-0.70777917],\n",
      "       [-0.67702913],\n",
      "       [-0.7220186 ],\n",
      "       [-0.6729278 ],\n",
      "       [-0.71517766],\n",
      "       [-0.6693313 ],\n",
      "       [-0.7404924 ],\n",
      "       [-0.6958525 ],\n",
      "       [-0.7282714 ],\n",
      "       [-0.71527773],\n",
      "       [-0.6940971 ],\n",
      "       [-0.699432  ],\n",
      "       [-0.69099057],\n",
      "       [-0.7063058 ],\n",
      "       [-0.73027235],\n",
      "       [-0.67506975],\n",
      "       [-0.6436169 ],\n",
      "       [-0.6770264 ],\n",
      "       [-0.6674838 ],\n",
      "       [-0.7153806 ],\n",
      "       [-0.719046  ],\n",
      "       [-0.7354003 ],\n",
      "       [-0.651323  ],\n",
      "       [-0.6578787 ],\n",
      "       [-0.6964284 ],\n",
      "       [-0.6925835 ],\n",
      "       [-0.7248672 ],\n",
      "       [-0.72420233],\n",
      "       [-0.6457764 ],\n",
      "       [-0.6907542 ],\n",
      "       [-0.695935  ],\n",
      "       [-0.67673814],\n",
      "       [-0.73884696],\n",
      "       [-0.7388662 ],\n",
      "       [-0.7388715 ],\n",
      "       [-0.72860485],\n",
      "       [-0.7227823 ],\n",
      "       [-0.70596594],\n",
      "       [-0.72330534],\n",
      "       [-0.67534643],\n",
      "       [-0.69104004],\n",
      "       [-0.67941374],\n",
      "       [-0.6766386 ],\n",
      "       [-0.71633154],\n",
      "       [-0.7058663 ],\n",
      "       [-0.645945  ],\n",
      "       [-0.7127123 ],\n",
      "       [-0.7270483 ],\n",
      "       [-0.7060868 ],\n",
      "       [-0.71476495],\n",
      "       [-0.6537322 ],\n",
      "       [-0.7156439 ],\n",
      "       [-0.7051763 ],\n",
      "       [-0.70435107],\n",
      "       [-0.72479725],\n",
      "       [-0.7135778 ],\n",
      "       [-0.7099304 ],\n",
      "       [-0.6457764 ],\n",
      "       [-0.72479725],\n",
      "       [-0.6678437 ],\n",
      "       [-0.654434  ],\n",
      "       [-0.6660085 ],\n",
      "       [-0.7231922 ],\n",
      "       [-0.6979235 ],\n",
      "       [-0.7065784 ],\n",
      "       [-0.7328884 ],\n",
      "       [-0.7159578 ],\n",
      "       [-0.7368411 ],\n",
      "       [-0.6922056 ],\n",
      "       [-0.7090062 ],\n",
      "       [-0.72691154],\n",
      "       [-0.7307699 ],\n",
      "       [-0.7057398 ],\n",
      "       [-0.713184  ],\n",
      "       [-0.7095132 ],\n",
      "       [-0.70660627],\n",
      "       [-0.65614724],\n",
      "       [-0.73054796],\n",
      "       [-0.71491605],\n",
      "       [-0.7193874 ],\n",
      "       [-0.71969277],\n",
      "       [-0.73884696],\n",
      "       [-0.70416623],\n",
      "       [-0.69035006],\n",
      "       [-0.6436169 ],\n",
      "       [-0.7200491 ],\n",
      "       [-0.69906646],\n",
      "       [-0.7215394 ],\n",
      "       [-0.71558535],\n",
      "       [-0.7060868 ],\n",
      "       [-0.71901166],\n",
      "       [-0.6978493 ],\n",
      "       [-0.6913806 ],\n",
      "       [-0.7004007 ],\n",
      "       [-0.6913806 ],\n",
      "       [-0.695935  ],\n",
      "       [-0.7386637 ],\n",
      "       [-0.7189929 ],\n",
      "       [-0.73915637],\n",
      "       [-0.7220036 ],\n",
      "       [-0.69257826],\n",
      "       [-0.6940971 ],\n",
      "       [-0.7331487 ],\n",
      "       [-0.7235864 ],\n",
      "       [-0.72551703],\n",
      "       [-0.6619655 ],\n",
      "       [-0.64402   ],\n",
      "       [-0.71868217],\n",
      "       [-0.73488915]], dtype=float32)>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob vec:  [<tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-0.8896171 ],\n",
      "       [-1.0314773 ],\n",
      "       [-0.9167702 ],\n",
      "       [-1.0794878 ],\n",
      "       [-1.3538914 ],\n",
      "       [-0.8762585 ],\n",
      "       [-0.8856162 ],\n",
      "       [-0.8669247 ],\n",
      "       [-0.887115  ],\n",
      "       [-1.0863314 ],\n",
      "       [-1.3853409 ],\n",
      "       [-1.0470366 ],\n",
      "       [-0.8767026 ],\n",
      "       [-1.375874  ],\n",
      "       [-1.0691506 ],\n",
      "       [-1.0881963 ],\n",
      "       [-0.89917725],\n",
      "       [-0.96040475],\n",
      "       [-1.0470366 ],\n",
      "       [-0.892238  ],\n",
      "       [-1.0577443 ],\n",
      "       [-1.3653429 ],\n",
      "       [-1.0666052 ],\n",
      "       [-0.8560283 ],\n",
      "       [-1.4169145 ],\n",
      "       [-0.90269685],\n",
      "       [-0.8895385 ],\n",
      "       [-0.87236375],\n",
      "       [-0.9151869 ],\n",
      "       [-0.8869134 ],\n",
      "       [-0.885551  ],\n",
      "       [-1.0776503 ],\n",
      "       [-0.86568844],\n",
      "       [-1.3563052 ],\n",
      "       [-0.95963836],\n",
      "       [-0.91759604],\n",
      "       [-1.4423327 ],\n",
      "       [-0.8911887 ],\n",
      "       [-1.0781261 ],\n",
      "       [-1.080787  ],\n",
      "       [-1.0831842 ],\n",
      "       [-0.9003215 ],\n",
      "       [-0.90276384],\n",
      "       [-0.9156685 ],\n",
      "       [-1.0721079 ],\n",
      "       [-0.8553368 ],\n",
      "       [-0.89587   ],\n",
      "       [-0.90003234],\n",
      "       [-1.0446279 ],\n",
      "       [-0.9167702 ],\n",
      "       [-0.88320386],\n",
      "       [-1.0566069 ],\n",
      "       [-0.90266585],\n",
      "       [-1.0748649 ],\n",
      "       [-1.0577443 ],\n",
      "       [-0.8686752 ],\n",
      "       [-0.8769922 ],\n",
      "       [-0.8941611 ],\n",
      "       [-1.368544  ],\n",
      "       [-1.0564884 ],\n",
      "       [-1.0446205 ],\n",
      "       [-1.3617959 ],\n",
      "       [-0.9122758 ],\n",
      "       [-1.4206989 ],\n",
      "       [-0.89447135],\n",
      "       [-0.90276384],\n",
      "       [-0.897593  ],\n",
      "       [-0.9180995 ],\n",
      "       [-1.0381204 ],\n",
      "       [-0.8931352 ],\n",
      "       [-0.88865185],\n",
      "       [-0.86774725],\n",
      "       [-1.0764605 ],\n",
      "       [-1.3784995 ],\n",
      "       [-1.0555792 ],\n",
      "       [-0.89000213],\n",
      "       [-1.027097  ],\n",
      "       [-0.8980403 ],\n",
      "       [-1.3927075 ],\n",
      "       [-1.0776218 ],\n",
      "       [-0.90161777],\n",
      "       [-1.072207  ],\n",
      "       [-1.0469785 ],\n",
      "       [-0.9606447 ],\n",
      "       [-1.0470517 ],\n",
      "       [-0.8726192 ],\n",
      "       [-0.9025805 ],\n",
      "       [-0.88657665],\n",
      "       [-0.8966042 ],\n",
      "       [-0.90053546],\n",
      "       [-0.8972671 ],\n",
      "       [-1.0569501 ],\n",
      "       [-0.9007497 ],\n",
      "       [-1.3927075 ],\n",
      "       [-1.101927  ],\n",
      "       [-0.8927548 ],\n",
      "       [-1.3956614 ],\n",
      "       [-0.8902893 ],\n",
      "       [-1.3933398 ],\n",
      "       [-0.8902893 ],\n",
      "       [-1.3867879 ],\n",
      "       [-1.0569501 ],\n",
      "       [-0.8903775 ],\n",
      "       [-0.89671737],\n",
      "       [-1.4023801 ],\n",
      "       [-0.87810063],\n",
      "       [-0.8706393 ],\n",
      "       [-1.0559549 ],\n",
      "       [-0.8856747 ],\n",
      "       [-0.8698305 ],\n",
      "       [-0.874495  ],\n",
      "       [-1.0536249 ],\n",
      "       [-1.3784995 ],\n",
      "       [-1.3563052 ],\n",
      "       [-0.9033829 ],\n",
      "       [-0.9142616 ],\n",
      "       [-0.8998681 ],\n",
      "       [-1.3918407 ],\n",
      "       [-0.885551  ],\n",
      "       [-1.0284406 ],\n",
      "       [-1.3639529 ],\n",
      "       [-0.88571787],\n",
      "       [-1.3910202 ],\n",
      "       [-0.9169968 ],\n",
      "       [-1.0736622 ],\n",
      "       [-0.897593  ],\n",
      "       [-0.9027838 ],\n",
      "       [-1.3889581 ]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-1.764745 ],\n",
      "       [-1.6652689],\n",
      "       [-1.6625855],\n",
      "       [-1.7029647],\n",
      "       [-1.7999513],\n",
      "       [-1.7227051],\n",
      "       [-1.7505426],\n",
      "       [-1.9854189],\n",
      "       [-1.775333 ],\n",
      "       [-1.79844  ],\n",
      "       [-1.7153747],\n",
      "       [-1.6740829],\n",
      "       [-1.8348482],\n",
      "       [-1.8132317],\n",
      "       [-1.8110065],\n",
      "       [-1.7918944],\n",
      "       [-1.6832321],\n",
      "       [-1.6806518],\n",
      "       [-1.6740829],\n",
      "       [-1.6844598],\n",
      "       [-1.709035 ],\n",
      "       [-1.8051267],\n",
      "       [-1.8115513],\n",
      "       [-1.7035906],\n",
      "       [-1.7147298],\n",
      "       [-1.693095 ],\n",
      "       [-1.764504 ],\n",
      "       [-1.7019241],\n",
      "       [-1.65328  ],\n",
      "       [-1.7113043],\n",
      "       [-1.7433442],\n",
      "       [-1.6873859],\n",
      "       [-1.6679561],\n",
      "       [-1.7638944],\n",
      "       [-1.672666 ],\n",
      "       [-1.6619056],\n",
      "       [-1.7102762],\n",
      "       [-1.8502464],\n",
      "       [-1.7039647],\n",
      "       [-1.7429643],\n",
      "       [-1.7977917],\n",
      "       [-1.7967621],\n",
      "       [-1.660264 ],\n",
      "       [-1.6575198],\n",
      "       [-1.6629349],\n",
      "       [-1.7045182],\n",
      "       [-1.8130659],\n",
      "       [-1.6641669],\n",
      "       [-1.6716694],\n",
      "       [-1.6625855],\n",
      "       [-1.6908028],\n",
      "       [-1.7059929],\n",
      "       [-1.6932385],\n",
      "       [-1.680961 ],\n",
      "       [-1.709035 ],\n",
      "       [-1.7341506],\n",
      "       [-1.716007 ],\n",
      "       [-1.6587355],\n",
      "       [-1.8086423],\n",
      "       [-1.7044283],\n",
      "       [-1.671589 ],\n",
      "       [-1.7898666],\n",
      "       [-1.6492343],\n",
      "       [-1.6698828],\n",
      "       [-1.6582896],\n",
      "       [-1.660264 ],\n",
      "       [-1.7749059],\n",
      "       [-1.8241972],\n",
      "       [-1.6622831],\n",
      "       [-1.8054103],\n",
      "       [-1.7072188],\n",
      "       [-1.748162 ],\n",
      "       [-1.8000913],\n",
      "       [-1.7364278],\n",
      "       [-1.705956 ],\n",
      "       [-1.6888944],\n",
      "       [-1.6534399],\n",
      "       [-1.6838617],\n",
      "       [-1.6823671],\n",
      "       [-1.6639606],\n",
      "       [-1.6626097],\n",
      "       [-1.7865076],\n",
      "       [-1.6729491],\n",
      "       [-1.6538283],\n",
      "       [-1.6739249],\n",
      "       [-1.6936865],\n",
      "       [-1.6950507],\n",
      "       [-1.675428 ],\n",
      "       [-1.6569879],\n",
      "       [-1.8010463],\n",
      "       [-1.7869323],\n",
      "       [-1.7079548],\n",
      "       [-1.7959803],\n",
      "       [-1.6823671],\n",
      "       [-1.7046052],\n",
      "       [-1.8572497],\n",
      "       [-1.7152133],\n",
      "       [-1.762191 ],\n",
      "       [-1.6973859],\n",
      "       [-1.762191 ],\n",
      "       [-1.6846621],\n",
      "       [-1.7079548],\n",
      "       [-1.7624456],\n",
      "       [-1.6572716],\n",
      "       [-1.6729314],\n",
      "       [-1.7088524],\n",
      "       [-1.7249688],\n",
      "       [-1.7055933],\n",
      "       [-1.7504739],\n",
      "       [-1.6945758],\n",
      "       [-1.6932366],\n",
      "       [-1.6980596],\n",
      "       [-1.7364278],\n",
      "       [-1.7638944],\n",
      "       [-1.6502888],\n",
      "       [-1.8076438],\n",
      "       [-1.6493672],\n",
      "       [-1.6941476],\n",
      "       [-1.7433442],\n",
      "       [-1.6597351],\n",
      "       [-1.7929083],\n",
      "       [-1.6900067],\n",
      "       [-1.8095208],\n",
      "       [-1.6623619],\n",
      "       [-1.7731614],\n",
      "       [-1.7749059],\n",
      "       [-1.6600857],\n",
      "       [-1.6861492]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-0.6208709 ],\n",
      "       [-0.57257324],\n",
      "       [-0.60219437],\n",
      "       [-0.7784256 ],\n",
      "       [-0.5818819 ],\n",
      "       [-0.63919264],\n",
      "       [-0.7552557 ],\n",
      "       [-0.7458338 ],\n",
      "       [-0.76699734],\n",
      "       [-0.77486175],\n",
      "       [-0.77560127],\n",
      "       [-0.58555496],\n",
      "       [-0.75259614],\n",
      "       [-0.5860546 ],\n",
      "       [-0.5910779 ],\n",
      "       [-0.61784136],\n",
      "       [-0.6239975 ],\n",
      "       [-0.55572146],\n",
      "       [-0.58555496],\n",
      "       [-0.63044983],\n",
      "       [-0.5923268 ],\n",
      "       [-0.5843629 ],\n",
      "       [-0.59258217],\n",
      "       [-0.7322254 ],\n",
      "       [-0.6360887 ],\n",
      "       [-0.61555344],\n",
      "       [-0.62096924],\n",
      "       [-0.6453138 ],\n",
      "       [-0.6103741 ],\n",
      "       [-0.6198083 ],\n",
      "       [-0.75410795],\n",
      "       [-0.61772144],\n",
      "       [-0.73251474],\n",
      "       [-0.6121611 ],\n",
      "       [-0.54970145],\n",
      "       [-0.60185665],\n",
      "       [-0.641078  ],\n",
      "       [-0.6233329 ],\n",
      "       [-0.77844495],\n",
      "       [-0.6318533 ],\n",
      "       [-0.7876246 ],\n",
      "       [-0.6215613 ],\n",
      "       [-0.61487275],\n",
      "       [-0.60654783],\n",
      "       [-0.59950894],\n",
      "       [-0.7325044 ],\n",
      "       [-0.61872584],\n",
      "       [-0.61693007],\n",
      "       [-0.588731  ],\n",
      "       [-0.60219437],\n",
      "       [-0.6363015 ],\n",
      "       [-0.59510523],\n",
      "       [-0.61545736],\n",
      "       [-0.7555085 ],\n",
      "       [-0.5923268 ],\n",
      "       [-0.75007963],\n",
      "       [-0.6394741 ],\n",
      "       [-0.7584652 ],\n",
      "       [-0.58483446],\n",
      "       [-0.59590137],\n",
      "       [-0.58851135],\n",
      "       [-0.7985968 ],\n",
      "       [-0.61679876],\n",
      "       [-0.6267347 ],\n",
      "       [-0.7571382 ],\n",
      "       [-0.61487275],\n",
      "       [-0.6252583 ],\n",
      "       [-0.5993147 ],\n",
      "       [-0.60012203],\n",
      "       [-0.62244606],\n",
      "       [-0.6191318 ],\n",
      "       [-0.6379299 ],\n",
      "       [-0.6114253 ],\n",
      "       [-0.5928839 ],\n",
      "       [-0.5986849 ],\n",
      "       [-0.6303322 ],\n",
      "       [-0.5551816 ],\n",
      "       [-0.62484604],\n",
      "       [-0.5954246 ],\n",
      "       [-0.59811366],\n",
      "       [-0.61497074],\n",
      "       [-0.7807932 ],\n",
      "       [-0.58424157],\n",
      "       [-0.5290383 ],\n",
      "       [-0.5854684 ],\n",
      "       [-0.64483166],\n",
      "       [-0.61441386],\n",
      "       [-0.6324109 ],\n",
      "       [-0.75521165],\n",
      "       [-0.62252045],\n",
      "       [-0.7761436 ],\n",
      "       [-0.5933881 ],\n",
      "       [-0.7776849 ],\n",
      "       [-0.5954246 ],\n",
      "       [-0.74438375],\n",
      "       [-0.62308615],\n",
      "       [-0.772018  ],\n",
      "       [-0.6210809 ],\n",
      "       [-0.61875254],\n",
      "       [-0.6210809 ],\n",
      "       [-0.6091806 ],\n",
      "       [-0.5933881 ],\n",
      "       [-0.6208812 ],\n",
      "       [-0.755854  ],\n",
      "       [-0.6066206 ],\n",
      "       [-0.63868064],\n",
      "       [-0.7491571 ],\n",
      "       [-0.5965507 ],\n",
      "       [-0.7552011 ],\n",
      "       [-0.6469157 ],\n",
      "       [-0.64353037],\n",
      "       [-0.60577196],\n",
      "       [-0.5928839 ],\n",
      "       [-0.6121611 ],\n",
      "       [-0.6215854 ],\n",
      "       [-0.5933367 ],\n",
      "       [-0.7502607 ],\n",
      "       [-0.6180935 ],\n",
      "       [-0.75410795],\n",
      "       [-0.56626105],\n",
      "       [-0.7863728 ],\n",
      "       [-0.63355345],\n",
      "       [-0.5975479 ],\n",
      "       [-0.6021258 ],\n",
      "       [-0.6119082 ],\n",
      "       [-0.6252583 ],\n",
      "       [-0.614837  ],\n",
      "       [-0.5911019 ]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-0.73049456],\n",
      "       [-0.778878  ],\n",
      "       [-0.70005465],\n",
      "       [-0.7204232 ],\n",
      "       [-0.7071982 ],\n",
      "       [-0.6232551 ],\n",
      "       [-0.64806914],\n",
      "       [-0.78674746],\n",
      "       [-0.7461654 ],\n",
      "       [-0.6690274 ],\n",
      "       [-0.74806637],\n",
      "       [-0.7026855 ],\n",
      "       [-0.7487872 ],\n",
      "       [-0.69826627],\n",
      "       [-0.7059548 ],\n",
      "       [-0.66780466],\n",
      "       [-0.712176  ],\n",
      "       [-0.7567617 ],\n",
      "       [-0.7026855 ],\n",
      "       [-0.65331686],\n",
      "       [-0.70037746],\n",
      "       [-0.6967606 ],\n",
      "       [-0.7052475 ],\n",
      "       [-0.60000193],\n",
      "       [-0.7366642 ],\n",
      "       [-0.70247173],\n",
      "       [-0.73063093],\n",
      "       [-0.72451496],\n",
      "       [-0.7045701 ],\n",
      "       [-0.6387876 ],\n",
      "       [-0.64540416],\n",
      "       [-0.6736151 ],\n",
      "       [-0.6209806 ],\n",
      "       [-0.6693401 ],\n",
      "       [-0.7731301 ],\n",
      "       [-0.70209503],\n",
      "       [-0.6499657 ],\n",
      "       [-0.64442366],\n",
      "       [-0.7209059 ],\n",
      "       [-0.6447708 ],\n",
      "       [-0.7320794 ],\n",
      "       [-0.67882085],\n",
      "       [-0.68594825],\n",
      "       [-0.7014991 ],\n",
      "       [-0.6778712 ],\n",
      "       [-0.60180795],\n",
      "       [-0.7164862 ],\n",
      "       [-0.6781276 ],\n",
      "       [-0.7038622 ],\n",
      "       [-0.70005465],\n",
      "       [-0.72200835],\n",
      "       [-0.7038235 ],\n",
      "       [-0.70220447],\n",
      "       [-0.6597393 ],\n",
      "       [-0.70037746],\n",
      "       [-0.75253904],\n",
      "       [-0.6197926 ],\n",
      "       [-0.734656  ],\n",
      "       [-0.6970239 ],\n",
      "       [-0.70748544],\n",
      "       [-0.70419407],\n",
      "       [-0.71724033],\n",
      "       [-0.7059137 ],\n",
      "       [-0.7181324 ],\n",
      "       [-0.7346902 ],\n",
      "       [-0.68594825],\n",
      "       [-0.66736287],\n",
      "       [-0.7055633 ],\n",
      "       [-0.7080276 ],\n",
      "       [-0.7165862 ],\n",
      "       [-0.6379313 ],\n",
      "       [-0.62991   ],\n",
      "       [-0.68095315],\n",
      "       [-0.719751  ],\n",
      "       [-0.70767725],\n",
      "       [-0.71987706],\n",
      "       [-0.7917716 ],\n",
      "       [-0.71386933],\n",
      "       [-0.69346106],\n",
      "       [-0.6760637 ],\n",
      "       [-0.6824442 ],\n",
      "       [-0.71468335],\n",
      "       [-0.70588386],\n",
      "       [-0.61351025],\n",
      "       [-0.703039  ],\n",
      "       [-0.72404397],\n",
      "       [-0.70159465],\n",
      "       [-0.6620391 ],\n",
      "       [-0.7348148 ],\n",
      "       [-0.6800594 ],\n",
      "       [-0.73283386],\n",
      "       [-0.70128083],\n",
      "       [-0.73383164],\n",
      "       [-0.69346106],\n",
      "       [-0.61662424],\n",
      "       [-0.6419065 ],\n",
      "       [-0.74857527],\n",
      "       [-0.73171204],\n",
      "       [-0.6252468 ],\n",
      "       [-0.73171204],\n",
      "       [-0.68833756],\n",
      "       [-0.70128083],\n",
      "       [-0.7315816 ],\n",
      "       [-0.7356587 ],\n",
      "       [-0.6990478 ],\n",
      "       [-0.616817  ],\n",
      "       [-0.75999045],\n",
      "       [-0.70559484],\n",
      "       [-0.6482221 ],\n",
      "       [-0.72559935],\n",
      "       [-0.7234343 ],\n",
      "       [-0.72086596],\n",
      "       [-0.719751  ],\n",
      "       [-0.6693401 ],\n",
      "       [-0.6873406 ],\n",
      "       [-0.7003665 ],\n",
      "       [-0.6647948 ],\n",
      "       [-0.62287617],\n",
      "       [-0.64540416],\n",
      "       [-0.7843734 ],\n",
      "       [-0.72807   ],\n",
      "       [-0.7214358 ],\n",
      "       [-0.70970577],\n",
      "       [-0.7007055 ],\n",
      "       [-0.7260103 ],\n",
      "       [-0.66736287],\n",
      "       [-0.6854135 ],\n",
      "       [-0.6907745 ]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-0.7269425 ],\n",
      "       [-0.7609219 ],\n",
      "       [-0.6111074 ],\n",
      "       [-0.76286715],\n",
      "       [-0.6072786 ],\n",
      "       [-0.708942  ],\n",
      "       [-0.6248483 ],\n",
      "       [-0.70156157],\n",
      "       [-0.769701  ],\n",
      "       [-0.60890996],\n",
      "       [-0.7204438 ],\n",
      "       [-0.6027206 ],\n",
      "       [-0.71023893],\n",
      "       [-0.6213556 ],\n",
      "       [-0.6059207 ],\n",
      "       [-0.6087377 ],\n",
      "       [-0.7421619 ],\n",
      "       [-0.61536604],\n",
      "       [-0.6027206 ],\n",
      "       [-0.7506273 ],\n",
      "       [-0.77315724],\n",
      "       [-0.61029345],\n",
      "       [-0.60912925],\n",
      "       [-0.69238865],\n",
      "       [-0.69785035],\n",
      "       [-0.76849073],\n",
      "       [-0.7268472 ],\n",
      "       [-0.6819754 ],\n",
      "       [-0.6231966 ],\n",
      "       [-0.71496147],\n",
      "       [-0.62017375],\n",
      "       [-0.76745015],\n",
      "       [-0.73359   ],\n",
      "       [-0.6057229 ],\n",
      "       [-0.78385353],\n",
      "       [-0.6106974 ],\n",
      "       [-0.67800933],\n",
      "       [-0.7371729 ],\n",
      "       [-0.7599174 ],\n",
      "       [-0.73222375],\n",
      "       [-0.60980546],\n",
      "       [-0.6220721 ],\n",
      "       [-0.6105116 ],\n",
      "       [-0.6173192 ],\n",
      "       [-0.5947658 ],\n",
      "       [-0.69139636],\n",
      "       [-0.72786564],\n",
      "       [-0.6042564 ],\n",
      "       [-0.60752994],\n",
      "       [-0.6111074 ],\n",
      "       [-0.70216054],\n",
      "       [-0.7696279 ],\n",
      "       [-0.7685304 ],\n",
      "       [-0.7407062 ],\n",
      "       [-0.77315724],\n",
      "       [-0.70582825],\n",
      "       [-0.7109951 ],\n",
      "       [-0.6250033 ],\n",
      "       [-0.61352855],\n",
      "       [-0.76777214],\n",
      "       [-0.6073846 ],\n",
      "       [-0.6140627 ],\n",
      "       [-0.6328786 ],\n",
      "       [-0.66289836],\n",
      "       [-0.628072  ],\n",
      "       [-0.6105116 ],\n",
      "       [-0.60680306],\n",
      "       [-0.6301035 ],\n",
      "       [-0.62549365],\n",
      "       [-0.72125626],\n",
      "       [-0.71689856],\n",
      "       [-0.68870467],\n",
      "       [-0.59612846],\n",
      "       [-0.75557214],\n",
      "       [-0.7629059 ],\n",
      "       [-0.7128742 ],\n",
      "       [-0.76373494],\n",
      "       [-0.73442453],\n",
      "       [-0.6220485 ],\n",
      "       [-0.5907327 ],\n",
      "       [-0.6071714 ],\n",
      "       [-0.7708906 ],\n",
      "       [-0.6019245 ],\n",
      "       [-0.5829865 ],\n",
      "       [-0.60267586],\n",
      "       [-0.68375015],\n",
      "       [-0.77081823],\n",
      "       [-0.7143542 ],\n",
      "       [-0.636794  ],\n",
      "       [-0.62835234],\n",
      "       [-0.6137906 ],\n",
      "       [-0.77154803],\n",
      "       [-0.62652624],\n",
      "       [-0.6220485 ],\n",
      "       [-0.7101558 ],\n",
      "       [-0.7417822 ],\n",
      "       [-0.70475984],\n",
      "       [-0.730508  ],\n",
      "       [-0.6945885 ],\n",
      "       [-0.730508  ],\n",
      "       [-0.62374747],\n",
      "       [-0.77154803],\n",
      "       [-0.73057836],\n",
      "       [-0.63661236],\n",
      "       [-0.63339305],\n",
      "       [-0.7110661 ],\n",
      "       [-0.7138699 ],\n",
      "       [-0.76699483],\n",
      "       [-0.6247133 ],\n",
      "       [-0.6787222 ],\n",
      "       [-0.68674165],\n",
      "       [-0.7496226 ],\n",
      "       [-0.75557214],\n",
      "       [-0.6057229 ],\n",
      "       [-0.62936956],\n",
      "       [-0.6063186 ],\n",
      "       [-0.6568055 ],\n",
      "       [-0.69539785],\n",
      "       [-0.62017375],\n",
      "       [-0.7623695 ],\n",
      "       [-0.6167623 ],\n",
      "       [-0.706337  ],\n",
      "       [-0.6441538 ],\n",
      "       [-0.61096877],\n",
      "       [-0.74265945],\n",
      "       [-0.60680306],\n",
      "       [-0.61059254],\n",
      "       [-0.6175137 ]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-5.6089181e-02],\n",
      "       [-6.3877463e-02],\n",
      "       [-3.8371944e-01],\n",
      "       [-3.6475736e-01],\n",
      "       [-3.2589737e-01],\n",
      "       [-3.8473517e-02],\n",
      "       [-1.5328224e-01],\n",
      "       [-2.0629978e-01],\n",
      "       [-1.4865653e-01],\n",
      "       [-3.6721456e-01],\n",
      "       [-3.4103206e-01],\n",
      "       [-3.6302358e-01],\n",
      "       [-2.1787233e+00],\n",
      "       [-1.9378923e-03],\n",
      "       [-3.5062310e-01],\n",
      "       [-5.9337270e-02],\n",
      "       [-7.1346337e-01],\n",
      "       [-7.8197247e-01],\n",
      "       [-3.6302358e-01],\n",
      "       [-1.7424142e-01],\n",
      "       [-3.5444379e-01],\n",
      "       [-3.4269741e-01],\n",
      "       [-1.4648828e-01],\n",
      "       [-5.4182924e-02],\n",
      "       [-4.5058460e+00],\n",
      "       [-4.0706493e-02],\n",
      "       [-5.6034021e-02],\n",
      "       [-1.2767832e+01],\n",
      "       [-4.2822871e-02],\n",
      "       [-1.0192851e-04],\n",
      "       [-3.6104414e-01],\n",
      "       [-3.7134138e-01],\n",
      "       [-3.9737424e-01],\n",
      "       [-3.5424560e-01],\n",
      "       [-1.2315993e+00],\n",
      "       [-3.8278148e-01],\n",
      "       [-6.4050469e+00],\n",
      "       [-2.2115066e-06],\n",
      "       [-1.5595412e-01],\n",
      "       [-3.7767506e-01],\n",
      "       [-3.4835660e-01],\n",
      "       [-4.9496487e-02],\n",
      "       [-3.8192935e-02],\n",
      "       [-1.7138487e-01],\n",
      "       [-1.5212746e-01],\n",
      "       [-5.4381717e-02],\n",
      "       [-7.1142954e-01],\n",
      "       [-3.6950642e-01],\n",
      "       [-1.5658264e-01],\n",
      "       [-3.8371944e-01],\n",
      "       [-5.3115711e+00],\n",
      "       [-3.0365879e-02],\n",
      "       [-4.0792476e-02],\n",
      "       [-1.7888451e-01],\n",
      "       [-3.5444379e-01],\n",
      "       [-7.4964249e-01],\n",
      "       [-1.6386357e-01],\n",
      "       [-3.8119158e-01],\n",
      "       [-1.4114964e-01],\n",
      "       [-1.4111251e-03],\n",
      "       [-1.5631060e-01],\n",
      "       [-3.3793771e-01],\n",
      "       [-4.3214951e-02],\n",
      "       [-2.8048854e+00],\n",
      "       [-1.6691190e-01],\n",
      "       [-3.8192935e-02],\n",
      "       [-3.6861080e-01],\n",
      "       [-4.3657234e-01],\n",
      "       [-4.1827968e-01],\n",
      "       [-1.1075593e+00],\n",
      "       [-3.9955080e-02],\n",
      "       [-7.3043519e-01],\n",
      "       [-3.5886723e-01],\n",
      "       [-3.4059319e-01],\n",
      "       [-6.0925256e-02],\n",
      "       [-3.5673013e+00],\n",
      "       [-6.8233632e-02],\n",
      "       [-1.1036288e+00],\n",
      "       [-4.1457711e-04],\n",
      "       [-3.5554925e-01],\n",
      "       [-1.6139533e-01],\n",
      "       [-1.6027766e-01],\n",
      "       [-3.6030170e-01],\n",
      "       [-2.4995983e-01],\n",
      "       [-3.6276406e-01],\n",
      "       [-1.1235854e+01],\n",
      "       [-1.6964991e-01],\n",
      "       [-1.5264988e+00],\n",
      "       [-4.7235500e-02],\n",
      "       [-1.8245538e-01],\n",
      "       [-3.5044271e-01],\n",
      "       [-1.4812218e-01],\n",
      "       [-1.5006668e-03],\n",
      "       [-4.1457711e-04],\n",
      "       [-3.7069884e-01],\n",
      "       [-1.7239851e-01],\n",
      "       [-7.1818158e-02],\n",
      "       [-3.4936938e-02],\n",
      "       [-2.3822421e-02],\n",
      "       [-3.4936938e-02],\n",
      "       [-5.1360875e-02],\n",
      "       [-1.4812218e-01],\n",
      "       [-3.4822796e-02],\n",
      "       [-4.7669496e-02],\n",
      "       [-1.8682660e-01],\n",
      "       [-3.7425965e-01],\n",
      "       [-3.5355993e-02],\n",
      "       [-1.3180103e-03],\n",
      "       [-1.5336584e-01],\n",
      "       [-1.4242321e+01],\n",
      "       [-9.8687000e+00],\n",
      "       [-1.1836648e+00],\n",
      "       [-3.4059319e-01],\n",
      "       [-3.5424560e-01],\n",
      "       [-3.9908108e-01],\n",
      "       [-3.5498437e-01],\n",
      "       [-1.5505632e+00],\n",
      "       [-1.3304889e-01],\n",
      "       [-3.6104414e-01],\n",
      "       [-6.5055341e-02],\n",
      "       [-3.4070438e-01],\n",
      "       [-4.3897123e+00],\n",
      "       [-1.1984065e+00],\n",
      "       [-3.8338441e-01],\n",
      "       [-3.5960317e-01],\n",
      "       [-3.6861080e-01],\n",
      "       [-3.8174398e-02],\n",
      "       [-3.4456309e-02]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-1.10055260e-01],\n",
      "       [-1.38343792e+01],\n",
      "       [-1.39097407e-01],\n",
      "       [-8.50721672e-02],\n",
      "       [-1.84086382e-01],\n",
      "       [-9.51752961e-02],\n",
      "       [-1.04586788e-01],\n",
      "       [-8.50563869e-02],\n",
      "       [-3.18226814e-02],\n",
      "       [-1.25983909e-01],\n",
      "       [-1.04329377e-01],\n",
      "       [-1.23599984e-01],\n",
      "       [-1.38281479e-01],\n",
      "       [-1.34934008e-01],\n",
      "       [-1.16732918e-01],\n",
      "       [-1.21556208e-01],\n",
      "       [-8.84338915e-02],\n",
      "       [-1.41843290e+01],\n",
      "       [-1.23599984e-01],\n",
      "       [-1.92518474e-03],\n",
      "       [-1.43184707e-01],\n",
      "       [-1.18472703e-01],\n",
      "       [-1.35207117e-01],\n",
      "       [-9.03475061e-02],\n",
      "       [-1.85163423e-01],\n",
      "       [-1.07775882e-01],\n",
      "       [-1.24734759e-01],\n",
      "       [-1.76365711e-02],\n",
      "       [-1.04473971e-01],\n",
      "       [-1.17617309e-01],\n",
      "       [-1.13317683e-01],\n",
      "       [-1.16096415e-01],\n",
      "       [-3.09342612e-02],\n",
      "       [-1.00804657e-01],\n",
      "       [-2.24025154e+01],\n",
      "       [-9.51252505e-02],\n",
      "       [-2.91688070e-02],\n",
      "       [-7.67854452e-02],\n",
      "       [-1.27715677e-01],\n",
      "       [-1.18147999e-01],\n",
      "       [-1.11993641e-01],\n",
      "       [-1.08648479e-01],\n",
      "       [-1.28809243e-01],\n",
      "       [-1.44945323e-01],\n",
      "       [-1.11052535e-01],\n",
      "       [-1.23147294e-02],\n",
      "       [-1.56855777e-01],\n",
      "       [-1.05736934e-01],\n",
      "       [-1.44227698e-01],\n",
      "       [-1.39097407e-01],\n",
      "       [-1.62028611e-01],\n",
      "       [-1.17286474e-01],\n",
      "       [-1.22314833e-01],\n",
      "       [-1.14223532e-01],\n",
      "       [-1.43184707e-01],\n",
      "       [-1.36454657e-01],\n",
      "       [-1.03364132e-01],\n",
      "       [-1.23925544e-01],\n",
      "       [-1.24638066e-01],\n",
      "       [-5.73845580e-02],\n",
      "       [-1.28290430e-01],\n",
      "       [-1.41303524e-01],\n",
      "       [-1.42420158e-01],\n",
      "       [-1.72838002e-01],\n",
      "       [-1.25982106e-01],\n",
      "       [-1.28809243e-01],\n",
      "       [-1.06030844e-01],\n",
      "       [-1.60985157e-01],\n",
      "       [-1.43419713e-01],\n",
      "       [-1.74760129e-02],\n",
      "       [-8.43834579e-02],\n",
      "       [-1.31935254e-01],\n",
      "       [-1.11089207e-01],\n",
      "       [-1.10352337e-01],\n",
      "       [-1.38443008e-01],\n",
      "       [-1.01012021e-01],\n",
      "       [-2.68280315e+01],\n",
      "       [-1.53439015e-01],\n",
      "       [-1.33794129e-01],\n",
      "       [-1.10193580e-01],\n",
      "       [-1.24295667e-01],\n",
      "       [-1.09497167e-01],\n",
      "       [-1.84845254e-02],\n",
      "       [-4.30447426e+01],\n",
      "       [-1.08919539e-01],\n",
      "       [-2.07052216e-01],\n",
      "       [-1.19330458e-01],\n",
      "       [-1.23927519e-01],\n",
      "       [-1.49182200e-01],\n",
      "       [-1.13829061e-01],\n",
      "       [-1.21135257e-01],\n",
      "       [-1.45590335e-01],\n",
      "       [-1.53416976e-01],\n",
      "       [-1.33794129e-01],\n",
      "       [-9.08299088e-02],\n",
      "       [-9.32440460e-02],\n",
      "       [-1.44725204e-01],\n",
      "       [-1.44449785e-01],\n",
      "       [-4.66884822e-02],\n",
      "       [-1.44449785e-01],\n",
      "       [-8.18094239e-02],\n",
      "       [-1.45590335e-01],\n",
      "       [-1.28659844e-01],\n",
      "       [-1.18055947e-01],\n",
      "       [-1.59792364e-01],\n",
      "       [-9.92148593e-02],\n",
      "       [-3.18337440e-01],\n",
      "       [-1.52077377e-01],\n",
      "       [-1.18836485e-01],\n",
      "       [-2.13411897e-01],\n",
      "       [-2.22657621e-01],\n",
      "       [-4.16768223e-01],\n",
      "       [-1.10352337e-01],\n",
      "       [-1.00804657e-01],\n",
      "       [-1.49080783e-01],\n",
      "       [-1.26321852e-01],\n",
      "       [-1.52455524e-01],\n",
      "       [-1.13721460e-01],\n",
      "       [-1.13317683e-01],\n",
      "       [-1.84903221e+01],\n",
      "       [-1.36374016e-04],\n",
      "       [-1.89864561e-01],\n",
      "       [-1.98042020e-02],\n",
      "       [-1.23503439e-01],\n",
      "       [-5.40585853e-02],\n",
      "       [-1.06030844e-01],\n",
      "       [-1.13851361e-01],\n",
      "       [-1.46227241e-01]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-0.7375207 ],\n",
      "       [-0.6961481 ],\n",
      "       [-0.71610314],\n",
      "       [-0.7240308 ],\n",
      "       [-0.69286126],\n",
      "       [-0.7249288 ],\n",
      "       [-0.6309507 ],\n",
      "       [-0.7552878 ],\n",
      "       [-0.63266724],\n",
      "       [-0.66681653],\n",
      "       [-0.72459626],\n",
      "       [-0.722999  ],\n",
      "       [-0.72603565],\n",
      "       [-0.7035271 ],\n",
      "       [-0.6888646 ],\n",
      "       [-0.66448766],\n",
      "       [-0.71980524],\n",
      "       [-0.7325597 ],\n",
      "       [-0.722999  ],\n",
      "       [-0.7127269 ],\n",
      "       [-0.6906934 ],\n",
      "       [-0.70416623],\n",
      "       [-0.6884438 ],\n",
      "       [-0.7512537 ],\n",
      "       [-0.73737824],\n",
      "       [-0.7007676 ],\n",
      "       [-0.7375859 ],\n",
      "       [-0.73292506],\n",
      "       [-0.7181512 ],\n",
      "       [-0.68880063],\n",
      "       [-0.6300092 ],\n",
      "       [-0.71908575],\n",
      "       [-0.73213506],\n",
      "       [-0.67702913],\n",
      "       [-0.7188585 ],\n",
      "       [-0.71466744],\n",
      "       [-0.72420406],\n",
      "       [-0.6883742 ],\n",
      "       [-0.72328854],\n",
      "       [-0.7409151 ],\n",
      "       [-0.658487  ],\n",
      "       [-0.6772582 ],\n",
      "       [-0.69904995],\n",
      "       [-0.7176309 ],\n",
      "       [-0.69880337],\n",
      "       [-0.7545631 ],\n",
      "       [-0.7070177 ],\n",
      "       [-0.6840092 ],\n",
      "       [-0.7220036 ],\n",
      "       [-0.71610314],\n",
      "       [-0.7283502 ],\n",
      "       [-0.6966923 ],\n",
      "       [-0.7003088 ],\n",
      "       [-0.7358552 ],\n",
      "       [-0.6906934 ],\n",
      "       [-0.7572102 ],\n",
      "       [-0.7267981 ],\n",
      "       [-0.6537322 ],\n",
      "       [-0.70346373],\n",
      "       [-0.7011124 ],\n",
      "       [-0.72200674],\n",
      "       [-0.67027664],\n",
      "       [-0.7187088 ],\n",
      "       [-0.6815147 ],\n",
      "       [-0.65403634],\n",
      "       [-0.69904995],\n",
      "       [-0.6682819 ],\n",
      "       [-0.6887049 ],\n",
      "       [-0.7183917 ],\n",
      "       [-0.71249545],\n",
      "       [-0.6900662 ],\n",
      "       [-0.7199707 ],\n",
      "       [-0.691264  ],\n",
      "       [-0.6808519 ],\n",
      "       [-0.70290124],\n",
      "       [-0.72816193],\n",
      "       [-0.68350524],\n",
      "       [-0.7200491 ],\n",
      "       [-0.7163837 ],\n",
      "       [-0.695935  ],\n",
      "       [-0.6920353 ],\n",
      "       [-0.6717191 ],\n",
      "       [-0.7220765 ],\n",
      "       [-0.7398846 ],\n",
      "       [-0.7229121 ],\n",
      "       [-0.72653896],\n",
      "       [-0.6985763 ],\n",
      "       [-0.69017184],\n",
      "       [-0.6554685 ],\n",
      "       [-0.6781862 ],\n",
      "       [-0.6484915 ],\n",
      "       [-0.6925835 ],\n",
      "       [-0.6490374 ],\n",
      "       [-0.7163837 ],\n",
      "       [-0.7667172 ],\n",
      "       [-0.6900075 ],\n",
      "       [-0.72581935],\n",
      "       [-0.7388715 ],\n",
      "       [-0.7167664 ],\n",
      "       [-0.7388715 ],\n",
      "       [-0.692791  ],\n",
      "       [-0.6925835 ],\n",
      "       [-0.7386637 ],\n",
      "       [-0.6550057 ],\n",
      "       [-0.7045152 ],\n",
      "       [-0.728177  ],\n",
      "       [-0.767661  ],\n",
      "       [-0.699432  ],\n",
      "       [-0.6312216 ],\n",
      "       [-0.7257606 ],\n",
      "       [-0.72644913],\n",
      "       [-0.7178134 ],\n",
      "       [-0.6808519 ],\n",
      "       [-0.67702913],\n",
      "       [-0.7169293 ],\n",
      "       [-0.6913806 ],\n",
      "       [-0.6791224 ],\n",
      "       [-0.7159554 ],\n",
      "       [-0.6300092 ],\n",
      "       [-0.69165176],\n",
      "       [-0.6543392 ],\n",
      "       [-0.7265691 ],\n",
      "       [-0.693501  ],\n",
      "       [-0.71558535],\n",
      "       [-0.72882473],\n",
      "       [-0.6682819 ],\n",
      "       [-0.6983296 ],\n",
      "       [-0.72106165]], dtype=float32)>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob vec:  [<tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-0.9586761 ],\n",
      "       [-1.3964126 ],\n",
      "       [-1.364038  ],\n",
      "       [-1.0772597 ],\n",
      "       [-1.0529472 ],\n",
      "       [-0.9083841 ],\n",
      "       [-0.91419035],\n",
      "       [-0.9149988 ],\n",
      "       [-0.86223936],\n",
      "       [-0.88847893],\n",
      "       [-0.91570204],\n",
      "       [-0.8895916 ],\n",
      "       [-0.9000486 ],\n",
      "       [-1.4013807 ],\n",
      "       [-1.4259698 ],\n",
      "       [-1.0775173 ],\n",
      "       [-0.88377833],\n",
      "       [-0.89201975],\n",
      "       [-0.9024838 ],\n",
      "       [-0.8935193 ],\n",
      "       [-0.90093005],\n",
      "       [-0.89226246],\n",
      "       [-1.0730274 ],\n",
      "       [-0.9021557 ],\n",
      "       [-1.0710069 ],\n",
      "       [-0.889045  ],\n",
      "       [-1.0730274 ],\n",
      "       [-1.0738791 ],\n",
      "       [-0.902886  ],\n",
      "       [-0.8996727 ],\n",
      "       [-1.057982  ],\n",
      "       [-0.9028171 ],\n",
      "       [-0.9006166 ],\n",
      "       [-1.394037  ],\n",
      "       [-1.3557712 ],\n",
      "       [-1.3560337 ],\n",
      "       [-0.91818637],\n",
      "       [-1.3736773 ],\n",
      "       [-1.3796228 ],\n",
      "       [-0.91554743],\n",
      "       [-0.8917303 ],\n",
      "       [-0.9197195 ],\n",
      "       [-0.885551  ],\n",
      "       [-1.320478  ],\n",
      "       [-0.9033829 ],\n",
      "       [-0.9156685 ],\n",
      "       [-1.063854  ],\n",
      "       [-1.3952391 ],\n",
      "       [-0.89048505],\n",
      "       [-0.9099008 ],\n",
      "       [-1.076818  ],\n",
      "       [-1.078317  ],\n",
      "       [-0.864095  ],\n",
      "       [-0.89875   ],\n",
      "       [-1.4002311 ],\n",
      "       [-0.9000486 ],\n",
      "       [-0.8927279 ],\n",
      "       [-0.89117724],\n",
      "       [-1.3624967 ],\n",
      "       [-0.8943868 ],\n",
      "       [-1.0788205 ],\n",
      "       [-1.0761187 ],\n",
      "       [-1.0713874 ],\n",
      "       [-0.9043632 ],\n",
      "       [-1.0469288 ],\n",
      "       [-0.88975203],\n",
      "       [-1.0470517 ],\n",
      "       [-0.8963332 ],\n",
      "       [-0.91598654],\n",
      "       [-0.88941807],\n",
      "       [-1.0761187 ],\n",
      "       [-1.3736086 ],\n",
      "       [-0.9169968 ],\n",
      "       [-0.8928903 ],\n",
      "       [-0.8991652 ],\n",
      "       [-1.0596201 ],\n",
      "       [-0.9142616 ],\n",
      "       [-0.8866856 ],\n",
      "       [-0.9266049 ],\n",
      "       [-0.9167702 ],\n",
      "       [-0.90590614],\n",
      "       [-0.87675005],\n",
      "       [-0.9000486 ],\n",
      "       [-1.3572998 ],\n",
      "       [-0.9026625 ],\n",
      "       [-0.87810063],\n",
      "       [-1.4006338 ],\n",
      "       [-1.0634966 ],\n",
      "       [-1.0916992 ],\n",
      "       [-0.8966042 ],\n",
      "       [-1.3563052 ],\n",
      "       [-0.9024737 ],\n",
      "       [-0.91727984],\n",
      "       [-1.0469288 ],\n",
      "       [-0.9169968 ],\n",
      "       [-0.897593  ],\n",
      "       [-0.88571787],\n",
      "       [-1.3759425 ],\n",
      "       [-1.3657103 ],\n",
      "       [-0.9142616 ],\n",
      "       [-1.3784617 ],\n",
      "       [-1.071693  ],\n",
      "       [-1.0390245 ],\n",
      "       [-1.0753088 ],\n",
      "       [-1.3597184 ],\n",
      "       [-0.904547  ],\n",
      "       [-1.3722807 ],\n",
      "       [-1.0935062 ],\n",
      "       [-1.3560337 ],\n",
      "       [-1.368544  ],\n",
      "       [-0.9171401 ],\n",
      "       [-1.4011133 ],\n",
      "       [-0.8973616 ],\n",
      "       [-0.9189219 ],\n",
      "       [-1.3748634 ],\n",
      "       [-1.372755  ],\n",
      "       [-1.0967562 ],\n",
      "       [-1.3738896 ],\n",
      "       [-1.0739244 ],\n",
      "       [-1.3872973 ],\n",
      "       [-0.897593  ],\n",
      "       [-1.0577443 ],\n",
      "       [-1.0666052 ],\n",
      "       [-1.4065253 ],\n",
      "       [-1.074302  ],\n",
      "       [-0.8931583 ],\n",
      "       [-0.87991315],\n",
      "       [-0.91677034]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-1.6653175],\n",
      "       [-1.7158382],\n",
      "       [-1.8041489],\n",
      "       [-1.6640737],\n",
      "       [-1.790163 ],\n",
      "       [-1.6593348],\n",
      "       [-1.8075929],\n",
      "       [-1.6477735],\n",
      "       [-1.6797773],\n",
      "       [-1.6613548],\n",
      "       [-1.8115112],\n",
      "       [-1.6889315],\n",
      "       [-1.6641973],\n",
      "       [-1.7174933],\n",
      "       [-1.8363347],\n",
      "       [-1.7028239],\n",
      "       [-1.765131 ],\n",
      "       [-1.6845542],\n",
      "       [-1.6954682],\n",
      "       [-1.6845928],\n",
      "       [-1.6842674],\n",
      "       [-1.7976975],\n",
      "       [-1.7717582],\n",
      "       [-1.6849979],\n",
      "       [-1.6622179],\n",
      "       [-1.7070646],\n",
      "       [-1.7717582],\n",
      "       [-1.7719659],\n",
      "       [-1.6903503],\n",
      "       [-1.7860299],\n",
      "       [-1.7090523],\n",
      "       [-1.6854236],\n",
      "       [-1.8172209],\n",
      "       [-1.6807642],\n",
      "       [-1.7639548],\n",
      "       [-1.7639397],\n",
      "       [-1.8183117],\n",
      "       [-1.7873421],\n",
      "       [-1.8043287],\n",
      "       [-1.8114772],\n",
      "       [-1.757288 ],\n",
      "       [-1.7694844],\n",
      "       [-1.7433442],\n",
      "       [-1.7817805],\n",
      "       [-1.6502888],\n",
      "       [-1.6575198],\n",
      "       [-1.8124706],\n",
      "       [-1.7483168],\n",
      "       [-1.8608218],\n",
      "       [-1.664674 ],\n",
      "       [-1.7490413],\n",
      "       [-1.7446862],\n",
      "       [-1.6926899],\n",
      "       [-1.6838793],\n",
      "       [-1.8446436],\n",
      "       [-1.6641973],\n",
      "       [-1.6871573],\n",
      "       [-1.7642827],\n",
      "       [-1.7702886],\n",
      "       [-1.6581612],\n",
      "       [-1.6809801],\n",
      "       [-1.662474 ],\n",
      "       [-1.771036 ],\n",
      "       [-1.6511328],\n",
      "       [-1.6742197],\n",
      "       [-1.674232 ],\n",
      "       [-1.6739249],\n",
      "       [-1.7738166],\n",
      "       [-1.6572696],\n",
      "       [-1.7965959],\n",
      "       [-1.662474 ],\n",
      "       [-1.6786299],\n",
      "       [-1.6623619],\n",
      "       [-1.7716911],\n",
      "       [-1.7618049],\n",
      "       [-1.7072332],\n",
      "       [-1.8076438],\n",
      "       [-1.7653443],\n",
      "       [-1.6529936],\n",
      "       [-1.6625855],\n",
      "       [-1.6643922],\n",
      "       [-1.6759349],\n",
      "       [-1.6641973],\n",
      "       [-1.7688885],\n",
      "       [-1.6880898],\n",
      "       [-1.7088524],\n",
      "       [-1.7009398],\n",
      "       [-1.7589291],\n",
      "       [-1.7286397],\n",
      "       [-1.6569879],\n",
      "       [-1.7638944],\n",
      "       [-1.6843224],\n",
      "       [-1.6621068],\n",
      "       [-1.6742197],\n",
      "       [-1.6623619],\n",
      "       [-1.7749059],\n",
      "       [-1.6900067],\n",
      "       [-1.6974591],\n",
      "       [-1.8052603],\n",
      "       [-1.8076438],\n",
      "       [-1.7725486],\n",
      "       [-1.7753743],\n",
      "       [-1.8292831],\n",
      "       [-1.7618327],\n",
      "       [-1.7676055],\n",
      "       [-1.6658618],\n",
      "       [-1.6788526],\n",
      "       [-1.7872833],\n",
      "       [-1.7639397],\n",
      "       [-1.8086423],\n",
      "       [-1.8227417],\n",
      "       [-1.8599386],\n",
      "       [-1.774688 ],\n",
      "       [-1.823627 ],\n",
      "       [-1.7841557],\n",
      "       [-1.8101827],\n",
      "       [-1.7840701],\n",
      "       [-1.8115635],\n",
      "       [-1.7718692],\n",
      "       [-1.7218186],\n",
      "       [-1.7749059],\n",
      "       [-1.709035 ],\n",
      "       [-1.8115513],\n",
      "       [-1.6674925],\n",
      "       [-1.688729 ],\n",
      "       [-1.6854159],\n",
      "       [-1.6772813],\n",
      "       [-1.6625855]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-0.5387464 ],\n",
      "       [-0.6142485 ],\n",
      "       [-0.60477144],\n",
      "       [-0.5983459 ],\n",
      "       [-0.6024393 ],\n",
      "       [-0.6363125 ],\n",
      "       [-0.5935371 ],\n",
      "       [-0.6173602 ],\n",
      "       [-0.7342952 ],\n",
      "       [-0.75464773],\n",
      "       [-0.5933268 ],\n",
      "       [-0.6303569 ],\n",
      "       [-0.6169703 ],\n",
      "       [-0.61494625],\n",
      "       [-0.6340888 ],\n",
      "       [-0.7771893 ],\n",
      "       [-0.62507653],\n",
      "       [-0.6307284 ],\n",
      "       [-0.6145422 ],\n",
      "       [-0.6279166 ],\n",
      "       [-0.6227954 ],\n",
      "       [-0.7595464 ],\n",
      "       [-0.61261654],\n",
      "       [-0.621557  ],\n",
      "       [-0.78717136],\n",
      "       [-0.61870325],\n",
      "       [-0.61261654],\n",
      "       [-0.6128359 ],\n",
      "       [-0.6174998 ],\n",
      "       [-0.6224663 ],\n",
      "       [-0.59219915],\n",
      "       [-0.6189068 ],\n",
      "       [-0.61632496],\n",
      "       [-0.612067  ],\n",
      "       [-0.61163485],\n",
      "       [-0.6118878 ],\n",
      "       [-0.59482574],\n",
      "       [-0.58930534],\n",
      "       [-0.79539835],\n",
      "       [-0.5939154 ],\n",
      "       [-0.6225591 ],\n",
      "       [-0.6012782 ],\n",
      "       [-0.75410795],\n",
      "       [-0.5779575 ],\n",
      "       [-0.6215854 ],\n",
      "       [-0.60654783],\n",
      "       [-0.59333473],\n",
      "       [-0.60795426],\n",
      "       [-0.6341189 ],\n",
      "       [-0.60590935],\n",
      "       [-0.62843823],\n",
      "       [-0.6297398 ],\n",
      "       [-0.6564789 ],\n",
      "       [-0.62399256],\n",
      "       [-0.77263653],\n",
      "       [-0.6169703 ],\n",
      "       [-0.62811327],\n",
      "       [-0.61933815],\n",
      "       [-0.60902834],\n",
      "       [-0.75678605],\n",
      "       [-0.7544452 ],\n",
      "       [-0.7914071 ],\n",
      "       [-0.61039585],\n",
      "       [-0.6197429 ],\n",
      "       [-0.58568794],\n",
      "       [-0.6301854 ],\n",
      "       [-0.5854684 ],\n",
      "       [-0.62584406],\n",
      "       [-0.6064361 ],\n",
      "       [-0.76278615],\n",
      "       [-0.7914071 ],\n",
      "       [-0.7877505 ],\n",
      "       [-0.6021258 ],\n",
      "       [-0.616145  ],\n",
      "       [-0.6126766 ],\n",
      "       [-0.59133536],\n",
      "       [-0.5933367 ],\n",
      "       [-0.62285805],\n",
      "       [-0.60548675],\n",
      "       [-0.60219437],\n",
      "       [-0.6188947 ],\n",
      "       [-0.74365914],\n",
      "       [-0.6169703 ],\n",
      "       [-0.60590935],\n",
      "       [-0.6194879 ],\n",
      "       [-0.63868064],\n",
      "       [-0.6196572 ],\n",
      "       [-0.6161848 ],\n",
      "       [-0.76259685],\n",
      "       [-0.75521165],\n",
      "       [-0.6121611 ],\n",
      "       [-0.61904025],\n",
      "       [-0.60201395],\n",
      "       [-0.58568794],\n",
      "       [-0.6021258 ],\n",
      "       [-0.6252583 ],\n",
      "       [-0.63355345],\n",
      "       [-0.78820616],\n",
      "       [-0.5844561 ],\n",
      "       [-0.5933367 ],\n",
      "       [-0.6106077 ],\n",
      "       [-0.60825944],\n",
      "       [-0.60267735],\n",
      "       [-0.61195296],\n",
      "       [-0.61061287],\n",
      "       [-0.6180244 ],\n",
      "       [-0.7857195 ],\n",
      "       [-0.6175839 ],\n",
      "       [-0.6118878 ],\n",
      "       [-0.58483446],\n",
      "       [-0.5972616 ],\n",
      "       [-0.5499117 ],\n",
      "       [-0.625371  ],\n",
      "       [-0.59764713],\n",
      "       [-0.5788199 ],\n",
      "       [-0.5850388 ],\n",
      "       [-0.61886406],\n",
      "       [-0.585691  ],\n",
      "       [-0.61298686],\n",
      "       [-0.6064744 ],\n",
      "       [-0.6252583 ],\n",
      "       [-0.5923268 ],\n",
      "       [-0.59258217],\n",
      "       [-0.6115083 ],\n",
      "       [-0.6165577 ],\n",
      "       [-0.6287731 ],\n",
      "       [-0.7440711 ],\n",
      "       [-0.60219437]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-0.7833905 ],\n",
      "       [-0.7244497 ],\n",
      "       [-0.6837006 ],\n",
      "       [-0.67641485],\n",
      "       [-0.70932543],\n",
      "       [-0.71178705],\n",
      "       [-0.7006271 ],\n",
      "       [-0.7120347 ],\n",
      "       [-0.5938363 ],\n",
      "       [-0.64907956],\n",
      "       [-0.69966954],\n",
      "       [-0.7196809 ],\n",
      "       [-0.6785685 ],\n",
      "       [-0.7266789 ],\n",
      "       [-0.73187846],\n",
      "       [-0.7233648 ],\n",
      "       [-0.73064363],\n",
      "       [-0.6538212 ],\n",
      "       [-0.7025961 ],\n",
      "       [-0.6623118 ],\n",
      "       [-0.7093226 ],\n",
      "       [-0.7310738 ],\n",
      "       [-0.72704375],\n",
      "       [-0.70824647],\n",
      "       [-0.7276563 ],\n",
      "       [-0.6380469 ],\n",
      "       [-0.72704375],\n",
      "       [-0.7269384 ],\n",
      "       [-0.70392656],\n",
      "       [-0.67371845],\n",
      "       [-0.70053124],\n",
      "       [-0.70893914],\n",
      "       [-0.7137991 ],\n",
      "       [-0.68920606],\n",
      "       [-0.66921645],\n",
      "       [-0.66928405],\n",
      "       [-0.701791  ],\n",
      "       [-0.7199748 ],\n",
      "       [-0.71919215],\n",
      "       [-0.7006171 ],\n",
      "       [-0.7342363 ],\n",
      "       [-0.68700975],\n",
      "       [-0.64540416],\n",
      "       [-0.64808357],\n",
      "       [-0.6873406 ],\n",
      "       [-0.7014991 ],\n",
      "       [-0.7047256 ],\n",
      "       [-0.7389515 ],\n",
      "       [-0.6624773 ],\n",
      "       [-0.70374876],\n",
      "       [-0.6466342 ],\n",
      "       [-0.64419264],\n",
      "       [-0.7304048 ],\n",
      "       [-0.7119393 ],\n",
      "       [-0.7390132 ],\n",
      "       [-0.6785685 ],\n",
      "       [-0.7187875 ],\n",
      "       [-0.7306659 ],\n",
      "       [-0.67343247],\n",
      "       [-0.7341621 ],\n",
      "       [-0.6600204 ],\n",
      "       [-0.72747934],\n",
      "       [-0.7271949 ],\n",
      "       [-0.6863928 ],\n",
      "       [-0.702316  ],\n",
      "       [-0.6639443 ],\n",
      "       [-0.703039  ],\n",
      "       [-0.6654267 ],\n",
      "       [-0.70217484],\n",
      "       [-0.7385227 ],\n",
      "       [-0.72747934],\n",
      "       [-0.72772163],\n",
      "       [-0.7007055 ],\n",
      "       [-0.7269502 ],\n",
      "       [-0.71987593],\n",
      "       [-0.7050233 ],\n",
      "       [-0.7003665 ],\n",
      "       [-0.73010236],\n",
      "       [-0.7246736 ],\n",
      "       [-0.70005465],\n",
      "       [-0.7150243 ],\n",
      "       [-0.74820024],\n",
      "       [-0.6785685 ],\n",
      "       [-0.6716208 ],\n",
      "       [-0.7058359 ],\n",
      "       [-0.616817  ],\n",
      "       [-0.62236637],\n",
      "       [-0.6613614 ],\n",
      "       [-0.7457898 ],\n",
      "       [-0.7348148 ],\n",
      "       [-0.6693401 ],\n",
      "       [-0.70967686],\n",
      "       [-0.7014137 ],\n",
      "       [-0.702316  ],\n",
      "       [-0.7007055 ],\n",
      "       [-0.66736287],\n",
      "       [-0.7214358 ],\n",
      "       [-0.7424135 ],\n",
      "       [-0.69644904],\n",
      "       [-0.7003665 ],\n",
      "       [-0.6807541 ],\n",
      "       [-0.72462523],\n",
      "       [-0.6868157 ],\n",
      "       [-0.72939324],\n",
      "       [-0.67170006],\n",
      "       [-0.70816875],\n",
      "       [-0.7212759 ],\n",
      "       [-0.66472477],\n",
      "       [-0.66928405],\n",
      "       [-0.6970239 ],\n",
      "       [-0.73494875],\n",
      "       [-0.76731706],\n",
      "       [-0.66704905],\n",
      "       [-0.70527405],\n",
      "       [-0.68271434],\n",
      "       [-0.70306903],\n",
      "       [-0.6627944 ],\n",
      "       [-0.7014389 ],\n",
      "       [-0.7270201 ],\n",
      "       [-0.6527665 ],\n",
      "       [-0.66736287],\n",
      "       [-0.70037746],\n",
      "       [-0.7052475 ],\n",
      "       [-0.7040724 ],\n",
      "       [-0.6758566 ],\n",
      "       [-0.6515128 ],\n",
      "       [-0.74432975],\n",
      "       [-0.70005465]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-0.6002936 ],\n",
      "       [-0.73170865],\n",
      "       [-0.6009933 ],\n",
      "       [-0.5910578 ],\n",
      "       [-0.7490966 ],\n",
      "       [-0.6736969 ],\n",
      "       [-0.6065089 ],\n",
      "       [-0.636177  ],\n",
      "       [-0.7052877 ],\n",
      "       [-0.6152085 ],\n",
      "       [-0.6096192 ],\n",
      "       [-0.7124736 ],\n",
      "       [-0.60429883],\n",
      "       [-0.7273127 ],\n",
      "       [-0.67874473],\n",
      "       [-0.75553745],\n",
      "       [-0.71701413],\n",
      "       [-0.7504161 ],\n",
      "       [-0.7718826 ],\n",
      "       [-0.7429229 ],\n",
      "       [-0.7493015 ],\n",
      "       [-0.63621163],\n",
      "       [-0.7391675 ],\n",
      "       [-0.7557702 ],\n",
      "       [-0.61475456],\n",
      "       [-0.7172718 ],\n",
      "       [-0.7391675 ],\n",
      "       [-0.74055886],\n",
      "       [-0.76545167],\n",
      "       [-0.61224484],\n",
      "       [-0.7735272 ],\n",
      "       [-0.7604282 ],\n",
      "       [-0.7461145 ],\n",
      "       [-0.6287551 ],\n",
      "       [-0.6050148 ],\n",
      "       [-0.60535645],\n",
      "       [-0.6193237 ],\n",
      "       [-0.75219774],\n",
      "       [-0.63998437],\n",
      "       [-0.6102024 ],\n",
      "       [-0.7347717 ],\n",
      "       [-0.6375346 ],\n",
      "       [-0.62017375],\n",
      "       [-0.57021534],\n",
      "       [-0.62936956],\n",
      "       [-0.6173192 ],\n",
      "       [-0.6119254 ],\n",
      "       [-0.72170377],\n",
      "       [-0.64067996],\n",
      "       [-0.7856166 ],\n",
      "       [-0.7209381 ],\n",
      "       [-0.7189732 ],\n",
      "       [-0.65793586],\n",
      "       [-0.74146384],\n",
      "       [-0.7207911 ],\n",
      "       [-0.60429883],\n",
      "       [-0.71796405],\n",
      "       [-0.7314977 ],\n",
      "       [-0.609717  ],\n",
      "       [-0.62810576],\n",
      "       [-0.7467034 ],\n",
      "       [-0.6074123 ],\n",
      "       [-0.7388622 ],\n",
      "       [-0.6238763 ],\n",
      "       [-0.6028352 ],\n",
      "       [-0.7222315 ],\n",
      "       [-0.60267586],\n",
      "       [-0.6077861 ],\n",
      "       [-0.6171791 ],\n",
      "       [-0.6363373 ],\n",
      "       [-0.6074123 ],\n",
      "       [-0.6296659 ],\n",
      "       [-0.61096877],\n",
      "       [-0.73136145],\n",
      "       [-0.7369273 ],\n",
      "       [-0.7762598 ],\n",
      "       [-0.6063186 ],\n",
      "       [-0.7210305 ],\n",
      "       [-0.634822  ],\n",
      "       [-0.6111074 ],\n",
      "       [-0.7537046 ],\n",
      "       [-0.7294389 ],\n",
      "       [-0.60429883],\n",
      "       [-0.6042139 ],\n",
      "       [-0.76149774],\n",
      "       [-0.7110661 ],\n",
      "       [-0.68543553],\n",
      "       [-0.72869253],\n",
      "       [-0.71956813],\n",
      "       [-0.636794  ],\n",
      "       [-0.6057229 ],\n",
      "       [-0.7595463 ],\n",
      "       [-0.6108494 ],\n",
      "       [-0.6028352 ],\n",
      "       [-0.61096877],\n",
      "       [-0.60680306],\n",
      "       [-0.706337  ],\n",
      "       [-0.7371346 ],\n",
      "       [-0.6104142 ],\n",
      "       [-0.6063186 ],\n",
      "       [-0.6222138 ],\n",
      "       [-0.74487907],\n",
      "       [-0.6331229 ],\n",
      "       [-0.7335982 ],\n",
      "       [-0.6078042 ],\n",
      "       [-0.75771916],\n",
      "       [-0.6249098 ],\n",
      "       [-0.59936845],\n",
      "       [-0.60535645],\n",
      "       [-0.61352855],\n",
      "       [-0.6376306 ],\n",
      "       [-0.7440928 ],\n",
      "       [-0.60704625],\n",
      "       [-0.6280938 ],\n",
      "       [-0.6057643 ],\n",
      "       [-0.6222911 ],\n",
      "       [-0.5948012 ],\n",
      "       [-0.6220584 ],\n",
      "       [-0.7402839 ],\n",
      "       [-0.7405075 ],\n",
      "       [-0.60680306],\n",
      "       [-0.77315724],\n",
      "       [-0.60912925],\n",
      "       [-0.63975096],\n",
      "       [-0.76620626],\n",
      "       [-0.7520656 ],\n",
      "       [-0.7477187 ],\n",
      "       [-0.6111074 ]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-1.26141638e-01],\n",
      "       [-1.18772984e+00],\n",
      "       [-1.50094032e-01],\n",
      "       [-3.56071472e-01],\n",
      "       [-3.63064587e-01],\n",
      "       [-1.58656435e+01],\n",
      "       [-3.55056316e-01],\n",
      "       [-1.74956143e-01],\n",
      "       [-3.68960738e-01],\n",
      "       [-3.74954104e-01],\n",
      "       [-1.49485424e-01],\n",
      "       [-3.56388450e+00],\n",
      "       [-3.69875282e-01],\n",
      "       [-1.18223417e+00],\n",
      "       [-2.37340546e+01],\n",
      "       [-3.44650298e-02],\n",
      "       [-7.52463996e-01],\n",
      "       [-1.73872858e-01],\n",
      "       [-3.86384815e-01],\n",
      "       [-4.53987755e-02],\n",
      "       [-4.03738171e-01],\n",
      "       [-3.66434157e-02],\n",
      "       [-1.51011243e-01],\n",
      "       [-1.82407364e-01],\n",
      "       [-3.42637114e-02],\n",
      "       [-3.99718918e-02],\n",
      "       [-1.51011243e-01],\n",
      "       [-3.59947473e-01],\n",
      "       [-6.16716206e-05],\n",
      "       [-3.60565074e-02],\n",
      "       [-3.54083419e-01],\n",
      "       [-1.93211046e-04],\n",
      "       [-4.68527079e-02],\n",
      "       [-1.87260479e-01],\n",
      "       [-3.53298515e-01],\n",
      "       [-3.53765398e-01],\n",
      "       [-1.09379506e-03],\n",
      "       [-2.41404787e-01],\n",
      "       [-4.63952452e-01],\n",
      "       [-1.49572238e-01],\n",
      "       [-3.71731579e-01],\n",
      "       [-7.22867072e-01],\n",
      "       [-3.61044139e-01],\n",
      "       [-1.13022901e-01],\n",
      "       [-3.99081081e-01],\n",
      "       [-1.71384871e-01],\n",
      "       [-2.98499241e-02],\n",
      "       [-2.35740438e-01],\n",
      "       [-5.00084236e-02],\n",
      "       [-1.62567496e-01],\n",
      "       [-5.21985963e-02],\n",
      "       [-5.25439121e-02],\n",
      "       [-2.75349541e+01],\n",
      "       [-7.10903168e-01],\n",
      "       [-5.71763329e-02],\n",
      "       [-3.69875282e-01],\n",
      "       [-2.81836629e+00],\n",
      "       [-3.38603482e-02],\n",
      "       [-3.10410578e-02],\n",
      "       [-1.67335376e-01],\n",
      "       [-3.97261858e-01],\n",
      "       [-3.62719029e-01],\n",
      "       [-1.42905151e-03],\n",
      "       [-1.81123883e-01],\n",
      "       [-3.63415420e-01],\n",
      "       [-1.05285442e+00],\n",
      "       [-3.62764060e-01],\n",
      "       [-3.67379338e-01],\n",
      "       [-1.71138600e-01],\n",
      "       [-3.66926610e-01],\n",
      "       [-3.62719029e-01],\n",
      "       [-3.33256647e-02],\n",
      "       [-3.83384407e-01],\n",
      "       [-3.30580249e-02],\n",
      "       [-4.06605452e-02],\n",
      "       [-1.44125208e-01],\n",
      "       [-3.54984373e-01],\n",
      "       [-4.31573987e-01],\n",
      "       [-1.85706869e-01],\n",
      "       [-3.83719444e-01],\n",
      "       [-1.13102853e+00],\n",
      "       [-1.11304426e+00],\n",
      "       [-3.69875282e-01],\n",
      "       [-2.86083687e-02],\n",
      "       [-4.72198576e-02],\n",
      "       [-3.74259651e-01],\n",
      "       [-7.27230385e-02],\n",
      "       [-2.63631024e-04],\n",
      "       [-2.11449787e-01],\n",
      "       [-4.72355001e-02],\n",
      "       [-3.54245603e-01],\n",
      "       [-2.15448861e-04],\n",
      "       [-3.83098364e-01],\n",
      "       [-3.63415420e-01],\n",
      "       [-3.83384407e-01],\n",
      "       [-3.68610799e-01],\n",
      "       [-4.38971233e+00],\n",
      "       [-3.35652977e-01],\n",
      "       [-3.43233347e-01],\n",
      "       [-3.54984373e-01],\n",
      "       [-3.78250808e-01],\n",
      "       [-1.46270856e-01],\n",
      "       [-1.85489476e-01],\n",
      "       [-1.21828377e+00],\n",
      "       [-1.49092674e-01],\n",
      "       [-7.25572526e-01],\n",
      "       [-3.67413014e-01],\n",
      "       [-3.04792672e-02],\n",
      "       [-3.53765398e-01],\n",
      "       [-1.41149640e-01],\n",
      "       [-3.38711321e-01],\n",
      "       [-1.80053921e+01],\n",
      "       [-3.68513167e-01],\n",
      "       [-2.04261944e-01],\n",
      "       [-1.45147905e-01],\n",
      "       [-2.57561682e-03],\n",
      "       [-3.54926705e-01],\n",
      "       [-2.30740476e-03],\n",
      "       [-3.60096961e-01],\n",
      "       [-1.51297674e-01],\n",
      "       [-3.68610799e-01],\n",
      "       [-3.54443789e-01],\n",
      "       [-1.46488279e-01],\n",
      "       [-4.08256471e-01],\n",
      "       [-1.60710633e-01],\n",
      "       [-1.73835561e-01],\n",
      "       [-9.88218599e-05],\n",
      "       [-3.83719385e-01]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-3.40785561e+01],\n",
      "       [-3.20957489e-02],\n",
      "       [-1.02990843e-01],\n",
      "       [-1.24991290e-01],\n",
      "       [-1.38334543e-01],\n",
      "       [-1.92131460e-01],\n",
      "       [-1.11628853e-01],\n",
      "       [-4.39310744e-02],\n",
      "       [-1.01244748e-01],\n",
      "       [-1.14427932e-01],\n",
      "       [-1.31694138e-01],\n",
      "       [-1.46250710e-01],\n",
      "       [-1.20027579e-01],\n",
      "       [-1.70628756e-01],\n",
      "       [-9.15968037e+00],\n",
      "       [-1.13551371e-01],\n",
      "       [-1.59415483e-01],\n",
      "       [-1.66890118e-02],\n",
      "       [-3.67250517e-02],\n",
      "       [-4.16231602e-01],\n",
      "       [-1.26237333e-01],\n",
      "       [-2.60773208e-03],\n",
      "       [-1.50387689e-01],\n",
      "       [-1.06361695e-01],\n",
      "       [-1.25631124e-01],\n",
      "       [-1.11353107e-01],\n",
      "       [-1.50387689e-01],\n",
      "       [-1.31978616e-01],\n",
      "       [-8.43617395e-02],\n",
      "       [-9.91160199e-02],\n",
      "       [-1.27100453e-01],\n",
      "       [-1.20216049e-01],\n",
      "       [-9.02392194e-02],\n",
      "       [-1.12865239e-01],\n",
      "       [-7.51129538e-02],\n",
      "       [-8.74881223e-02],\n",
      "       [-1.15195513e-01],\n",
      "       [-9.25591066e-02],\n",
      "       [-8.57941657e-02],\n",
      "       [-8.94952491e-02],\n",
      "       [-1.30687550e-01],\n",
      "       [-1.45499602e-01],\n",
      "       [-1.13317683e-01],\n",
      "       [-1.07005911e+01],\n",
      "       [-1.49080783e-01],\n",
      "       [-1.44945323e-01],\n",
      "       [-1.23229675e-01],\n",
      "       [-1.34767249e-01],\n",
      "       [-1.02993600e-01],\n",
      "       [-1.32782385e-01],\n",
      "       [-4.48343381e-02],\n",
      "       [-1.31361127e-01],\n",
      "       [-2.39123821e-01],\n",
      "       [-1.47036880e-01],\n",
      "       [-1.14828207e-01],\n",
      "       [-1.20027579e-01],\n",
      "       [-1.39370218e-01],\n",
      "       [-2.74901465e-02],\n",
      "       [-9.15152356e-02],\n",
      "       [-1.41613156e-01],\n",
      "       [-1.25299647e-01],\n",
      "       [-1.37826949e-01],\n",
      "       [-5.85364290e-02],\n",
      "       [-1.13344111e-01],\n",
      "       [-1.39270857e-01],\n",
      "       [-1.30107030e-01],\n",
      "       [-1.08919539e-01],\n",
      "       [-3.89549248e-02],\n",
      "       [-1.29050553e-01],\n",
      "       [-4.04908150e-01],\n",
      "       [-1.37826949e-01],\n",
      "       [-2.57581715e-02],\n",
      "       [-1.23503439e-01],\n",
      "       [-2.61774480e-01],\n",
      "       [-1.23128839e-01],\n",
      "       [-3.12130712e-03],\n",
      "       [-1.26321852e-01],\n",
      "       [-1.70251980e-01],\n",
      "       [-1.35308012e-01],\n",
      "       [-1.39097407e-01],\n",
      "       [-7.99324084e-03],\n",
      "       [-1.05722860e-01],\n",
      "       [-1.20027579e-01],\n",
      "       [-4.99641348e-04],\n",
      "       [-1.15278147e-01],\n",
      "       [-9.92148593e-02],\n",
      "       [-1.13066696e-01],\n",
      "       [-1.03161201e-01],\n",
      "       [-1.43548265e-01],\n",
      "       [-1.49182200e-01],\n",
      "       [-1.00804657e-01],\n",
      "       [-1.85936138e-01],\n",
      "       [-1.08865514e-01],\n",
      "       [-1.39270857e-01],\n",
      "       [-1.23503439e-01],\n",
      "       [-1.06030844e-01],\n",
      "       [-1.89864561e-01],\n",
      "       [-9.16806012e-02],\n",
      "       [-1.33692160e-01],\n",
      "       [-1.26321852e-01],\n",
      "       [-1.19687967e-01],\n",
      "       [-1.51572749e-01],\n",
      "       [-1.34061217e-01],\n",
      "       [-2.19336245e-02],\n",
      "       [-1.03129804e-01],\n",
      "       [-1.33578658e-01],\n",
      "       [-1.33617789e-01],\n",
      "       [-1.14549965e-01],\n",
      "       [-8.74881223e-02],\n",
      "       [-1.24638066e-01],\n",
      "       [-3.09010100e+00],\n",
      "       [-5.29321747e+01],\n",
      "       [-9.25715119e-02],\n",
      "       [-7.22838640e-02],\n",
      "       [-3.38657037e-03],\n",
      "       [-2.22509261e-03],\n",
      "       [-8.45781118e-02],\n",
      "       [-2.38189027e-02],\n",
      "       [-1.48300037e-01],\n",
      "       [-8.28984231e-02],\n",
      "       [-1.06030844e-01],\n",
      "       [-1.43184707e-01],\n",
      "       [-1.35207117e-01],\n",
      "       [-1.47996262e-01],\n",
      "       [-1.32365093e-01],\n",
      "       [-8.69008824e-02],\n",
      "       [-7.82378465e-02],\n",
      "       [-1.39097378e-01]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-0.71927536],\n",
      "       [-0.72240853],\n",
      "       [-0.6943448 ],\n",
      "       [-0.6960421 ],\n",
      "       [-0.69265366],\n",
      "       [-0.69730407],\n",
      "       [-0.69100255],\n",
      "       [-0.7177981 ],\n",
      "       [-0.7583424 ],\n",
      "       [-0.6436169 ],\n",
      "       [-0.6925822 ],\n",
      "       [-0.72676843],\n",
      "       [-0.68474853],\n",
      "       [-0.71876025],\n",
      "       [-0.7185633 ],\n",
      "       [-0.72583103],\n",
      "       [-0.73280257],\n",
      "       [-0.7148899 ],\n",
      "       [-0.6997517 ],\n",
      "       [-0.7170789 ],\n",
      "       [-0.7154682 ],\n",
      "       [-0.6422464 ],\n",
      "       [-0.72872096],\n",
      "       [-0.7129057 ],\n",
      "       [-0.6782204 ],\n",
      "       [-0.6890781 ],\n",
      "       [-0.72872096],\n",
      "       [-0.7297676 ],\n",
      "       [-0.7040355 ],\n",
      "       [-0.6729278 ],\n",
      "       [-0.6908982 ],\n",
      "       [-0.714491  ],\n",
      "       [-0.7041578 ],\n",
      "       [-0.6945867 ],\n",
      "       [-0.6770047 ],\n",
      "       [-0.6770264 ],\n",
      "       [-0.6922249 ],\n",
      "       [-0.6902519 ],\n",
      "       [-0.67491364],\n",
      "       [-0.69129986],\n",
      "       [-0.735556  ],\n",
      "       [-0.73020285],\n",
      "       [-0.6300092 ],\n",
      "       [-0.6696877 ],\n",
      "       [-0.7169293 ],\n",
      "       [-0.7176309 ],\n",
      "       [-0.6884431 ],\n",
      "       [-0.6992624 ],\n",
      "       [-0.68280286],\n",
      "       [-0.6996304 ],\n",
      "       [-0.7353418 ],\n",
      "       [-0.7358861 ],\n",
      "       [-0.72309124],\n",
      "       [-0.7182657 ],\n",
      "       [-0.7172025 ],\n",
      "       [-0.68474853],\n",
      "       [-0.72581035],\n",
      "       [-0.7367555 ],\n",
      "       [-0.68198556],\n",
      "       [-0.6543292 ],\n",
      "       [-0.7364829 ],\n",
      "       [-0.6795045 ],\n",
      "       [-0.723601  ],\n",
      "       [-0.7120613 ],\n",
      "       [-0.7231922 ],\n",
      "       [-0.68928236],\n",
      "       [-0.7229121 ],\n",
      "       [-0.66564834],\n",
      "       [-0.71721256],\n",
      "       [-0.63419837],\n",
      "       [-0.6795045 ],\n",
      "       [-0.66632736],\n",
      "       [-0.71558535],\n",
      "       [-0.73138916],\n",
      "       [-0.6990797 ],\n",
      "       [-0.6965388 ],\n",
      "       [-0.6913806 ],\n",
      "       [-0.7345439 ],\n",
      "       [-0.7145321 ],\n",
      "       [-0.71610314],\n",
      "       [-0.6794556 ],\n",
      "       [-0.7405019 ],\n",
      "       [-0.68474853],\n",
      "       [-0.6794203 ],\n",
      "       [-0.7076699 ],\n",
      "       [-0.728177  ],\n",
      "       [-0.7124241 ],\n",
      "       [-0.7132418 ],\n",
      "       [-0.7473303 ],\n",
      "       [-0.6554685 ],\n",
      "       [-0.67702913],\n",
      "       [-0.71582013],\n",
      "       [-0.7151179 ],\n",
      "       [-0.7231922 ],\n",
      "       [-0.71558535],\n",
      "       [-0.6682819 ],\n",
      "       [-0.7265691 ],\n",
      "       [-0.70862484],\n",
      "       [-0.7045055 ],\n",
      "       [-0.6913806 ],\n",
      "       [-0.6853081 ],\n",
      "       [-0.72370905],\n",
      "       [-0.71694076],\n",
      "       [-0.7214216 ],\n",
      "       [-0.6800052 ],\n",
      "       [-0.68691164],\n",
      "       [-0.66742325],\n",
      "       [-0.6645316 ],\n",
      "       [-0.6770264 ],\n",
      "       [-0.70346373],\n",
      "       [-0.6708786 ],\n",
      "       [-0.6711503 ],\n",
      "       [-0.6678437 ],\n",
      "       [-0.6896356 ],\n",
      "       [-0.7289095 ],\n",
      "       [-0.69975686],\n",
      "       [-0.66350466],\n",
      "       [-0.7008242 ],\n",
      "       [-0.72994053],\n",
      "       [-0.69975805],\n",
      "       [-0.6682819 ],\n",
      "       [-0.6906934 ],\n",
      "       [-0.6884438 ],\n",
      "       [-0.6983367 ],\n",
      "       [-0.715638  ],\n",
      "       [-0.7039946 ],\n",
      "       [-0.73837394],\n",
      "       [-0.71610314]], dtype=float32)>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob vec:  [<tf.Tensor: shape=(106, 1), dtype=float32, numpy=\n",
      "array([[-0.91643035],\n",
      "       [-1.397269  ],\n",
      "       [-0.87575126],\n",
      "       [-1.0666637 ],\n",
      "       [-0.8973598 ],\n",
      "       [-1.0570917 ],\n",
      "       [-0.90242565],\n",
      "       [-0.89447135],\n",
      "       [-1.3597184 ],\n",
      "       [-0.8973598 ],\n",
      "       [-0.9635291 ],\n",
      "       [-0.90161777],\n",
      "       [-0.92877156],\n",
      "       [-1.3563052 ],\n",
      "       [-0.8743166 ],\n",
      "       [-0.8941344 ],\n",
      "       [-1.0736923 ],\n",
      "       [-0.90353924],\n",
      "       [-0.86976385],\n",
      "       [-0.9120265 ],\n",
      "       [-1.0728282 ],\n",
      "       [-1.0741494 ],\n",
      "       [-1.0625994 ],\n",
      "       [-0.8936907 ],\n",
      "       [-0.91634196],\n",
      "       [-1.047049  ],\n",
      "       [-1.0691506 ],\n",
      "       [-0.8703322 ],\n",
      "       [-0.89662653],\n",
      "       [-0.87637645],\n",
      "       [-0.8917303 ],\n",
      "       [-0.90273935],\n",
      "       [-0.8931583 ],\n",
      "       [-1.0709431 ],\n",
      "       [-0.8971822 ],\n",
      "       [-1.0215619 ],\n",
      "       [-0.9142881 ],\n",
      "       [-1.0790639 ],\n",
      "       [-1.0571662 ],\n",
      "       [-1.3932161 ],\n",
      "       [-1.0455775 ],\n",
      "       [-1.0591698 ],\n",
      "       [-0.9156685 ],\n",
      "       [-1.3594253 ],\n",
      "       [-1.0469288 ],\n",
      "       [-1.0565735 ],\n",
      "       [-0.91727984],\n",
      "       [-0.885551  ],\n",
      "       [-1.394865  ],\n",
      "       [-0.8800671 ],\n",
      "       [-1.0596201 ],\n",
      "       [-1.072966  ],\n",
      "       [-0.9007832 ],\n",
      "       [-0.8928332 ],\n",
      "       [-1.0793395 ],\n",
      "       [-1.0558388 ],\n",
      "       [-1.3679874 ],\n",
      "       [-0.8931563 ],\n",
      "       [-0.86622524],\n",
      "       [-1.3686192 ],\n",
      "       [-0.9142616 ],\n",
      "       [-0.897593  ],\n",
      "       [-0.8730819 ],\n",
      "       [-0.91863525],\n",
      "       [-0.92661417],\n",
      "       [-0.9142616 ],\n",
      "       [-1.3871958 ],\n",
      "       [-1.0470366 ],\n",
      "       [-1.3925819 ],\n",
      "       [-1.057982  ],\n",
      "       [-0.90053546],\n",
      "       [-0.9169968 ],\n",
      "       [-0.92580336],\n",
      "       [-0.8909191 ],\n",
      "       [-1.3794627 ],\n",
      "       [-1.0446279 ],\n",
      "       [-0.91146785],\n",
      "       [-0.8934767 ],\n",
      "       [-1.0691125 ],\n",
      "       [-1.4254259 ],\n",
      "       [-1.0764605 ],\n",
      "       [-1.0469288 ],\n",
      "       [-0.91161716],\n",
      "       [-1.0666052 ],\n",
      "       [-1.368544  ],\n",
      "       [-1.3893956 ],\n",
      "       [-0.8827658 ],\n",
      "       [-1.3634102 ],\n",
      "       [-0.904619  ],\n",
      "       [-1.3781117 ],\n",
      "       [-1.0876354 ],\n",
      "       [-0.8643743 ],\n",
      "       [-1.4226892 ],\n",
      "       [-0.91819227],\n",
      "       [-0.897593  ],\n",
      "       [-0.8909191 ],\n",
      "       [-0.8636962 ],\n",
      "       [-1.4373149 ],\n",
      "       [-0.8893292 ],\n",
      "       [-1.0470455 ],\n",
      "       [-0.89207757],\n",
      "       [-1.3563052 ],\n",
      "       [-0.87850666],\n",
      "       [-0.8661777 ],\n",
      "       [-0.86055017],\n",
      "       [-0.8710275 ]], dtype=float32)>, <tf.Tensor: shape=(106, 1), dtype=float32, numpy=\n",
      "array([[-1.776135 ],\n",
      "       [-1.678335 ],\n",
      "       [-1.7183127],\n",
      "       [-1.8114806],\n",
      "       [-1.7869034],\n",
      "       [-1.7079118],\n",
      "       [-1.6955942],\n",
      "       [-1.6582896],\n",
      "       [-1.7676055],\n",
      "       [-1.7869034],\n",
      "       [-1.6487615],\n",
      "       [-1.6626097],\n",
      "       [-1.8676848],\n",
      "       [-1.7638944],\n",
      "       [-1.7218878],\n",
      "       [-1.6585505],\n",
      "       [-1.7729563],\n",
      "       [-1.6552864],\n",
      "       [-1.6948882],\n",
      "       [-1.6855123],\n",
      "       [-1.7738769],\n",
      "       [-1.7732577],\n",
      "       [-1.6898023],\n",
      "       [-1.7666157],\n",
      "       [-1.7761939],\n",
      "       [-1.6730014],\n",
      "       [-1.8110065],\n",
      "       [-1.7442983],\n",
      "       [-1.6571372],\n",
      "       [-1.6756774],\n",
      "       [-1.757288 ],\n",
      "       [-1.6904839],\n",
      "       [-1.6854159],\n",
      "       [-1.7767876],\n",
      "       [-1.6845762],\n",
      "       [-1.6631176],\n",
      "       [-1.6539469],\n",
      "       [-1.7034036],\n",
      "       [-1.7095039],\n",
      "       [-1.6823931],\n",
      "       [-1.6719923],\n",
      "       [-1.709021 ],\n",
      "       [-1.6575198],\n",
      "       [-1.7674387],\n",
      "       [-1.6742197],\n",
      "       [-1.7060685],\n",
      "       [-1.6621068],\n",
      "       [-1.7433442],\n",
      "       [-1.6813285],\n",
      "       [-1.76791  ],\n",
      "       [-1.7072332],\n",
      "       [-1.7720473],\n",
      "       [-1.6830798],\n",
      "       [-1.6877425],\n",
      "       [-1.7439473],\n",
      "       [-1.7078292],\n",
      "       [-1.7366927],\n",
      "       [-1.7698051],\n",
      "       [-1.6994503],\n",
      "       [-1.7995455],\n",
      "       [-1.8076438],\n",
      "       [-1.7749059],\n",
      "       [-1.684552 ],\n",
      "       [-1.8265973],\n",
      "       [-1.6541435],\n",
      "       [-1.8076438],\n",
      "       [-1.6799031],\n",
      "       [-1.6740829],\n",
      "       [-1.6849622],\n",
      "       [-1.7090523],\n",
      "       [-1.8010463],\n",
      "       [-1.6623619],\n",
      "       [-1.673847 ],\n",
      "       [-1.7593426],\n",
      "       [-1.8132576],\n",
      "       [-1.6716694],\n",
      "       [-1.6668983],\n",
      "       [-1.6823766],\n",
      "       [-1.8110372],\n",
      "       [-1.7434205],\n",
      "       [-1.8000913],\n",
      "       [-1.6742197],\n",
      "       [-1.6503769],\n",
      "       [-1.8115513],\n",
      "       [-1.8086423],\n",
      "       [-1.7145967],\n",
      "       [-1.6906625],\n",
      "       [-1.7735598],\n",
      "       [-1.8076491],\n",
      "       [-1.7726886],\n",
      "       [-1.6570468],\n",
      "       [-1.7389717],\n",
      "       [-1.7160914],\n",
      "       [-1.8182975],\n",
      "       [-1.7749059],\n",
      "       [-1.7593426],\n",
      "       [-1.668266 ],\n",
      "       [-1.6844562],\n",
      "       [-1.6889155],\n",
      "       [-1.673763 ],\n",
      "       [-1.6829805],\n",
      "       [-1.7638944],\n",
      "       [-1.825748 ],\n",
      "       [-1.6684525],\n",
      "       [-1.6847322],\n",
      "       [-1.7314401]], dtype=float32)>, <tf.Tensor: shape=(106, 1), dtype=float32, numpy=\n",
      "array([[-0.5818754 ],\n",
      "       [-0.6004513 ],\n",
      "       [-0.74942493],\n",
      "       [-0.59251416],\n",
      "       [-0.776268  ],\n",
      "       [-0.5932177 ],\n",
      "       [-0.61458814],\n",
      "       [-0.7571382 ],\n",
      "       [-0.61061287],\n",
      "       [-0.776268  ],\n",
      "       [-0.5341196 ],\n",
      "       [-0.61497074],\n",
      "       [-0.59218955],\n",
      "       [-0.6121611 ],\n",
      "       [-0.75034577],\n",
      "       [-0.7582145 ],\n",
      "       [-0.61208767],\n",
      "       [-0.6174481 ],\n",
      "       [-0.6468863 ],\n",
      "       [-0.60898256],\n",
      "       [-0.60951734],\n",
      "       [-0.6099758 ],\n",
      "       [-0.57748246],\n",
      "       [-0.61695474],\n",
      "       [-0.58178854],\n",
      "       [-0.58367103],\n",
      "       [-0.5910779 ],\n",
      "       [-0.63670784],\n",
      "       [-0.7555473 ],\n",
      "       [-0.7427202 ],\n",
      "       [-0.6225591 ],\n",
      "       [-0.6173477 ],\n",
      "       [-0.6287731 ],\n",
      "       [-0.60940796],\n",
      "       [-0.6253193 ],\n",
      "       [-0.57121575],\n",
      "       [-0.61071455],\n",
      "       [-0.77858305],\n",
      "       [-0.60296804],\n",
      "       [-0.5957526 ],\n",
      "       [-0.5802817 ],\n",
      "       [-0.5914876 ],\n",
      "       [-0.60654783],\n",
      "       [-0.61048675],\n",
      "       [-0.58568794],\n",
      "       [-0.59528375],\n",
      "       [-0.60201395],\n",
      "       [-0.75410795],\n",
      "       [-0.61251926],\n",
      "       [-0.74401736],\n",
      "       [-0.59133536],\n",
      "       [-0.6124504 ],\n",
      "       [-0.62510115],\n",
      "       [-0.62965435],\n",
      "       [-0.6309469 ],\n",
      "       [-0.6005826 ],\n",
      "       [-0.58952427],\n",
      "       [-0.6156992 ],\n",
      "       [-0.64944637],\n",
      "       [-0.8008909 ],\n",
      "       [-0.5933367 ],\n",
      "       [-0.6252583 ],\n",
      "       [-0.64726174],\n",
      "       [-0.59985644],\n",
      "       [-0.6036614 ],\n",
      "       [-0.5933367 ],\n",
      "       [-0.76137024],\n",
      "       [-0.58555496],\n",
      "       [-0.7621132 ],\n",
      "       [-0.59219915],\n",
      "       [-0.62252045],\n",
      "       [-0.6021258 ],\n",
      "       [-0.6028222 ],\n",
      "       [-0.6216168 ],\n",
      "       [-0.5870436 ],\n",
      "       [-0.588731  ],\n",
      "       [-0.603328  ],\n",
      "       [-0.62727016],\n",
      "       [-0.5910852 ],\n",
      "       [-0.62866944],\n",
      "       [-0.6114253 ],\n",
      "       [-0.58568794],\n",
      "       [-0.62892437],\n",
      "       [-0.59258217],\n",
      "       [-0.58483446],\n",
      "       [-0.6104821 ],\n",
      "       [-0.6363112 ],\n",
      "       [-0.6057976 ],\n",
      "       [-0.61335796],\n",
      "       [-0.6104757 ],\n",
      "       [-0.7798039 ],\n",
      "       [-0.748662  ],\n",
      "       [-0.6394562 ],\n",
      "       [-0.5946578 ],\n",
      "       [-0.6252583 ],\n",
      "       [-0.6216168 ],\n",
      "       [-0.6584953 ],\n",
      "       [-0.64089185],\n",
      "       [-0.6304104 ],\n",
      "       [-0.58538276],\n",
      "       [-0.62982774],\n",
      "       [-0.6121611 ],\n",
      "       [-0.63362104],\n",
      "       [-0.73361593],\n",
      "       [-0.7335816 ],\n",
      "       [-0.7503139 ]], dtype=float32)>, <tf.Tensor: shape=(106, 1), dtype=float32, numpy=\n",
      "array([[-0.6784901 ],\n",
      "       [-0.69557565],\n",
      "       [-0.7621596 ],\n",
      "       [-0.7053508 ],\n",
      "       [-0.7326779 ],\n",
      "       [-0.7013949 ],\n",
      "       [-0.70210326],\n",
      "       [-0.7346902 ],\n",
      "       [-0.67170006],\n",
      "       [-0.7326779 ],\n",
      "       [-0.8163082 ],\n",
      "       [-0.6824442 ],\n",
      "       [-0.7353356 ],\n",
      "       [-0.6693401 ],\n",
      "       [-0.7597792 ],\n",
      "       [-0.73423773],\n",
      "       [-0.72617984],\n",
      "       [-0.68799895],\n",
      "       [-0.72552973],\n",
      "       [-0.7346564 ],\n",
      "       [-0.7250319 ],\n",
      "       [-0.7248131 ],\n",
      "       [-0.7034084 ],\n",
      "       [-0.72891736],\n",
      "       [-0.6781695 ],\n",
      "       [-0.7063402 ],\n",
      "       [-0.7059548 ],\n",
      "       [-0.63367075],\n",
      "       [-0.735247  ],\n",
      "       [-0.7485655 ],\n",
      "       [-0.7342363 ],\n",
      "       [-0.7033845 ],\n",
      "       [-0.6515128 ],\n",
      "       [-0.7240209 ],\n",
      "       [-0.71562433],\n",
      "       [-0.7789525 ],\n",
      "       [-0.702866  ],\n",
      "       [-0.7200137 ],\n",
      "       [-0.71148956],\n",
      "       [-0.69317394],\n",
      "       [-0.71290064],\n",
      "       [-0.70143044],\n",
      "       [-0.7014991 ],\n",
      "       [-0.6715838 ],\n",
      "       [-0.702316  ],\n",
      "       [-0.70360136],\n",
      "       [-0.7014137 ],\n",
      "       [-0.64540416],\n",
      "       [-0.6900499 ],\n",
      "       [-0.6480162 ],\n",
      "       [-0.7050233 ],\n",
      "       [-0.72692263],\n",
      "       [-0.7156571 ],\n",
      "       [-0.64814156],\n",
      "       [-0.6446119 ],\n",
      "       [-0.7095019 ],\n",
      "       [-0.716732  ],\n",
      "       [-0.7279566 ],\n",
      "       [-0.7265929 ],\n",
      "       [-0.7162852 ],\n",
      "       [-0.7003665 ],\n",
      "       [-0.66736287],\n",
      "       [-0.66287124],\n",
      "       [-0.731997  ],\n",
      "       [-0.7228697 ],\n",
      "       [-0.7003665 ],\n",
      "       [-0.6090235 ],\n",
      "       [-0.7026855 ],\n",
      "       [-0.614869  ],\n",
      "       [-0.70053124],\n",
      "       [-0.6800594 ],\n",
      "       [-0.7007055 ],\n",
      "       [-0.7599772 ],\n",
      "       [-0.73289794],\n",
      "       [-0.69925725],\n",
      "       [-0.7038622 ],\n",
      "       [-0.70942795],\n",
      "       [-0.6570378 ],\n",
      "       [-0.7058232 ],\n",
      "       [-0.75433385],\n",
      "       [-0.68095315],\n",
      "       [-0.702316  ],\n",
      "       [-0.7175363 ],\n",
      "       [-0.7052475 ],\n",
      "       [-0.6970239 ],\n",
      "       [-0.65978104],\n",
      "       [-0.7222094 ],\n",
      "       [-0.6740954 ],\n",
      "       [-0.691102  ],\n",
      "       [-0.6805932 ],\n",
      "       [-0.65015376],\n",
      "       [-0.7519608 ],\n",
      "       [-0.7389049 ],\n",
      "       [-0.70153564],\n",
      "       [-0.66736287],\n",
      "       [-0.73289794],\n",
      "       [-0.6490091 ],\n",
      "       [-0.71390116],\n",
      "       [-0.71954286],\n",
      "       [-0.70339775],\n",
      "       [-0.6747291 ],\n",
      "       [-0.6693401 ],\n",
      "       [-0.65077823],\n",
      "       [-0.62093925],\n",
      "       [-0.59440535],\n",
      "       [-0.7559137 ]], dtype=float32)>, <tf.Tensor: shape=(106, 1), dtype=float32, numpy=\n",
      "array([[-0.60303515],\n",
      "       [-0.6269684 ],\n",
      "       [-0.7190835 ],\n",
      "       [-0.60908985],\n",
      "       [-0.6135802 ],\n",
      "       [-0.77190095],\n",
      "       [-0.7716364 ],\n",
      "       [-0.628072  ],\n",
      "       [-0.6078042 ],\n",
      "       [-0.6135802 ],\n",
      "       [-0.765159  ],\n",
      "       [-0.6071714 ],\n",
      "       [-0.6412999 ],\n",
      "       [-0.6057229 ],\n",
      "       [-0.71730137],\n",
      "       [-0.6249458 ],\n",
      "       [-0.74228275],\n",
      "       [-0.6191855 ],\n",
      "       [-0.6786899 ],\n",
      "       [-0.75066024],\n",
      "       [-0.74524796],\n",
      "       [-0.7465426 ],\n",
      "       [-0.80386883],\n",
      "       [-0.736569  ],\n",
      "       [-0.60303926],\n",
      "       [-0.60140246],\n",
      "       [-0.6059207 ],\n",
      "       [-0.6944019 ],\n",
      "       [-0.63665676],\n",
      "       [-0.7326325 ],\n",
      "       [-0.7347717 ],\n",
      "       [-0.7654805 ],\n",
      "       [-0.7520656 ],\n",
      "       [-0.74001265],\n",
      "       [-0.7285725 ],\n",
      "       [-0.75588953],\n",
      "       [-0.62338257],\n",
      "       [-0.76273423],\n",
      "       [-0.75543314],\n",
      "       [-0.62237513],\n",
      "       [-0.60118484],\n",
      "       [-0.7755529 ],\n",
      "       [-0.6173192 ],\n",
      "       [-0.6076233 ],\n",
      "       [-0.6028352 ],\n",
      "       [-0.76938623],\n",
      "       [-0.6108494 ],\n",
      "       [-0.62017375],\n",
      "       [-0.6290319 ],\n",
      "       [-0.64589524],\n",
      "       [-0.7762598 ],\n",
      "       [-0.73952466],\n",
      "       [-0.7365368 ],\n",
      "       [-0.7556443 ],\n",
      "       [-0.7285651 ],\n",
      "       [-0.7588975 ],\n",
      "       [-0.7653355 ],\n",
      "       [-0.7340529 ],\n",
      "       [-0.672309  ],\n",
      "       [-0.6247507 ],\n",
      "       [-0.6063186 ],\n",
      "       [-0.60680306],\n",
      "       [-0.67493707],\n",
      "       [-0.63572174],\n",
      "       [-0.6297951 ],\n",
      "       [-0.6063186 ],\n",
      "       [-0.697359  ],\n",
      "       [-0.6027206 ],\n",
      "       [-0.6919608 ],\n",
      "       [-0.7735272 ],\n",
      "       [-0.62835234],\n",
      "       [-0.61096877],\n",
      "       [-0.7490054 ],\n",
      "       [-0.73241127],\n",
      "       [-0.6254675 ],\n",
      "       [-0.60752994],\n",
      "       [-0.78831804],\n",
      "       [-0.7468885 ],\n",
      "       [-0.60592395],\n",
      "       [-0.68318415],\n",
      "       [-0.59612846],\n",
      "       [-0.6028352 ],\n",
      "       [-0.659717  ],\n",
      "       [-0.60912925],\n",
      "       [-0.61352855],\n",
      "       [-0.741501  ],\n",
      "       [-0.7017377 ],\n",
      "       [-0.61115503],\n",
      "       [-0.62453973],\n",
      "       [-0.62220854],\n",
      "       [-0.6078748 ],\n",
      "       [-0.7028401 ],\n",
      "       [-0.68995756],\n",
      "       [-0.6191334 ],\n",
      "       [-0.60680306],\n",
      "       [-0.73241127],\n",
      "       [-0.64795023],\n",
      "       [-0.68496466],\n",
      "       [-0.71216846],\n",
      "       [-0.6026469 ],\n",
      "       [-0.7130167 ],\n",
      "       [-0.6057229 ],\n",
      "       [-0.70936656],\n",
      "       [-0.732877  ],\n",
      "       [-0.70213836],\n",
      "       [-0.7149911 ]], dtype=float32)>, <tf.Tensor: shape=(106, 1), dtype=float32, numpy=\n",
      "array([[-3.5753757e-01],\n",
      "       [-5.1221363e-02],\n",
      "       [-3.5306938e-02],\n",
      "       [-1.4631960e-01],\n",
      "       [-3.5036620e-01],\n",
      "       [-1.4791815e-01],\n",
      "       [-3.8768846e-01],\n",
      "       [-1.6691190e-01],\n",
      "       [-1.4909267e-01],\n",
      "       [-3.5036620e-01],\n",
      "       [-6.9326356e-02],\n",
      "       [-1.6139533e-01],\n",
      "       [-1.9111272e-03],\n",
      "       [-3.5424560e-01],\n",
      "       [-5.9923669e-04],\n",
      "       [-3.8158691e-01],\n",
      "       [-3.5968781e-01],\n",
      "       [-4.7817089e-02],\n",
      "       [-1.4244436e+01],\n",
      "       [-1.5358920e-01],\n",
      "       [-1.4827250e-01],\n",
      "       [-3.5618150e-01],\n",
      "       [-3.4199306e-01],\n",
      "       [-3.6582780e-01],\n",
      "       [-3.5749641e-01],\n",
      "       [-3.5937202e-01],\n",
      "       [-3.5062310e-01],\n",
      "       [-1.8994202e-01],\n",
      "       [-4.7472466e-02],\n",
      "       [-7.1497983e-01],\n",
      "       [-3.7173158e-01],\n",
      "       [-5.1760890e-05],\n",
      "       [-1.7383556e-01],\n",
      "       [-1.6419822e-03],\n",
      "       [-1.5860491e+00],\n",
      "       [-7.7329522e-01],\n",
      "       [-4.3028869e-02],\n",
      "       [-3.6554375e-01],\n",
      "       [-4.4574648e-01],\n",
      "       [-3.9585840e-04],\n",
      "       [-1.4811447e-01],\n",
      "       [-3.5191995e-01],\n",
      "       [-1.7138487e-01],\n",
      "       [-1.4884730e-01],\n",
      "       [-3.6341542e-01],\n",
      "       [-3.0453220e-02],\n",
      "       [-3.8309836e-01],\n",
      "       [-3.6104414e-01],\n",
      "       [-1.8650106e-01],\n",
      "       [-3.9137504e-01],\n",
      "       [-1.4412521e-01],\n",
      "       [-1.5099184e-01],\n",
      "       [-1.1165279e+00],\n",
      "       [-3.8763434e-01],\n",
      "       [-1.6318408e-01],\n",
      "       [-2.0911561e-01],\n",
      "       [-3.3060640e-01],\n",
      "       [-1.5430236e-01],\n",
      "       [-1.7637259e+01],\n",
      "       [-2.7003728e-02],\n",
      "       [-3.5498437e-01],\n",
      "       [-3.6861080e-01],\n",
      "       [-9.8045845e+00],\n",
      "       [-3.5476485e-01],\n",
      "       [-4.9098521e-02],\n",
      "       [-3.5498437e-01],\n",
      "       [-3.4218517e-01],\n",
      "       [-3.6302358e-01],\n",
      "       [-2.4865450e-02],\n",
      "       [-3.5408342e-01],\n",
      "       [-1.8245538e-01],\n",
      "       [-3.8338441e-01],\n",
      "       [-3.0378319e-02],\n",
      "       [-1.5860267e-01],\n",
      "       [-6.4399332e-02],\n",
      "       [-1.5658264e-01],\n",
      "       [-1.5786998e-01],\n",
      "       [-1.4995151e-05],\n",
      "       [-3.5078412e-01],\n",
      "       [-7.8472457e+00],\n",
      "       [-3.5886723e-01],\n",
      "       [-3.6341542e-01],\n",
      "       [-6.3290467e+00],\n",
      "       [-1.4648828e-01],\n",
      "       [-1.4114964e-01],\n",
      "       [-5.7804268e-02],\n",
      "       [-5.3082495e+00],\n",
      "       [-6.1541565e-02],\n",
      "       [-3.9140144e-01],\n",
      "       [-3.7751809e-01],\n",
      "       [-3.6472043e-01],\n",
      "       [-1.6724896e+00],\n",
      "       [-6.4628224e+00],\n",
      "       [-1.0938029e-03],\n",
      "       [-3.6861080e-01],\n",
      "       [-1.5860267e-01],\n",
      "       [-2.5237206e+01],\n",
      "       [-2.1328545e+01],\n",
      "       [-3.5614982e+00],\n",
      "       [-3.6251214e-01],\n",
      "       [-1.0639472e+00],\n",
      "       [-3.5424560e-01],\n",
      "       [-3.7937742e-01],\n",
      "       [-3.9818874e-01],\n",
      "       [-1.5697102e-01],\n",
      "       [-3.5693422e-02]], dtype=float32)>, <tf.Tensor: shape=(106, 1), dtype=float32, numpy=\n",
      "array([[-1.11181125e-01],\n",
      "       [-1.54070929e-01],\n",
      "       [-1.27371298e-02],\n",
      "       [-1.19767345e-01],\n",
      "       [-1.36507243e-01],\n",
      "       [-1.29439145e-01],\n",
      "       [-7.73338005e-02],\n",
      "       [-1.25982106e-01],\n",
      "       [-1.03129804e-01],\n",
      "       [-1.36507243e-01],\n",
      "       [-5.20488777e+01],\n",
      "       [-1.24295667e-01],\n",
      "       [-1.15288839e+01],\n",
      "       [-1.00804657e-01],\n",
      "       [-1.50589431e-02],\n",
      "       [-1.39398009e-01],\n",
      "       [-6.46681711e-02],\n",
      "       [-1.38089597e-01],\n",
      "       [-1.94391206e-01],\n",
      "       [-6.00888395e+00],\n",
      "       [-6.64000050e-04],\n",
      "       [-6.00361273e-05],\n",
      "       [-1.29281908e-01],\n",
      "       [-2.43456513e-01],\n",
      "       [-1.26127377e-01],\n",
      "       [-8.54032766e-03],\n",
      "       [-1.16732918e-01],\n",
      "       [-3.31951194e-02],\n",
      "       [-1.33122280e-01],\n",
      "       [-8.84054974e-02],\n",
      "       [-1.30687550e-01],\n",
      "       [-1.25962034e-01],\n",
      "       [-8.69008824e-02],\n",
      "       [-1.94577619e-01],\n",
      "       [-1.60127223e-01],\n",
      "       [-1.48772736e+01],\n",
      "       [-1.50209650e-01],\n",
      "       [-1.12142093e-01],\n",
      "       [-1.56182796e-01],\n",
      "       [-1.50152385e-01],\n",
      "       [-1.13245539e-01],\n",
      "       [-6.11575246e-02],\n",
      "       [-1.44945323e-01],\n",
      "       [-8.96455199e-02],\n",
      "       [-1.39270857e-01],\n",
      "       [-1.32529020e-01],\n",
      "       [-1.08865514e-01],\n",
      "       [-1.13317683e-01],\n",
      "       [-1.43722117e-01],\n",
      "       [-1.17909886e-01],\n",
      "       [-3.12130712e-03],\n",
      "       [-1.33960575e-01],\n",
      "       [-1.33580936e-03],\n",
      "       [-9.32993367e-02],\n",
      "       [-1.21559672e-01],\n",
      "       [-1.39801696e-01],\n",
      "       [-3.77838388e-02],\n",
      "       [-2.12300569e-01],\n",
      "       [-1.75312459e-02],\n",
      "       [-1.36628047e-01],\n",
      "       [-1.26321852e-01],\n",
      "       [-1.06030844e-01],\n",
      "       [-5.42921498e-02],\n",
      "       [-5.35195637e+00],\n",
      "       [-9.56934467e-02],\n",
      "       [-1.26321852e-01],\n",
      "       [-1.07508749e-01],\n",
      "       [-1.23599984e-01],\n",
      "       [-1.24035515e-02],\n",
      "       [-1.27100453e-01],\n",
      "       [-1.13829061e-01],\n",
      "       [-1.23503439e-01],\n",
      "       [-1.39547644e+01],\n",
      "       [-1.38242543e-01],\n",
      "       [-1.39536992e-01],\n",
      "       [-1.44227698e-01],\n",
      "       [-1.46025955e-03],\n",
      "       [-8.52354020e-02],\n",
      "       [-1.31979570e-01],\n",
      "       [-5.92436716e-02],\n",
      "       [-1.11089207e-01],\n",
      "       [-1.39270857e-01],\n",
      "       [-1.69441253e-01],\n",
      "       [-1.35207117e-01],\n",
      "       [-1.24638066e-01],\n",
      "       [-4.76148278e-02],\n",
      "       [-1.98341027e-01],\n",
      "       [-8.84792022e-03],\n",
      "       [-1.18449420e-01],\n",
      "       [-1.05345838e-01],\n",
      "       [-1.19606629e-01],\n",
      "       [-1.98560059e-02],\n",
      "       [-1.95409402e-01],\n",
      "       [-1.30057096e-01],\n",
      "       [-1.06030844e-01],\n",
      "       [-1.38242543e-01],\n",
      "       [-7.58374333e-02],\n",
      "       [-2.15507388e-01],\n",
      "       [-1.81242317e-01],\n",
      "       [-9.51886699e-02],\n",
      "       [-8.39548111e-01],\n",
      "       [-1.00804657e-01],\n",
      "       [-2.10575342e-01],\n",
      "       [-8.12203288e-02],\n",
      "       [-5.60595617e-02],\n",
      "       [-9.37086165e-01]], dtype=float32)>, <tf.Tensor: shape=(106, 1), dtype=float32, numpy=\n",
      "array([[-0.7400059 ],\n",
      "       [-0.7115905 ],\n",
      "       [-0.7657093 ],\n",
      "       [-0.688351  ],\n",
      "       [-0.64871246],\n",
      "       [-0.6927229 ],\n",
      "       [-0.6987646 ],\n",
      "       [-0.65403634],\n",
      "       [-0.6800052 ],\n",
      "       [-0.64871246],\n",
      "       [-0.70437807],\n",
      "       [-0.6920353 ],\n",
      "       [-0.68706465],\n",
      "       [-0.67702913],\n",
      "       [-0.76556313],\n",
      "       [-0.65406686],\n",
      "       [-0.7290095 ],\n",
      "       [-0.7090062 ],\n",
      "       [-0.72597146],\n",
      "       [-0.7276306 ],\n",
      "       [-0.7256925 ],\n",
      "       [-0.7269393 ],\n",
      "       [-0.7070793 ],\n",
      "       [-0.7334825 ],\n",
      "       [-0.74076617],\n",
      "       [-0.7219343 ],\n",
      "       [-0.6888646 ],\n",
      "       [-0.72824675],\n",
      "       [-0.6551966 ],\n",
      "       [-0.74044675],\n",
      "       [-0.735556  ],\n",
      "       [-0.70300937],\n",
      "       [-0.7039946 ],\n",
      "       [-0.72221494],\n",
      "       [-0.7219375 ],\n",
      "       [-0.6851184 ],\n",
      "       [-0.7189589 ],\n",
      "       [-0.7231119 ],\n",
      "       [-0.7087673 ],\n",
      "       [-0.7166463 ],\n",
      "       [-0.720074  ],\n",
      "       [-0.6922026 ],\n",
      "       [-0.7176309 ],\n",
      "       [-0.6799112 ],\n",
      "       [-0.7231922 ],\n",
      "       [-0.69656175],\n",
      "       [-0.7151179 ],\n",
      "       [-0.6300092 ],\n",
      "       [-0.69524366],\n",
      "       [-0.64032435],\n",
      "       [-0.6965388 ],\n",
      "       [-0.72860485],\n",
      "       [-0.72694445],\n",
      "       [-0.7055478 ],\n",
      "       [-0.73931926],\n",
      "       [-0.7058663 ],\n",
      "       [-0.6930009 ],\n",
      "       [-0.7317591 ],\n",
      "       [-0.72933877],\n",
      "       [-0.6744092 ],\n",
      "       [-0.6913806 ],\n",
      "       [-0.6682819 ],\n",
      "       [-0.70727783],\n",
      "       [-0.6699725 ],\n",
      "       [-0.7161339 ],\n",
      "       [-0.6913806 ],\n",
      "       [-0.7422414 ],\n",
      "       [-0.722999  ],\n",
      "       [-0.7408653 ],\n",
      "       [-0.6908982 ],\n",
      "       [-0.6781862 ],\n",
      "       [-0.71558535],\n",
      "       [-0.7288798 ],\n",
      "       [-0.73737264],\n",
      "       [-0.70355207],\n",
      "       [-0.7220036 ],\n",
      "       [-0.69391185],\n",
      "       [-0.700463  ],\n",
      "       [-0.6889955 ],\n",
      "       [-0.7319617 ],\n",
      "       [-0.691264  ],\n",
      "       [-0.7231922 ],\n",
      "       [-0.6971945 ],\n",
      "       [-0.6884438 ],\n",
      "       [-0.70346373],\n",
      "       [-0.71642846],\n",
      "       [-0.7275004 ],\n",
      "       [-0.68041444],\n",
      "       [-0.7033262 ],\n",
      "       [-0.68499786],\n",
      "       [-0.66453713],\n",
      "       [-0.76098084],\n",
      "       [-0.7375861 ],\n",
      "       [-0.69257194],\n",
      "       [-0.6682819 ],\n",
      "       [-0.73737264],\n",
      "       [-0.6996152 ],\n",
      "       [-0.67098904],\n",
      "       [-0.7258554 ],\n",
      "       [-0.7228342 ],\n",
      "       [-0.7139111 ],\n",
      "       [-0.67702913],\n",
      "       [-0.70091677],\n",
      "       [-0.73011327],\n",
      "       [-0.757451  ],\n",
      "       [-0.7679399 ]], dtype=float32)>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation vlb: -8.134406031142547, Best vlb: -8.134406031142547\n",
      "\n",
      "Epoch_1, vlb: -7.582987511291065, took: 1.3045690059661865\n",
      "log prob vec:  [<tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-0.7912493 ],\n",
      "       [-0.8246449 ],\n",
      "       [-0.8279821 ],\n",
      "       [-1.1524996 ],\n",
      "       [-1.1542068 ],\n",
      "       [-0.8102668 ],\n",
      "       [-0.792876  ],\n",
      "       [-0.80407596],\n",
      "       [-0.81973845],\n",
      "       [-0.8053821 ],\n",
      "       [-1.132041  ],\n",
      "       [-0.82122684],\n",
      "       [-1.1424447 ],\n",
      "       [-0.83084434],\n",
      "       [-1.1231536 ],\n",
      "       [-1.4009038 ],\n",
      "       [-1.3862531 ],\n",
      "       [-1.4614077 ],\n",
      "       [-1.1278279 ],\n",
      "       [-0.80660063],\n",
      "       [-0.8383669 ],\n",
      "       [-0.92378706],\n",
      "       [-1.1280193 ],\n",
      "       [-1.4128485 ],\n",
      "       [-1.488291  ],\n",
      "       [-1.411783  ],\n",
      "       [-1.1465693 ],\n",
      "       [-0.80486196],\n",
      "       [-0.84188527],\n",
      "       [-0.8184558 ],\n",
      "       [-1.1595165 ],\n",
      "       [-1.1731285 ],\n",
      "       [-0.81873584],\n",
      "       [-0.8328578 ],\n",
      "       [-0.8393327 ],\n",
      "       [-0.8234563 ],\n",
      "       [-1.3915045 ],\n",
      "       [-0.8401024 ],\n",
      "       [-1.1562666 ],\n",
      "       [-0.8183138 ],\n",
      "       [-1.396328  ],\n",
      "       [-0.8393327 ],\n",
      "       [-0.84188527],\n",
      "       [-1.1469975 ],\n",
      "       [-1.4273753 ],\n",
      "       [-0.8017065 ],\n",
      "       [-0.7998731 ],\n",
      "       [-0.79385304],\n",
      "       [-0.8222481 ],\n",
      "       [-0.8107212 ],\n",
      "       [-1.3906888 ],\n",
      "       [-0.8261281 ],\n",
      "       [-0.8145135 ],\n",
      "       [-0.809702  ],\n",
      "       [-0.84802824],\n",
      "       [-1.1419103 ],\n",
      "       [-0.81856906],\n",
      "       [-0.77490103],\n",
      "       [-0.8038726 ],\n",
      "       [-1.1363308 ],\n",
      "       [-1.1264046 ],\n",
      "       [-1.1600938 ],\n",
      "       [-0.7728709 ],\n",
      "       [-1.1235732 ],\n",
      "       [-1.4272612 ],\n",
      "       [-0.8120034 ],\n",
      "       [-0.829106  ],\n",
      "       [-1.1433905 ],\n",
      "       [-0.79114306],\n",
      "       [-0.8208091 ],\n",
      "       [-0.81002676],\n",
      "       [-0.81041414],\n",
      "       [-0.8079123 ],\n",
      "       [-0.8210228 ],\n",
      "       [-0.8100637 ],\n",
      "       [-0.78384006],\n",
      "       [-1.1297092 ],\n",
      "       [-0.80919874],\n",
      "       [-0.8181332 ],\n",
      "       [-0.8326942 ],\n",
      "       [-0.86459935],\n",
      "       [-0.8200531 ],\n",
      "       [-1.4287168 ],\n",
      "       [-1.1464967 ],\n",
      "       [-0.805442  ],\n",
      "       [-0.82036555],\n",
      "       [-1.1378509 ],\n",
      "       [-1.0832857 ],\n",
      "       [-1.4228032 ],\n",
      "       [-1.4139018 ],\n",
      "       [-0.7900942 ],\n",
      "       [-0.80320406],\n",
      "       [-0.8512756 ],\n",
      "       [-0.77007824],\n",
      "       [-0.8102668 ],\n",
      "       [-0.8015728 ],\n",
      "       [-0.81872475],\n",
      "       [-0.79266256],\n",
      "       [-0.81907105],\n",
      "       [-0.8200531 ],\n",
      "       [-0.8410262 ],\n",
      "       [-0.80913335],\n",
      "       [-0.836671  ],\n",
      "       [-0.8326942 ],\n",
      "       [-1.1251798 ],\n",
      "       [-0.81615245],\n",
      "       [-1.1260161 ],\n",
      "       [-0.81615245],\n",
      "       [-1.1165994 ],\n",
      "       [-1.3894956 ],\n",
      "       [-1.1625075 ],\n",
      "       [-0.81295186],\n",
      "       [-0.8195892 ],\n",
      "       [-0.8392375 ],\n",
      "       [-0.82476974],\n",
      "       [-0.832992  ],\n",
      "       [-0.82476974],\n",
      "       [-1.3826742 ],\n",
      "       [-1.4522669 ],\n",
      "       [-0.8377492 ],\n",
      "       [-1.3949194 ],\n",
      "       [-0.82330287],\n",
      "       [-0.8199576 ],\n",
      "       [-1.1538116 ],\n",
      "       [-0.8401024 ],\n",
      "       [-0.7942958 ],\n",
      "       [-0.80678415],\n",
      "       [-0.8240101 ]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-1.5072385],\n",
      "       [-1.8786292],\n",
      "       [-1.5068254],\n",
      "       [-1.8798015],\n",
      "       [-1.6930822],\n",
      "       [-1.5116268],\n",
      "       [-1.6262614],\n",
      "       [-1.8328158],\n",
      "       [-1.5392162],\n",
      "       [-1.645676 ],\n",
      "       [-1.5336404],\n",
      "       [-1.526226 ],\n",
      "       [-1.5396128],\n",
      "       [-1.5010886],\n",
      "       [-1.8427213],\n",
      "       [-1.5489569],\n",
      "       [-1.5018369],\n",
      "       [-1.5488173],\n",
      "       [-1.5608537],\n",
      "       [-1.9094481],\n",
      "       [-1.7017936],\n",
      "       [-1.4040468],\n",
      "       [-1.5607457],\n",
      "       [-1.8274682],\n",
      "       [-1.5091598],\n",
      "       [-1.5344262],\n",
      "       [-1.5514925],\n",
      "       [-1.6167809],\n",
      "       [-1.8366815],\n",
      "       [-1.536459 ],\n",
      "       [-1.6319567],\n",
      "       [-1.6648133],\n",
      "       [-1.6720102],\n",
      "       [-1.5190567],\n",
      "       [-1.7054529],\n",
      "       [-1.5139558],\n",
      "       [-1.6978704],\n",
      "       [-1.7084345],\n",
      "       [-1.5168885],\n",
      "       [-1.6970677],\n",
      "       [-1.6376753],\n",
      "       [-1.7054529],\n",
      "       [-1.8366815],\n",
      "       [-1.7032523],\n",
      "       [-1.6413155],\n",
      "       [-1.6748483],\n",
      "       [-1.5310013],\n",
      "       [-1.5996742],\n",
      "       [-1.4937164],\n",
      "       [-1.9208052],\n",
      "       [-1.6961292],\n",
      "       [-1.8445097],\n",
      "       [-1.5310922],\n",
      "       [-1.5263516],\n",
      "       [-1.5061811],\n",
      "       [-1.5318501],\n",
      "       [-1.4988394],\n",
      "       [-1.5678011],\n",
      "       [-1.5278656],\n",
      "       [-1.5385747],\n",
      "       [-1.5587282],\n",
      "       [-1.5292891],\n",
      "       [-1.5985271],\n",
      "       [-1.5534868],\n",
      "       [-1.5866585],\n",
      "       [-1.5277478],\n",
      "       [-1.50679  ],\n",
      "       [-1.6620803],\n",
      "       [-1.5216087],\n",
      "       [-1.8779359],\n",
      "       [-1.6630043],\n",
      "       [-1.5115767],\n",
      "       [-1.5061853],\n",
      "       [-1.8687904],\n",
      "       [-1.6701592],\n",
      "       [-1.9621716],\n",
      "       [-1.5290579],\n",
      "       [-1.6524869],\n",
      "       [-1.5358789],\n",
      "       [-1.519061 ],\n",
      "       [-1.5124552],\n",
      "       [-1.5161471],\n",
      "       [-1.5580182],\n",
      "       [-1.5384796],\n",
      "       [-1.5257064],\n",
      "       [-1.5157319],\n",
      "       [-1.5418518],\n",
      "       [-1.7853681],\n",
      "       [-1.6399319],\n",
      "       [-1.5129687],\n",
      "       [-1.4910306],\n",
      "       [-1.538239 ],\n",
      "       [-1.7489362],\n",
      "       [-1.6241146],\n",
      "       [-1.5116268],\n",
      "       [-1.5178996],\n",
      "       [-1.6537303],\n",
      "       [-1.600384 ],\n",
      "       [-1.5262333],\n",
      "       [-1.5161471],\n",
      "       [-1.7116243],\n",
      "       [-1.6524987],\n",
      "       [-1.5084351],\n",
      "       [-1.519061 ],\n",
      "       [-1.5528275],\n",
      "       [-1.8257742],\n",
      "       [-1.5538496],\n",
      "       [-1.8257742],\n",
      "       [-1.8464398],\n",
      "       [-1.5495147],\n",
      "       [-1.512041 ],\n",
      "       [-1.5272459],\n",
      "       [-1.680356 ],\n",
      "       [-1.7419556],\n",
      "       [-1.8545873],\n",
      "       [-1.4970688],\n",
      "       [-1.8545873],\n",
      "       [-1.6434462],\n",
      "       [-1.5556931],\n",
      "       [-1.7016895],\n",
      "       [-1.8527379],\n",
      "       [-1.691237 ],\n",
      "       [-1.5161607],\n",
      "       [-1.6529908],\n",
      "       [-1.7084345],\n",
      "       [-1.6000472],\n",
      "       [-1.5066718],\n",
      "       [-1.882446 ]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-0.56034964],\n",
      "       [-0.5470717 ],\n",
      "       [-0.5465165 ],\n",
      "       [-0.54596287],\n",
      "       [-0.8798583 ],\n",
      "       [-0.85172707],\n",
      "       [-0.5509805 ],\n",
      "       [-0.5437048 ],\n",
      "       [-0.5480416 ],\n",
      "       [-0.8632871 ],\n",
      "       [-0.52659583],\n",
      "       [-0.55336344],\n",
      "       [-0.54740715],\n",
      "       [-0.55140996],\n",
      "       [-0.52205986],\n",
      "       [-0.5274091 ],\n",
      "       [-0.47211552],\n",
      "       [-0.54704475],\n",
      "       [-0.5343498 ],\n",
      "       [-0.54518014],\n",
      "       [-0.5283665 ],\n",
      "       [-0.36852524],\n",
      "       [-0.53427154],\n",
      "       [-0.87059474],\n",
      "       [-0.5634247 ],\n",
      "       [-0.869666  ],\n",
      "       [-0.8694333 ],\n",
      "       [-0.8618681 ],\n",
      "       [-0.5263824 ],\n",
      "       [-0.5500293 ],\n",
      "       [-0.54974365],\n",
      "       [-0.5441389 ],\n",
      "       [-0.5475211 ],\n",
      "       [-0.5403496 ],\n",
      "       [-0.52781045],\n",
      "       [-0.5494932 ],\n",
      "       [-0.5231771 ],\n",
      "       [-0.52808166],\n",
      "       [-0.8715259 ],\n",
      "       [-0.5502994 ],\n",
      "       [-0.8718157 ],\n",
      "       [-0.52781045],\n",
      "       [-0.5263824 ],\n",
      "       [-0.52756864],\n",
      "       [-0.53357697],\n",
      "       [-0.54761916],\n",
      "       [-0.56370944],\n",
      "       [-0.5470739 ],\n",
      "       [-0.5671327 ],\n",
      "       [-0.5439369 ],\n",
      "       [-0.52313876],\n",
      "       [-0.54844743],\n",
      "       [-0.5532207 ],\n",
      "       [-0.55644727],\n",
      "       [-0.54929245],\n",
      "       [-0.5487477 ],\n",
      "       [-0.55381787],\n",
      "       [-0.85508937],\n",
      "       [-0.5535117 ],\n",
      "       [-0.5274738 ],\n",
      "       [-0.5355624 ],\n",
      "       [-0.8495449 ],\n",
      "       [-0.851172  ],\n",
      "       [-0.5382348 ],\n",
      "       [-0.5326379 ],\n",
      "       [-0.55519164],\n",
      "       [-0.54627025],\n",
      "       [-0.5385411 ],\n",
      "       [-0.84419256],\n",
      "       [-0.54271084],\n",
      "       [-0.54456764],\n",
      "       [-0.8512695 ],\n",
      "       [-0.8440213 ],\n",
      "       [-0.86189544],\n",
      "       [-0.54325336],\n",
      "       [-0.85424405],\n",
      "       [-0.52908283],\n",
      "       [-0.5466118 ],\n",
      "       [-0.5509796 ],\n",
      "       [-0.54051816],\n",
      "       [-0.533995  ],\n",
      "       [-0.5537948 ],\n",
      "       [-0.86848485],\n",
      "       [-0.547981  ],\n",
      "       [-0.55715466],\n",
      "       [-0.55354184],\n",
      "       [-0.5259773 ],\n",
      "       [-0.53714573],\n",
      "       [-0.53115827],\n",
      "       [-0.49394268],\n",
      "       [-0.82494843],\n",
      "       [-0.5529384 ],\n",
      "       [-0.5227172 ],\n",
      "       [-0.8525622 ],\n",
      "       [-0.85172707],\n",
      "       [-0.55471885],\n",
      "       [-0.5467068 ],\n",
      "       [-0.8565177 ],\n",
      "       [-0.5529072 ],\n",
      "       [-0.5537948 ],\n",
      "       [-0.52958095],\n",
      "       [-0.54672754],\n",
      "       [-0.5440705 ],\n",
      "       [-0.54051816],\n",
      "       [-0.5370032 ],\n",
      "       [-0.5516442 ],\n",
      "       [-0.5359919 ],\n",
      "       [-0.5516442 ],\n",
      "       [-0.5307387 ],\n",
      "       [-0.5215877 ],\n",
      "       [-0.8644149 ],\n",
      "       [-0.55506694],\n",
      "       [-0.54800534],\n",
      "       [-0.89374804],\n",
      "       [-0.5474802 ],\n",
      "       [-0.55551505],\n",
      "       [-0.5474802 ],\n",
      "       [-0.5358341 ],\n",
      "       [-0.5472968 ],\n",
      "       [-0.52903175],\n",
      "       [-0.5197334 ],\n",
      "       [-0.8753609 ],\n",
      "       [-0.55373055],\n",
      "       [-0.5472665 ],\n",
      "       [-0.52808166],\n",
      "       [-0.54720795],\n",
      "       [-0.84778225],\n",
      "       [-0.5475539 ]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-0.7469216 ],\n",
      "       [-0.773468  ],\n",
      "       [-0.6201063 ],\n",
      "       [-0.77106225],\n",
      "       [-0.6373712 ],\n",
      "       [-0.6375537 ],\n",
      "       [-0.719297  ],\n",
      "       [-0.64395773],\n",
      "       [-0.6204284 ],\n",
      "       [-0.749812  ],\n",
      "       [-0.6150753 ],\n",
      "       [-0.6260524 ],\n",
      "       [-0.771931  ],\n",
      "       [-0.63061684],\n",
      "       [-0.60392445],\n",
      "       [-0.61106205],\n",
      "       [-0.67759085],\n",
      "       [-0.74797046],\n",
      "       [-0.61399186],\n",
      "       [-0.7350194 ],\n",
      "       [-0.6198461 ],\n",
      "       [-0.6979156 ],\n",
      "       [-0.61407554],\n",
      "       [-0.7528515 ],\n",
      "       [-0.7672715 ],\n",
      "       [-0.63313925],\n",
      "       [-0.6278686 ],\n",
      "       [-0.7451995 ],\n",
      "       [-0.59675366],\n",
      "       [-0.6213986 ],\n",
      "       [-0.74031925],\n",
      "       [-0.7570926 ],\n",
      "       [-0.7617737 ],\n",
      "       [-0.6171957 ],\n",
      "       [-0.6191463 ],\n",
      "       [-0.7620083 ],\n",
      "       [-0.6186249 ],\n",
      "       [-0.61860806],\n",
      "       [-0.6324047 ],\n",
      "       [-0.7689006 ],\n",
      "       [-0.75354505],\n",
      "       [-0.6191463 ],\n",
      "       [-0.59675366],\n",
      "       [-0.62229884],\n",
      "       [-0.64042103],\n",
      "       [-0.6398844 ],\n",
      "       [-0.62596506],\n",
      "       [-0.7136066 ],\n",
      "       [-0.6267888 ],\n",
      "       [-0.73682404],\n",
      "       [-0.621879  ],\n",
      "       [-0.7792447 ],\n",
      "       [-0.6251212 ],\n",
      "       [-0.62959605],\n",
      "       [-0.63674736],\n",
      "       [-0.7676992 ],\n",
      "       [-0.7725444 ],\n",
      "       [-0.70266074],\n",
      "       [-0.7457168 ],\n",
      "       [-0.6162217 ],\n",
      "       [-0.61415386],\n",
      "       [-0.7481845 ],\n",
      "       [-0.70669174],\n",
      "       [-0.6205252 ],\n",
      "       [-0.7158535 ],\n",
      "       [-0.6273899 ],\n",
      "       [-0.6217263 ],\n",
      "       [-0.6297994 ],\n",
      "       [-0.6531187 ],\n",
      "       [-0.62679625],\n",
      "       [-0.63338214],\n",
      "       [-0.6378798 ],\n",
      "       [-0.7536631 ],\n",
      "       [-0.62452936],\n",
      "       [-0.6324792 ],\n",
      "       [-0.706152  ],\n",
      "       [-0.61801064],\n",
      "       [-0.6400228 ],\n",
      "       [-0.62247884],\n",
      "       [-0.6168757 ],\n",
      "       [-0.6480982 ],\n",
      "       [-0.7643945 ],\n",
      "       [-0.64481515],\n",
      "       [-0.77070045],\n",
      "       [-0.6293914 ],\n",
      "       [-0.7649371 ],\n",
      "       [-0.6152982 ],\n",
      "       [-0.65013486],\n",
      "       [-0.63862413],\n",
      "       [-0.67469764],\n",
      "       [-0.7388669 ],\n",
      "       [-0.7394552 ],\n",
      "       [-0.6435957 ],\n",
      "       [-0.7173879 ],\n",
      "       [-0.6375537 ],\n",
      "       [-0.7485058 ],\n",
      "       [-0.7591063 ],\n",
      "       [-0.6602267 ],\n",
      "       [-0.6266805 ],\n",
      "       [-0.7643945 ],\n",
      "       [-0.61896694],\n",
      "       [-0.640118  ],\n",
      "       [-0.63135517],\n",
      "       [-0.6168757 ],\n",
      "       [-0.617803  ],\n",
      "       [-0.7660638 ],\n",
      "       [-0.61733776],\n",
      "       [-0.7660638 ],\n",
      "       [-0.62249833],\n",
      "       [-0.6133379 ],\n",
      "       [-0.7522849 ],\n",
      "       [-0.627047  ],\n",
      "       [-0.7636489 ],\n",
      "       [-0.66528046],\n",
      "       [-0.61489016],\n",
      "       [-0.63194364],\n",
      "       [-0.61489016],\n",
      "       [-0.7590178 ],\n",
      "       [-0.75978625],\n",
      "       [-0.62058926],\n",
      "       [-0.6035258 ],\n",
      "       [-0.6418523 ],\n",
      "       [-0.7642695 ],\n",
      "       [-0.747703  ],\n",
      "       [-0.61860806],\n",
      "       [-0.71438456],\n",
      "       [-0.7500034 ],\n",
      "       [-0.76993316]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-0.67375666],\n",
      "       [-0.6116691 ],\n",
      "       [-0.628481  ],\n",
      "       [-0.6041249 ],\n",
      "       [-0.6214751 ],\n",
      "       [-0.6457063 ],\n",
      "       [-0.6836068 ],\n",
      "       [-0.68504024],\n",
      "       [-0.7453105 ],\n",
      "       [-0.65422344],\n",
      "       [-0.61032766],\n",
      "       [-0.7039857 ],\n",
      "       [-0.743649  ],\n",
      "       [-0.64581364],\n",
      "       [-0.61173314],\n",
      "       [-0.6186401 ],\n",
      "       [-0.6266386 ],\n",
      "       [-0.69038033],\n",
      "       [-0.74953485],\n",
      "       [-0.71592015],\n",
      "       [-0.6073903 ],\n",
      "       [-0.7259042 ],\n",
      "       [-0.7497972 ],\n",
      "       [-0.6591818 ],\n",
      "       [-0.69305456],\n",
      "       [-0.6393537 ],\n",
      "       [-0.7317272 ],\n",
      "       [-0.6413249 ],\n",
      "       [-0.6246104 ],\n",
      "       [-0.74234784],\n",
      "       [-0.7098789 ],\n",
      "       [-0.60482   ],\n",
      "       [-0.62040555],\n",
      "       [-0.6164263 ],\n",
      "       [-0.6108787 ],\n",
      "       [-0.62314534],\n",
      "       [-0.61462843],\n",
      "       [-0.615154  ],\n",
      "       [-0.61579394],\n",
      "       [-0.6385621 ],\n",
      "       [-0.6344665 ],\n",
      "       [-0.6108787 ],\n",
      "       [-0.6246104 ],\n",
      "       [-0.60763514],\n",
      "       [-0.7003442 ],\n",
      "       [-0.68082774],\n",
      "       [-0.65500504],\n",
      "       [-0.6850711 ],\n",
      "       [-0.6841309 ],\n",
      "       [-0.717224  ],\n",
      "       [-0.6143026 ],\n",
      "       [-0.64824915],\n",
      "       [-0.7268511 ],\n",
      "       [-0.71155727],\n",
      "       [-0.6703843 ],\n",
      "       [-0.7417173 ],\n",
      "       [-0.62899554],\n",
      "       [-0.6777776 ],\n",
      "       [-0.7236918 ],\n",
      "       [-0.7828557 ],\n",
      "       [-0.74680996],\n",
      "       [-0.72297454],\n",
      "       [-0.67042375],\n",
      "       [-0.72939616],\n",
      "       [-0.67530614],\n",
      "       [-0.71673226],\n",
      "       [-0.6290911 ],\n",
      "       [-0.71415156],\n",
      "       [-0.72332126],\n",
      "       [-0.7293548 ],\n",
      "       [-0.70735615],\n",
      "       [-0.6480231 ],\n",
      "       [-0.6514516 ],\n",
      "       [-0.6311325 ],\n",
      "       [-0.7023057 ],\n",
      "       [-0.69317627],\n",
      "       [-0.6160284 ],\n",
      "       [-0.7128838 ],\n",
      "       [-0.73864764],\n",
      "       [-0.6161022 ],\n",
      "       [-0.6877387 ],\n",
      "       [-0.6136931 ],\n",
      "       [-0.71278787],\n",
      "       [-0.74474597],\n",
      "       [-0.6904039 ],\n",
      "       [-0.6164445 ],\n",
      "       [-0.7861191 ],\n",
      "       [-0.69187236],\n",
      "       [-0.70760125],\n",
      "       [-0.71217746],\n",
      "       [-0.749127  ],\n",
      "       [-0.7262404 ],\n",
      "       [-0.6762026 ],\n",
      "       [-0.6574669 ],\n",
      "       [-0.6457063 ],\n",
      "       [-0.714744  ],\n",
      "       [-0.61828256],\n",
      "       [-0.7002739 ],\n",
      "       [-0.71700126],\n",
      "       [-0.6136931 ],\n",
      "       [-0.62041044],\n",
      "       [-0.7130972 ],\n",
      "       [-0.63232553],\n",
      "       [-0.6161022 ],\n",
      "       [-0.74152315],\n",
      "       [-0.63759273],\n",
      "       [-0.74486643],\n",
      "       [-0.63759273],\n",
      "       [-0.7272748 ],\n",
      "       [-0.6109449 ],\n",
      "       [-0.62319   ],\n",
      "       [-0.71634924],\n",
      "       [-0.62299985],\n",
      "       [-0.669174  ],\n",
      "       [-0.62979674],\n",
      "       [-0.6574178 ],\n",
      "       [-0.62979674],\n",
      "       [-0.6161404 ],\n",
      "       [-0.6904836 ],\n",
      "       [-0.6086278 ],\n",
      "       [-0.6086241 ],\n",
      "       [-0.6382977 ],\n",
      "       [-0.61385447],\n",
      "       [-0.70536065],\n",
      "       [-0.615154  ],\n",
      "       [-0.6860892 ],\n",
      "       [-0.6449348 ],\n",
      "       [-0.6098968 ]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-3.95648265e+00],\n",
      "       [-1.92961678e-01],\n",
      "       [-6.13616258e-02],\n",
      "       [-4.10836428e-01],\n",
      "       [-1.86369181e-01],\n",
      "       [-2.11120620e-01],\n",
      "       [-2.00070092e-03],\n",
      "       [-9.51747894e-01],\n",
      "       [-7.07435906e-02],\n",
      "       [-8.57929292e-04],\n",
      "       [-4.19758022e-01],\n",
      "       [-1.39596128e+00],\n",
      "       [-6.75970018e-02],\n",
      "       [-1.36777595e-01],\n",
      "       [-4.27148134e-01],\n",
      "       [-1.96918190e-01],\n",
      "       [-3.84032607e-01],\n",
      "       [-9.57104802e-01],\n",
      "       [-4.59249705e-01],\n",
      "       [-7.04184771e-02],\n",
      "       [-4.06946063e-01],\n",
      "       [-5.54837704e-01],\n",
      "       [-4.58557576e-01],\n",
      "       [-4.35131699e-01],\n",
      "       [-1.52915840e+01],\n",
      "       [-2.00937644e-01],\n",
      "       [-4.55415815e-01],\n",
      "       [-4.34221745e-01],\n",
      "       [-1.67036697e-03],\n",
      "       [-3.32569098e-03],\n",
      "       [-4.47064877e-01],\n",
      "       [-4.04183596e-01],\n",
      "       [-5.16742133e-02],\n",
      "       [-4.30295050e-01],\n",
      "       [-1.84764624e-01],\n",
      "       [-5.52252457e-02],\n",
      "       [-1.83743730e-01],\n",
      "       [-4.93348911e-02],\n",
      "       [-4.21932876e-01],\n",
      "       [-3.52466017e-01],\n",
      "       [-1.90182641e-01],\n",
      "       [-1.84764624e-01],\n",
      "       [-1.67036697e-03],\n",
      "       [-4.04546380e-01],\n",
      "       [-6.80560013e-04],\n",
      "       [-1.44738102e+00],\n",
      "       [-1.19924603e+01],\n",
      "       [-4.43776518e-01],\n",
      "       [-1.22343855e+01],\n",
      "       [-4.56015348e-01],\n",
      "       [-1.82346210e-01],\n",
      "       [-6.15610123e-01],\n",
      "       [-3.19636852e-01],\n",
      "       [-1.39459312e+00],\n",
      "       [-1.45410013e+00],\n",
      "       [-6.60994202e-02],\n",
      "       [-1.46424547e-01],\n",
      "       [-4.41875905e-01],\n",
      "       [-4.08704160e-03],\n",
      "       [-4.92603928e-02],\n",
      "       [-2.20756978e-01],\n",
      "       [-4.60775644e-01],\n",
      "       [-2.74198279e-02],\n",
      "       [-3.28787506e-01],\n",
      "       [-4.17846262e-01],\n",
      "       [-9.52301204e-01],\n",
      "       [-6.11121766e-02],\n",
      "       [-4.52445090e-01],\n",
      "       [-4.54684764e-01],\n",
      "       [-3.91987944e-03],\n",
      "       [-2.09298963e-03],\n",
      "       [-6.25301003e-02],\n",
      "       [-1.41729861e-01],\n",
      "       [-2.02704430e-01],\n",
      "       [-1.38351500e-01],\n",
      "       [-4.49698269e-01],\n",
      "       [-1.94758818e-01],\n",
      "       [-4.53441143e-01],\n",
      "       [-3.34449182e-03],\n",
      "       [-4.30342585e-01],\n",
      "       [-2.66908836e+00],\n",
      "       [-4.22408372e-01],\n",
      "       [-4.48627055e-01],\n",
      "       [-2.18697771e-01],\n",
      "       [-4.02992392e+00],\n",
      "       [-1.94874570e-01],\n",
      "       [-4.11370903e-01],\n",
      "       [-3.27550769e-01],\n",
      "       [-1.99111685e-01],\n",
      "       [-1.49392754e-01],\n",
      "       [-4.54660505e-01],\n",
      "       [-4.58191544e-01],\n",
      "       [-2.96344012e-01],\n",
      "       [-3.34375113e-01],\n",
      "       [-2.11120620e-01],\n",
      "       [-3.00423026e-01],\n",
      "       [-4.15270418e-01],\n",
      "       [-4.42944348e-01],\n",
      "       [-1.22130752e-01],\n",
      "       [-4.22408372e-01],\n",
      "       [-2.39266214e-04],\n",
      "       [-4.53438729e-01],\n",
      "       [-6.03515320e-02],\n",
      "       [-4.30342585e-01],\n",
      "       [-2.33820616e-03],\n",
      "       [-4.54376876e-01],\n",
      "       [-6.56001866e-02],\n",
      "       [-4.54376876e-01],\n",
      "       [-2.27479339e-01],\n",
      "       [-4.11423773e-01],\n",
      "       [-1.96405396e-01],\n",
      "       [-9.54102516e-01],\n",
      "       [-4.38763789e-04],\n",
      "       [-5.19439578e-02],\n",
      "       [-4.52656627e-01],\n",
      "       [-1.44487572e+00],\n",
      "       [-4.52656627e-01],\n",
      "       [-4.14415628e-01],\n",
      "       [-1.40942585e+00],\n",
      "       [-4.07564312e-01],\n",
      "       [-4.20291096e-01],\n",
      "       [-2.96736718e-04],\n",
      "       [-4.22391176e-01],\n",
      "       [-1.92934822e-03],\n",
      "       [-4.93348911e-02],\n",
      "       [-4.43551332e-01],\n",
      "       [-1.58000167e-03],\n",
      "       [-4.15728241e-01]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-3.96775790e-02],\n",
      "       [-5.74845858e-02],\n",
      "       [-1.12032838e-01],\n",
      "       [-7.62872472e-02],\n",
      "       [-9.18539017e-02],\n",
      "       [-9.60100591e-02],\n",
      "       [-6.52610883e-02],\n",
      "       [-1.14155814e-01],\n",
      "       [-6.79924563e-02],\n",
      "       [-5.10643283e-03],\n",
      "       [-1.07391775e-01],\n",
      "       [-5.91694713e-01],\n",
      "       [-6.81730509e-02],\n",
      "       [-1.36877075e-02],\n",
      "       [-9.65243950e-02],\n",
      "       [-9.14057270e-02],\n",
      "       [-1.83074512e+01],\n",
      "       [-1.09789699e-01],\n",
      "       [-9.87950563e-02],\n",
      "       [-1.70794465e-02],\n",
      "       [-6.58965632e-02],\n",
      "       [-1.52746796e+02],\n",
      "       [-8.57770517e-02],\n",
      "       [-7.34840706e-02],\n",
      "       [-1.48280144e-01],\n",
      "       [-1.09549634e-01],\n",
      "       [-7.80605376e-02],\n",
      "       [-6.38970733e-02],\n",
      "       [-1.01170041e-01],\n",
      "       [-8.30281302e-02],\n",
      "       [-3.77489924e-02],\n",
      "       [-5.77164367e-02],\n",
      "       [-6.79069981e-02],\n",
      "       [-9.18910652e-02],\n",
      "       [-8.16534534e-02],\n",
      "       [-8.35585743e-02],\n",
      "       [-5.85830621e-02],\n",
      "       [-9.90010947e-02],\n",
      "       [-7.98684433e-02],\n",
      "       [-8.64782184e-02],\n",
      "       [-3.89158316e-02],\n",
      "       [-8.16534534e-02],\n",
      "       [-1.01170041e-01],\n",
      "       [-7.09728375e-02],\n",
      "       [-7.41016120e-02],\n",
      "       [-1.27076462e-01],\n",
      "       [-2.44232684e-01],\n",
      "       [-5.20416163e-02],\n",
      "       [-1.43484771e-01],\n",
      "       [-4.90400493e-02],\n",
      "       [-1.56876427e-04],\n",
      "       [-2.17482731e-01],\n",
      "       [-8.28523338e-02],\n",
      "       [-9.55251977e-02],\n",
      "       [-6.80647612e-01],\n",
      "       [-1.55648321e-01],\n",
      "       [-8.97437036e-02],\n",
      "       [-3.68285850e-02],\n",
      "       [-4.50007915e-02],\n",
      "       [-1.74461002e-03],\n",
      "       [-1.00763008e-01],\n",
      "       [-7.44021982e-02],\n",
      "       [-6.56118318e-02],\n",
      "       [-9.83830020e-02],\n",
      "       [-3.05977855e-02],\n",
      "       [-6.81479648e-02],\n",
      "       [-7.37072602e-02],\n",
      "       [-9.28497314e-02],\n",
      "       [-3.53286266e-02],\n",
      "       [-8.09527338e-02],\n",
      "       [-8.43012109e-02],\n",
      "       [-8.57508704e-02],\n",
      "       [-7.41732493e-02],\n",
      "       [-8.37984085e-02],\n",
      "       [-8.25862959e-02],\n",
      "       [-3.29216570e-02],\n",
      "       [-6.24842346e-02],\n",
      "       [-6.66434914e-02],\n",
      "       [-1.92140869e-04],\n",
      "       [-1.04895219e-01],\n",
      "       [-4.31439018e+00],\n",
      "       [-8.12537968e-02],\n",
      "       [-6.53591380e-02],\n",
      "       [-9.12497565e-02],\n",
      "       [-3.96678410e-03],\n",
      "       [-1.57850664e-02],\n",
      "       [-1.01598844e-01],\n",
      "       [-1.81143856e+01],\n",
      "       [-4.48965319e-02],\n",
      "       [-2.26570854e+01],\n",
      "       [-5.66085167e-02],\n",
      "       [-5.17924018e-02],\n",
      "       [-2.83648357e+01],\n",
      "       [-1.20713271e-01],\n",
      "       [-9.60100591e-02],\n",
      "       [-4.77235578e-02],\n",
      "       [-5.91455586e-02],\n",
      "       [-6.22911565e-02],\n",
      "       [-6.64223552e-01],\n",
      "       [-8.12537968e-02],\n",
      "       [-9.19280723e-02],\n",
      "       [-7.80603588e-02],\n",
      "       [-7.23944418e-03],\n",
      "       [-1.04895219e-01],\n",
      "       [-4.04193029e-02],\n",
      "       [-5.88070340e-02],\n",
      "       [-1.26612652e-02],\n",
      "       [-5.88070340e-02],\n",
      "       [-7.04245642e-02],\n",
      "       [-1.80231722e-03],\n",
      "       [-1.01328768e-01],\n",
      "       [-1.31074935e-02],\n",
      "       [-7.22353458e-02],\n",
      "       [-1.22463608e+01],\n",
      "       [-7.77606070e-02],\n",
      "       [-1.24699650e-02],\n",
      "       [-7.77606070e-02],\n",
      "       [-5.66417761e-02],\n",
      "       [-1.99665278e-01],\n",
      "       [-3.73474099e-02],\n",
      "       [-6.20016977e-02],\n",
      "       [-3.99089903e-02],\n",
      "       [-6.98253736e-02],\n",
      "       [-2.20193341e-01],\n",
      "       [-9.90010947e-02],\n",
      "       [-2.00184118e-02],\n",
      "       [-7.80562311e-02],\n",
      "       [-6.51368499e-02]], dtype=float32)>, <tf.Tensor: shape=(128, 1), dtype=float32, numpy=\n",
      "array([[-0.7200519 ],\n",
      "       [-0.6585082 ],\n",
      "       [-0.6579489 ],\n",
      "       [-0.6431154 ],\n",
      "       [-0.61608744],\n",
      "       [-0.6126151 ],\n",
      "       [-0.7443386 ],\n",
      "       [-0.7244846 ],\n",
      "       [-0.7350952 ],\n",
      "       [-0.60302293],\n",
      "       [-0.6565674 ],\n",
      "       [-0.7572806 ],\n",
      "       [-0.74967873],\n",
      "       [-0.65867114],\n",
      "       [-0.67011684],\n",
      "       [-0.6566631 ],\n",
      "       [-0.6333773 ],\n",
      "       [-0.73909897],\n",
      "       [-0.73699576],\n",
      "       [-0.7187895 ],\n",
      "       [-0.6384225 ],\n",
      "       [-0.78585386],\n",
      "       [-0.73732823],\n",
      "       [-0.60900813],\n",
      "       [-0.6452912 ],\n",
      "       [-0.6174628 ],\n",
      "       [-0.7531991 ],\n",
      "       [-0.6008471 ],\n",
      "       [-0.68586344],\n",
      "       [-0.7364454 ],\n",
      "       [-0.76522875],\n",
      "       [-0.62137455],\n",
      "       [-0.63191944],\n",
      "       [-0.65307665],\n",
      "       [-0.6398454 ],\n",
      "       [-0.63274735],\n",
      "       [-0.6440162 ],\n",
      "       [-0.64081174],\n",
      "       [-0.6264161 ],\n",
      "       [-0.6369998 ],\n",
      "       [-0.6149528 ],\n",
      "       [-0.6398454 ],\n",
      "       [-0.68586344],\n",
      "       [-0.635246  ],\n",
      "       [-0.7403633 ],\n",
      "       [-0.7589818 ],\n",
      "       [-0.7570569 ],\n",
      "       [-0.75065833],\n",
      "       [-0.65554535],\n",
      "       [-0.72292256],\n",
      "       [-0.641321  ],\n",
      "       [-0.66044456],\n",
      "       [-0.7439531 ],\n",
      "       [-0.74570376],\n",
      "       [-0.65266573],\n",
      "       [-0.7547412 ],\n",
      "       [-0.65728194],\n",
      "       [-0.7707925 ],\n",
      "       [-0.7271553 ],\n",
      "       [-0.64992964],\n",
      "       [-0.7371976 ],\n",
      "       [-0.7575792 ],\n",
      "       [-0.76262933],\n",
      "       [-0.74761856],\n",
      "       [-0.74809897],\n",
      "       [-0.74632573],\n",
      "       [-0.6564232 ],\n",
      "       [-0.76063704],\n",
      "       [-0.75236905],\n",
      "       [-0.7317474 ],\n",
      "       [-0.7682913 ],\n",
      "       [-0.613522  ],\n",
      "       [-0.61968285],\n",
      "       [-0.6314713 ],\n",
      "       [-0.766912  ],\n",
      "       [-0.7313181 ],\n",
      "       [-0.6559213 ],\n",
      "       [-0.76214224],\n",
      "       [-0.74263656],\n",
      "       [-0.6535667 ],\n",
      "       [-0.65739065],\n",
      "       [-0.6332829 ],\n",
      "       [-0.7521974 ],\n",
      "       [-0.7537707 ],\n",
      "       [-0.7551418 ],\n",
      "       [-0.63432693],\n",
      "       [-0.6496552 ],\n",
      "       [-0.75755954],\n",
      "       [-0.7370101 ],\n",
      "       [-0.77112836],\n",
      "       [-0.61938584],\n",
      "       [-0.73408437],\n",
      "       [-0.7693027 ],\n",
      "       [-0.7741622 ],\n",
      "       [-0.6126151 ],\n",
      "       [-0.7168056 ],\n",
      "       [-0.6278858 ],\n",
      "       [-0.77484554],\n",
      "       [-0.75768054],\n",
      "       [-0.6332829 ],\n",
      "       [-0.6413814 ],\n",
      "       [-0.7619634 ],\n",
      "       [-0.6502129 ],\n",
      "       [-0.6535667 ],\n",
      "       [-0.7445529 ],\n",
      "       [-0.6441598 ],\n",
      "       [-0.7438161 ],\n",
      "       [-0.6441598 ],\n",
      "       [-0.7325424 ],\n",
      "       [-0.6531029 ],\n",
      "       [-0.6169491 ],\n",
      "       [-0.7488918 ],\n",
      "       [-0.6335598 ],\n",
      "       [-0.612604  ],\n",
      "       [-0.6509572 ],\n",
      "       [-0.6611047 ],\n",
      "       [-0.6509572 ],\n",
      "       [-0.6322837 ],\n",
      "       [-0.76031876],\n",
      "       [-0.63725305],\n",
      "       [-0.6688101 ],\n",
      "       [-0.61165303],\n",
      "       [-0.6326761 ],\n",
      "       [-0.76361305],\n",
      "       [-0.64081174],\n",
      "       [-0.75128174],\n",
      "       [-0.6108299 ],\n",
      "       [-0.65336555]], dtype=float32)>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_32779/1274224122.py\", line 27, in <module>\n",
      "    vlb_train, vlb_val, best_epoch, best_vlb, curr_epoch = train_VAEAC(model, x_train, x_test, masker, nb_epochs, early_stop=early_stop, flatten = False)\n",
      "  File \"/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_32779/204414277.py\", line 89, in train_VAEAC\n",
      "    vlb, kl_divergence, rec_loss, regularizer = eval(model, x_batch, x_batch_flat, x_batch_flat_masked_concat, mask_flat)\n",
      "  File \"/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_32779/267379911.py\", line 142, in eval\n",
      "    rec_loss = model.reconstruction_loss(rec_params, x_flat)\n",
      "  File \"/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_32779/267379911.py\", line 57, in reconstruction_loss\n",
      "    time.sleep(2)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 1515, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 1473, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/posixpath.py\", line 391, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/posixpath.py\", line 425, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/posixpath.py\", line 167, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_32779/1274224122.py\", line 27, in <module>\n",
      "    vlb_train, vlb_val, best_epoch, best_vlb, curr_epoch = train_VAEAC(model, x_train, x_test, masker, nb_epochs, early_stop=early_stop, flatten = False)\n",
      "  File \"/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_32779/204414277.py\", line 89, in train_VAEAC\n",
      "    vlb, kl_divergence, rec_loss, regularizer = eval(model, x_batch, x_batch_flat, x_batch_flat_masked_concat, mask_flat)\n",
      "  File \"/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_32779/267379911.py\", line 142, in eval\n",
      "    rec_loss = model.reconstruction_loss(rec_params, x_flat)\n",
      "  File \"/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_32779/267379911.py\", line 57, in reconstruction_loss\n",
      "    time.sleep(2)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3364, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3461, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2066, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 1515, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 1473, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 751, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 720, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 705, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_32779/1274224122.py\", line 27, in <module>\n",
      "    vlb_train, vlb_val, best_epoch, best_vlb, curr_epoch = train_VAEAC(model, x_train, x_test, masker, nb_epochs, early_stop=early_stop, flatten = False)\n",
      "  File \"/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_32779/204414277.py\", line 89, in train_VAEAC\n",
      "    vlb, kl_divergence, rec_loss, regularizer = eval(model, x_batch, x_batch_flat, x_batch_flat_masked_concat, mask_flat)\n",
      "  File \"/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_32779/267379911.py\", line 142, in eval\n",
      "    rec_loss = model.reconstruction_loss(rec_params, x_flat)\n",
      "  File \"/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_32779/267379911.py\", line 57, in reconstruction_loss\n",
      "    time.sleep(2)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3364, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3461, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2066, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2947, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3172, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3383, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2066, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n",
      "    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 1515, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 1473, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/opt/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 753, in getmodule\n",
      "    modulesbyfile[f] = modulesbyfile[\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "masker = top_masker_tensorflow(p=1)\n",
    "\n",
    "x_train, x_test, x_means, x_stds, y_train, y_test, feature_names, X_dims = \\\n",
    "    get_my_COMPAS(rseed=42, separate_test=True, test_ratio=0.1, save_dir='../data/')\n",
    "\n",
    "x_train, x_test, input_dim_vec = join_compas_targets(x_train, x_test, y_train, y_test, X_dims)\n",
    "\n",
    "print('Compas', x_train.shape, x_test.shape)\n",
    "print(input_dim_vec)\n",
    "\n",
    "dname = 'compas'\n",
    "print(dname)\n",
    "\n",
    "width = widths[names.index(dname)] # 350\n",
    "depth = depths[names.index(dname)] # number of hidden layers # 3\n",
    "latent_dim = latent_dims[names.index(dname)] # 4\n",
    "\n",
    "batch_size = 128\n",
    "nb_epochs = 2000\n",
    "early_stop = 200\n",
    "lr = 1e-4\n",
    "\n",
    "optimizer = tfa.optimizers.RectifiedAdam(lr = lr, epsilon = 1e-8)\n",
    "\n",
    "model = VAEAC_gauss_cat(width, depth, latent_dim, input_dim_vec, batch_size, lr, optimizer, save_model = False)\n",
    "\n",
    "vlb_train, vlb_val, best_epoch, best_vlb, curr_epoch = train_VAEAC(model, x_train, x_test, masker, nb_epochs, early_stop=early_stop, flatten = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c94ecff",
   "metadata": {},
   "source": [
    "## Train VAEAC (COMPAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9551adf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compas (5554, 19) (618, 19)\n",
      "[3 6 2 2 2 1 1 2]\n",
      "compas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow_addons/optimizers/rectified_adam.py:120: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "2021-12-11 10:34:50.038726: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_0, vlb: -8.681355768972306, took: 10.464995861053467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-11 10:35:02.245212: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation vlb: -8.305243485953815, Best vlb: -8.305243485953815\n",
      "\n",
      "Epoch_1, vlb: -7.753174734064088, took: 1.5554118156433105\n",
      "Validation vlb: -7.78951622367291, Best vlb: -7.78951622367291\n",
      "\n",
      "Epoch_2, vlb: -7.270990914305981, took: 1.3150877952575684\n",
      "Validation vlb: -7.420550860247566, Best vlb: -7.420550860247566\n",
      "\n",
      "Epoch_3, vlb: -6.904767925034421, took: 1.4372706413269043\n",
      "Validation vlb: -6.989337527636185, Best vlb: -6.989337527636185\n",
      "\n",
      "Epoch_4, vlb: -6.481826809528366, took: 1.3424499034881592\n",
      "Validation vlb: -6.313973465397906, Best vlb: -6.313973465397906\n",
      "\n",
      "Epoch_5, vlb: -6.0440584116802825, took: 1.6763591766357422\n",
      "Validation vlb: -5.627160942670211, Best vlb: -5.627160942670211\n",
      "\n",
      "Epoch_6, vlb: -5.6088894207699935, took: 1.3605060577392578\n",
      "Validation vlb: -4.99425472796542, Best vlb: -4.99425472796542\n",
      "\n",
      "Epoch_7, vlb: -5.240122656707159, took: 1.1718788146972656\n",
      "Validation vlb: -4.658706757628802, Best vlb: -4.658706757628802\n",
      "\n",
      "Epoch_8, vlb: -5.040081029492915, took: 1.3337459564208984\n",
      "Validation vlb: -4.474220698705383, Best vlb: -4.474220698705383\n",
      "\n",
      "Epoch_9, vlb: -4.8561399313542415, took: 1.7001888751983643\n",
      "Validation vlb: -4.357474387270733, Best vlb: -4.357474387270733\n",
      "\n",
      "Epoch_10, vlb: -4.74542897443871, took: 1.0000898838043213\n",
      "Validation vlb: -4.263980153309103, Best vlb: -4.263980153309103\n",
      "\n",
      "Epoch_11, vlb: -4.634117330455677, took: 1.3274998664855957\n",
      "Validation vlb: -4.239526338176049, Best vlb: -4.239526338176049\n",
      "\n",
      "Epoch_12, vlb: -4.547014993173766, took: 2.113776922225952\n",
      "Validation vlb: -4.015772435271624, Best vlb: -4.015772435271624\n",
      "\n",
      "Epoch_13, vlb: -4.500731827416865, took: 1.6047542095184326\n",
      "Validation vlb: -3.868905007260517, Best vlb: -3.868905007260517\n",
      "\n",
      "Epoch_14, vlb: -4.447000867907504, took: 1.3548579216003418\n",
      "Validation vlb: -3.8115677864420374, Best vlb: -3.8115677864420374\n",
      "\n",
      "Epoch_15, vlb: -4.3324055645935475, took: 1.5316250324249268\n",
      "Validation vlb: -3.7236143879134294, Best vlb: -3.7236143879134294\n",
      "\n",
      "Epoch_16, vlb: -4.261622181804136, took: 1.5432100296020508\n",
      "Validation vlb: -3.5905998192944573, Best vlb: -3.5905998192944573\n",
      "\n",
      "Epoch_17, vlb: -4.190360140019198, took: 1.4033029079437256\n",
      "Validation vlb: -3.4129538242870936, Best vlb: -3.4129538242870936\n",
      "\n",
      "Epoch_18, vlb: -4.086560984588502, took: 1.460010051727295\n",
      "Validation vlb: -3.3733401468270805, Best vlb: -3.3733401468270805\n",
      "\n",
      "Epoch_19, vlb: -4.061175603251857, took: 1.39778733253479\n",
      "Validation vlb: -3.312043619773149, Best vlb: -3.312043619773149\n",
      "\n",
      "Epoch_20, vlb: -3.969725368260736, took: 1.5239412784576416\n",
      "Validation vlb: -3.1165391905022286, Best vlb: -3.1165391905022286\n",
      "\n",
      "Epoch_21, vlb: -3.9427398502032536, took: 1.274212121963501\n",
      "Validation vlb: -3.1189684759837526, Best vlb: -3.1165391905022286\n",
      "\n",
      "Epoch_22, vlb: -3.866525703683619, took: 1.420684814453125\n",
      "Validation vlb: -3.0820183190713037, Best vlb: -3.0820183190713037\n",
      "\n",
      "Epoch_23, vlb: -3.8348825454368494, took: 1.5052082538604736\n",
      "Validation vlb: -3.000346301057192, Best vlb: -3.000346301057192\n",
      "\n",
      "Epoch_24, vlb: -3.817299249063175, took: 1.5421600341796875\n",
      "Validation vlb: -2.9916815271655333, Best vlb: -2.9916815271655333\n",
      "\n",
      "Epoch_25, vlb: -3.7823509811110148, took: 1.4938509464263916\n",
      "Validation vlb: -2.989891172612755, Best vlb: -2.989891172612755\n",
      "\n",
      "Epoch_26, vlb: -3.716115727199682, took: 1.4072060585021973\n",
      "Validation vlb: -3.1242155897578763, Best vlb: -2.989891172612755\n",
      "\n",
      "Epoch_27, vlb: -3.7270282443839524, took: 1.4063329696655273\n",
      "Validation vlb: -2.8522665207440028, Best vlb: -2.8522665207440028\n",
      "\n",
      "Epoch_28, vlb: -3.6667598658085385, took: 1.3967828750610352\n",
      "Validation vlb: -2.8984097176770947, Best vlb: -2.8522665207440028\n",
      "\n",
      "Epoch_29, vlb: -3.677920227991749, took: 1.5613319873809814\n",
      "Validation vlb: -2.9608013089806517, Best vlb: -2.8522665207440028\n",
      "\n",
      "Epoch_30, vlb: -3.6418008355301064, took: 1.7204229831695557\n",
      "Validation vlb: -2.9401619110292603, Best vlb: -2.8522665207440028\n",
      "\n",
      "Epoch_31, vlb: -3.5829328292129623, took: 1.6103370189666748\n",
      "Validation vlb: -2.881443997417067, Best vlb: -2.8522665207440028\n",
      "\n",
      "Epoch_32, vlb: -3.628092077167334, took: 2.2201521396636963\n",
      "Validation vlb: -2.8694671504320066, Best vlb: -2.8522665207440028\n",
      "\n",
      "Epoch_33, vlb: -3.581172677738271, took: 1.2358050346374512\n",
      "Validation vlb: -2.733207523629889, Best vlb: -2.733207523629889\n",
      "\n",
      "Epoch_34, vlb: -3.6031922854491176, took: 1.388681173324585\n",
      "Validation vlb: -2.689175045606002, Best vlb: -2.689175045606002\n",
      "\n",
      "Epoch_35, vlb: -3.5570107692859994, took: 1.2340548038482666\n",
      "Validation vlb: -2.85510774414902, Best vlb: -2.689175045606002\n",
      "\n",
      "Epoch_36, vlb: -3.545934402732313, took: 1.3518650531768799\n",
      "Validation vlb: -2.772095579159684, Best vlb: -2.689175045606002\n",
      "\n",
      "Epoch_37, vlb: -3.539329725845194, took: 1.3404819965362549\n",
      "Validation vlb: -2.754573183152282, Best vlb: -2.689175045606002\n",
      "\n",
      "Epoch_38, vlb: -3.5560155249689394, took: 1.1081516742706299\n",
      "Validation vlb: -2.8718072881976378, Best vlb: -2.689175045606002\n",
      "\n",
      "Epoch_39, vlb: -3.511532454427129, took: 1.2946250438690186\n",
      "Validation vlb: -2.687594555728258, Best vlb: -2.687594555728258\n",
      "\n",
      "Epoch_40, vlb: -3.532937714982299, took: 1.1385860443115234\n",
      "Validation vlb: -2.7267722520241846, Best vlb: -2.687594555728258\n",
      "\n",
      "Epoch_41, vlb: -3.5054690718522035, took: 1.0266036987304688\n",
      "Validation vlb: -2.7308435594379707, Best vlb: -2.687594555728258\n",
      "\n",
      "Epoch_42, vlb: -3.480322204221699, took: 1.0212302207946777\n",
      "Validation vlb: -2.7190784867913207, Best vlb: -2.687594555728258\n",
      "\n",
      "Epoch_43, vlb: -3.4914166750476907, took: 1.038266897201538\n",
      "Validation vlb: -2.693872054417928, Best vlb: -2.687594555728258\n",
      "\n",
      "Epoch_44, vlb: -3.506884886113525, took: 1.0436043739318848\n",
      "Validation vlb: -2.770405034031297, Best vlb: -2.687594555728258\n",
      "\n",
      "Epoch_45, vlb: -3.4434104326349133, took: 1.0342378616333008\n",
      "Validation vlb: -2.6963590850336265, Best vlb: -2.687594555728258\n",
      "\n",
      "Epoch_46, vlb: -3.457401999647685, took: 1.0183019638061523\n",
      "Validation vlb: -2.664160473832806, Best vlb: -2.664160473832806\n",
      "\n",
      "Epoch_47, vlb: -3.4246890823919225, took: 1.0986101627349854\n",
      "Validation vlb: -2.7586040473678737, Best vlb: -2.664160473832806\n",
      "\n",
      "Epoch_48, vlb: -3.4209362605427684, took: 1.027796983718872\n",
      "Validation vlb: -2.615826090562691, Best vlb: -2.615826090562691\n",
      "\n",
      "Epoch_49, vlb: -3.4248012809561104, took: 1.0233008861541748\n",
      "Validation vlb: -2.6473531206063083, Best vlb: -2.615826090562691\n",
      "\n",
      "Epoch_50, vlb: -3.3909987389375975, took: 1.0858900547027588\n",
      "Validation vlb: -2.5707578605046937, Best vlb: -2.5707578605046937\n",
      "\n",
      "Epoch_51, vlb: -3.3656430554381367, took: 1.275630235671997\n",
      "Validation vlb: -2.755190839273644, Best vlb: -2.5707578605046937\n",
      "\n",
      "Epoch_52, vlb: -3.3583562867779677, took: 1.6512880325317383\n",
      "Validation vlb: -2.777109093650645, Best vlb: -2.5707578605046937\n",
      "\n",
      "Epoch_53, vlb: -3.358013242966072, took: 1.2507221698760986\n",
      "Validation vlb: -2.6595393862924914, Best vlb: -2.5707578605046937\n",
      "\n",
      "Epoch_54, vlb: -3.350707710926851, took: 1.505976915359497\n",
      "Validation vlb: -2.6042637639832726, Best vlb: -2.5707578605046937\n",
      "\n",
      "Epoch_55, vlb: -3.345936566666386, took: 1.072037935256958\n",
      "Validation vlb: -2.5661090253626258, Best vlb: -2.5661090253626258\n",
      "\n",
      "Epoch_56, vlb: -3.282826411539503, took: 1.3317039012908936\n",
      "Validation vlb: -2.6469675383521514, Best vlb: -2.5661090253626258\n",
      "\n",
      "Epoch_57, vlb: -3.3061765671463204, took: 1.1894631385803223\n",
      "Validation vlb: -2.590922891900763, Best vlb: -2.5661090253626258\n",
      "\n",
      "Epoch_58, vlb: -3.294991366932793, took: 1.3871569633483887\n",
      "Validation vlb: -2.5520522702473265, Best vlb: -2.5520522702473265\n",
      "\n",
      "Epoch_59, vlb: -3.2912384181044967, took: 1.268406867980957\n",
      "Validation vlb: -2.593542416118881, Best vlb: -2.5520522702473265\n",
      "\n",
      "Epoch_60, vlb: -3.307719831978369, took: 1.035614013671875\n",
      "Validation vlb: -2.6052368651701796, Best vlb: -2.5520522702473265\n",
      "\n",
      "Epoch_61, vlb: -3.284301493122374, took: 1.153559923171997\n",
      "Validation vlb: -2.5925665889357288, Best vlb: -2.5520522702473265\n",
      "\n",
      "Epoch_62, vlb: -3.2647258060030855, took: 1.3833599090576172\n",
      "Validation vlb: -2.6964321545412626, Best vlb: -2.5520522702473265\n",
      "\n",
      "Epoch_63, vlb: -3.2724643683407777, took: 1.6167240142822266\n",
      "Validation vlb: -2.6005230868132756, Best vlb: -2.5520522702473265\n",
      "\n",
      "Epoch_64, vlb: -3.2285307550928257, took: 1.3616001605987549\n",
      "Validation vlb: -2.5651062653674277, Best vlb: -2.5520522702473265\n",
      "\n",
      "Epoch_65, vlb: -3.2741082582621424, took: 1.1427009105682373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation vlb: -2.669371763093572, Best vlb: -2.5520522702473265\n",
      "\n",
      "Epoch_66, vlb: -3.2696308395954445, took: 1.3473799228668213\n",
      "Validation vlb: -2.6947437181441916, Best vlb: -2.5520522702473265\n",
      "\n",
      "Epoch_67, vlb: -3.208865483458797, took: 1.2846782207489014\n",
      "Validation vlb: -2.596956087161808, Best vlb: -2.5520522702473265\n",
      "\n",
      "Epoch_68, vlb: -3.19845939352223, took: 1.4237151145935059\n",
      "Validation vlb: -2.621121026165663, Best vlb: -2.5520522702473265\n",
      "\n",
      "Epoch_69, vlb: -3.1936041935339308, took: 1.124847173690796\n",
      "Validation vlb: -2.468284576070347, Best vlb: -2.468284576070347\n",
      "\n",
      "Epoch_70, vlb: -3.2085145259015113, took: 1.0010669231414795\n",
      "Validation vlb: -2.4914471964234286, Best vlb: -2.468284576070347\n",
      "\n",
      "Epoch_71, vlb: -3.2278447578740628, took: 1.0008630752563477\n",
      "Validation vlb: -2.373121224560784, Best vlb: -2.373121224560784\n",
      "\n",
      "Epoch_72, vlb: -3.173219781064455, took: 0.9948468208312988\n",
      "Validation vlb: -2.487571926950251, Best vlb: -2.373121224560784\n",
      "\n",
      "Epoch_73, vlb: -3.177923118377015, took: 0.971668004989624\n",
      "Validation vlb: -2.484302901141466, Best vlb: -2.373121224560784\n",
      "\n",
      "Epoch_74, vlb: -3.2006068027916683, took: 0.9699239730834961\n",
      "Validation vlb: -2.549726620461177, Best vlb: -2.373121224560784\n",
      "\n",
      "Epoch_75, vlb: -3.1671318524511416, took: 0.9902122020721436\n",
      "Validation vlb: -2.534451748560933, Best vlb: -2.373121224560784\n",
      "\n",
      "Epoch_76, vlb: -3.1760997113711036, took: 0.982759952545166\n",
      "Validation vlb: -2.5158810731276726, Best vlb: -2.373121224560784\n",
      "\n",
      "Epoch_77, vlb: -3.113972398073157, took: 0.9809129238128662\n",
      "Validation vlb: -2.5986180861019395, Best vlb: -2.373121224560784\n",
      "\n",
      "Epoch_78, vlb: -3.14814108784009, took: 0.9674382209777832\n",
      "Validation vlb: -2.5854686433057568, Best vlb: -2.373121224560784\n",
      "\n",
      "Epoch_79, vlb: -3.128959589052595, took: 1.1092510223388672\n",
      "Validation vlb: -2.545262685485642, Best vlb: -2.373121224560784\n",
      "\n",
      "Epoch_80, vlb: -3.135444037211126, took: 0.9931340217590332\n",
      "Validation vlb: -2.509419049259914, Best vlb: -2.373121224560784\n",
      "\n",
      "Epoch_81, vlb: -3.1118616631102296, took: 0.9691939353942871\n",
      "Validation vlb: -2.4951837170857054, Best vlb: -2.373121224560784\n",
      "\n",
      "Epoch_82, vlb: -3.099987714720751, took: 0.9687774181365967\n",
      "Validation vlb: -2.386310458569079, Best vlb: -2.373121224560784\n",
      "\n",
      "Epoch_83, vlb: -3.1609827360318725, took: 0.9832210540771484\n",
      "Validation vlb: -2.443030368162976, Best vlb: -2.373121224560784\n",
      "\n",
      "Epoch_84, vlb: -3.1046935759905994, took: 0.9998583793640137\n",
      "Validation vlb: -2.4645081191386993, Best vlb: -2.373121224560784\n",
      "\n",
      "Epoch_85, vlb: -3.1363440232179043, took: 0.9780521392822266\n",
      "Validation vlb: -2.4686690032675043, Best vlb: -2.373121224560784\n",
      "\n",
      "Epoch_86, vlb: -3.106416363831171, took: 1.0397748947143555\n",
      "Validation vlb: -2.4807813244730137, Best vlb: -2.373121224560784\n",
      "\n",
      "Epoch_87, vlb: -3.0676130566787583, took: 1.042768955230713\n",
      "Validation vlb: -2.4805700215706934, Best vlb: -2.373121224560784\n",
      "\n",
      "Epoch_88, vlb: -3.1504503652093905, took: 1.0427300930023193\n",
      "Validation vlb: -2.315096635262943, Best vlb: -2.315096635262943\n",
      "\n",
      "Epoch_89, vlb: -3.0750657620084856, took: 1.08001708984375\n",
      "Validation vlb: -2.4459271361526933, Best vlb: -2.315096635262943\n",
      "\n",
      "Epoch_90, vlb: -3.065495885186887, took: 1.0022900104522705\n",
      "Validation vlb: -2.5180186091117487, Best vlb: -2.315096635262943\n",
      "\n",
      "Epoch_91, vlb: -3.0458602547259224, took: 1.0479729175567627\n",
      "Validation vlb: -2.310715446194399, Best vlb: -2.310715446194399\n",
      "\n",
      "Epoch_92, vlb: -3.069056342220753, took: 1.0016839504241943\n",
      "Validation vlb: -2.4822035669123084, Best vlb: -2.310715446194399\n",
      "\n",
      "Epoch_93, vlb: -3.074458527831152, took: 0.994067907333374\n",
      "Validation vlb: -2.4875051357985316, Best vlb: -2.310715446194399\n",
      "\n",
      "Epoch_94, vlb: -3.040118718516453, took: 0.9764978885650635\n",
      "Validation vlb: -2.489891828456743, Best vlb: -2.310715446194399\n",
      "\n",
      "Epoch_95, vlb: -3.0913014871249063, took: 0.9754428863525391\n",
      "Validation vlb: -2.395844076829435, Best vlb: -2.310715446194399\n",
      "\n",
      "Epoch_96, vlb: -3.0224894095033803, took: 0.9834311008453369\n",
      "Validation vlb: -2.2998013697010027, Best vlb: -2.2998013697010027\n",
      "\n",
      "Epoch_97, vlb: -3.073019761596174, took: 1.0117073059082031\n",
      "Validation vlb: -2.2982332799041156, Best vlb: -2.2982332799041156\n",
      "\n",
      "Epoch_98, vlb: -3.0178297140377652, took: 0.9984109401702881\n",
      "Validation vlb: -2.3510163617365567, Best vlb: -2.2982332799041156\n",
      "\n",
      "Epoch_99, vlb: -3.045232843228025, took: 1.023029088973999\n",
      "Validation vlb: -2.205753731496126, Best vlb: -2.205753731496126\n",
      "\n",
      "Epoch_100, vlb: -2.9889595194347134, took: 1.0032520294189453\n",
      "Validation vlb: -2.2611101221498164, Best vlb: -2.205753731496126\n",
      "\n",
      "Epoch_101, vlb: -2.9595300997595673, took: 1.0265898704528809\n",
      "Validation vlb: -2.498729112464633, Best vlb: -2.205753731496126\n",
      "\n",
      "Epoch_102, vlb: -3.0361516558098027, took: 1.0150320529937744\n",
      "Validation vlb: -2.49471444065131, Best vlb: -2.205753731496126\n",
      "\n",
      "Epoch_103, vlb: -2.9976984219109792, took: 0.9957189559936523\n",
      "Validation vlb: -2.3915078755721306, Best vlb: -2.205753731496126\n",
      "\n",
      "Epoch_104, vlb: -2.9908442860418916, took: 1.0147819519042969\n",
      "Validation vlb: -2.4025129006518515, Best vlb: -2.205753731496126\n",
      "\n",
      "Epoch_105, vlb: -2.931592797134092, took: 0.9983198642730713\n",
      "Validation vlb: -2.3474175081283915, Best vlb: -2.205753731496126\n",
      "\n",
      "Epoch_106, vlb: -2.960051016967246, took: 1.009545087814331\n",
      "Validation vlb: -2.321804411588749, Best vlb: -2.205753731496126\n",
      "\n",
      "Epoch_107, vlb: -2.9667995684345208, took: 1.0282008647918701\n",
      "Validation vlb: -2.332798125288633, Best vlb: -2.205753731496126\n",
      "\n",
      "Epoch_108, vlb: -2.942290524248172, took: 1.0261120796203613\n",
      "Validation vlb: -2.34796965855225, Best vlb: -2.205753731496126\n",
      "\n",
      "Epoch_109, vlb: -2.968923274758998, took: 1.0272529125213623\n",
      "Validation vlb: -2.482264135261955, Best vlb: -2.205753731496126\n",
      "\n",
      "Epoch_110, vlb: -2.940091692289518, took: 1.1140496730804443\n",
      "Validation vlb: -2.34599943068421, Best vlb: -2.205753731496126\n",
      "\n",
      "Epoch_111, vlb: -2.9945154512877252, took: 1.028062343597412\n",
      "Validation vlb: -2.3538854832016534, Best vlb: -2.205753731496126\n",
      "\n",
      "Epoch_112, vlb: -2.9654884604528564, took: 1.038020133972168\n",
      "Validation vlb: -2.3959130716169534, Best vlb: -2.205753731496126\n",
      "\n",
      "Epoch_113, vlb: -2.8991303510875377, took: 1.0276899337768555\n",
      "Validation vlb: -2.2481134709416857, Best vlb: -2.205753731496126\n",
      "\n",
      "Epoch_114, vlb: -2.9037890483202333, took: 1.0294780731201172\n",
      "Validation vlb: -2.2416416016982983, Best vlb: -2.205753731496126\n",
      "\n",
      "Epoch_115, vlb: -2.925144127903115, took: 1.0445411205291748\n",
      "Validation vlb: -2.446223566833052, Best vlb: -2.205753731496126\n",
      "\n",
      "Epoch_116, vlb: -2.964447667712911, took: 1.0262987613677979\n",
      "Validation vlb: -2.2603051268938676, Best vlb: -2.205753731496126\n",
      "\n",
      "Epoch_117, vlb: -2.907752004165979, took: 1.0373470783233643\n",
      "Validation vlb: -2.3364434049353244, Best vlb: -2.205753731496126\n",
      "\n",
      "Epoch_118, vlb: -2.9335793437483844, took: 1.264983892440796\n",
      "Validation vlb: -2.461466970567179, Best vlb: -2.205753731496126\n",
      "\n",
      "Epoch_119, vlb: -2.9657025242779036, took: 1.0377700328826904\n",
      "Validation vlb: -2.327356784474888, Best vlb: -2.205753731496126\n",
      "\n",
      "Epoch_120, vlb: -2.922224331983869, took: 1.049604892730713\n",
      "Validation vlb: -2.1748531111621547, Best vlb: -2.1748531111621547\n",
      "\n",
      "Epoch_121, vlb: -2.8948251194004366, took: 1.0079472064971924\n",
      "Validation vlb: -2.336815881111861, Best vlb: -2.1748531111621547\n",
      "\n",
      "Epoch_122, vlb: -2.934608141037471, took: 1.108476161956787\n",
      "Validation vlb: -2.1703253857140403, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_123, vlb: -2.906202731220079, took: 1.015760898590088\n",
      "Validation vlb: -2.314464137006346, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_124, vlb: -2.893423092034724, took: 0.9976000785827637\n",
      "Validation vlb: -2.4815047419958516, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_125, vlb: -2.912907062952236, took: 1.0053620338439941\n",
      "Validation vlb: -2.4096921115245635, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_126, vlb: -2.9467877251319763, took: 1.0034570693969727\n",
      "Validation vlb: -2.3891093460873107, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_127, vlb: -2.905403119606812, took: 1.1083157062530518\n",
      "Validation vlb: -2.2068213691217613, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_128, vlb: -2.867096910641526, took: 0.9944519996643066\n",
      "Validation vlb: -2.1746180316005326, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_129, vlb: -2.910082042067009, took: 0.981691837310791\n",
      "Validation vlb: -2.2252092778104022, Best vlb: -2.1703253857140403\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_130, vlb: -2.9275649907479924, took: 0.988610029220581\n",
      "Validation vlb: -2.329179861784753, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_131, vlb: -2.8638955992161907, took: 1.0236401557922363\n",
      "Validation vlb: -2.3199446147313783, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_132, vlb: -2.929069550208235, took: 0.9887650012969971\n",
      "Validation vlb: -2.29801800335881, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_133, vlb: -2.8546627908032876, took: 0.9835090637207031\n",
      "Validation vlb: -2.2659223789536065, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_134, vlb: -2.9184747726143714, took: 1.0122077465057373\n",
      "Validation vlb: -2.4458970012788246, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_135, vlb: -2.9027435718614676, took: 1.0772161483764648\n",
      "Validation vlb: -2.2129414783712344, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_136, vlb: -2.8404596844378847, took: 1.0498511791229248\n",
      "Validation vlb: -2.354422272987736, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_137, vlb: -2.8258675292622586, took: 0.9975242614746094\n",
      "Validation vlb: -2.328844082779869, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_138, vlb: -2.820502044229916, took: 1.0845568180084229\n",
      "Validation vlb: -2.294956723463188, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_139, vlb: -2.849790363806291, took: 1.0228228569030762\n",
      "Validation vlb: -2.2484416745627196, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_140, vlb: -2.8528882861352027, took: 1.0189521312713623\n",
      "Validation vlb: -2.3119115061744515, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_141, vlb: -2.84019375208002, took: 1.0077340602874756\n",
      "Validation vlb: -2.358755789142596, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_142, vlb: -2.8943404042124103, took: 1.0072600841522217\n",
      "Validation vlb: -2.175678159040926, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_143, vlb: -2.839329424117339, took: 1.0065279006958008\n",
      "Validation vlb: -2.2667713797979756, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_144, vlb: -2.801502400007787, took: 1.1050686836242676\n",
      "Validation vlb: -2.30210299013502, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_145, vlb: -2.834272203747643, took: 0.9939007759094238\n",
      "Validation vlb: -2.2222681261574952, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_146, vlb: -2.848582485145401, took: 1.0109009742736816\n",
      "Validation vlb: -2.2142897308065668, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_147, vlb: -2.8281059726176605, took: 1.0031607151031494\n",
      "Validation vlb: -2.274261171377978, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_148, vlb: -2.8433365901683274, took: 1.0056278705596924\n",
      "Validation vlb: -2.3528786399989454, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_149, vlb: -2.8266407098107345, took: 1.008025884628296\n",
      "Validation vlb: -2.2978131038085543, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_150, vlb: -2.842308498605104, took: 1.0157628059387207\n",
      "Validation vlb: -2.2743196109351986, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_151, vlb: -2.8392028982878847, took: 0.9954183101654053\n",
      "Validation vlb: -2.340828840015004, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_152, vlb: -2.7702509864955487, took: 1.1030631065368652\n",
      "Validation vlb: -2.270618772043765, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_153, vlb: -2.7960728342944354, took: 1.0392630100250244\n",
      "Validation vlb: -2.309576246730718, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_154, vlb: -2.7616604011523282, took: 0.9948608875274658\n",
      "Validation vlb: -2.3223456179054036, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_155, vlb: -2.7880724733323126, took: 1.020759105682373\n",
      "Validation vlb: -2.2269653594995393, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_156, vlb: -2.8133031355205698, took: 1.013639211654663\n",
      "Validation vlb: -2.2939091631509725, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_157, vlb: -2.8044550463498217, took: 1.0135998725891113\n",
      "Validation vlb: -2.2349219870027217, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_158, vlb: -2.8082050926014808, took: 0.9911911487579346\n",
      "Validation vlb: -2.3339239277885957, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_159, vlb: -2.7883481440202424, took: 1.008042812347412\n",
      "Validation vlb: -2.2841816025644444, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_160, vlb: -2.769593167399433, took: 1.024703025817871\n",
      "Validation vlb: -2.230779893961539, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_161, vlb: -2.6819733678757993, took: 1.1605923175811768\n",
      "Validation vlb: -2.252606747605654, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_162, vlb: -2.816466656442078, took: 1.0743539333343506\n",
      "Validation vlb: -2.251850341130229, Best vlb: -2.1703253857140403\n",
      "\n",
      "Epoch_163, vlb: -2.751652872128842, took: 0.9984958171844482\n",
      "Validation vlb: -2.0741851765746824, Best vlb: -2.0741851765746824\n",
      "\n",
      "Epoch_164, vlb: -2.803052190485986, took: 0.996088981628418\n",
      "Validation vlb: -2.1520749913064408, Best vlb: -2.0741851765746824\n",
      "\n",
      "Epoch_165, vlb: -2.824979515353853, took: 1.068922996520996\n",
      "Validation vlb: -2.278176171879938, Best vlb: -2.0741851765746824\n",
      "\n",
      "Epoch_166, vlb: -2.7963015706066097, took: 0.9972009658813477\n",
      "Validation vlb: -2.166543542374299, Best vlb: -2.0741851765746824\n",
      "\n",
      "Epoch_167, vlb: -2.7594452663252667, took: 0.9948408603668213\n",
      "Validation vlb: -2.239651499442684, Best vlb: -2.0741851765746824\n",
      "\n",
      "Epoch_168, vlb: -2.7912655018369286, took: 0.9962759017944336\n",
      "Validation vlb: -2.1121513642925276, Best vlb: -2.0741851765746824\n",
      "\n",
      "Epoch_169, vlb: -2.7623362836538994, took: 0.9988868236541748\n",
      "Validation vlb: -2.3308419552435766, Best vlb: -2.0741851765746824\n",
      "\n",
      "Epoch_170, vlb: -2.7561058471818427, took: 0.9934861660003662\n",
      "Validation vlb: -2.22051387813099, Best vlb: -2.0741851765746824\n",
      "\n",
      "Epoch_171, vlb: -2.7926342993019553, took: 1.0011918544769287\n",
      "Validation vlb: -2.0307714777085386, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_172, vlb: -2.7291576149740964, took: 1.011763095855713\n",
      "Validation vlb: -2.1299786536824743, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_173, vlb: -2.772336271627369, took: 1.0084271430969238\n",
      "Validation vlb: -2.2438535952645213, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_174, vlb: -2.7971675304101016, took: 0.9872009754180908\n",
      "Validation vlb: -2.2751570442347853, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_175, vlb: -2.7349038057117787, took: 0.9948549270629883\n",
      "Validation vlb: -2.2107231879311473, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_176, vlb: -2.7834545224100125, took: 0.9904541969299316\n",
      "Validation vlb: -2.2820640667356717, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_177, vlb: -2.785312297063463, took: 1.0025641918182373\n",
      "Validation vlb: -2.1500646531774774, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_178, vlb: -2.751235809092647, took: 1.1071958541870117\n",
      "Validation vlb: -2.2353833616744354, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_179, vlb: -2.749126315245664, took: 0.9987800121307373\n",
      "Validation vlb: -2.2448801392490423, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_180, vlb: -2.755045664495043, took: 1.0030198097229004\n",
      "Validation vlb: -2.2230317222261893, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_181, vlb: -2.7849232983065457, took: 1.0043408870697021\n",
      "Validation vlb: -2.2064160976595093, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_182, vlb: -2.7778449301501595, took: 0.9899561405181885\n",
      "Validation vlb: -2.1116725580591984, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_183, vlb: -2.796109637898166, took: 1.1467211246490479\n",
      "Validation vlb: -2.1157165869925785, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_184, vlb: -2.6819854509843353, took: 1.004422903060913\n",
      "Validation vlb: -2.270036234438998, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_185, vlb: -2.791809684473875, took: 0.9914829730987549\n",
      "Validation vlb: -2.268681293166571, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_186, vlb: -2.7554264395280756, took: 0.9898989200592041\n",
      "Validation vlb: -2.263350715143395, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_187, vlb: -2.736531591432576, took: 1.0610160827636719\n",
      "Validation vlb: -2.2271326136048946, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_188, vlb: -2.707635920469307, took: 0.9859051704406738\n",
      "Validation vlb: -2.2275903155502763, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_189, vlb: -2.762706158785327, took: 0.989304780960083\n",
      "Validation vlb: -2.1980208825910745, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_190, vlb: -2.768952979773637, took: 0.9915928840637207\n",
      "Validation vlb: -2.303013830894791, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_191, vlb: -2.771090206328939, took: 0.9877948760986328\n",
      "Validation vlb: -2.352706921524986, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_192, vlb: -2.7262703786876763, took: 1.055783748626709\n",
      "Validation vlb: -2.231330418663889, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_193, vlb: -2.7217558464816802, took: 0.995945930480957\n",
      "Validation vlb: -2.2134187692191607, Best vlb: -2.0307714777085386\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_194, vlb: -2.765193069500592, took: 1.0329411029815674\n",
      "Validation vlb: -2.319398938645051, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_195, vlb: -2.7344246509051984, took: 1.1054201126098633\n",
      "Validation vlb: -2.199128885485208, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_196, vlb: -2.7636551494687676, took: 0.9807431697845459\n",
      "Validation vlb: -2.073803064892593, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_197, vlb: -2.7491575321105475, took: 0.9993181228637695\n",
      "Validation vlb: -2.144934144992273, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_198, vlb: -2.728283724254269, took: 1.1154398918151855\n",
      "Validation vlb: -2.2586316445113, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_199, vlb: -2.714788226288001, took: 1.0132911205291748\n",
      "Validation vlb: -2.275484467015683, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_200, vlb: -2.7192260614607373, took: 0.9958851337432861\n",
      "Validation vlb: -2.1853117857936133, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_201, vlb: -2.7621817642036084, took: 0.9963479042053223\n",
      "Validation vlb: -2.2269049446945437, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_202, vlb: -2.754341140427004, took: 0.9801898002624512\n",
      "Validation vlb: -2.262645445981072, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_203, vlb: -2.6969843704064287, took: 0.9909110069274902\n",
      "Validation vlb: -2.3356093604202024, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_204, vlb: -2.705998238016471, took: 1.0708370208740234\n",
      "Validation vlb: -2.162174770361397, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_205, vlb: -2.7637477454239403, took: 0.9930589199066162\n",
      "Validation vlb: -2.2177270986501454, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_206, vlb: -2.6935741024820934, took: 0.9872491359710693\n",
      "Validation vlb: -2.216114815770615, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_207, vlb: -2.716909290948359, took: 0.9907279014587402\n",
      "Validation vlb: -2.1685831392467216, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_208, vlb: -2.723097524670418, took: 1.000760793685913\n",
      "Validation vlb: -2.315657144225531, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_209, vlb: -2.7085240022029473, took: 1.005760908126831\n",
      "Validation vlb: -2.2609460238114143, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_210, vlb: -2.6668606386538416, took: 0.9918830394744873\n",
      "Validation vlb: -2.0452399855678522, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_211, vlb: -2.741668238758311, took: 1.0071098804473877\n",
      "Validation vlb: -2.236606686632224, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_212, vlb: -2.69244741818267, took: 1.0756289958953857\n",
      "Validation vlb: -2.142145453919099, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_213, vlb: -2.663051022442717, took: 1.006079912185669\n",
      "Validation vlb: -2.084933773985187, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_214, vlb: -2.7161921369630226, took: 0.9966850280761719\n",
      "Validation vlb: -2.2585346559876376, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_215, vlb: -2.6658839133732775, took: 1.0051043033599854\n",
      "Validation vlb: -2.279635593729112, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_216, vlb: -2.721994585213062, took: 1.0016200542449951\n",
      "Validation vlb: -2.2731626921101293, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_217, vlb: -2.7863431748701677, took: 1.0046451091766357\n",
      "Validation vlb: -2.258559969250824, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_218, vlb: -2.7416486115948433, took: 1.0063941478729248\n",
      "Validation vlb: -2.3429459383572575, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_219, vlb: -2.729766438494151, took: 1.0187411308288574\n",
      "Validation vlb: -2.150199916756269, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_220, vlb: -2.6728608080239273, took: 0.9895930290222168\n",
      "Validation vlb: -2.352941673550405, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_221, vlb: -2.729978574022012, took: 1.1006550788879395\n",
      "Validation vlb: -2.2344451606466547, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_222, vlb: -2.739231238229523, took: 1.001309871673584\n",
      "Validation vlb: -2.1529079108562286, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_223, vlb: -2.7433609610458993, took: 1.0005288124084473\n",
      "Validation vlb: -2.178037501847474, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_224, vlb: -2.694186335894372, took: 0.9935541152954102\n",
      "Validation vlb: -2.3328468390653048, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_225, vlb: -2.6842461446656314, took: 1.0074071884155273\n",
      "Validation vlb: -2.15692174550399, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_226, vlb: -2.6701435370543125, took: 1.000525951385498\n",
      "Validation vlb: -2.18663878888374, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_227, vlb: -2.6973915580025585, took: 1.0073630809783936\n",
      "Validation vlb: -2.144534699739376, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_228, vlb: -2.6779499524438988, took: 1.00099778175354\n",
      "Validation vlb: -2.1572738646689356, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_229, vlb: -2.6847462252313643, took: 1.0856828689575195\n",
      "Validation vlb: -2.163528757959508, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_230, vlb: -2.7347262110004418, took: 0.9879028797149658\n",
      "Validation vlb: -2.147893095479428, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_231, vlb: -2.7575820375785236, took: 1.0025737285614014\n",
      "Validation vlb: -2.3109613082169713, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_232, vlb: -2.6642846415529675, took: 1.0138671398162842\n",
      "Validation vlb: -2.222139157909406, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_233, vlb: -2.6498569537290875, took: 1.0058879852294922\n",
      "Validation vlb: -2.3157947672995163, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_234, vlb: -2.6985474576355957, took: 0.9906008243560791\n",
      "Validation vlb: -2.262857407042124, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_235, vlb: -2.658162038018045, took: 0.9903981685638428\n",
      "Validation vlb: -2.041073070760684, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_236, vlb: -2.6303910398637815, took: 0.988743782043457\n",
      "Validation vlb: -2.1183602470410294, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_237, vlb: -2.698001441656792, took: 0.9743108749389648\n",
      "Validation vlb: -2.141658741679392, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_238, vlb: -2.7009252226949383, took: 1.0991840362548828\n",
      "Validation vlb: -2.130973660830155, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_239, vlb: -2.7101218290881883, took: 0.9975459575653076\n",
      "Validation vlb: -2.0905910614624763, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_240, vlb: -2.705313078997416, took: 0.982839822769165\n",
      "Validation vlb: -2.2922647863530035, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_241, vlb: -2.6908914924397673, took: 1.0027618408203125\n",
      "Validation vlb: -2.2129536516072297, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_242, vlb: -2.682667906781431, took: 0.9915661811828613\n",
      "Validation vlb: -2.1783565070636834, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_243, vlb: -2.6809555761325603, took: 0.9988570213317871\n",
      "Validation vlb: -2.2900465777005192, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_244, vlb: -2.7082618112224104, took: 1.0050990581512451\n",
      "Validation vlb: -2.1648798775904385, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_245, vlb: -2.6358747203271937, took: 0.9995748996734619\n",
      "Validation vlb: -2.220562339986412, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_246, vlb: -2.6710416894616693, took: 1.0463721752166748\n",
      "Validation vlb: -2.230251663325288, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_247, vlb: -2.7083863733825297, took: 0.9824256896972656\n",
      "Validation vlb: -2.2489405974601078, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_248, vlb: -2.6440956027635707, took: 0.990117073059082\n",
      "Validation vlb: -2.1486442251112856, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_249, vlb: -2.7124250223622033, took: 1.0440900325775146\n",
      "Validation vlb: -2.1800627554118828, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_250, vlb: -2.6401289805775288, took: 1.0039300918579102\n",
      "Validation vlb: -2.1833090257490335, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_251, vlb: -2.704608956729503, took: 1.0723381042480469\n",
      "Validation vlb: -2.3384537264752927, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_252, vlb: -2.6563761061715443, took: 0.9851152896881104\n",
      "Validation vlb: -2.183894604155161, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_253, vlb: -2.7196922556376086, took: 0.9828932285308838\n",
      "Validation vlb: -2.13234391104442, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_254, vlb: -2.653495777413104, took: 0.9975907802581787\n",
      "Validation vlb: -2.383232099724433, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_255, vlb: -2.6448240839065646, took: 1.1152617931365967\n",
      "Validation vlb: -2.2935434437106728, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_256, vlb: -2.64901789538844, took: 0.98944091796875\n",
      "Validation vlb: -2.1410122041177595, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_257, vlb: -2.6719383655111266, took: 1.0180091857910156\n",
      "Validation vlb: -2.259584043404045, Best vlb: -2.0307714777085386\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_258, vlb: -2.709066877959227, took: 1.0033729076385498\n",
      "Validation vlb: -2.215249571599621, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_259, vlb: -2.646652789952131, took: 1.0148489475250244\n",
      "Validation vlb: -2.2262961154616767, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_260, vlb: -2.7000197173452736, took: 1.0061800479888916\n",
      "Validation vlb: -2.144450298790793, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_261, vlb: -2.661945887422407, took: 0.985274076461792\n",
      "Validation vlb: -2.285256249233357, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_262, vlb: -2.667781721657027, took: 0.9986450672149658\n",
      "Validation vlb: -2.323839980035924, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_263, vlb: -2.6797949361165823, took: 1.0627119541168213\n",
      "Validation vlb: -2.080953021651333, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_264, vlb: -2.6857938506339667, took: 0.9944126605987549\n",
      "Validation vlb: -2.2768658195112903, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_265, vlb: -2.6638091163408597, took: 1.002389907836914\n",
      "Validation vlb: -2.109498820644366, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_266, vlb: -2.699805534783138, took: 1.0092296600341797\n",
      "Validation vlb: -2.1777109474811738, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_267, vlb: -2.6680284697759653, took: 1.0170741081237793\n",
      "Validation vlb: -2.3150054945529086, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_268, vlb: -2.6604609423332777, took: 1.003265142440796\n",
      "Validation vlb: -2.1512540493968237, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_269, vlb: -2.657366973614581, took: 0.9972269535064697\n",
      "Validation vlb: -2.2307186381330766, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_270, vlb: -2.6785242834836405, took: 1.0008819103240967\n",
      "Validation vlb: -2.2680909147540342, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_271, vlb: -2.6785885764833917, took: 0.9955768585205078\n",
      "Validation vlb: -2.0523628501830364, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_272, vlb: -2.6672939652369783, took: 1.1084117889404297\n",
      "Validation vlb: -2.1990604500940316, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_273, vlb: -2.6618092405225116, took: 1.0003609657287598\n",
      "Validation vlb: -2.3388857108489596, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_274, vlb: -2.6953185667526305, took: 0.9959700107574463\n",
      "Validation vlb: -2.2641751858794574, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_275, vlb: -2.6549044920894196, took: 0.9857437610626221\n",
      "Validation vlb: -2.256484405122529, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_276, vlb: -2.6597694410537702, took: 1.0157930850982666\n",
      "Validation vlb: -2.2313137779729653, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_277, vlb: -2.65578173148705, took: 0.9921360015869141\n",
      "Validation vlb: -2.186724194816787, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_278, vlb: -2.7036176279890136, took: 0.9987900257110596\n",
      "Validation vlb: -2.271274421593132, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_279, vlb: -2.659754999430007, took: 1.012544870376587\n",
      "Validation vlb: -2.3161605538673773, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_280, vlb: -2.6690467689206456, took: 1.06294584274292\n",
      "Validation vlb: -2.0694528428482006, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_281, vlb: -2.6853464593293213, took: 0.9819121360778809\n",
      "Validation vlb: -2.1670010645412705, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_282, vlb: -2.64991801489932, took: 1.0068681240081787\n",
      "Validation vlb: -2.1033463802152466, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_283, vlb: -2.7218905313434814, took: 0.9886636734008789\n",
      "Validation vlb: -2.2797478688187582, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_284, vlb: -2.6729019249691826, took: 0.9977288246154785\n",
      "Validation vlb: -2.138353199634737, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_285, vlb: -2.63821654625406, took: 0.9855868816375732\n",
      "Validation vlb: -2.3058005764260647, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_286, vlb: -2.6562708768648857, took: 0.9941489696502686\n",
      "Validation vlb: -2.267211624719564, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_287, vlb: -2.6313222248776595, took: 0.9973146915435791\n",
      "Validation vlb: -2.193079564949455, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_288, vlb: -2.6818959581624444, took: 1.0417520999908447\n",
      "Validation vlb: -2.1415401897769915, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_289, vlb: -2.6524390083961364, took: 1.109093189239502\n",
      "Validation vlb: -2.2189328408164113, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_290, vlb: -2.6583272252616843, took: 0.9987361431121826\n",
      "Validation vlb: -2.226674167855272, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_291, vlb: -2.6644800940476867, took: 1.0020098686218262\n",
      "Validation vlb: -2.3021447450211903, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_292, vlb: -2.593116984163036, took: 1.0249278545379639\n",
      "Validation vlb: -2.126078375720669, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_293, vlb: -2.6219902906565515, took: 1.0614399909973145\n",
      "Validation vlb: -2.316488145238759, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_294, vlb: -2.6569815426367325, took: 1.015275001525879\n",
      "Validation vlb: -2.1582791639377383, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_295, vlb: -2.652142720725582, took: 1.0045058727264404\n",
      "Validation vlb: -2.2325789604372193, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_296, vlb: -2.643516593070437, took: 1.0016331672668457\n",
      "Validation vlb: -2.180410333435898, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_297, vlb: -2.646027656008453, took: 1.065446138381958\n",
      "Validation vlb: -2.2942684074821598, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_298, vlb: -2.657315732336405, took: 0.9974241256713867\n",
      "Validation vlb: -2.046544341207708, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_299, vlb: -2.6208081765702014, took: 0.9951519966125488\n",
      "Validation vlb: -2.20631895173329, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_300, vlb: -2.673905362231207, took: 0.990703821182251\n",
      "Validation vlb: -2.2488941405583356, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_301, vlb: -2.6111156779053317, took: 0.9950599670410156\n",
      "Validation vlb: -2.320738742266658, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_302, vlb: -2.653652502471352, took: 1.0164828300476074\n",
      "Validation vlb: -2.180649929447853, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_303, vlb: -2.6527586986918137, took: 0.9876859188079834\n",
      "Validation vlb: -2.2802677486320917, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_304, vlb: -2.657750263117954, took: 0.9880540370941162\n",
      "Validation vlb: -2.139219606578543, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_305, vlb: -2.610037851127567, took: 0.9942209720611572\n",
      "Validation vlb: -2.2431540473765152, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_306, vlb: -2.639900859843753, took: 1.1234221458435059\n",
      "Validation vlb: -2.1972181256920775, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_307, vlb: -2.5975331907268906, took: 0.9898827075958252\n",
      "Validation vlb: -2.208624287330603, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_308, vlb: -2.663112558733013, took: 1.0144710540771484\n",
      "Validation vlb: -2.2700815883654992, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_309, vlb: -2.6499502961911183, took: 0.9834959506988525\n",
      "Validation vlb: -2.18497551606311, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_310, vlb: -2.6625204436591, took: 1.0139470100402832\n",
      "Validation vlb: -2.175561031477351, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_311, vlb: -2.6208836291391813, took: 0.9967191219329834\n",
      "Validation vlb: -2.1790212034021765, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_312, vlb: -2.6366909037058757, took: 0.9882268905639648\n",
      "Validation vlb: -2.0906710570684144, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_313, vlb: -2.649146077699012, took: 0.9943380355834961\n",
      "Validation vlb: -2.1986248817258667, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_314, vlb: -2.6891328186966508, took: 1.0329959392547607\n",
      "Validation vlb: -2.300991866966667, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_315, vlb: -2.666381587731482, took: 1.0032379627227783\n",
      "Validation vlb: -2.1447310131344595, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_316, vlb: -2.646678783243665, took: 1.0242972373962402\n",
      "Validation vlb: -2.286621107638461, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_317, vlb: -2.6479729038371773, took: 0.993175745010376\n",
      "Validation vlb: -2.3111213673279893, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_318, vlb: -2.6380826825927306, took: 0.991875171661377\n",
      "Validation vlb: -2.1680232128279107, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_319, vlb: -2.683969228697802, took: 1.0223779678344727\n",
      "Validation vlb: -2.0775582065088463, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_320, vlb: -2.620103603994241, took: 0.9891149997711182\n",
      "Validation vlb: -2.2472660734429715, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_321, vlb: -2.658827869077359, took: 1.0087838172912598\n",
      "Validation vlb: -2.246223088992838, Best vlb: -2.0307714777085386\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_322, vlb: -2.6518364684977547, took: 1.0098240375518799\n",
      "Validation vlb: -2.1919077785269727, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_323, vlb: -2.6129113056696616, took: 1.130194902420044\n",
      "Validation vlb: -2.206555508487047, Best vlb: -2.0307714777085386\n",
      "\n",
      "Epoch_324, vlb: -2.633322009089013, took: 1.0080246925354004\n",
      "Validation vlb: -1.9928766068517196, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_325, vlb: -2.635609570277609, took: 1.0041089057922363\n",
      "Validation vlb: -2.1730436453155715, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_326, vlb: -2.5897098077815945, took: 1.0053372383117676\n",
      "Validation vlb: -2.1553930603570537, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_327, vlb: -2.622505002885488, took: 1.0854852199554443\n",
      "Validation vlb: -2.287704399874295, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_328, vlb: -2.6001838573157725, took: 0.9953429698944092\n",
      "Validation vlb: -2.30103920038464, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_329, vlb: -2.640720779620189, took: 1.0059831142425537\n",
      "Validation vlb: -2.2372574489865102, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_330, vlb: -2.5824559960040765, took: 0.9907703399658203\n",
      "Validation vlb: -2.211171870092744, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_331, vlb: -2.661451027038761, took: 1.0088279247283936\n",
      "Validation vlb: -2.230581687106284, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_332, vlb: -2.601498168794895, took: 0.9839389324188232\n",
      "Validation vlb: -2.1805415577873055, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_333, vlb: -2.585022574841311, took: 1.0078439712524414\n",
      "Validation vlb: -2.215750651066357, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_334, vlb: -2.609374360897406, took: 1.0057110786437988\n",
      "Validation vlb: -2.2817316695710215, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_335, vlb: -2.666851679884705, took: 1.0193798542022705\n",
      "Validation vlb: -2.17577305735122, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_336, vlb: -2.6097270727243447, took: 1.1542189121246338\n",
      "Validation vlb: -2.0812571029447042, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_337, vlb: -2.616964033622728, took: 0.9886500835418701\n",
      "Validation vlb: -2.025422365534267, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_338, vlb: -2.6159242793815705, took: 1.004654884338379\n",
      "Validation vlb: -2.285904390526435, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_339, vlb: -2.6449380840463608, took: 1.0505552291870117\n",
      "Validation vlb: -2.156737406276962, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_340, vlb: -2.6669140298202563, took: 1.1497080326080322\n",
      "Validation vlb: -2.1559611084391768, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_341, vlb: -2.563949900820481, took: 1.2233338356018066\n",
      "Validation vlb: -2.157910307634224, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_342, vlb: -2.6342257903898885, took: 1.152944803237915\n",
      "Validation vlb: -2.1372439768707867, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_343, vlb: -2.6368738075018703, took: 1.064877986907959\n",
      "Validation vlb: -2.18300858786191, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_344, vlb: -2.6188387658727397, took: 1.2230761051177979\n",
      "Validation vlb: -2.3529256763581703, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_345, vlb: -2.668242391597293, took: 0.9972720146179199\n",
      "Validation vlb: -2.2546893809605573, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_346, vlb: -2.5818912224843427, took: 1.0495359897613525\n",
      "Validation vlb: -2.213958626811944, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_347, vlb: -2.5623238234370573, took: 0.9897480010986328\n",
      "Validation vlb: -2.24972807396577, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_348, vlb: -2.6526894484400794, took: 1.0051729679107666\n",
      "Validation vlb: -2.2573938369750977, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_349, vlb: -2.607839582118363, took: 1.0055139064788818\n",
      "Validation vlb: -2.280869766346459, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_350, vlb: -2.5571634713634124, took: 1.0608789920806885\n",
      "Validation vlb: -2.1731985224875046, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_351, vlb: -2.6660494174208624, took: 1.3432440757751465\n",
      "Validation vlb: -2.2123818451532653, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_352, vlb: -2.6572117925677574, took: 1.415060043334961\n",
      "Validation vlb: -2.156815289679469, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_353, vlb: -2.5985189333295153, took: 1.0054640769958496\n",
      "Validation vlb: -2.1923825709950964, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_354, vlb: -2.6167506339698545, took: 0.9918220043182373\n",
      "Validation vlb: -2.2520228142105645, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_355, vlb: -2.6346291638897354, took: 0.9934089183807373\n",
      "Validation vlb: -2.2440610757537645, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_356, vlb: -2.6401459925201327, took: 0.9884862899780273\n",
      "Validation vlb: -2.042259148023661, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_357, vlb: -2.6530369051849836, took: 1.036823034286499\n",
      "Validation vlb: -2.086025784702363, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_358, vlb: -2.6171366291849743, took: 0.9889998435974121\n",
      "Validation vlb: -2.2100412992211993, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_359, vlb: -2.6322035559399075, took: 1.000267744064331\n",
      "Validation vlb: -2.1692135442036253, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_360, vlb: -2.629929886303834, took: 1.1003789901733398\n",
      "Validation vlb: -2.1103440469136903, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_361, vlb: -2.6479960346462317, took: 0.9965059757232666\n",
      "Validation vlb: -2.176167301375503, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_362, vlb: -2.574229225392903, took: 0.9864082336425781\n",
      "Validation vlb: -2.1067785429723056, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_363, vlb: -2.647704526765938, took: 1.0386507511138916\n",
      "Validation vlb: -2.04531515454783, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_364, vlb: -2.595595813029354, took: 0.9895238876342773\n",
      "Validation vlb: -2.2108058462636757, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_365, vlb: -2.6691120575776752, took: 0.995290994644165\n",
      "Validation vlb: -2.256496832208726, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_366, vlb: -2.6525630876234123, took: 1.0024471282958984\n",
      "Validation vlb: -2.0579044664561943, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_367, vlb: -2.6946430924044353, took: 1.0024981498718262\n",
      "Validation vlb: -2.205273385958378, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_368, vlb: -2.602060031410083, took: 1.035254955291748\n",
      "Validation vlb: -2.0515176663506764, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_369, vlb: -2.64802545671803, took: 0.9999759197235107\n",
      "Validation vlb: -2.190338932966337, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_370, vlb: -2.6118340527448973, took: 1.0015530586242676\n",
      "Validation vlb: -2.177930789472216, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_371, vlb: -2.641452368103346, took: 0.9961230754852295\n",
      "Validation vlb: -2.077679451229503, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_372, vlb: -2.6248415874670767, took: 1.0067870616912842\n",
      "Validation vlb: -2.1938709533716096, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_373, vlb: -2.58167577081762, took: 0.9947829246520996\n",
      "Validation vlb: -2.104184855920983, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_374, vlb: -2.5717487214149912, took: 1.0144717693328857\n",
      "Validation vlb: -2.3539577718691533, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_375, vlb: -2.614376697687953, took: 1.0003929138183594\n",
      "Validation vlb: -2.1889593878996023, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_376, vlb: -2.596997644903851, took: 1.0064592361450195\n",
      "Validation vlb: -2.1327193716968917, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_377, vlb: -2.581472602558514, took: 1.1162428855895996\n",
      "Validation vlb: -2.040334243218876, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_378, vlb: -2.576145512018008, took: 1.015197992324829\n",
      "Validation vlb: -2.0980838718537758, Best vlb: -1.9928766068517196\n",
      "\n",
      "Epoch_379, vlb: -2.600899806673423, took: 0.9977118968963623\n",
      "Validation vlb: -1.9800331037021377, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_380, vlb: -2.6485846750076014, took: 1.0146722793579102\n",
      "Validation vlb: -2.1402323238286387, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_381, vlb: -2.5970850305931577, took: 1.0561401844024658\n",
      "Validation vlb: -2.073762916824193, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_382, vlb: -2.618879174344781, took: 1.0302820205688477\n",
      "Validation vlb: -2.3278017136657123, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_383, vlb: -2.5989052225970775, took: 1.0149469375610352\n",
      "Validation vlb: -2.1419909517356106, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_384, vlb: -2.5937281797633656, took: 1.0140080451965332\n",
      "Validation vlb: -2.104892170544967, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_385, vlb: -2.6233810788657026, took: 1.0074796676635742\n",
      "Validation vlb: -2.1895611972870563, Best vlb: -1.9800331037021377\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_386, vlb: -2.5996141794305676, took: 1.0658817291259766\n",
      "Validation vlb: -2.153241313391133, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_387, vlb: -2.598332994568893, took: 1.0199878215789795\n",
      "Validation vlb: -2.0602382737841807, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_388, vlb: -2.6147470357480733, took: 1.0039169788360596\n",
      "Validation vlb: -2.1644407985279863, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_389, vlb: -2.650330875089025, took: 1.0184340476989746\n",
      "Validation vlb: -2.0750813522770954, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_390, vlb: -2.547227197471264, took: 1.1150383949279785\n",
      "Validation vlb: -2.088457249514879, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_391, vlb: -2.634709146240009, took: 1.003939151763916\n",
      "Validation vlb: -2.3042277780551355, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_392, vlb: -2.611340104224315, took: 1.0061781406402588\n",
      "Validation vlb: -2.1431781612939433, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_393, vlb: -2.6543531744094646, took: 1.0237998962402344\n",
      "Validation vlb: -2.088069070890112, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_394, vlb: -2.6085141420278526, took: 1.0236480236053467\n",
      "Validation vlb: -1.991197979565963, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_395, vlb: -2.589243871684035, took: 1.0076138973236084\n",
      "Validation vlb: -2.1977619283793426, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_396, vlb: -2.572242072043249, took: 1.0160188674926758\n",
      "Validation vlb: -2.224966434984917, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_397, vlb: -2.6062050790378075, took: 1.0013036727905273\n",
      "Validation vlb: -2.0654448081760344, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_398, vlb: -2.612994549297625, took: 1.1347830295562744\n",
      "Validation vlb: -2.2155789264197487, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_399, vlb: -2.587412024643252, took: 1.0004868507385254\n",
      "Validation vlb: -2.0025851417899516, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_400, vlb: -2.642780085626505, took: 0.9947311878204346\n",
      "Validation vlb: -2.1259353052836794, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_401, vlb: -2.6073530850154616, took: 0.9948379993438721\n",
      "Validation vlb: -2.1266338562888234, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_402, vlb: -2.6196809873591236, took: 1.0171010494232178\n",
      "Validation vlb: -2.1256320522826853, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_403, vlb: -2.5907751594381363, took: 1.0120232105255127\n",
      "Validation vlb: -2.196402040885876, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_404, vlb: -2.5754360525050863, took: 0.9978370666503906\n",
      "Validation vlb: -2.11106913876765, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_405, vlb: -2.571523017921077, took: 1.000791072845459\n",
      "Validation vlb: -2.04856775802316, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_406, vlb: -2.5806917257346735, took: 1.018366813659668\n",
      "Validation vlb: -2.174427963769166, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_407, vlb: -2.562373896404097, took: 1.1193780899047852\n",
      "Validation vlb: -2.1754153939898346, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_408, vlb: -2.6075090706069393, took: 1.0138449668884277\n",
      "Validation vlb: -2.1505328937641623, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_409, vlb: -2.594207890565163, took: 1.00557279586792\n",
      "Validation vlb: -2.1762707916278283, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_410, vlb: -2.601788180570702, took: 1.0238430500030518\n",
      "Validation vlb: -2.080435612826671, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_411, vlb: -2.608024136557354, took: 1.0148289203643799\n",
      "Validation vlb: -2.23820521684912, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_412, vlb: -2.625380417061745, took: 1.0042288303375244\n",
      "Validation vlb: -2.1224863066256625, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_413, vlb: -2.5995218470494095, took: 1.00518798828125\n",
      "Validation vlb: -2.0384443188176573, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_414, vlb: -2.6226017793649214, took: 1.0254831314086914\n",
      "Validation vlb: -2.250761008957057, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_415, vlb: -2.5803298706367888, took: 1.1169319152832031\n",
      "Validation vlb: -2.1868288347250435, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_416, vlb: -2.620578102383804, took: 1.007737159729004\n",
      "Validation vlb: -2.043832967968049, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_417, vlb: -2.6592337636154335, took: 0.9978220462799072\n",
      "Validation vlb: -2.071165169712795, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_418, vlb: -2.5986804146109015, took: 1.025923728942871\n",
      "Validation vlb: -2.218116692354764, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_419, vlb: -2.612949615337027, took: 1.0121448040008545\n",
      "Validation vlb: -2.2093467442348937, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_420, vlb: -2.596825038094636, took: 0.9924812316894531\n",
      "Validation vlb: -2.166803390848598, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_421, vlb: -2.618505093877076, took: 1.0106620788574219\n",
      "Validation vlb: -2.151523096275947, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_422, vlb: -2.624463510461793, took: 1.0340092182159424\n",
      "Validation vlb: -2.1019809500685014, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_423, vlb: -2.6212768255913046, took: 1.0435612201690674\n",
      "Validation vlb: -2.316843245793315, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_424, vlb: -2.5808548171785524, took: 1.0177271366119385\n",
      "Validation vlb: -2.154736644627593, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_425, vlb: -2.596305199479902, took: 1.0256199836730957\n",
      "Validation vlb: -2.283536199612911, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_426, vlb: -2.597700900554142, took: 1.0126657485961914\n",
      "Validation vlb: -2.1995263435308217, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_427, vlb: -2.625909974174101, took: 0.9998199939727783\n",
      "Validation vlb: -2.302070889658141, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_428, vlb: -2.5377286974371533, took: 1.0034830570220947\n",
      "Validation vlb: -2.2653488210104045, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_429, vlb: -2.621102016264902, took: 1.0165059566497803\n",
      "Validation vlb: -2.205363355022418, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_430, vlb: -2.6518065449484913, took: 1.0224549770355225\n",
      "Validation vlb: -2.1954679026187045, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_431, vlb: -2.645316965596299, took: 1.021791934967041\n",
      "Validation vlb: -2.2075409858357946, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_432, vlb: -2.548566640947292, took: 1.1129271984100342\n",
      "Validation vlb: -2.152499406854697, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_433, vlb: -2.597113843869081, took: 1.0214509963989258\n",
      "Validation vlb: -2.2919332572171602, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_434, vlb: -2.5757350699267114, took: 1.076352834701538\n",
      "Validation vlb: -2.1457218846071116, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_435, vlb: -2.6237515759287784, took: 1.000378131866455\n",
      "Validation vlb: -2.1343038807409096, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_436, vlb: -2.5773209393088834, took: 1.0067269802093506\n",
      "Validation vlb: -2.1595805354874495, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_437, vlb: -2.672296294901583, took: 1.0098681449890137\n",
      "Validation vlb: -2.096661629414481, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_438, vlb: -2.573632446260902, took: 1.0315830707550049\n",
      "Validation vlb: -2.0020274251795893, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_439, vlb: -2.5608746428518874, took: 1.0016920566558838\n",
      "Validation vlb: -2.2975949268896603, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_440, vlb: -2.6347048947997433, took: 1.1055340766906738\n",
      "Validation vlb: -2.04597232642683, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_441, vlb: -2.57405995000469, took: 1.0099949836730957\n",
      "Validation vlb: -2.175576530999736, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_442, vlb: -2.603972718232306, took: 1.0141029357910156\n",
      "Validation vlb: -2.1217747606505855, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_443, vlb: -2.598379524397172, took: 1.0220341682434082\n",
      "Validation vlb: -2.127590674026884, Best vlb: -1.9800331037021377\n",
      "\n",
      "Epoch_444, vlb: -2.6039989940288217, took: 1.008824110031128\n",
      "Validation vlb: -1.9793315662921054, Best vlb: -1.9793315662921054\n",
      "\n",
      "Epoch_445, vlb: -2.601285403007477, took: 1.0575189590454102\n",
      "Validation vlb: -2.116834683711475, Best vlb: -1.9793315662921054\n",
      "\n",
      "Epoch_446, vlb: -2.603573504050601, took: 1.0871670246124268\n",
      "Validation vlb: -2.2310211951678625, Best vlb: -1.9793315662921054\n",
      "\n",
      "Epoch_447, vlb: -2.607821037497158, took: 1.0310087203979492\n",
      "Validation vlb: -2.103474276737102, Best vlb: -1.9793315662921054\n",
      "\n",
      "Epoch_448, vlb: -2.5916447452302025, took: 1.0302989482879639\n",
      "Validation vlb: -2.236923544152269, Best vlb: -1.9793315662921054\n",
      "\n",
      "Epoch_449, vlb: -2.5778269912855722, took: 1.0415058135986328\n",
      "Validation vlb: -2.171617844343957, Best vlb: -1.9793315662921054\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_450, vlb: -2.599944710001741, took: 1.0251829624176025\n",
      "Validation vlb: -2.0023717097094145, Best vlb: -1.9793315662921054\n",
      "\n",
      "Epoch_451, vlb: -2.5853882653274507, took: 1.0006132125854492\n",
      "Validation vlb: -2.19181636660616, Best vlb: -1.9793315662921054\n",
      "\n",
      "Epoch_452, vlb: -2.603328851645569, took: 1.0109031200408936\n",
      "Validation vlb: -2.0461827802040817, Best vlb: -1.9793315662921054\n",
      "\n",
      "Epoch_453, vlb: -2.5935836240117998, took: 1.0734272003173828\n",
      "Validation vlb: -2.114174094400745, Best vlb: -1.9793315662921054\n",
      "\n",
      "Epoch_454, vlb: -2.5838893386241897, took: 1.0238289833068848\n",
      "Validation vlb: -2.1142269419234934, Best vlb: -1.9793315662921054\n",
      "\n",
      "Epoch_455, vlb: -2.6317859585438867, took: 1.0036070346832275\n",
      "Validation vlb: -2.168232874191309, Best vlb: -1.9793315662921054\n",
      "\n",
      "Epoch_456, vlb: -2.6077536608531484, took: 1.0045690536499023\n",
      "Validation vlb: -2.2759075450279953, Best vlb: -1.9793315662921054\n",
      "\n",
      "Epoch_457, vlb: -2.612858440449309, took: 1.0175187587738037\n",
      "Validation vlb: -2.265697143996032, Best vlb: -1.9793315662921054\n",
      "\n",
      "Epoch_458, vlb: -2.619569727959803, took: 1.0369060039520264\n",
      "Validation vlb: -2.0544681934862847, Best vlb: -1.9793315662921054\n",
      "\n",
      "Epoch_459, vlb: -2.611083453449363, took: 1.0015549659729004\n",
      "Validation vlb: -2.184528296819397, Best vlb: -1.9793315662921054\n",
      "\n",
      "Epoch_460, vlb: -2.593172774196401, took: 1.1672923564910889\n",
      "Validation vlb: -2.130273045845402, Best vlb: -1.9793315662921054\n",
      "\n",
      "Epoch_461, vlb: -2.58985394713773, took: 1.3607852458953857\n",
      "Validation vlb: -2.1314967434768923, Best vlb: -1.9793315662921054\n",
      "\n",
      "Epoch_462, vlb: -2.602452206791928, took: 1.1235990524291992\n",
      "Validation vlb: -2.182903751200457, Best vlb: -1.9793315662921054\n",
      "\n",
      "Epoch_463, vlb: -2.6373985994192695, took: 1.1934618949890137\n",
      "Validation vlb: -2.079636550644069, Best vlb: -1.9793315662921054\n",
      "\n",
      "Epoch_464, vlb: -2.6199136905889953, took: 1.217069149017334\n",
      "Validation vlb: -2.0389749556297625, Best vlb: -1.9793315662921054\n",
      "\n",
      "Epoch_465, vlb: -2.5907982133213547, took: 1.3049368858337402\n",
      "Validation vlb: -2.031371951489001, Best vlb: -1.9793315662921054\n",
      "\n",
      "Epoch_466, vlb: -2.5994246480788608, took: 1.135118007659912\n",
      "Validation vlb: -2.137193049427761, Best vlb: -1.9793315662921054\n",
      "\n",
      "Epoch_467, vlb: -2.5844357460146976, took: 1.2651069164276123\n",
      "Validation vlb: -2.157775649746645, Best vlb: -1.9793315662921054\n",
      "\n",
      "Epoch_468, vlb: -2.598322311665113, took: 1.394197940826416\n",
      "Validation vlb: -2.054135219564716, Best vlb: -1.9793315662921054\n",
      "\n",
      "Epoch_469, vlb: -2.509187164004432, took: 1.2659950256347656\n",
      "Validation vlb: -1.926753595808949, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_470, vlb: -2.604538545361259, took: 1.2639570236206055\n",
      "Validation vlb: -2.2186364926952375, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_471, vlb: -2.5926477949439857, took: 1.2777800559997559\n",
      "Validation vlb: -2.093846967690971, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_472, vlb: -2.5610920502034547, took: 1.0440130233764648\n",
      "Validation vlb: -2.1961961498538267, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_473, vlb: -2.573233555665667, took: 1.0477309226989746\n",
      "Validation vlb: -2.1435044868864286, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_474, vlb: -2.577861917074696, took: 1.25675630569458\n",
      "Validation vlb: -2.176913770657141, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_475, vlb: -2.5458783055965877, took: 1.3005099296569824\n",
      "Validation vlb: -2.1157445232459255, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_476, vlb: -2.585325873151373, took: 1.5041449069976807\n",
      "Validation vlb: -2.1763428939584775, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_477, vlb: -2.574297801513315, took: 1.1893398761749268\n",
      "Validation vlb: -2.0492859203067026, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_478, vlb: -2.5723360784198896, took: 1.2649509906768799\n",
      "Validation vlb: -2.1755902512559615, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_479, vlb: -2.578873891644998, took: 1.0907042026519775\n",
      "Validation vlb: -2.153200550758337, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_480, vlb: -2.5671711267650577, took: 0.9929478168487549\n",
      "Validation vlb: -2.1488832054014733, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_481, vlb: -2.628836532870256, took: 1.0886898040771484\n",
      "Validation vlb: -2.0243058721610256, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_482, vlb: -2.5846798012142433, took: 1.1770639419555664\n",
      "Validation vlb: -2.0800862613233546, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_483, vlb: -2.5775243076953607, took: 1.2533071041107178\n",
      "Validation vlb: -2.178671205699637, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_484, vlb: -2.5927738133719984, took: 1.1156299114227295\n",
      "Validation vlb: -2.1380727507149904, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_485, vlb: -2.6324199328630313, took: 1.0715827941894531\n",
      "Validation vlb: -2.2352893113318384, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_486, vlb: -2.615593395604047, took: 1.111771821975708\n",
      "Validation vlb: -2.1260511470072476, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_487, vlb: -2.6180822319550012, took: 1.029930830001831\n",
      "Validation vlb: -2.2093589822837063, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_488, vlb: -2.540711000491957, took: 1.0192389488220215\n",
      "Validation vlb: -2.332204557931153, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_489, vlb: -2.571170638008516, took: 1.0422179698944092\n",
      "Validation vlb: -2.0502288426396142, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_490, vlb: -2.6297325792611397, took: 0.9847958087921143\n",
      "Validation vlb: -2.0686216763308134, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_491, vlb: -2.5666818075313986, took: 0.9787158966064453\n",
      "Validation vlb: -2.160561925384991, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_492, vlb: -2.5546721607650578, took: 0.9986081123352051\n",
      "Validation vlb: -2.2214314335758245, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_493, vlb: -2.5560111727352233, took: 0.9884891510009766\n",
      "Validation vlb: -2.233364416943399, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_494, vlb: -2.610301649566172, took: 0.9846558570861816\n",
      "Validation vlb: -2.1487536584675118, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_495, vlb: -2.557056825110326, took: 1.1283080577850342\n",
      "Validation vlb: -2.1676336168085486, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_496, vlb: -2.5898625164698217, took: 1.0117347240447998\n",
      "Validation vlb: -2.125782527969879, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_497, vlb: -2.585104575893799, took: 0.9973490238189697\n",
      "Validation vlb: -2.1336696533709283, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_498, vlb: -2.570676255492285, took: 1.006883144378662\n",
      "Validation vlb: -2.0265141004111773, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_499, vlb: -2.61182273340002, took: 0.9886178970336914\n",
      "Validation vlb: -2.18023840972135, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_500, vlb: -2.529167178383739, took: 1.0101239681243896\n",
      "Validation vlb: -2.2255261191272426, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_501, vlb: -2.5798801344678175, took: 1.0999412536621094\n",
      "Validation vlb: -2.1879705321827365, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_502, vlb: -2.561438073775388, took: 1.034693956375122\n",
      "Validation vlb: -2.0545429955794203, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_503, vlb: -2.54271505091059, took: 1.3601107597351074\n",
      "Validation vlb: -2.0968540602131567, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_504, vlb: -2.6315708582805653, took: 1.0776119232177734\n",
      "Validation vlb: -2.1045648025463315, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_505, vlb: -2.5441121764006183, took: 1.1292641162872314\n",
      "Validation vlb: -2.0766033393279635, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_506, vlb: -2.5901351007661417, took: 1.0301318168640137\n",
      "Validation vlb: -2.077278616358933, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_507, vlb: -2.5774091155485235, took: 0.9992098808288574\n",
      "Validation vlb: -2.321502776593452, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_508, vlb: -2.6017515873969095, took: 1.1092309951782227\n",
      "Validation vlb: -2.2431205221750203, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_509, vlb: -2.6442335616954162, took: 1.0787007808685303\n",
      "Validation vlb: -2.15313921854334, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_510, vlb: -2.5718292688610833, took: 1.094198226928711\n",
      "Validation vlb: -2.187721404442895, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_511, vlb: -2.5717737498710944, took: 1.108698844909668\n",
      "Validation vlb: -2.087496504428703, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_512, vlb: -2.5705631448933235, took: 1.0644338130950928\n",
      "Validation vlb: -2.1614750477102582, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_513, vlb: -2.5867932663575153, took: 0.9926371574401855\n",
      "Validation vlb: -2.1755022740286916, Best vlb: -1.926753595808949\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_514, vlb: -2.5546903933901817, took: 1.1109511852264404\n",
      "Validation vlb: -2.284532070931493, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_515, vlb: -2.5823454773593473, took: 1.283508062362671\n",
      "Validation vlb: -2.095894829740802, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_516, vlb: -2.5995747307285284, took: 1.2556028366088867\n",
      "Validation vlb: -2.084596474193832, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_517, vlb: -2.571248101982059, took: 1.286679983139038\n",
      "Validation vlb: -2.2134870618678217, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_518, vlb: -2.5743481228580825, took: 1.368934154510498\n",
      "Validation vlb: -2.0192106383518107, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_519, vlb: -2.6003022806911447, took: 1.0619497299194336\n",
      "Validation vlb: -2.145714635602093, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_520, vlb: -2.600879646180386, took: 1.2627449035644531\n",
      "Validation vlb: -2.0281928920437218, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_521, vlb: -2.582542248754395, took: 0.9859638214111328\n",
      "Validation vlb: -2.285402598890286, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_522, vlb: -2.544136580521141, took: 1.0453202724456787\n",
      "Validation vlb: -2.05748969297193, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_523, vlb: -2.585710845836685, took: 1.3113868236541748\n",
      "Validation vlb: -2.1476567269914746, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_524, vlb: -2.6035909657861245, took: 1.2944231033325195\n",
      "Validation vlb: -2.117414668539967, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_525, vlb: -2.5742459707565777, took: 1.3462889194488525\n",
      "Validation vlb: -2.244591544746967, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_526, vlb: -2.533441701122527, took: 1.491569995880127\n",
      "Validation vlb: -2.0850261021586296, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_527, vlb: -2.5508266826048573, took: 1.104910135269165\n",
      "Validation vlb: -2.071907851302508, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_528, vlb: -2.618808233012473, took: 1.2479329109191895\n",
      "Validation vlb: -2.1188376694435442, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_529, vlb: -2.6019041897969837, took: 1.2100849151611328\n",
      "Validation vlb: -2.15757881746323, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_530, vlb: -2.565577755826044, took: 0.9880189895629883\n",
      "Validation vlb: -2.093052621026641, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_531, vlb: -2.5918856112700297, took: 1.0220990180969238\n",
      "Validation vlb: -2.1633888965285712, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_532, vlb: -2.5825123662436913, took: 1.085627794265747\n",
      "Validation vlb: -2.2101348904730047, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_533, vlb: -2.574905256251444, took: 1.1098036766052246\n",
      "Validation vlb: -2.22284065018194, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_534, vlb: -2.537344326335919, took: 1.0928559303283691\n",
      "Validation vlb: -2.171670395965329, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_535, vlb: -2.543519014843428, took: 1.0847151279449463\n",
      "Validation vlb: -2.0732628280676684, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_536, vlb: -2.5922860188324504, took: 0.9970128536224365\n",
      "Validation vlb: -2.3406121067244645, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_537, vlb: -2.6179575408410805, took: 0.9907519817352295\n",
      "Validation vlb: -2.1123653908763504, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_538, vlb: -2.5446273636637637, took: 1.1106691360473633\n",
      "Validation vlb: -2.19339006849863, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_539, vlb: -2.534245080384669, took: 1.1942100524902344\n",
      "Validation vlb: -2.0645227193060816, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_540, vlb: -2.5638917970022024, took: 1.0002148151397705\n",
      "Validation vlb: -2.1002295121405887, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_541, vlb: -2.556245070461923, took: 1.2235469818115234\n",
      "Validation vlb: -2.1169104367783924, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_542, vlb: -2.6094168977081496, took: 1.0550799369812012\n",
      "Validation vlb: -2.2705647189254514, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_543, vlb: -2.536434785384775, took: 1.2635278701782227\n",
      "Validation vlb: -2.0537330054928185, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_544, vlb: -2.5687567979086636, took: 1.0783107280731201\n",
      "Validation vlb: -2.2051503820326723, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_545, vlb: -2.5768343188327383, took: 1.0499310493469238\n",
      "Validation vlb: -2.2000205435799165, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_546, vlb: -2.6294774392289053, took: 1.2178173065185547\n",
      "Validation vlb: -2.089223367110811, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_547, vlb: -2.6020866553045923, took: 1.2537469863891602\n",
      "Validation vlb: -2.2631940131820136, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_548, vlb: -2.615527357950372, took: 1.0921218395233154\n",
      "Validation vlb: -2.200270261579347, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_549, vlb: -2.6430518101735814, took: 1.2824933528900146\n",
      "Validation vlb: -2.087524556805015, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_550, vlb: -2.5806554513394855, took: 1.245635986328125\n",
      "Validation vlb: -2.2795663142281444, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_551, vlb: -2.5529163757759674, took: 0.9960718154907227\n",
      "Validation vlb: -2.174918553590003, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_552, vlb: -2.5686721286629446, took: 1.0095419883728027\n",
      "Validation vlb: -2.2796872750069332, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_553, vlb: -2.555722542190277, took: 1.12461519241333\n",
      "Validation vlb: -2.1306089791665186, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_554, vlb: -2.6085382040172846, took: 1.380418062210083\n",
      "Validation vlb: -2.2263835205615146, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_555, vlb: -2.5510332736343972, took: 1.1295161247253418\n",
      "Validation vlb: -1.9427038099387703, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_556, vlb: -2.5634464269754136, took: 1.291869878768921\n",
      "Validation vlb: -2.112236735504422, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_557, vlb: -2.567882337886136, took: 1.0542809963226318\n",
      "Validation vlb: -2.123202203161122, Best vlb: -1.926753595808949\n",
      "\n",
      "Epoch_558, vlb: -2.6141919768624997, took: 0.988983154296875\n",
      "Validation vlb: -1.9233951958252002, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_559, vlb: -2.5812699281358014, took: 1.2583301067352295\n",
      "Validation vlb: -2.254236717440164, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_560, vlb: -2.561556883067762, took: 1.3272428512573242\n",
      "Validation vlb: -2.084813586716513, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_561, vlb: -2.5809658021003083, took: 1.167160987854004\n",
      "Validation vlb: -2.1405878352501633, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_562, vlb: -2.5977802270601495, took: 1.078190803527832\n",
      "Validation vlb: -2.0624647063344814, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_563, vlb: -2.5351518247039104, took: 1.2577579021453857\n",
      "Validation vlb: -2.1917252363124713, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_564, vlb: -2.5422770360583318, took: 1.2390060424804688\n",
      "Validation vlb: -2.085105629800593, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_565, vlb: -2.536311382950318, took: 1.126786708831787\n",
      "Validation vlb: -2.1515229527618507, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_566, vlb: -2.5444425882347033, took: 1.1553869247436523\n",
      "Validation vlb: -2.133053563173535, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_567, vlb: -2.5649514986449624, took: 1.0361223220825195\n",
      "Validation vlb: -2.0961996914885193, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_568, vlb: -2.522681018882919, took: 1.0918960571289062\n",
      "Validation vlb: -2.1125878442838353, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_569, vlb: -2.627620328103719, took: 1.0978219509124756\n",
      "Validation vlb: -2.1454995913027175, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_570, vlb: -2.561816281022294, took: 1.1581692695617676\n",
      "Validation vlb: -2.1804444643286054, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_571, vlb: -2.563267147708463, took: 1.0402522087097168\n",
      "Validation vlb: -2.1409803979990936, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_572, vlb: -2.6009157122920046, took: 1.007357120513916\n",
      "Validation vlb: -2.1071219922655224, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_573, vlb: -2.5764475334965584, took: 1.1337769031524658\n",
      "Validation vlb: -2.206851713094125, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_574, vlb: -2.580508955528636, took: 1.0976428985595703\n",
      "Validation vlb: -2.113226670663334, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_575, vlb: -2.5976008598779576, took: 1.1624319553375244\n",
      "Validation vlb: -2.175810866371328, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_576, vlb: -2.5689291614055807, took: 1.150526762008667\n",
      "Validation vlb: -2.197607700878748, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_577, vlb: -2.5520687020850947, took: 1.0744388103485107\n",
      "Validation vlb: -2.183969390816673, Best vlb: -1.9233951958252002\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_578, vlb: -2.5760666250023516, took: 1.4607298374176025\n",
      "Validation vlb: -1.9774302811298556, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_579, vlb: -2.5961246418360546, took: 1.0989549160003662\n",
      "Validation vlb: -2.311947414404366, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_580, vlb: -2.548599834445573, took: 1.3489511013031006\n",
      "Validation vlb: -2.0075773166607114, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_581, vlb: -2.5429148886930957, took: 1.385612964630127\n",
      "Validation vlb: -2.1315249254788395, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_582, vlb: -2.5252848290521377, took: 1.3636548519134521\n",
      "Validation vlb: -2.1558749968951574, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_583, vlb: -2.6021890820124445, took: 1.3197941780090332\n",
      "Validation vlb: -2.2479673836223517, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_584, vlb: -2.5612029536490737, took: 1.1171128749847412\n",
      "Validation vlb: -2.2493257939236835, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_585, vlb: -2.600009230602376, took: 1.0790600776672363\n",
      "Validation vlb: -2.2435430299888535, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_586, vlb: -2.5529478972534245, took: 1.0622050762176514\n",
      "Validation vlb: -2.165959214701236, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_587, vlb: -2.570173731458071, took: 1.4667787551879883\n",
      "Validation vlb: -2.142490126168458, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_588, vlb: -2.54617387646602, took: 1.110205888748169\n",
      "Validation vlb: -2.177200134518077, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_589, vlb: -2.5090934484606815, took: 1.010369062423706\n",
      "Validation vlb: -2.1474755342724254, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_590, vlb: -2.578391883370685, took: 1.0645158290863037\n",
      "Validation vlb: -2.1128371471726006, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_591, vlb: -2.569253036518255, took: 0.9890730381011963\n",
      "Validation vlb: -2.1778457866903262, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_592, vlb: -2.543745159458934, took: 1.021134853363037\n",
      "Validation vlb: -2.1373112788092357, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_593, vlb: -2.5326833723258493, took: 1.4425339698791504\n",
      "Validation vlb: -2.2028990467003635, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_594, vlb: -2.606756379203398, took: 1.1891698837280273\n",
      "Validation vlb: -2.2059920659728807, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_595, vlb: -2.521160677263193, took: 1.0026280879974365\n",
      "Validation vlb: -2.1701277675752113, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_596, vlb: -2.570712581056891, took: 1.2044081687927246\n",
      "Validation vlb: -2.131415031102869, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_597, vlb: -2.532448800639536, took: 1.1097769737243652\n",
      "Validation vlb: -2.1361856445139664, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_598, vlb: -2.536125793360874, took: 0.9980480670928955\n",
      "Validation vlb: -2.161371141961477, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_599, vlb: -2.540393977359254, took: 1.1249561309814453\n",
      "Validation vlb: -2.0845382730551907, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_600, vlb: -2.57219000209524, took: 1.0825231075286865\n",
      "Validation vlb: -2.2009269519917014, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_601, vlb: -2.5524611897635126, took: 1.118346929550171\n",
      "Validation vlb: -2.0618983773351873, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_602, vlb: -2.5571980057408323, took: 0.991281270980835\n",
      "Validation vlb: -2.182276123164155, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_603, vlb: -2.6059703503231972, took: 1.0134460926055908\n",
      "Validation vlb: -2.244137300256772, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_604, vlb: -2.582547326074215, took: 0.9873249530792236\n",
      "Validation vlb: -2.067183070969813, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_605, vlb: -2.5363931631844467, took: 1.2762501239776611\n",
      "Validation vlb: -2.1367634214629634, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_606, vlb: -2.521144757392727, took: 1.3648161888122559\n",
      "Validation vlb: -2.218634847298409, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_607, vlb: -2.587421456471939, took: 1.067471981048584\n",
      "Validation vlb: -2.0659510070837817, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_608, vlb: -2.5320817464368996, took: 0.9955630302429199\n",
      "Validation vlb: -2.112187319203102, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_609, vlb: -2.5487856456260696, took: 1.0033390522003174\n",
      "Validation vlb: -2.1078809966547203, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_610, vlb: -2.533768378190098, took: 1.0212090015411377\n",
      "Validation vlb: -2.2215231853781394, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_611, vlb: -2.5460193159771687, took: 0.9965720176696777\n",
      "Validation vlb: -2.1924513782883923, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_612, vlb: -2.5204457223608547, took: 1.0838677883148193\n",
      "Validation vlb: -2.0507746255899324, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_613, vlb: -2.557202441339489, took: 1.3553500175476074\n",
      "Validation vlb: -2.1479207504914415, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_614, vlb: -2.5718276948863585, took: 1.211780071258545\n",
      "Validation vlb: -2.1143428276271883, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_615, vlb: -2.555381153743388, took: 1.3155996799468994\n",
      "Validation vlb: -2.212585339654225, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_616, vlb: -2.5814451134200573, took: 1.1900999546051025\n",
      "Validation vlb: -2.0172953100266193, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_617, vlb: -2.5721446113874897, took: 1.1504590511322021\n",
      "Validation vlb: -2.1098263001364796, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_618, vlb: -2.540078996710525, took: 1.056006908416748\n",
      "Validation vlb: -2.196846914522856, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_619, vlb: -2.5862007915522924, took: 0.9830770492553711\n",
      "Validation vlb: -2.0399642715947914, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_620, vlb: -2.5321641737581113, took: 1.226515769958496\n",
      "Validation vlb: -2.142243920792268, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_621, vlb: -2.5773226661908786, took: 1.2578608989715576\n",
      "Validation vlb: -2.1166404651592465, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_622, vlb: -2.574160842188065, took: 1.315244197845459\n",
      "Validation vlb: -1.9643751865065986, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_623, vlb: -2.6123156327758283, took: 1.0105242729187012\n",
      "Validation vlb: -2.1967706695729476, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_624, vlb: -2.5675489914516, took: 1.0193941593170166\n",
      "Validation vlb: -2.048988402468487, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_625, vlb: -2.503826146422851, took: 1.0314178466796875\n",
      "Validation vlb: -2.0326105637843557, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_626, vlb: -2.520738899257057, took: 1.0645558834075928\n",
      "Validation vlb: -2.0863508198253546, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_627, vlb: -2.554767167177568, took: 1.2994670867919922\n",
      "Validation vlb: -2.323541647408001, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_628, vlb: -2.5802269767504353, took: 1.2980859279632568\n",
      "Validation vlb: -2.1473828596590407, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_629, vlb: -2.580925559757165, took: 1.0561211109161377\n",
      "Validation vlb: -2.1015178906493204, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_630, vlb: -2.5743736213688204, took: 1.1135048866271973\n",
      "Validation vlb: -2.0565216753475104, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_631, vlb: -2.5857342392753515, took: 1.109632968902588\n",
      "Validation vlb: -2.188879836724414, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_632, vlb: -2.566925731888339, took: 1.1230289936065674\n",
      "Validation vlb: -2.247755785204446, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_633, vlb: -2.534041959603227, took: 1.239819049835205\n",
      "Validation vlb: -2.1468891549650517, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_634, vlb: -2.5802752874633326, took: 1.1191291809082031\n",
      "Validation vlb: -2.2097893579106502, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_635, vlb: -2.581732003260397, took: 1.141390085220337\n",
      "Validation vlb: -2.112121319693655, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_636, vlb: -2.622510501794949, took: 1.4071311950683594\n",
      "Validation vlb: -2.1591740372111494, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_637, vlb: -2.5367705821990967, took: 1.2267069816589355\n",
      "Validation vlb: -2.093076159653154, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_638, vlb: -2.5741186485386685, took: 1.2345671653747559\n",
      "Validation vlb: -2.0378555051717173, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_639, vlb: -2.567285966564092, took: 1.2025368213653564\n",
      "Validation vlb: -2.054453057378627, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_640, vlb: -2.588127666726832, took: 1.1924591064453125\n",
      "Validation vlb: -2.0825504913299215, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_641, vlb: -2.5571721144447146, took: 1.1681292057037354\n",
      "Validation vlb: -2.0604232993326526, Best vlb: -1.9233951958252002\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_642, vlb: -2.540474414052747, took: 1.3383450508117676\n",
      "Validation vlb: -2.0510981599875637, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_643, vlb: -2.56586555375184, took: 1.0448100566864014\n",
      "Validation vlb: -2.167811950047811, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_644, vlb: -2.5361257718113372, took: 1.0254199504852295\n",
      "Validation vlb: -2.209473437090136, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_645, vlb: -2.5691121350014687, took: 1.0444848537445068\n",
      "Validation vlb: -2.177261201309155, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_646, vlb: -2.523121460247349, took: 1.2563581466674805\n",
      "Validation vlb: -2.1113188116295825, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_647, vlb: -2.5729380684187397, took: 1.2175788879394531\n",
      "Validation vlb: -2.2246805857686165, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_648, vlb: -2.5637509979101063, took: 1.0816500186920166\n",
      "Validation vlb: -2.2562545480080023, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_649, vlb: -2.557182183573046, took: 0.9987459182739258\n",
      "Validation vlb: -2.0771512969797867, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_650, vlb: -2.4947207313155717, took: 1.1060550212860107\n",
      "Validation vlb: -2.0899220508279153, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_651, vlb: -2.524867108540437, took: 0.9693799018859863\n",
      "Validation vlb: -2.006619262849629, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_652, vlb: -2.544146489702114, took: 1.1729559898376465\n",
      "Validation vlb: -2.2353186950714456, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_653, vlb: -2.5614678652371525, took: 0.9963250160217285\n",
      "Validation vlb: -2.179645824200899, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_654, vlb: -2.5499527825268644, took: 1.4423978328704834\n",
      "Validation vlb: -2.043436853631029, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_655, vlb: -2.5323728401539864, took: 1.4272780418395996\n",
      "Validation vlb: -2.092743199234256, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_656, vlb: -2.518352535589161, took: 1.3605997562408447\n",
      "Validation vlb: -2.160554405940775, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_657, vlb: -2.4694720125902374, took: 1.3624241352081299\n",
      "Validation vlb: -2.157551015465005, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_658, vlb: -2.527041062546326, took: 1.2767069339752197\n",
      "Validation vlb: -2.226988456010047, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_659, vlb: -2.5546188415449045, took: 1.3369641304016113\n",
      "Validation vlb: -1.97439712961129, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_660, vlb: -2.5385294523778303, took: 1.2469522953033447\n",
      "Validation vlb: -2.361750159063, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_661, vlb: -2.539363736594629, took: 1.3860552310943604\n",
      "Validation vlb: -2.043147638777699, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_662, vlb: -2.5681550370560817, took: 1.218883991241455\n",
      "Validation vlb: -2.108356431849952, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_663, vlb: -2.544125543380754, took: 1.2081952095031738\n",
      "Validation vlb: -2.223615827298087, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_664, vlb: -2.565020993582671, took: 1.3895149230957031\n",
      "Validation vlb: -2.0555470190387712, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_665, vlb: -2.5394152455678465, took: 1.2031669616699219\n",
      "Validation vlb: -2.286372935501889, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_666, vlb: -2.523573638554052, took: 1.5207791328430176\n",
      "Validation vlb: -2.112243184765566, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_667, vlb: -2.5337463934558047, took: 1.002406120300293\n",
      "Validation vlb: -2.1623440560399523, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_668, vlb: -2.557145443330105, took: 0.9779820442199707\n",
      "Validation vlb: -2.2108221123519454, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_669, vlb: -2.5462303996987594, took: 0.9723861217498779\n",
      "Validation vlb: -2.1558492631202375, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_670, vlb: -2.57909234779792, took: 0.9995791912078857\n",
      "Validation vlb: -2.1110902241518583, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_671, vlb: -2.566661796940717, took: 1.1262211799621582\n",
      "Validation vlb: -2.135275883196241, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_672, vlb: -2.5035092527161495, took: 1.024271011352539\n",
      "Validation vlb: -2.160763120574087, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_673, vlb: -2.565961075121008, took: 0.9845020771026611\n",
      "Validation vlb: -2.0071148787501563, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_674, vlb: -2.5114418560539082, took: 1.160254955291748\n",
      "Validation vlb: -2.1020880540211997, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_675, vlb: -2.54234135043591, took: 1.1241788864135742\n",
      "Validation vlb: -2.2266226864169716, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_676, vlb: -2.5267950441067195, took: 0.9758780002593994\n",
      "Validation vlb: -2.105229501971149, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_677, vlb: -2.537665867316109, took: 0.980255126953125\n",
      "Validation vlb: -2.2094660638605506, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_678, vlb: -2.5195030034511348, took: 1.0046489238739014\n",
      "Validation vlb: -2.164098931747733, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_679, vlb: -2.51929068608296, took: 1.1528761386871338\n",
      "Validation vlb: -2.1154830779844116, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_680, vlb: -2.5448454962302165, took: 1.0187392234802246\n",
      "Validation vlb: -2.079413378894522, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_681, vlb: -2.5501749903596815, took: 0.9893021583557129\n",
      "Validation vlb: -2.40990809252347, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_682, vlb: -2.5239667169559246, took: 1.0075879096984863\n",
      "Validation vlb: -2.0612067288951197, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_683, vlb: -2.572954188931705, took: 0.9812960624694824\n",
      "Validation vlb: -2.151091401245216, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_684, vlb: -2.5736025282063415, took: 0.988029956817627\n",
      "Validation vlb: -2.0527424658000664, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_685, vlb: -2.524072668987682, took: 0.9720101356506348\n",
      "Validation vlb: -2.315833254539465, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_686, vlb: -2.5468123223569523, took: 0.990466833114624\n",
      "Validation vlb: -2.1686847580289377, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_687, vlb: -2.559061979019775, took: 0.9755649566650391\n",
      "Validation vlb: -2.22317683812484, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_688, vlb: -2.5650425993541957, took: 1.0800549983978271\n",
      "Validation vlb: -1.9969667470185117, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_689, vlb: -2.5044134762118007, took: 0.9714770317077637\n",
      "Validation vlb: -2.249157005914978, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_690, vlb: -2.560569638522529, took: 0.9890239238739014\n",
      "Validation vlb: -2.1238974568141704, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_691, vlb: -2.5421613906843867, took: 0.9853529930114746\n",
      "Validation vlb: -2.070325272754558, Best vlb: -1.9233951958252002\n",
      "\n",
      "Epoch_692, vlb: -2.5526328009583774, took: 1.013071060180664\n",
      "Validation vlb: -1.8541494619499133, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_693, vlb: -2.4896734670035308, took: 1.1652560234069824\n",
      "Validation vlb: -2.125463075236595, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_694, vlb: -2.5746069181124254, took: 0.9990289211273193\n",
      "Validation vlb: -2.1624852530778806, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_695, vlb: -2.551258572580834, took: 0.986793041229248\n",
      "Validation vlb: -2.1790867233739317, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_696, vlb: -2.5686689162365943, took: 1.2328078746795654\n",
      "Validation vlb: -2.1748148445944184, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_697, vlb: -2.543888516077516, took: 1.5986111164093018\n",
      "Validation vlb: -2.1937433577664076, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_698, vlb: -2.567249787038378, took: 1.6956720352172852\n",
      "Validation vlb: -2.0370751113953327, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_699, vlb: -2.5786882366342514, took: 1.3906629085540771\n",
      "Validation vlb: -2.1225052890654137, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_700, vlb: -2.5820009700677615, took: 1.2387657165527344\n",
      "Validation vlb: -2.271178950769616, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_701, vlb: -2.503571111433876, took: 0.9860129356384277\n",
      "Validation vlb: -2.186297973768611, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_702, vlb: -2.5972901158166266, took: 1.0720949172973633\n",
      "Validation vlb: -2.1876569164609445, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_703, vlb: -2.5521029515965274, took: 1.1212151050567627\n",
      "Validation vlb: -2.1393780777755294, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_704, vlb: -2.5526760440101115, took: 1.1860802173614502\n",
      "Validation vlb: -2.104614389367088, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_705, vlb: -2.549348850665781, took: 1.2698569297790527\n",
      "Validation vlb: -2.046922875453739, Best vlb: -1.8541494619499133\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_706, vlb: -2.5754030208429675, took: 1.1219708919525146\n",
      "Validation vlb: -2.0589675127881244, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_707, vlb: -2.5383118686806525, took: 1.1278700828552246\n",
      "Validation vlb: -2.2550817492710347, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_708, vlb: -2.559733250607335, took: 1.0664598941802979\n",
      "Validation vlb: -2.2828557174954214, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_709, vlb: -2.5175603905040065, took: 1.1255009174346924\n",
      "Validation vlb: -2.1365513894164447, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_710, vlb: -2.5505065389962174, took: 0.9871678352355957\n",
      "Validation vlb: -2.2122904976594797, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_711, vlb: -2.521979978799734, took: 1.007314920425415\n",
      "Validation vlb: -2.2418739363599363, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_712, vlb: -2.557880043081985, took: 1.0165119171142578\n",
      "Validation vlb: -2.1333765273726875, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_713, vlb: -2.5364092677284327, took: 1.0981860160827637\n",
      "Validation vlb: -2.0184287588958987, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_714, vlb: -2.517311505718458, took: 1.0007247924804688\n",
      "Validation vlb: -2.055452109926341, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_715, vlb: -2.509586853220345, took: 1.0620670318603516\n",
      "Validation vlb: -2.1514990769543694, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_716, vlb: -2.5850871810391087, took: 1.0643830299377441\n",
      "Validation vlb: -2.0931853592202887, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_717, vlb: -2.5346353481603177, took: 1.1376750469207764\n",
      "Validation vlb: -2.1326082524358263, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_718, vlb: -2.5129531619786616, took: 1.2546141147613525\n",
      "Validation vlb: -2.2801321903092964, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_719, vlb: -2.5820562836932415, took: 1.3273251056671143\n",
      "Validation vlb: -2.1348109114131497, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_720, vlb: -2.5148134081321607, took: 1.197866439819336\n",
      "Validation vlb: -2.205393190137005, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_721, vlb: -2.520647057192249, took: 1.2762260437011719\n",
      "Validation vlb: -2.1363190161757486, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_722, vlb: -2.566081580215965, took: 1.1088762283325195\n",
      "Validation vlb: -2.1252143915417125, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_723, vlb: -2.456074699893631, took: 1.1624538898468018\n",
      "Validation vlb: -2.0839931146998234, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_724, vlb: -2.5325822654198005, took: 1.1409249305725098\n",
      "Validation vlb: -2.352022777483301, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_725, vlb: -2.5309180186556364, took: 1.1708900928497314\n",
      "Validation vlb: -2.1962964696791567, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_726, vlb: -2.582901428447252, took: 1.3207550048828125\n",
      "Validation vlb: -2.21858334001214, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_727, vlb: -2.5674378613924267, took: 1.1373977661132812\n",
      "Validation vlb: -2.3020455771665356, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_728, vlb: -2.5093580866014524, took: 1.3864750862121582\n",
      "Validation vlb: -2.1280941415373174, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_729, vlb: -2.4935998472757035, took: 1.1433160305023193\n",
      "Validation vlb: -2.0995268632678923, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_730, vlb: -2.5279636234527962, took: 1.1125710010528564\n",
      "Validation vlb: -2.1181949458075957, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_731, vlb: -2.5549024508760954, took: 1.0039339065551758\n",
      "Validation vlb: -2.2559617507033365, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_732, vlb: -2.559641849921683, took: 0.9852490425109863\n",
      "Validation vlb: -2.1993806292709794, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_733, vlb: -2.545296663511645, took: 1.087458848953247\n",
      "Validation vlb: -2.2511840013238604, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_734, vlb: -2.5176858777488182, took: 1.2130959033966064\n",
      "Validation vlb: -2.1483138564335102, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_735, vlb: -2.574362315760699, took: 1.7989239692687988\n",
      "Validation vlb: -2.2139777828574565, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_736, vlb: -2.52891134992537, took: 1.2683360576629639\n",
      "Validation vlb: -2.0644732810146986, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_737, vlb: -2.528050546556829, took: 1.4322516918182373\n",
      "Validation vlb: -2.1475327115228646, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_738, vlb: -2.5052615195072767, took: 1.5367369651794434\n",
      "Validation vlb: -2.156313938616163, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_739, vlb: -2.5673812440199777, took: 1.2314910888671875\n",
      "Validation vlb: -2.053965418084154, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_740, vlb: -2.526504313211712, took: 1.2306742668151855\n",
      "Validation vlb: -2.151806039717591, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_741, vlb: -2.5146922677980723, took: 1.4314920902252197\n",
      "Validation vlb: -2.1267649402124595, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_742, vlb: -2.587434084843851, took: 1.4152028560638428\n",
      "Validation vlb: -2.092683547523029, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_743, vlb: -2.552771145206241, took: 1.2649850845336914\n",
      "Validation vlb: -2.2852495835436972, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_744, vlb: -2.5287922010775477, took: 1.0242271423339844\n",
      "Validation vlb: -2.211518161890962, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_745, vlb: -2.587165152893506, took: 1.227370262145996\n",
      "Validation vlb: -2.1564238931754645, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_746, vlb: -2.5565748373552926, took: 1.2512187957763672\n",
      "Validation vlb: -2.0447862341180203, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_747, vlb: -2.538084798293617, took: 1.3186299800872803\n",
      "Validation vlb: -2.1516806230575907, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_748, vlb: -2.518357811533894, took: 1.3310160636901855\n",
      "Validation vlb: -2.2264138271121916, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_749, vlb: -2.5024294287427793, took: 1.0105831623077393\n",
      "Validation vlb: -2.205807367189031, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_750, vlb: -2.5375749196858557, took: 0.9823107719421387\n",
      "Validation vlb: -2.198523147206476, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_751, vlb: -2.5632891978640586, took: 1.0332059860229492\n",
      "Validation vlb: -2.0914718371764742, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_752, vlb: -2.514944830943579, took: 1.317836046218872\n",
      "Validation vlb: -2.0960685215335833, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_753, vlb: -2.5059394421747445, took: 1.1824100017547607\n",
      "Validation vlb: -2.030095230028467, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_754, vlb: -2.5459468633440876, took: 1.100672960281372\n",
      "Validation vlb: -2.174578072569517, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_755, vlb: -2.561248918810808, took: 1.0273642539978027\n",
      "Validation vlb: -2.2064855993758514, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_756, vlb: -2.5125202382259073, took: 1.0912561416625977\n",
      "Validation vlb: -2.2779105473490593, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_757, vlb: -2.5207212891645643, took: 1.1498119831085205\n",
      "Validation vlb: -2.1375593857471995, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_758, vlb: -2.586508335636209, took: 1.0041401386260986\n",
      "Validation vlb: -1.981729516705263, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_759, vlb: -2.538655155375573, took: 1.0139398574829102\n",
      "Validation vlb: -2.1185189018743324, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_760, vlb: -2.5492667859090847, took: 0.989703893661499\n",
      "Validation vlb: -2.115839038080382, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_761, vlb: -2.5565635415346106, took: 1.0077009201049805\n",
      "Validation vlb: -2.169816468525859, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_762, vlb: -2.552306101225158, took: 1.0319931507110596\n",
      "Validation vlb: -2.2304768080078667, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_763, vlb: -2.5148846191341687, took: 0.9736678600311279\n",
      "Validation vlb: -2.0781331174196165, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_764, vlb: -2.519174854919384, took: 1.0676069259643555\n",
      "Validation vlb: -2.0562603265336414, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_765, vlb: -2.4732792301061215, took: 1.2142250537872314\n",
      "Validation vlb: -2.086438364195592, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_766, vlb: -2.555196487006413, took: 1.2305469512939453\n",
      "Validation vlb: -2.2802564164195633, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_767, vlb: -2.5192751109192257, took: 1.2779951095581055\n",
      "Validation vlb: -2.11489795481117, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_768, vlb: -2.497948688270808, took: 1.3504581451416016\n",
      "Validation vlb: -2.0354870704385455, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_769, vlb: -2.5586641363673475, took: 1.3670856952667236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation vlb: -2.164329890680159, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_770, vlb: -2.5214693808152084, took: 1.122248888015747\n",
      "Validation vlb: -2.2333268880072534, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_771, vlb: -2.530628820696794, took: 1.179579734802246\n",
      "Validation vlb: -2.178893680325604, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_772, vlb: -2.5191900913862173, took: 1.580995798110962\n",
      "Validation vlb: -2.062140961680983, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_773, vlb: -2.5444034131516817, took: 1.3203151226043701\n",
      "Validation vlb: -2.1172669848192087, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_774, vlb: -2.558447038178655, took: 1.1672580242156982\n",
      "Validation vlb: -2.1632364922742626, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_775, vlb: -2.5903482289463655, took: 0.988692045211792\n",
      "Validation vlb: -2.1431367181265624, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_776, vlb: -2.5328927809358457, took: 1.1802082061767578\n",
      "Validation vlb: -2.1350364731353464, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_777, vlb: -2.503625465976192, took: 1.1270179748535156\n",
      "Validation vlb: -2.021946486917514, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_778, vlb: -2.5433295546652905, took: 1.1076500415802002\n",
      "Validation vlb: -2.1847068287408082, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_779, vlb: -2.5872515027455214, took: 1.1819357872009277\n",
      "Validation vlb: -2.1043839385208574, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_780, vlb: -2.5663435060771027, took: 0.9760088920593262\n",
      "Validation vlb: -2.1695520376310378, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_781, vlb: -2.4684311468737734, took: 1.049213171005249\n",
      "Validation vlb: -2.095623922193706, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_782, vlb: -2.4916891860240735, took: 0.9664709568023682\n",
      "Validation vlb: -1.978987791391638, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_783, vlb: -2.4904275462487466, took: 1.0920660495758057\n",
      "Validation vlb: -2.1990123288916923, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_784, vlb: -2.547324181032301, took: 1.0417728424072266\n",
      "Validation vlb: -2.164784658302381, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_785, vlb: -2.561003088135491, took: 0.981348991394043\n",
      "Validation vlb: -2.0976281906794574, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_786, vlb: -2.5285340117000183, took: 1.2482070922851562\n",
      "Validation vlb: -2.229073266381199, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_787, vlb: -2.5274329799347144, took: 1.1674590110778809\n",
      "Validation vlb: -2.077746643603427, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_788, vlb: -2.539119153560217, took: 1.2214128971099854\n",
      "Validation vlb: -2.1665497404857748, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_789, vlb: -2.51009574982173, took: 1.3540699481964111\n",
      "Validation vlb: -2.234900470690434, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_790, vlb: -2.569966270977703, took: 1.0980749130249023\n",
      "Validation vlb: -2.1458685259217196, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_791, vlb: -2.547365158206059, took: 0.993297815322876\n",
      "Validation vlb: -2.085983606603925, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_792, vlb: -2.510733053758928, took: 0.9829549789428711\n",
      "Validation vlb: -2.1690688981979025, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_793, vlb: -2.5287346464719627, took: 1.0942721366882324\n",
      "Validation vlb: -2.2543847722914614, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_794, vlb: -2.5417964182442625, took: 1.0179250240325928\n",
      "Validation vlb: -1.9837218773789391, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_795, vlb: -2.526245372004332, took: 0.991469144821167\n",
      "Validation vlb: -2.19233800369559, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_796, vlb: -2.560545736995583, took: 1.0024182796478271\n",
      "Validation vlb: -2.137346101038664, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_797, vlb: -2.5219618243024122, took: 1.305077075958252\n",
      "Validation vlb: -2.111330661958861, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_798, vlb: -2.504068231342248, took: 1.3989520072937012\n",
      "Validation vlb: -2.1295616765624112, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_799, vlb: -2.532763580731621, took: 1.4832549095153809\n",
      "Validation vlb: -2.3285054651278894, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_800, vlb: -2.5323260670134093, took: 1.2257249355316162\n",
      "Validation vlb: -2.128724971635442, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_801, vlb: -2.5032644792130236, took: 1.3195161819458008\n",
      "Validation vlb: -2.313837542117221, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_802, vlb: -2.554442859700484, took: 1.2053120136260986\n",
      "Validation vlb: -2.077758241239875, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_803, vlb: -2.4930973974199744, took: 1.1225829124450684\n",
      "Validation vlb: -2.2218784761274515, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_804, vlb: -2.5877026399777954, took: 1.1159141063690186\n",
      "Validation vlb: -2.20625417209366, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_805, vlb: -2.5651897113444266, took: 1.0019540786743164\n",
      "Validation vlb: -2.1760661995526656, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_806, vlb: -2.528679354825293, took: 1.2684543132781982\n",
      "Validation vlb: -2.246208346391573, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_807, vlb: -2.5302862867761267, took: 1.3025550842285156\n",
      "Validation vlb: -2.0581956851058023, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_808, vlb: -2.540167555175413, took: 1.1517300605773926\n",
      "Validation vlb: -2.079205032305424, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_809, vlb: -2.509795084161041, took: 1.2408409118652344\n",
      "Validation vlb: -2.1037116243615506, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_810, vlb: -2.5279035312390215, took: 1.2182509899139404\n",
      "Validation vlb: -2.1714500601623437, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_811, vlb: -2.4860575369978104, took: 1.0391290187835693\n",
      "Validation vlb: -2.214577468081971, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_812, vlb: -2.511376628010317, took: 1.2039239406585693\n",
      "Validation vlb: -2.1325114923001878, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_813, vlb: -2.486372598865112, took: 1.1302049160003662\n",
      "Validation vlb: -2.105845223352747, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_814, vlb: -2.577504043146394, took: 1.3725168704986572\n",
      "Validation vlb: -2.115514655329263, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_815, vlb: -2.511962077838292, took: 1.2023909091949463\n",
      "Validation vlb: -2.0661444324505753, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_816, vlb: -2.557714590461382, took: 1.1644470691680908\n",
      "Validation vlb: -2.130312038471012, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_817, vlb: -2.4918229176408317, took: 1.0155668258666992\n",
      "Validation vlb: -2.080530608356192, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_818, vlb: -2.5575743615305333, took: 1.231123924255371\n",
      "Validation vlb: -2.1267408645654573, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_819, vlb: -2.5159908354947755, took: 1.1373980045318604\n",
      "Validation vlb: -2.162404905245142, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_820, vlb: -2.536610473176752, took: 1.1302659511566162\n",
      "Validation vlb: -2.0676126549544844, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_821, vlb: -2.4967661805405306, took: 0.985957145690918\n",
      "Validation vlb: -2.125931132572754, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_822, vlb: -2.538142916793054, took: 1.0957207679748535\n",
      "Validation vlb: -2.1756219107742063, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_823, vlb: -2.5129703894166324, took: 1.3457751274108887\n",
      "Validation vlb: -2.2047647187624935, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_824, vlb: -2.5458244078872787, took: 1.1218559741973877\n",
      "Validation vlb: -2.153871949436595, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_825, vlb: -2.5317298986519416, took: 1.0863020420074463\n",
      "Validation vlb: -2.1685326184269678, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_826, vlb: -2.501941209952094, took: 1.0166349411010742\n",
      "Validation vlb: -2.360951069489266, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_827, vlb: -2.512521779146562, took: 1.1109390258789062\n",
      "Validation vlb: -2.101550108406536, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_828, vlb: -2.5500470238020405, took: 1.0034050941467285\n",
      "Validation vlb: -2.1393868066732167, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_829, vlb: -2.471610842139162, took: 1.1265027523040771\n",
      "Validation vlb: -2.245391029756046, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_830, vlb: -2.564935346537457, took: 1.2710001468658447\n",
      "Validation vlb: -2.073462039521597, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_831, vlb: -2.5572342348047243, took: 1.1902122497558594\n",
      "Validation vlb: -2.1758773203420794, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_832, vlb: -2.524874686851405, took: 1.1445591449737549\n",
      "Validation vlb: -2.190516838363845, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_833, vlb: -2.5153977820286872, took: 1.002573013305664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation vlb: -2.121098374857486, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_834, vlb: -2.57924326873315, took: 0.9776871204376221\n",
      "Validation vlb: -2.1468687003484437, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_835, vlb: -2.5278700645509966, took: 1.338210105895996\n",
      "Validation vlb: -2.1365942276025667, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_836, vlb: -2.5412094072768445, took: 1.3769729137420654\n",
      "Validation vlb: -2.1621496368766215, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_837, vlb: -2.5549116417964157, took: 1.263221025466919\n",
      "Validation vlb: -2.1741106093508527, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_838, vlb: -2.5485129477439443, took: 0.9930641651153564\n",
      "Validation vlb: -2.164952610688688, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_839, vlb: -2.560967719507681, took: 0.980644941329956\n",
      "Validation vlb: -2.2626762143230748, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_840, vlb: -2.5631388252829748, took: 0.9799239635467529\n",
      "Validation vlb: -2.2006586986838035, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_841, vlb: -2.528038037694698, took: 1.0778472423553467\n",
      "Validation vlb: -2.2319877464022837, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_842, vlb: -2.5319486794215895, took: 1.0162560939788818\n",
      "Validation vlb: -2.1424798641390015, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_843, vlb: -2.5609078292242264, took: 1.2482240200042725\n",
      "Validation vlb: -2.1655582170270407, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_844, vlb: -2.498683048230433, took: 1.3583471775054932\n",
      "Validation vlb: -2.1164220992029676, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_845, vlb: -2.492476795489813, took: 1.17740797996521\n",
      "Validation vlb: -2.115962903476456, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_846, vlb: -2.5688539027824304, took: 0.9846251010894775\n",
      "Validation vlb: -2.112556983352093, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_847, vlb: -2.506937584603996, took: 0.987617015838623\n",
      "Validation vlb: -2.2131461695945767, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_848, vlb: -2.5119841128834555, took: 0.9842748641967773\n",
      "Validation vlb: -2.1246892753928224, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_849, vlb: -2.5300172396945917, took: 0.9900839328765869\n",
      "Validation vlb: -2.149370811517956, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_850, vlb: -2.535824347624128, took: 1.143376111984253\n",
      "Validation vlb: -2.113477028689338, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_851, vlb: -2.524105128256331, took: 1.2497336864471436\n",
      "Validation vlb: -2.190125363544353, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_852, vlb: -2.5505735176177087, took: 1.0841689109802246\n",
      "Validation vlb: -2.047071419873284, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_853, vlb: -2.5991974668533717, took: 1.4931120872497559\n",
      "Validation vlb: -2.0551574808879964, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_854, vlb: -2.5324517445122976, took: 1.1657960414886475\n",
      "Validation vlb: -2.1886203775128115, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_855, vlb: -2.541250914260128, took: 1.1030020713806152\n",
      "Validation vlb: -2.20915080930037, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_856, vlb: -2.5121833607581263, took: 1.2502400875091553\n",
      "Validation vlb: -2.0325470661268263, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_857, vlb: -2.528248006678503, took: 1.1010057926177979\n",
      "Validation vlb: -2.1790903231858434, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_858, vlb: -2.5604599949950404, took: 1.0287511348724365\n",
      "Validation vlb: -2.1722883761507794, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_859, vlb: -2.5664253379957773, took: 0.9969830513000488\n",
      "Validation vlb: -2.09239615205808, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_860, vlb: -2.524667055090855, took: 1.0776071548461914\n",
      "Validation vlb: -2.086109958420294, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_861, vlb: -2.502619185377292, took: 1.0133709907531738\n",
      "Validation vlb: -2.0247845506976723, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_862, vlb: -2.5327039046557123, took: 1.023146629333496\n",
      "Validation vlb: -2.162940109817727, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_863, vlb: -2.525868119547854, took: 1.017078161239624\n",
      "Validation vlb: -2.2140823826434928, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_864, vlb: -2.5400432284718844, took: 1.0038070678710938\n",
      "Validation vlb: -2.0792157256487505, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_865, vlb: -2.502509184436571, took: 0.980288028717041\n",
      "Validation vlb: -2.088521364051547, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_866, vlb: -2.483684093865466, took: 0.9794261455535889\n",
      "Validation vlb: -2.1421071121221993, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_867, vlb: -2.517120926595653, took: 0.9859001636505127\n",
      "Validation vlb: -2.0162581351968463, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_868, vlb: -2.5313514639758616, took: 0.9865829944610596\n",
      "Validation vlb: -2.1874513402340097, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_869, vlb: -2.5369307368961564, took: 1.1057720184326172\n",
      "Validation vlb: -2.233078609392481, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_870, vlb: -2.5823978940746017, took: 1.2209961414337158\n",
      "Validation vlb: -2.1482211563579474, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_871, vlb: -2.4749824494220047, took: 1.0839369297027588\n",
      "Validation vlb: -2.099362850189209, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_872, vlb: -2.533455239468699, took: 0.9795989990234375\n",
      "Validation vlb: -2.046549578314846, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_873, vlb: -2.494783007416057, took: 0.9897279739379883\n",
      "Validation vlb: -2.2607739018005075, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_874, vlb: -2.5498212659620836, took: 1.070849895477295\n",
      "Validation vlb: -2.138660942852304, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_875, vlb: -2.5068897447184257, took: 1.0389418601989746\n",
      "Validation vlb: -2.1314244980179375, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_876, vlb: -2.518821199463132, took: 1.1706328392028809\n",
      "Validation vlb: -2.028358337177042, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_877, vlb: -2.492268879893533, took: 1.2322111129760742\n",
      "Validation vlb: -2.191328565665433, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_878, vlb: -2.5398181485837856, took: 0.9725148677825928\n",
      "Validation vlb: -2.1789014177414976, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_879, vlb: -2.5642729787720078, took: 1.3509800434112549\n",
      "Validation vlb: -2.1157819005663727, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_880, vlb: -2.495668613013493, took: 1.2589261531829834\n",
      "Validation vlb: -2.0015743947723537, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_881, vlb: -2.5456645855767976, took: 1.394881010055542\n",
      "Validation vlb: -2.101795402159583, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_882, vlb: -2.525557682588884, took: 1.586210012435913\n",
      "Validation vlb: -2.094774521670295, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_883, vlb: -2.5176982622933037, took: 1.325368881225586\n",
      "Validation vlb: -2.228917182070538, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_884, vlb: -2.476763987051827, took: 1.292691946029663\n",
      "Validation vlb: -2.0529475196665543, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_885, vlb: -2.528262596058253, took: 1.1612038612365723\n",
      "Validation vlb: -2.2151676420255, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_886, vlb: -2.5193916098608744, took: 1.202125072479248\n",
      "Validation vlb: -2.144764763637654, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_887, vlb: -2.4883556349654397, took: 1.2669811248779297\n",
      "Validation vlb: -2.095192131486911, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_888, vlb: -2.5292770556936515, took: 1.3904948234558105\n",
      "Validation vlb: -2.10211181640625, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_889, vlb: -2.5250229996058593, took: 1.2615690231323242\n",
      "Validation vlb: -2.2005101068120174, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_890, vlb: -2.4995467711748476, took: 1.1605377197265625\n",
      "Validation vlb: -2.194036800884506, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_891, vlb: -2.4893931277930674, took: 1.3411121368408203\n",
      "Validation vlb: -2.222007291602471, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_892, vlb: -2.503722800940629, took: 1.147778034210205\n",
      "Validation vlb: -2.087889767773329, Best vlb: -1.8541494619499133\n",
      "\n",
      "Epoch_893, vlb: -2.586370694109292, took: 0.99294114112854\n",
      "Validation vlb: -2.2118012326434977, Best vlb: -1.8541494619499133\n",
      "\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "masker = top_masker_tensorflow(p=1)\n",
    "\n",
    "x_train, x_test, x_means, x_stds, y_train, y_test, feature_names, X_dims = \\\n",
    "    get_my_COMPAS(rseed=42, separate_test=True, test_ratio=0.1, save_dir='../data/')\n",
    "\n",
    "x_train, x_test, input_dim_vec = join_compas_targets(x_train, x_test, y_train, y_test, X_dims)\n",
    "\n",
    "print('Compas', x_train.shape, x_test.shape)\n",
    "print(input_dim_vec)\n",
    "\n",
    "dname = 'compas'\n",
    "print(dname)\n",
    "\n",
    "width = widths[names.index(dname)] # 350\n",
    "depth = depths[names.index(dname)] # number of hidden layers # 3\n",
    "latent_dim = latent_dims[names.index(dname)] # 4\n",
    "\n",
    "batch_size = 128\n",
    "nb_epochs = 2000\n",
    "early_stop = 200\n",
    "lr = 1e-4\n",
    "\n",
    "optimizer = tfa.optimizers.RectifiedAdam(lr = lr, epsilon = 1e-8)\n",
    "\n",
    "model = VAEAC_gauss_cat(width, depth, latent_dim, input_dim_vec, batch_size, lr, optimizer, save_model = False)\n",
    "\n",
    "vlb_train, vlb_val, best_epoch, best_vlb, curr_epoch = train_VAEAC(model, x_train, x_test, masker, nb_epochs, early_stop=early_stop, flatten = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f3ac60b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_42549/2474145594.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8e2b58",
   "metadata": {},
   "source": [
    "## Train VAEAC (default credit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fa76c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow_addons/optimizers/rectified_adam.py:120: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "2021-12-11 09:16:23.922815: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_31061/2988879061.py\", line 202, in train_step_VAEAC  *\n        loss, vlb, kl_divergence, rec_loss, regularizer = compute_loss_VAEAC(model, x_batch, x_flat, x_masked, mask)\n    File \"/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_31061/2988879061.py\", line 151, in compute_loss_VAEAC  *\n        prior_params = model.prior_encoder(x_masked)\n    File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"prior_encoder_model\" is incompatible with the layer: expected shape=(None, 62), found shape=(64, 48)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_31061/2727803489.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munnormalise_cat_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_means\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_stds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mvlb_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvlb_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_vlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_VAEAC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_31061/2361816603.py\u001b[0m in \u001b[0;36mtrain_VAEAC\u001b[0;34m(model, x_train, x_test, masker, nb_epochs, early_stop, flatten)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mx_batch_flat_masked_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_batch_flat_masked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_flat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_divergence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step_VAEAC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch_flat_masked_concat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mvlb_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_31061/2988879061.py\", line 202, in train_step_VAEAC  *\n        loss, vlb, kl_divergence, rec_loss, regularizer = compute_loss_VAEAC(model, x_batch, x_flat, x_masked, mask)\n    File \"/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_31061/2988879061.py\", line 151, in compute_loss_VAEAC  *\n        prior_params = model.prior_encoder(x_masked)\n    File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"prior_encoder_model\" is incompatible with the layer: expected shape=(None, 62), found shape=(64, 48)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "nb_epochs = 2000 # 2000\n",
    "early_stop = 200\n",
    "lr = 7e-4        # Maybe this should be 1e-4, but it makes the performance terrible...\n",
    "\n",
    "optimizer = tfa.optimizers.RectifiedAdam(lr = lr, epsilon = 1e-8)\n",
    "\n",
    "width = 350\n",
    "depth = 3\n",
    "latent_dim = 8\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "masker = top_masker_tensorflow(p=1)\n",
    "\n",
    "x_train, x_test, x_means, x_stds = \\\n",
    "load_UCI(dset_name=dname, splits=10, seed=42, separate_targets=False, save_dir='../data/') # np.arrays\n",
    "\n",
    "model = VAEAC_gauss_cat(width, depth, latent_dim, input_dim_vec, batch_size, lr, optimizer, save_model = True)\n",
    "\n",
    "x_train = unnormalise_cat_vars(x_train, x_means, x_stds, input_dim_vec) # np.array\n",
    "x_test = unnormalise_cat_vars(x_test, x_means, x_stds, input_dim_vec) \n",
    "\n",
    "vlb_train, vlb_val, best_epoch, best_vlb, curr_epoch = train_VAEAC(model, x_train, x_test, masker, nb_epochs, early_stop=early_stop, flatten = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc9ac90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621e0c18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55f7efcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 619\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vlb_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_32779/2189333610.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvlb_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcurr_epoch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvlb_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcurr_epoch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cost_train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cost_dev'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vlb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vlb_train' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.clip(vlb_train[:curr_epoch], -1000, 1000), 'r')\n",
    "plt.plot(np.clip(vlb_val[:curr_epoch], -1000, 1000), 'b')\n",
    "plt.legend(['cost_train', 'cost_dev'])\n",
    "plt.ylabel('vlb')\n",
    "plt.xlabel('it')\n",
    "plt.grid(True)\n",
    "#plt.savefig( str(dname) + '_vlb_lr_' + str(model.lr) + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad555fe9",
   "metadata": {},
   "source": [
    "## Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9b8ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e427ab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the total number of trainable variables \n",
    "\"\"\"\n",
    "total_parameters = 0\n",
    "for variable in model.trainable_variables:\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    #print(shape)\n",
    "    #print(len(shape))\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        #print(dim)\n",
    "        variable_parameters *= dim\n",
    "    #print(variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)\n",
    "\n",
    "time.sleep(10)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b0e8fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469f0d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20896d98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae708749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b7c650e",
   "metadata": {},
   "source": [
    "## Generate sample parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e71b276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55cd3a92",
   "metadata": {},
   "source": [
    "## UNDER VAEAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b107d2c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af920dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class under_VAEAC(tf.keras.Model):\n",
    "    def __init__(self, base_VAE, width, depth, latent_dim, batch_size, lr, optimizer, save_model = True):\n",
    "        super(under_VAEAC, self).__init__()\n",
    "        \n",
    "        self.base_VAEAC = base_VAE\n",
    "        self.input_dim = self.base_VAEAC.latent_dim # 8 for default credit\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.latent_dim = latent_dim # 6 for default credit\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.prior_encoder = tfd.Normal(loc=tf.zeros(latent_dim), scale=tf.ones(latent_dim))\n",
    "        \n",
    "        # self.input_dim is put in a list to make sum(input_dim_vec in recognition_encoder work)\n",
    "        self.recognition_encoder = create_recognition_encoder(width, depth, latent_dim, [self.input_dim])\n",
    "        self.decoder = create_decoder(width, depth, latent_dim, [self.input_dim])\n",
    "        \n",
    "        self.vlb_scale = 1 / self.input_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer\n",
    "        self.save_model = save_model\n",
    "\n",
    "    # Inspiration taken from \n",
    "    # https://github.com/joocxi/tf2-VAEAC/blob/d2b1bbc258ec77ee0975ea7eb68e63c4efcda6f0/model/vaeac.py\n",
    "\n",
    "    def reconstruction_loss(self, x, y):\n",
    "        log_prob_vec = []\n",
    "        reshape_dim = self.batch_size\n",
    "        for idx in range(self.input_dim):\n",
    "            # Gaussian_case\n",
    "            log_prob_vec.append(tf.expand_dims(-(x[:, idx] - y[:, idx])**2, 1))\n",
    "\n",
    "        log_prob_vec = tf.reshape(log_prob_vec, [reshape_dim, self.input_dim])\n",
    "        log_prob_vec = tf.math.reduce_sum(log_prob_vec, axis= -1)\n",
    "        \n",
    "        return log_prob_vec\n",
    "    \n",
    "def compute_loss_under_VAEAC(model, x_flat, proposal_params_VAEAC):\n",
    "    \n",
    "    # Essentially CLUEs normal_parse_params\n",
    "    proposal_distribution_VAEAC = tfd.Normal(\n",
    "      loc=proposal_params_VAEAC[..., :model.input_dim],\n",
    "      scale=tf.clip_by_value(\n",
    "        tf.nn.softplus(proposal_params_VAEAC[..., model.input_dim:]),\n",
    "        1e-3,\n",
    "        tf.float32.max),\n",
    "      name=\"proposal\")\n",
    "    \n",
    "    z_sample = proposal_distribution_VAEAC.sample() # tensor with dim (base_VAEAC.latent_dim,)\n",
    "    \n",
    "    proposal_params_VAE = model.recognition_encoder(z_sample) \n",
    "\n",
    "    proposal_distribution_VAE = tfd.Normal(\n",
    "      loc=proposal_params_VAE[..., :model.latent_dim],\n",
    "      scale=tf.clip_by_value(\n",
    "        tf.nn.softplus(proposal_params_VAE[..., model.latent_dim:]),\n",
    "        1e-3,\n",
    "        tf.float32.max),\n",
    "      name=\"priors\")\n",
    "\n",
    "    u_sample = proposal_distribution_VAE.sample() \n",
    "    \n",
    "    rec_params = model.decoder(u_sample) \n",
    "    \n",
    "    # True distribution, Estimated distribution in this order\n",
    "    kl_divergence = tf.reduce_sum(\n",
    "      tf.reshape(\n",
    "        tfd.kl_divergence(proposal_distribution_VAE, model.prior_encoder),\n",
    "        (model.batch_size, -1)), -1)\n",
    "    \n",
    "    rec_loss = model.reconstruction_loss(rec_params, z_sample)\n",
    "    \n",
    "    vlb = tf.reduce_mean(-kl_divergence + rec_loss) # For comparing\n",
    "    loss = tf.reduce_mean((kl_divergence - rec_loss) * model.vlb_scale) \n",
    "    return loss, vlb, kl_divergence, rec_loss\n",
    "\n",
    "@tf.function # Converts all numpy arrays to tensors\n",
    "def train_step_under_VAEAC(model, x_flat, proposal_params_VAEAC):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, vlb, kl_divergence, rec_loss = compute_loss_under_VAEAC(model, x_flat, proposal_params_VAEAC)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss, vlb, kl_divergence, rec_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e843dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "def train_VAE(model, x_train, x_test, nb_epochs, early_stop = None, flatten = False):\n",
    "    \n",
    "    vlb_train = np.zeros(nb_epochs)\n",
    "    vlb_val = np.zeros(nb_epochs)\n",
    "    best_vlb = -np.inf\n",
    "    best_epoch = 0\n",
    "    \n",
    "    overall_batch_size = model.batch_size\n",
    "    \n",
    "    test_data = []\n",
    "    for x in batch(x_test, n = overall_batch_size):\n",
    "        test_data.append(x)\n",
    "    \n",
    "    epoch = 0\n",
    "    for epoch in range(0, nb_epochs):\n",
    "        tic = time.time()\n",
    "        \n",
    "        train_data = []\n",
    "        np.random.shuffle(x_train)\n",
    "        for x in batch(x_train, n = overall_batch_size):\n",
    "            train_data.append(x)\n",
    "        \n",
    "        ## Training\n",
    "        nb_samples = 0\n",
    "        for x_batch in train_data:\n",
    "\n",
    "            model.batch_size = x_batch.shape[0] # TODO: FIX THIS: Very ugly solution now to make sure batches \n",
    "                                                # that do not have the full size\n",
    "            \n",
    "            # If data is not already flattened (default credit)\n",
    "            if flatten:\n",
    "                x_batch_flat = gauss_cat_to_flat(x_batch, model.input_dim_vec) # numpy\n",
    "            # If data is already flattened (COMPAS from join_compas_targets)\n",
    "            else:\n",
    "                x_batch_flat = x_batch\n",
    "            \n",
    "            x_batch_flat = tf.convert_to_tensor(x_batch_flat)\n",
    "    \n",
    "            proposal_params_VAEAC = model.base_VAEAC.recognition_encoder(x_batch_flat) # tensor with dim (16,)\n",
    "            \n",
    "            loss, vlb, kl_divergence, rec_loss = train_step_under_VAEAC(model, x_batch_flat, proposal_params_VAEAC)\n",
    "\n",
    "            vlb_train[epoch] += vlb.numpy() * x_batch.shape[0]\n",
    "            nb_samples += x_batch.shape[0]\n",
    "\n",
    "        vlb_train[epoch] /= nb_samples\n",
    "        toc = time.time()\n",
    "        print(\"Epoch\" + str(epoch) + \", vlb: \" + str(vlb_train[epoch]) + \", took: \" + str(toc-tic))\n",
    "        \n",
    "        ## Validation\n",
    "        nb_samples = 0\n",
    "        \n",
    "        for x_batch in test_data:\n",
    "            model.batch_size = x_batch.shape[0] # TODO: FIX THIS: Very ugly solution now to make sure batches \n",
    "                                                # that do not have the full size\n",
    "            \n",
    "            # If data is not already flattened (default credit)\n",
    "            if flatten:\n",
    "                x_batch_flat = gauss_cat_to_flat(x_batch, model.input_dim_vec) # numpy\n",
    "            # If data is already flattened (COMPAS from join_compas_targets)\n",
    "            else:\n",
    "                x_batch_flat = x_batch\n",
    "            \n",
    "            x_batch_flat = tf.convert_to_tensor(x_batch_flat)\n",
    "    \n",
    "            proposal_params_VAEAC = model.base_VAEAC.recognition_encoder(x_batch_flat) # tensor with dim (16,)\n",
    "            \n",
    "            # In CLUE there is actually no difference between eval and fitother than that we should not update the weights.\n",
    "            # Therefore ok to just call compute_loss_under_VAEAC directly instead of a special eval func\n",
    "            loss, vlb, kl_divergence, rec_loss = compute_loss_under_VAEAC(model, x_batch_flat, proposal_params_VAEAC)\n",
    "\n",
    "            vlb_val[epoch] += vlb.numpy() * x_batch.shape[0]\n",
    "            nb_samples += x_batch.shape[0]\n",
    "\n",
    "        vlb_val[epoch] /= nb_samples\n",
    "\n",
    "        if vlb_val[epoch] > best_vlb:\n",
    "            best_vlb = vlb_val[epoch]\n",
    "            best_epoch = epoch\n",
    "            if(model.save_model):\n",
    "                #open text file\n",
    "                text_file = open(\"./COMPAS_under_VAEAC_18/\" + str(dname) + \"_best_epoch_under_VAEAC_lr_\" + str(model.lr) + \".txt\", \"w\")\n",
    "\n",
    "                #write string to file\n",
    "                text_file.write(str(epoch))\n",
    "\n",
    "                #close file\n",
    "                text_file.close()\n",
    "\n",
    "                model.recognition_encoder.save(\"./COMPAS_under_VAEAC_18/\" + str(dname) + \"_under_recog_encoder_lr_\" + str(model.lr))\n",
    "                model.decoder.save(\"./COMPAS_under_VAEAC_18/\" + str(dname) + \"_under_decoder_lr_\" + str(model.lr))\n",
    "\n",
    "        print(\"Validation vlb: \" + str(vlb_val[epoch]) + \", Best vlb: \" + str(best_vlb) + \"\\n\")\n",
    "\n",
    "        if early_stop is not None and (epoch - best_epoch) > early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    \n",
    "    np.save(\"./COMPAS_under_VAEAC_18/\" + str(dname) + \"_under_vlb_train_lr_\" + str(model.lr), vlb_train)\n",
    "    np.save(\"./COMPAS_under_VAEAC_18/\" + str(dname) + \"_under_vlb_val_lr_\" + str(model.lr), vlb_val)\n",
    "    return vlb_train, vlb_val, best_epoch, best_vlb, epoch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbf0a1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compas\n",
      "Compas (5554, 19) (618, 19)\n",
      "[3 6 2 2 2 1 1 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow_addons/optimizers/rectified_adam.py:120: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "2021-12-11 11:05:36.824306: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-11 11:05:40.156040: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0, vlb: -16.163664724056694, took: 4.89443302154541\n",
      "Validation vlb: -16.08613402480832, Best vlb: -16.08613402480832\n",
      "\n",
      "Epoch1, vlb: -15.439203943672222, took: 0.714231014251709\n",
      "Validation vlb: -14.881131014777619, Best vlb: -14.881131014777619\n",
      "\n",
      "Epoch2, vlb: -14.576955498917565, took: 0.6364648342132568\n",
      "Validation vlb: -14.166119266868023, Best vlb: -14.166119266868023\n",
      "\n",
      "Epoch3, vlb: -13.609398480924634, took: 0.725053071975708\n",
      "Validation vlb: -13.634096161061507, Best vlb: -13.634096161061507\n",
      "\n",
      "Epoch4, vlb: -12.817848996116739, took: 0.646914005279541\n",
      "Validation vlb: -12.66442160621816, Best vlb: -12.66442160621816\n",
      "\n",
      "Epoch5, vlb: -11.889560465594611, took: 0.6413393020629883\n",
      "Validation vlb: -11.70616201135333, Best vlb: -11.70616201135333\n",
      "\n",
      "Epoch6, vlb: -10.987123188888335, took: 0.8162600994110107\n",
      "Validation vlb: -10.77046710079156, Best vlb: -10.77046710079156\n",
      "\n",
      "Epoch7, vlb: -10.160024353442195, took: 0.854423999786377\n",
      "Validation vlb: -9.860607758309078, Best vlb: -9.860607758309078\n",
      "\n",
      "Epoch8, vlb: -9.182563190714335, took: 0.8176209926605225\n",
      "Validation vlb: -9.28992859059553, Best vlb: -9.28992859059553\n",
      "\n",
      "Epoch9, vlb: -8.79191227185885, took: 0.8032717704772949\n",
      "Validation vlb: -8.843920791033403, Best vlb: -8.843920791033403\n",
      "\n",
      "Epoch10, vlb: -8.321014409963286, took: 0.6225180625915527\n",
      "Validation vlb: -8.084691988997475, Best vlb: -8.084691988997475\n",
      "\n",
      "Epoch11, vlb: -7.89786273625243, took: 0.6872930526733398\n",
      "Validation vlb: -7.994815314086124, Best vlb: -7.994815314086124\n",
      "\n",
      "Epoch12, vlb: -7.627626991374998, took: 0.7320802211761475\n",
      "Validation vlb: -7.7774425648562735, Best vlb: -7.7774425648562735\n",
      "\n",
      "Epoch13, vlb: -7.26137914406897, took: 0.6719543933868408\n",
      "Validation vlb: -7.252183054643155, Best vlb: -7.252183054643155\n",
      "\n",
      "Epoch14, vlb: -6.926224315685895, took: 1.0889570713043213\n",
      "Validation vlb: -6.977544992872812, Best vlb: -6.977544992872812\n",
      "\n",
      "Epoch15, vlb: -6.494618420296965, took: 0.8088369369506836\n",
      "Validation vlb: -6.35039101449417, Best vlb: -6.35039101449417\n",
      "\n",
      "Epoch16, vlb: -6.202285259200122, took: 0.7589211463928223\n",
      "Validation vlb: -6.011578448767801, Best vlb: -6.011578448767801\n",
      "\n",
      "Epoch17, vlb: -5.942120518751354, took: 0.7154169082641602\n",
      "Validation vlb: -5.974381647449481, Best vlb: -5.974381647449481\n",
      "\n",
      "Epoch18, vlb: -5.787330425511087, took: 0.660750150680542\n",
      "Validation vlb: -5.778891126700589, Best vlb: -5.778891126700589\n",
      "\n",
      "Epoch19, vlb: -5.736257243336728, took: 0.6065459251403809\n",
      "Validation vlb: -6.025214144327109, Best vlb: -5.778891126700589\n",
      "\n",
      "Epoch20, vlb: -5.652214016637916, took: 0.5640659332275391\n",
      "Validation vlb: -5.687025749181853, Best vlb: -5.687025749181853\n",
      "\n",
      "Epoch21, vlb: -5.658534558076072, took: 0.5705416202545166\n",
      "Validation vlb: -5.715077437243416, Best vlb: -5.687025749181853\n",
      "\n",
      "Epoch22, vlb: -5.605685657333632, took: 0.5636937618255615\n",
      "Validation vlb: -5.602697644033093, Best vlb: -5.602697644033093\n",
      "\n",
      "Epoch23, vlb: -5.619776970455018, took: 0.5680899620056152\n",
      "Validation vlb: -5.716892387488899, Best vlb: -5.602697644033093\n",
      "\n",
      "Epoch24, vlb: -5.584550094123705, took: 0.567835807800293\n",
      "Validation vlb: -5.571951079137117, Best vlb: -5.571951079137117\n",
      "\n",
      "Epoch25, vlb: -5.571448656823251, took: 0.5725970268249512\n",
      "Validation vlb: -5.608402056215651, Best vlb: -5.571951079137117\n",
      "\n",
      "Epoch26, vlb: -5.557376169754869, took: 0.5672712326049805\n",
      "Validation vlb: -5.531826741487077, Best vlb: -5.531826741487077\n",
      "\n",
      "Epoch27, vlb: -5.594734951022034, took: 0.6407692432403564\n",
      "Validation vlb: -5.640521126657628, Best vlb: -5.531826741487077\n",
      "\n",
      "Epoch28, vlb: -5.571704725932422, took: 0.620236873626709\n",
      "Validation vlb: -5.635849906402884, Best vlb: -5.531826741487077\n",
      "\n",
      "Epoch29, vlb: -5.591008092071841, took: 0.6410748958587646\n",
      "Validation vlb: -5.643002764692584, Best vlb: -5.531826741487077\n",
      "\n",
      "Epoch30, vlb: -5.550998121102937, took: 0.6310708522796631\n",
      "Validation vlb: -5.5277548638748115, Best vlb: -5.5277548638748115\n",
      "\n",
      "Epoch31, vlb: -5.529025471206187, took: 0.6640639305114746\n",
      "Validation vlb: -5.611469980196659, Best vlb: -5.5277548638748115\n",
      "\n",
      "Epoch32, vlb: -5.53896281098478, took: 0.7175331115722656\n",
      "Validation vlb: -5.483471444509561, Best vlb: -5.483471444509561\n",
      "\n",
      "Epoch33, vlb: -5.576675378636658, took: 0.6918320655822754\n",
      "Validation vlb: -5.6047619029542, Best vlb: -5.483471444509561\n",
      "\n",
      "Epoch34, vlb: -5.508756643067945, took: 0.8841731548309326\n",
      "Validation vlb: -5.5711819176535, Best vlb: -5.483471444509561\n",
      "\n",
      "Epoch35, vlb: -5.526790440833779, took: 0.6745059490203857\n",
      "Validation vlb: -5.525919438951609, Best vlb: -5.483471444509561\n",
      "\n",
      "Epoch36, vlb: -5.546861625207084, took: 0.6581840515136719\n",
      "Validation vlb: -5.65661294406286, Best vlb: -5.483471444509561\n",
      "\n",
      "Epoch37, vlb: -5.517297767073206, took: 0.995297908782959\n",
      "Validation vlb: -5.569359841084403, Best vlb: -5.483471444509561\n",
      "\n",
      "Epoch38, vlb: -5.523257080925683, took: 0.703298807144165\n",
      "Validation vlb: -5.5397504571957885, Best vlb: -5.483471444509561\n",
      "\n",
      "Epoch39, vlb: -5.475674266732765, took: 0.6995728015899658\n",
      "Validation vlb: -5.556175653216909, Best vlb: -5.483471444509561\n",
      "\n",
      "Epoch40, vlb: -5.498217513675436, took: 0.6746890544891357\n",
      "Validation vlb: -5.520883736101169, Best vlb: -5.483471444509561\n",
      "\n",
      "Epoch41, vlb: -5.4987920950675635, took: 0.6123528480529785\n",
      "Validation vlb: -5.486275572607046, Best vlb: -5.483471444509561\n",
      "\n",
      "Epoch42, vlb: -5.4891883259760545, took: 0.6123189926147461\n",
      "Validation vlb: -5.557787984706051, Best vlb: -5.483471444509561\n",
      "\n",
      "Epoch43, vlb: -5.512144732827285, took: 0.5789330005645752\n",
      "Validation vlb: -5.549397400667752, Best vlb: -5.483471444509561\n",
      "\n",
      "Epoch44, vlb: -5.499465916798104, took: 0.5590460300445557\n",
      "Validation vlb: -5.496851927254192, Best vlb: -5.483471444509561\n",
      "\n",
      "Epoch45, vlb: -5.504447698507285, took: 0.5720040798187256\n",
      "Validation vlb: -5.577863963290712, Best vlb: -5.483471444509561\n",
      "\n",
      "Epoch46, vlb: -5.471040269647694, took: 0.5596070289611816\n",
      "Validation vlb: -5.568559705246614, Best vlb: -5.483471444509561\n",
      "\n",
      "Epoch47, vlb: -5.515098751900562, took: 0.5657131671905518\n",
      "Validation vlb: -5.492294893295634, Best vlb: -5.483471444509561\n",
      "\n",
      "Epoch48, vlb: -5.464997275939381, took: 0.7464001178741455\n",
      "Validation vlb: -5.5613539488956, Best vlb: -5.483471444509561\n",
      "\n",
      "Epoch49, vlb: -5.48715540038024, took: 0.7622151374816895\n",
      "Validation vlb: -5.459021782797903, Best vlb: -5.459021782797903\n",
      "\n",
      "Epoch50, vlb: -5.493535208710673, took: 0.5706431865692139\n",
      "Validation vlb: -5.589778139367459, Best vlb: -5.459021782797903\n",
      "\n",
      "Epoch51, vlb: -5.4860405308933275, took: 0.6545529365539551\n",
      "Validation vlb: -5.629623352902607, Best vlb: -5.459021782797903\n",
      "\n",
      "Epoch52, vlb: -5.45077130946488, took: 0.8312327861785889\n",
      "Validation vlb: -5.5364549661531415, Best vlb: -5.459021782797903\n",
      "\n",
      "Epoch53, vlb: -5.474327954191676, took: 0.8777878284454346\n",
      "Validation vlb: -5.470600351932365, Best vlb: -5.459021782797903\n",
      "\n",
      "Epoch54, vlb: -5.457577809439231, took: 0.6145739555358887\n",
      "Validation vlb: -5.5344323865032505, Best vlb: -5.459021782797903\n",
      "\n",
      "Epoch55, vlb: -5.461881603918189, took: 0.5831770896911621\n",
      "Validation vlb: -5.420925915048346, Best vlb: -5.420925915048346\n",
      "\n",
      "Epoch56, vlb: -5.477056113172015, took: 0.6743259429931641\n",
      "Validation vlb: -5.475014509120806, Best vlb: -5.420925915048346\n",
      "\n",
      "Epoch57, vlb: -5.4726038022053345, took: 0.6439549922943115\n",
      "Validation vlb: -5.575918905943343, Best vlb: -5.420925915048346\n",
      "\n",
      "Epoch58, vlb: -5.4508807672027375, took: 0.6472852230072021\n",
      "Validation vlb: -5.5313695703895345, Best vlb: -5.420925915048346\n",
      "\n",
      "Epoch59, vlb: -5.502743070916302, took: 0.631464958190918\n",
      "Validation vlb: -5.5399064909678835, Best vlb: -5.420925915048346\n",
      "\n",
      "Epoch60, vlb: -5.436440455127973, took: 0.6613078117370605\n",
      "Validation vlb: -5.539518302312561, Best vlb: -5.420925915048346\n",
      "\n",
      "Epoch61, vlb: -5.422647320112394, took: 0.6783440113067627\n",
      "Validation vlb: -5.533121622881843, Best vlb: -5.420925915048346\n",
      "\n",
      "Epoch62, vlb: -5.452049852576581, took: 0.7111690044403076\n",
      "Validation vlb: -5.497422722936834, Best vlb: -5.420925915048346\n",
      "\n",
      "Epoch63, vlb: -5.425605231323901, took: 0.6316089630126953\n",
      "Validation vlb: -5.539807747097077, Best vlb: -5.420925915048346\n",
      "\n",
      "Epoch64, vlb: -5.4359684948617275, took: 0.6357238292694092\n",
      "Validation vlb: -5.35070708500143, Best vlb: -5.35070708500143\n",
      "\n",
      "Epoch65, vlb: -5.453916940493338, took: 0.6356918811798096\n",
      "Validation vlb: -5.5030960388553956, Best vlb: -5.35070708500143\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch66, vlb: -5.434534754562515, took: 0.7896530628204346\n",
      "Validation vlb: -5.574560984824467, Best vlb: -5.35070708500143\n",
      "\n",
      "Epoch67, vlb: -5.463906265824743, took: 0.8926150798797607\n",
      "Validation vlb: -5.492505329712309, Best vlb: -5.35070708500143\n",
      "\n",
      "Epoch68, vlb: -5.420391473746102, took: 0.6041228771209717\n",
      "Validation vlb: -5.446987715353858, Best vlb: -5.35070708500143\n",
      "\n",
      "Epoch69, vlb: -5.455777298086274, took: 0.62742018699646\n",
      "Validation vlb: -5.465476924933276, Best vlb: -5.35070708500143\n",
      "\n",
      "Epoch70, vlb: -5.4364865831561255, took: 0.7837352752685547\n",
      "Validation vlb: -5.439863877774828, Best vlb: -5.35070708500143\n",
      "\n",
      "Epoch71, vlb: -5.425827115827814, took: 0.9195420742034912\n",
      "Validation vlb: -5.506247637727114, Best vlb: -5.35070708500143\n",
      "\n",
      "Epoch72, vlb: -5.4207813089856325, took: 0.6800887584686279\n",
      "Validation vlb: -5.505746981858436, Best vlb: -5.35070708500143\n",
      "\n",
      "Epoch73, vlb: -5.4183431964664095, took: 0.6453359127044678\n",
      "Validation vlb: -5.522513516126713, Best vlb: -5.35070708500143\n",
      "\n",
      "Epoch74, vlb: -5.436825855283974, took: 0.5889158248901367\n",
      "Validation vlb: -5.457788291486722, Best vlb: -5.35070708500143\n",
      "\n",
      "Epoch75, vlb: -5.4340962511968565, took: 0.7203681468963623\n",
      "Validation vlb: -5.430262821777739, Best vlb: -5.35070708500143\n",
      "\n",
      "Epoch76, vlb: -5.430768021242542, took: 0.7264842987060547\n",
      "Validation vlb: -5.446856361376815, Best vlb: -5.35070708500143\n",
      "\n",
      "Epoch77, vlb: -5.416796475809858, took: 0.7092349529266357\n",
      "Validation vlb: -5.301155662845254, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch78, vlb: -5.412692202910443, took: 0.6433119773864746\n",
      "Validation vlb: -5.610699145925084, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch79, vlb: -5.425269969642098, took: 0.6272788047790527\n",
      "Validation vlb: -5.502403371928193, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch80, vlb: -5.4215989682929395, took: 1.0024609565734863\n",
      "Validation vlb: -5.707220162388576, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch81, vlb: -5.407619679107227, took: 0.6876471042633057\n",
      "Validation vlb: -5.465941370498015, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch82, vlb: -5.433856753252116, took: 0.5717368125915527\n",
      "Validation vlb: -5.482150276887764, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch83, vlb: -5.430883807847515, took: 0.6187891960144043\n",
      "Validation vlb: -5.442147423148541, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch84, vlb: -5.379137610461929, took: 0.6306219100952148\n",
      "Validation vlb: -5.5517319336678215, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch85, vlb: -5.412711808696083, took: 0.8071653842926025\n",
      "Validation vlb: -5.381728917649649, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch86, vlb: -5.421468763760109, took: 0.7000281810760498\n",
      "Validation vlb: -5.4951993223147095, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch87, vlb: -5.409235633187986, took: 0.6002938747406006\n",
      "Validation vlb: -5.352105520303967, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch88, vlb: -5.442331973966352, took: 0.7430799007415771\n",
      "Validation vlb: -5.4997064025656694, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch89, vlb: -5.41908670167164, took: 0.5917689800262451\n",
      "Validation vlb: -5.398889248425135, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch90, vlb: -5.4271444293892035, took: 0.5668869018554688\n",
      "Validation vlb: -5.472526778680993, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch91, vlb: -5.419133976718956, took: 0.7857339382171631\n",
      "Validation vlb: -5.520846744957094, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch92, vlb: -5.407530492529149, took: 0.7355539798736572\n",
      "Validation vlb: -5.405297726874984, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch93, vlb: -5.407932949959052, took: 0.7271039485931396\n",
      "Validation vlb: -5.437262010420024, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch94, vlb: -5.450986603416467, took: 0.7395179271697998\n",
      "Validation vlb: -5.4272795819156, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch95, vlb: -5.379025983861937, took: 0.7000682353973389\n",
      "Validation vlb: -5.501358063089809, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch96, vlb: -5.443771999003349, took: 1.2365450859069824\n",
      "Validation vlb: -5.413887577921056, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch97, vlb: -5.421817435606976, took: 0.6204206943511963\n",
      "Validation vlb: -5.452837444046168, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch98, vlb: -5.412591377006918, took: 0.5942630767822266\n",
      "Validation vlb: -5.495825335431639, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch99, vlb: -5.397077788454962, took: 0.6550140380859375\n",
      "Validation vlb: -5.36175862407993, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch100, vlb: -5.4022218220346385, took: 0.7753732204437256\n",
      "Validation vlb: -5.451677063136425, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch101, vlb: -5.405447980333328, took: 0.6985909938812256\n",
      "Validation vlb: -5.461188564794349, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch102, vlb: -5.418303427526236, took: 0.6031742095947266\n",
      "Validation vlb: -5.438050044778867, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch103, vlb: -5.435417046854296, took: 0.5884339809417725\n",
      "Validation vlb: -5.369970539241161, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch104, vlb: -5.374926689506479, took: 0.7597780227661133\n",
      "Validation vlb: -5.505823305123832, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch105, vlb: -5.376144564920164, took: 0.7439777851104736\n",
      "Validation vlb: -5.3342512356039, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch106, vlb: -5.365608447139796, took: 0.6013050079345703\n",
      "Validation vlb: -5.447379876109003, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch107, vlb: -5.3839961267608, took: 0.6547470092773438\n",
      "Validation vlb: -5.429491592456608, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch108, vlb: -5.388295958357231, took: 0.6166729927062988\n",
      "Validation vlb: -5.399334842718921, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch109, vlb: -5.349508591165292, took: 0.6066849231719971\n",
      "Validation vlb: -5.4907043620606455, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch110, vlb: -5.417014500285208, took: 0.792269229888916\n",
      "Validation vlb: -5.455902951434978, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch111, vlb: -5.356813914835088, took: 0.773766279220581\n",
      "Validation vlb: -5.410795531226594, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch112, vlb: -5.387740167474249, took: 0.6790351867675781\n",
      "Validation vlb: -5.4972660749861335, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch113, vlb: -5.389265656084906, took: 0.6050281524658203\n",
      "Validation vlb: -5.506917606279688, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch114, vlb: -5.383150873228849, took: 0.5722477436065674\n",
      "Validation vlb: -5.360459048385374, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch115, vlb: -5.387358980724869, took: 0.5628106594085693\n",
      "Validation vlb: -5.474096719501088, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch116, vlb: -5.412235106329803, took: 0.6501278877258301\n",
      "Validation vlb: -5.4652808352967295, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch117, vlb: -5.37389566780725, took: 0.5647308826446533\n",
      "Validation vlb: -5.432611397554959, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch118, vlb: -5.387860500945606, took: 0.5679910182952881\n",
      "Validation vlb: -5.40280362163161, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch119, vlb: -5.385066608838311, took: 0.5715451240539551\n",
      "Validation vlb: -5.547301952892909, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch120, vlb: -5.393331101870511, took: 0.5760908126831055\n",
      "Validation vlb: -5.434216738518773, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch121, vlb: -5.405561043111206, took: 0.5722439289093018\n",
      "Validation vlb: -5.491409761620186, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch122, vlb: -5.417457502438604, took: 1.1477370262145996\n",
      "Validation vlb: -5.425923845914575, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch123, vlb: -5.412495558494538, took: 0.9418051242828369\n",
      "Validation vlb: -5.390416768762286, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch124, vlb: -5.404276604183638, took: 0.735692024230957\n",
      "Validation vlb: -5.328348199912259, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch125, vlb: -5.354077270145162, took: 0.7352397441864014\n",
      "Validation vlb: -5.413755760995316, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch126, vlb: -5.387046453546697, took: 0.623729944229126\n",
      "Validation vlb: -5.447340743055621, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch127, vlb: -5.429852472778529, took: 0.7398898601531982\n",
      "Validation vlb: -5.371139671424446, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch128, vlb: -5.420156065748886, took: 1.034466028213501\n",
      "Validation vlb: -5.423723007868794, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch129, vlb: -5.3777345824593645, took: 0.7023510932922363\n",
      "Validation vlb: -5.314812887062147, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch130, vlb: -5.357498030547491, took: 0.7704360485076904\n",
      "Validation vlb: -5.324335902254172, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch131, vlb: -5.387272027056553, took: 0.8074009418487549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation vlb: -5.465943105012467, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch132, vlb: -5.3911606280718685, took: 0.7654478549957275\n",
      "Validation vlb: -5.397088720574734, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch133, vlb: -5.379396009840122, took: 0.6729803085327148\n",
      "Validation vlb: -5.444101139179711, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch134, vlb: -5.394951287559095, took: 0.6154019832611084\n",
      "Validation vlb: -5.333385356421609, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch135, vlb: -5.398394488670176, took: 0.576185941696167\n",
      "Validation vlb: -5.38704809941906, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch136, vlb: -5.373212961657767, took: 0.5722620487213135\n",
      "Validation vlb: -5.401562156800699, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch137, vlb: -5.376520516076533, took: 0.5617308616638184\n",
      "Validation vlb: -5.389176629507812, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch138, vlb: -5.399800530345396, took: 0.5693261623382568\n",
      "Validation vlb: -5.49616369537551, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch139, vlb: -5.35546128733535, took: 0.5656750202178955\n",
      "Validation vlb: -5.455796848223048, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch140, vlb: -5.398778302745936, took: 0.5682399272918701\n",
      "Validation vlb: -5.459032555614089, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch141, vlb: -5.403298568416509, took: 0.6102659702301025\n",
      "Validation vlb: -5.526953262032815, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch142, vlb: -5.404060119255266, took: 0.559729814529419\n",
      "Validation vlb: -5.4378477022485825, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch143, vlb: -5.356767937567837, took: 0.6160480976104736\n",
      "Validation vlb: -5.335071832230947, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch144, vlb: -5.387665432650793, took: 0.6751708984375\n",
      "Validation vlb: -5.400834864397265, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch145, vlb: -5.424529887293155, took: 1.0091450214385986\n",
      "Validation vlb: -5.402105721840966, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch146, vlb: -5.411525192817081, took: 0.6078908443450928\n",
      "Validation vlb: -5.443444000478701, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch147, vlb: -5.3970273017539885, took: 0.5716233253479004\n",
      "Validation vlb: -5.350215473607134, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch148, vlb: -5.382488318557657, took: 0.6935970783233643\n",
      "Validation vlb: -5.401241466065441, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch149, vlb: -5.348784555236684, took: 0.6577737331390381\n",
      "Validation vlb: -5.315376809499796, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch150, vlb: -5.397857583079958, took: 0.6001138687133789\n",
      "Validation vlb: -5.353572791448303, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch151, vlb: -5.385967718082493, took: 0.5937600135803223\n",
      "Validation vlb: -5.469842844410621, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch152, vlb: -5.366389916103694, took: 0.7431979179382324\n",
      "Validation vlb: -5.515644136755983, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch153, vlb: -5.403236393624602, took: 1.2864649295806885\n",
      "Validation vlb: -5.459319642446573, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch154, vlb: -5.4081029569154, took: 0.7219960689544678\n",
      "Validation vlb: -5.54599284199835, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch155, vlb: -5.330551946425034, took: 0.8174028396606445\n",
      "Validation vlb: -5.449961370634801, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch156, vlb: -5.372429898886703, took: 0.9275612831115723\n",
      "Validation vlb: -5.429257608926026, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch157, vlb: -5.3757727907680115, took: 0.6573669910430908\n",
      "Validation vlb: -5.4642207198158435, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch158, vlb: -5.400021045809819, took: 0.6640539169311523\n",
      "Validation vlb: -5.466542080382313, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch159, vlb: -5.391985224784601, took: 0.694025993347168\n",
      "Validation vlb: -5.408648404488671, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch160, vlb: -5.400938679341752, took: 0.6300742626190186\n",
      "Validation vlb: -5.492479628343798, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch161, vlb: -5.385571981532393, took: 0.6162130832672119\n",
      "Validation vlb: -5.354821990608783, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch162, vlb: -5.376717191743559, took: 0.8057739734649658\n",
      "Validation vlb: -5.450508418592435, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch163, vlb: -5.3840243142416115, took: 0.7248690128326416\n",
      "Validation vlb: -5.38094688156276, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch164, vlb: -5.38737035424408, took: 0.7437460422515869\n",
      "Validation vlb: -5.424712977363068, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch165, vlb: -5.369228270142294, took: 0.6632049083709717\n",
      "Validation vlb: -5.366378532644228, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch166, vlb: -5.350304194564737, took: 0.7893998622894287\n",
      "Validation vlb: -5.367263530064555, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch167, vlb: -5.368307742104069, took: 0.7728030681610107\n",
      "Validation vlb: -5.326030238932391, Best vlb: -5.301155662845254\n",
      "\n",
      "Epoch168, vlb: -5.363252995895919, took: 0.7377660274505615\n",
      "Validation vlb: -5.2363137411839755, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch169, vlb: -5.371798571812235, took: 0.7749099731445312\n",
      "Validation vlb: -5.376441423175405, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch170, vlb: -5.369261061840449, took: 0.7356760501861572\n",
      "Validation vlb: -5.349534797051192, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch171, vlb: -5.392612219629504, took: 0.7311630249023438\n",
      "Validation vlb: -5.34065973411486, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch172, vlb: -5.347148275907665, took: 0.652407169342041\n",
      "Validation vlb: -5.466659297449303, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch173, vlb: -5.3455106989702905, took: 0.6493721008300781\n",
      "Validation vlb: -5.51259427548998, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch174, vlb: -5.354862131057645, took: 0.5699031352996826\n",
      "Validation vlb: -5.340190410614014, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch175, vlb: -5.395313021859377, took: 0.588507890701294\n",
      "Validation vlb: -5.375330946592065, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch176, vlb: -5.404303061691685, took: 0.6255910396575928\n",
      "Validation vlb: -5.447532022655203, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch177, vlb: -5.371731415557312, took: 0.684406042098999\n",
      "Validation vlb: -5.435883393951219, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch178, vlb: -5.34217459218812, took: 0.783350944519043\n",
      "Validation vlb: -5.495410929991589, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch179, vlb: -5.383978012958619, took: 0.7370896339416504\n",
      "Validation vlb: -5.340238694619978, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch180, vlb: -5.386541024016784, took: 0.7770419120788574\n",
      "Validation vlb: -5.3846482150377195, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch181, vlb: -5.369943442428803, took: 0.823735237121582\n",
      "Validation vlb: -5.47105117754643, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch182, vlb: -5.418381275270412, took: 0.7542409896850586\n",
      "Validation vlb: -5.355421373373482, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch183, vlb: -5.424688885956305, took: 0.8498039245605469\n",
      "Validation vlb: -5.521005218468823, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch184, vlb: -5.35139504207052, took: 0.747185230255127\n",
      "Validation vlb: -5.428339962820405, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch185, vlb: -5.386864540600116, took: 0.6091172695159912\n",
      "Validation vlb: -5.407781386452585, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch186, vlb: -5.39590536899862, took: 0.6752691268920898\n",
      "Validation vlb: -5.36204027743787, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch187, vlb: -5.320719500003549, took: 0.6404070854187012\n",
      "Validation vlb: -5.362620399993601, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch188, vlb: -5.357338470051003, took: 0.6718831062316895\n",
      "Validation vlb: -5.33465372320132, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch189, vlb: -5.3435577604467595, took: 0.6141090393066406\n",
      "Validation vlb: -5.379179366583963, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch190, vlb: -5.380923290410658, took: 0.5750479698181152\n",
      "Validation vlb: -5.410971298958492, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch191, vlb: -5.356693846998259, took: 0.5624749660491943\n",
      "Validation vlb: -5.335723218022813, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch192, vlb: -5.359559972465661, took: 0.5667579174041748\n",
      "Validation vlb: -5.332190984275349, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch193, vlb: -5.388094364587978, took: 0.6582131385803223\n",
      "Validation vlb: -5.355351812245391, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch194, vlb: -5.386340401436212, took: 0.8050050735473633\n",
      "Validation vlb: -5.368475474200203, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch195, vlb: -5.359633601480223, took: 0.675330638885498\n",
      "Validation vlb: -5.5093506038381825, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch196, vlb: -5.390328659185369, took: 0.6284990310668945\n",
      "Validation vlb: -5.35815333394171, Best vlb: -5.2363137411839755\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch197, vlb: -5.400699112198101, took: 0.7432470321655273\n",
      "Validation vlb: -5.5463428883105035, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch198, vlb: -5.337142048527707, took: 0.705895185470581\n",
      "Validation vlb: -5.382479784943911, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch199, vlb: -5.393199895928135, took: 0.7588729858398438\n",
      "Validation vlb: -5.452928094030584, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch200, vlb: -5.380723820343952, took: 0.6931769847869873\n",
      "Validation vlb: -5.452719441509555, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch201, vlb: -5.367195075649471, took: 0.8547811508178711\n",
      "Validation vlb: -5.3656013374575515, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch202, vlb: -5.343015167667662, took: 0.7919449806213379\n",
      "Validation vlb: -5.453960614682787, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch203, vlb: -5.406760437092766, took: 0.7385251522064209\n",
      "Validation vlb: -5.370881295127004, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch204, vlb: -5.353121007652131, took: 0.6964981555938721\n",
      "Validation vlb: -5.359061606879373, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch205, vlb: -5.376910284692793, took: 0.708406925201416\n",
      "Validation vlb: -5.3069519842326836, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch206, vlb: -5.360933357406358, took: 0.6989092826843262\n",
      "Validation vlb: -5.333446240347952, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch207, vlb: -5.364300254612979, took: 0.643096923828125\n",
      "Validation vlb: -5.405829264508096, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch208, vlb: -5.361395429096764, took: 0.6126160621643066\n",
      "Validation vlb: -5.390841286545046, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch209, vlb: -5.347693695024402, took: 0.6796250343322754\n",
      "Validation vlb: -5.365000315854465, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch210, vlb: -5.34707423496212, took: 0.6654078960418701\n",
      "Validation vlb: -5.382138367995475, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch211, vlb: -5.351821727361188, took: 0.6803748607635498\n",
      "Validation vlb: -5.361542033531905, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch212, vlb: -5.362136286885609, took: 0.6203241348266602\n",
      "Validation vlb: -5.345005981358895, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch213, vlb: -5.3736216739480085, took: 0.617246150970459\n",
      "Validation vlb: -5.386490139760632, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch214, vlb: -5.322027792465271, took: 0.6260502338409424\n",
      "Validation vlb: -5.5492970333901805, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch215, vlb: -5.3713528563060216, took: 0.6107101440429688\n",
      "Validation vlb: -5.441509197444978, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch216, vlb: -5.376380647048703, took: 0.6219518184661865\n",
      "Validation vlb: -5.416794205946444, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch217, vlb: -5.34878670160488, took: 0.6111979484558105\n",
      "Validation vlb: -5.395886873350174, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch218, vlb: -5.331940223211688, took: 0.6439981460571289\n",
      "Validation vlb: -5.531597608115681, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch219, vlb: -5.370946262223625, took: 0.6245639324188232\n",
      "Validation vlb: -5.400671688869933, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch220, vlb: -5.346540044956942, took: 0.6440937519073486\n",
      "Validation vlb: -5.435808582984899, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch221, vlb: -5.3826966270251715, took: 0.6523299217224121\n",
      "Validation vlb: -5.420505832314105, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch222, vlb: -5.363937792092894, took: 0.6677570343017578\n",
      "Validation vlb: -5.4561530702708225, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch223, vlb: -5.381412715073923, took: 0.628807783126831\n",
      "Validation vlb: -5.413146258172094, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch224, vlb: -5.373749764222312, took: 0.6146507263183594\n",
      "Validation vlb: -5.343710755838932, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch225, vlb: -5.340248600372875, took: 0.5952062606811523\n",
      "Validation vlb: -5.407613493478028, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch226, vlb: -5.345028694902859, took: 0.6914989948272705\n",
      "Validation vlb: -5.383907449284032, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch227, vlb: -5.369771224198086, took: 0.5905311107635498\n",
      "Validation vlb: -5.40668655444889, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch228, vlb: -5.353787597152111, took: 0.5971536636352539\n",
      "Validation vlb: -5.387037156855018, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch229, vlb: -5.345534204964506, took: 0.642404317855835\n",
      "Validation vlb: -5.307701124728305, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch230, vlb: -5.331679171952662, took: 0.6366720199584961\n",
      "Validation vlb: -5.422562606897941, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch231, vlb: -5.34559593262499, took: 0.6033759117126465\n",
      "Validation vlb: -5.395144802081161, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch232, vlb: -5.367941024967093, took: 0.6638047695159912\n",
      "Validation vlb: -5.429966992930687, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch233, vlb: -5.3892781565332735, took: 0.8558230400085449\n",
      "Validation vlb: -5.37846984832418, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch234, vlb: -5.35834193169577, took: 0.6236898899078369\n",
      "Validation vlb: -5.3522991347081454, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch235, vlb: -5.3443845172644435, took: 0.7136728763580322\n",
      "Validation vlb: -5.365436007675616, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch236, vlb: -5.3427991266082335, took: 0.6202442646026611\n",
      "Validation vlb: -5.539316797719418, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch237, vlb: -5.363919443907307, took: 0.582024097442627\n",
      "Validation vlb: -5.440708388788415, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch238, vlb: -5.351440012261813, took: 0.5647220611572266\n",
      "Validation vlb: -5.445392537657111, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch239, vlb: -5.370977772110694, took: 0.5690369606018066\n",
      "Validation vlb: -5.325798138059844, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch240, vlb: -5.400871860324199, took: 0.6256802082061768\n",
      "Validation vlb: -5.330667410853612, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch241, vlb: -5.373414971650742, took: 0.5629968643188477\n",
      "Validation vlb: -5.309186782651735, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch242, vlb: -5.350847636962221, took: 0.5773749351501465\n",
      "Validation vlb: -5.403225090125618, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch243, vlb: -5.3690730624805045, took: 0.5610020160675049\n",
      "Validation vlb: -5.328955591689422, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch244, vlb: -5.363407738053374, took: 0.5597329139709473\n",
      "Validation vlb: -5.411487094792733, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch245, vlb: -5.360235139368076, took: 0.5629870891571045\n",
      "Validation vlb: -5.336733427634131, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch246, vlb: -5.33133142287241, took: 0.6971089839935303\n",
      "Validation vlb: -5.320377714039824, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch247, vlb: -5.3840069672938595, took: 0.7199270725250244\n",
      "Validation vlb: -5.362966907834544, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch248, vlb: -5.362941065923232, took: 0.6729772090911865\n",
      "Validation vlb: -5.352395486677349, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch249, vlb: -5.391945344233281, took: 0.6416430473327637\n",
      "Validation vlb: -5.396083020083727, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch250, vlb: -5.361815049230173, took: 0.5589921474456787\n",
      "Validation vlb: -5.33823646002217, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch251, vlb: -5.344563660194087, took: 0.6311149597167969\n",
      "Validation vlb: -5.422238079861144, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch252, vlb: -5.355737082941909, took: 0.6626498699188232\n",
      "Validation vlb: -5.391597102760882, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch253, vlb: -5.341266895482383, took: 0.7143080234527588\n",
      "Validation vlb: -5.418827385578341, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch254, vlb: -5.341307955162534, took: 0.777573823928833\n",
      "Validation vlb: -5.459684492314904, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch255, vlb: -5.356487444677411, took: 0.8208820819854736\n",
      "Validation vlb: -5.460393110911052, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch256, vlb: -5.387564141929772, took: 0.7814357280731201\n",
      "Validation vlb: -5.40679519307652, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch257, vlb: -5.3653719391718075, took: 0.6758749485015869\n",
      "Validation vlb: -5.4121390129756, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch258, vlb: -5.344620603514009, took: 0.6078910827636719\n",
      "Validation vlb: -5.50007141909553, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch259, vlb: -5.353003420330098, took: 0.5933430194854736\n",
      "Validation vlb: -5.287751895324312, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch260, vlb: -5.34431372746573, took: 0.5720241069793701\n",
      "Validation vlb: -5.4022579393726335, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch261, vlb: -5.36020806954411, took: 0.6233899593353271\n",
      "Validation vlb: -5.33566070760338, Best vlb: -5.2363137411839755\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch262, vlb: -5.365419561243933, took: 0.5729620456695557\n",
      "Validation vlb: -5.39040454845984, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch263, vlb: -5.352704696359933, took: 0.563791036605835\n",
      "Validation vlb: -5.299207563924944, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch264, vlb: -5.3439167137406995, took: 0.5759620666503906\n",
      "Validation vlb: -5.5237282657314655, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch265, vlb: -5.361720737296555, took: 0.570065975189209\n",
      "Validation vlb: -5.456005597963302, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch266, vlb: -5.332548550319706, took: 0.559553861618042\n",
      "Validation vlb: -5.333235103335581, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch267, vlb: -5.368086802689, took: 0.5691220760345459\n",
      "Validation vlb: -5.4567485750686, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch268, vlb: -5.369892702112963, took: 0.5676679611206055\n",
      "Validation vlb: -5.56429355584302, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch269, vlb: -5.339506758173722, took: 0.6491978168487549\n",
      "Validation vlb: -5.27024977569827, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch270, vlb: -5.376720313421462, took: 0.6035587787628174\n",
      "Validation vlb: -5.372046078678859, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch271, vlb: -5.370198065400939, took: 0.5654358863830566\n",
      "Validation vlb: -5.399219900273197, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch272, vlb: -5.364647182578615, took: 0.5902149677276611\n",
      "Validation vlb: -5.4120564476186015, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch273, vlb: -5.350896625841613, took: 0.8252780437469482\n",
      "Validation vlb: -5.353857254904836, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch274, vlb: -5.336304168028071, took: 0.6060550212860107\n",
      "Validation vlb: -5.346245128359995, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch275, vlb: -5.346269883137621, took: 0.6430401802062988\n",
      "Validation vlb: -5.440303794774422, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch276, vlb: -5.376197918653918, took: 0.7865650653839111\n",
      "Validation vlb: -5.404991111323286, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch277, vlb: -5.32513615721887, took: 0.6460559368133545\n",
      "Validation vlb: -5.32610331075477, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch278, vlb: -5.3411016836842915, took: 0.5980591773986816\n",
      "Validation vlb: -5.534491966457429, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch279, vlb: -5.374488944409776, took: 0.5629112720489502\n",
      "Validation vlb: -5.305338972208955, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch280, vlb: -5.349443390852778, took: 0.569758415222168\n",
      "Validation vlb: -5.355880991926471, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch281, vlb: -5.354119730287634, took: 0.5654571056365967\n",
      "Validation vlb: -5.382909676017885, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch282, vlb: -5.326312509852002, took: 0.566058874130249\n",
      "Validation vlb: -5.5279604902545225, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch283, vlb: -5.3537066493654235, took: 0.5795509815216064\n",
      "Validation vlb: -5.363261080868422, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch284, vlb: -5.401528221607037, took: 0.5734150409698486\n",
      "Validation vlb: -5.3936234137772745, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch285, vlb: -5.349566342378203, took: 0.5914628505706787\n",
      "Validation vlb: -5.379201822682106, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch286, vlb: -5.36520926523947, took: 0.5670197010040283\n",
      "Validation vlb: -5.382554322770498, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch287, vlb: -5.34986968653469, took: 0.5653731822967529\n",
      "Validation vlb: -5.421608508211895, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch288, vlb: -5.353598447338815, took: 0.5644950866699219\n",
      "Validation vlb: -5.310799094079767, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch289, vlb: -5.307166845797634, took: 0.6156349182128906\n",
      "Validation vlb: -5.391346071916105, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch290, vlb: -5.3121170303913425, took: 0.5686290264129639\n",
      "Validation vlb: -5.365961729129927, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch291, vlb: -5.354705709753425, took: 0.5613701343536377\n",
      "Validation vlb: -5.377217522716831, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch292, vlb: -5.38586517559954, took: 0.5666117668151855\n",
      "Validation vlb: -5.327163332102754, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch293, vlb: -5.356257743443952, took: 0.5661938190460205\n",
      "Validation vlb: -5.457046425458297, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch294, vlb: -5.363477660204207, took: 0.5587818622589111\n",
      "Validation vlb: -5.417798352472991, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch295, vlb: -5.368112318885811, took: 0.5685951709747314\n",
      "Validation vlb: -5.388607665558849, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch296, vlb: -5.352743172499616, took: 0.5673041343688965\n",
      "Validation vlb: -5.324927405632044, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch297, vlb: -5.345160233789869, took: 0.5835189819335938\n",
      "Validation vlb: -5.494115858787858, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch298, vlb: -5.366470540218058, took: 0.6408917903900146\n",
      "Validation vlb: -5.37292935238687, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch299, vlb: -5.365274157161802, took: 0.8362612724304199\n",
      "Validation vlb: -5.366823034379089, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch300, vlb: -5.359017937570585, took: 0.6629810333251953\n",
      "Validation vlb: -5.370701425669648, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch301, vlb: -5.348695073661763, took: 0.6060171127319336\n",
      "Validation vlb: -5.427894098473213, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch302, vlb: -5.340888825087913, took: 0.5659310817718506\n",
      "Validation vlb: -5.49830335783727, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch303, vlb: -5.305275740707612, took: 0.6149077415466309\n",
      "Validation vlb: -5.393643015025117, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch304, vlb: -5.363385027589066, took: 0.5691471099853516\n",
      "Validation vlb: -5.377225616603222, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch305, vlb: -5.357460350452845, took: 0.5652098655700684\n",
      "Validation vlb: -5.377473630565656, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch306, vlb: -5.410683886198677, took: 0.6444318294525146\n",
      "Validation vlb: -5.489341053762097, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch307, vlb: -5.324141839789451, took: 0.5999269485473633\n",
      "Validation vlb: -5.402642412093079, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch308, vlb: -5.369867205319321, took: 0.5681071281433105\n",
      "Validation vlb: -5.387275206618324, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch309, vlb: -5.363376042891802, took: 0.560513973236084\n",
      "Validation vlb: -5.371884679331363, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch310, vlb: -5.34053029866368, took: 0.5598680973052979\n",
      "Validation vlb: -5.416460858193802, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch311, vlb: -5.360560974715895, took: 0.5629119873046875\n",
      "Validation vlb: -5.399204389948675, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch312, vlb: -5.406211756698339, took: 0.5647633075714111\n",
      "Validation vlb: -5.398954587461107, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch313, vlb: -5.370890043002476, took: 0.5692501068115234\n",
      "Validation vlb: -5.363253996210191, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch314, vlb: -5.338481518103572, took: 0.5547189712524414\n",
      "Validation vlb: -5.410192926338961, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch315, vlb: -5.362401054280673, took: 0.5692169666290283\n",
      "Validation vlb: -5.4355710085155895, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch316, vlb: -5.327728402670399, took: 0.5647859573364258\n",
      "Validation vlb: -5.28077712876897, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch317, vlb: -5.36428708913389, took: 0.5843291282653809\n",
      "Validation vlb: -5.502105293150473, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch318, vlb: -5.324836389426924, took: 0.5687899589538574\n",
      "Validation vlb: -5.447651299843896, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch319, vlb: -5.354897109990846, took: 0.5766792297363281\n",
      "Validation vlb: -5.325465057274284, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch320, vlb: -5.379147215030328, took: 0.5599238872528076\n",
      "Validation vlb: -5.463988674497141, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch321, vlb: -5.3526149668365575, took: 0.5608291625976562\n",
      "Validation vlb: -5.32885367044739, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch322, vlb: -5.349571529034019, took: 0.5734398365020752\n",
      "Validation vlb: -5.330652949879471, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch323, vlb: -5.328168655583702, took: 0.5660910606384277\n",
      "Validation vlb: -5.401288595785987, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch324, vlb: -5.380390527826185, took: 0.5519399642944336\n",
      "Validation vlb: -5.407652038586564, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch325, vlb: -5.332992667382597, took: 0.7652387619018555\n",
      "Validation vlb: -5.3605055700999635, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch326, vlb: -5.394323342130646, took: 0.760645866394043\n",
      "Validation vlb: -5.321661601946192, Best vlb: -5.2363137411839755\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch327, vlb: -5.365971355417627, took: 0.7530410289764404\n",
      "Validation vlb: -5.4321699065297935, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch328, vlb: -5.313838278083953, took: 0.6656429767608643\n",
      "Validation vlb: -5.423446795701209, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch329, vlb: -5.349746360510942, took: 0.6082549095153809\n",
      "Validation vlb: -5.262684087537253, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch330, vlb: -5.326122036673931, took: 0.6763319969177246\n",
      "Validation vlb: -5.341652592409004, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch331, vlb: -5.360563846728249, took: 0.6147069931030273\n",
      "Validation vlb: -5.411175862099361, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch332, vlb: -5.345939755911959, took: 0.6136918067932129\n",
      "Validation vlb: -5.4864817036008375, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch333, vlb: -5.309965540618402, took: 0.6093201637268066\n",
      "Validation vlb: -5.332327930672655, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch334, vlb: -5.357385519987222, took: 0.7245111465454102\n",
      "Validation vlb: -5.377660839303026, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch335, vlb: -5.343850993320244, took: 0.7932820320129395\n",
      "Validation vlb: -5.3914209461520795, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch336, vlb: -5.367363197579416, took: 0.6099908351898193\n",
      "Validation vlb: -5.30675097653781, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch337, vlb: -5.337560875706849, took: 0.5632829666137695\n",
      "Validation vlb: -5.307275458832775, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch338, vlb: -5.362115614955958, took: 0.586359977722168\n",
      "Validation vlb: -5.456750835801405, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch339, vlb: -5.349991648155105, took: 0.5629918575286865\n",
      "Validation vlb: -5.4606172938177115, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch340, vlb: -5.326642792474721, took: 0.5595798492431641\n",
      "Validation vlb: -5.281054968972808, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch341, vlb: -5.38962758235636, took: 0.5627281665802002\n",
      "Validation vlb: -5.4184090929123965, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch342, vlb: -5.342558401971143, took: 0.5953328609466553\n",
      "Validation vlb: -5.422951624231431, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch343, vlb: -5.352514439364067, took: 0.5701949596405029\n",
      "Validation vlb: -5.367460564116444, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch344, vlb: -5.353424076386652, took: 0.6324212551116943\n",
      "Validation vlb: -5.33581558863322, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch345, vlb: -5.365470905290218, took: 0.6704380512237549\n",
      "Validation vlb: -5.356574603269015, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch346, vlb: -5.348337273225107, took: 0.5733680725097656\n",
      "Validation vlb: -5.452804738263868, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch347, vlb: -5.326809750901395, took: 0.5754659175872803\n",
      "Validation vlb: -5.38165494307731, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch348, vlb: -5.36669789092078, took: 0.5946412086486816\n",
      "Validation vlb: -5.486673984712767, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch349, vlb: -5.3541216814221775, took: 0.5708789825439453\n",
      "Validation vlb: -5.258316368732638, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch350, vlb: -5.336382622078335, took: 0.5772008895874023\n",
      "Validation vlb: -5.296413264228303, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch351, vlb: -5.363738107904497, took: 0.5779409408569336\n",
      "Validation vlb: -5.359882905645278, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch352, vlb: -5.347895198302772, took: 0.5936448574066162\n",
      "Validation vlb: -5.30663818834669, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch353, vlb: -5.322845308566892, took: 0.5682258605957031\n",
      "Validation vlb: -5.296302489864016, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch354, vlb: -5.318347785126876, took: 0.559345006942749\n",
      "Validation vlb: -5.304381277954694, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch355, vlb: -5.393053308081018, took: 0.6076970100402832\n",
      "Validation vlb: -5.3442198373739, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch356, vlb: -5.327918036704361, took: 0.5735599994659424\n",
      "Validation vlb: -5.3963857962475625, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch357, vlb: -5.34685177557495, took: 0.567241907119751\n",
      "Validation vlb: -5.337357369827221, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch358, vlb: -5.359226232129633, took: 0.6799921989440918\n",
      "Validation vlb: -5.27494203388498, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch359, vlb: -5.381432462004736, took: 0.5697019100189209\n",
      "Validation vlb: -5.297542494863368, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch360, vlb: -5.366121978780371, took: 0.5622079372406006\n",
      "Validation vlb: -5.329863716483502, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch361, vlb: -5.356856039972412, took: 0.5656890869140625\n",
      "Validation vlb: -5.472789792181219, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch362, vlb: -5.316191098223841, took: 0.5590338706970215\n",
      "Validation vlb: -5.393340891618944, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch363, vlb: -5.375466413535701, took: 0.5610959529876709\n",
      "Validation vlb: -5.356402019080992, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch364, vlb: -5.341907324359964, took: 0.559196949005127\n",
      "Validation vlb: -5.402518364989642, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch365, vlb: -5.360728603324574, took: 0.5629441738128662\n",
      "Validation vlb: -5.330848135222895, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch366, vlb: -5.316468236941763, took: 0.5731208324432373\n",
      "Validation vlb: -5.417529672480709, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch367, vlb: -5.360786143678275, took: 0.5713119506835938\n",
      "Validation vlb: -5.256620968815578, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch368, vlb: -5.326510217321147, took: 0.572746992111206\n",
      "Validation vlb: -5.466168948361789, Best vlb: -5.2363137411839755\n",
      "\n",
      "Epoch369, vlb: -5.3434200025866865, took: 0.564528226852417\n",
      "Validation vlb: -5.30521584945975, Best vlb: -5.2363137411839755\n",
      "\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "# For Default credit\n",
    "\"\"\"\n",
    "input_dim_vec = [1, 2, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ,1 ,1 ,1 ,1 ,1, 2]\n",
    "width = 350\n",
    "depth = 3\n",
    "latent_dim = 8\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "batch_size = 128\n",
    "nb_epochs = 2000 # 2000\n",
    "early_stop = 200\n",
    "lr = 1e-4        # Maybe this should be 1e-4, but it makes the performance terrible...\n",
    "\n",
    "x_train, x_test, x_means, x_stds = \\\n",
    "load_UCI(dset_name=dname, splits=10, seed=42, separate_targets=False, save_dir='../data/') # np.arrays\n",
    "\n",
    "x_train = unnormalise_cat_vars(x_train, x_means, x_stds, input_dim_vec) # np.array\n",
    "x_test = unnormalise_cat_vars(x_test, x_means, x_stds, input_dim_vec) \n",
    "\"\"\"\n",
    "\n",
    "# For COMPAS\n",
    "#\"\"\"\n",
    "dname = 'compas'\n",
    "print(dname)\n",
    "\n",
    "input_dim_vec = [3, 6, 2, 2, 2, 1, 1, 2]\n",
    "batch_size = 128\n",
    "nb_epochs = 2000\n",
    "early_stop = 200\n",
    "lr = 1e-4\n",
    "width = widths[names.index(dname)] # 350\n",
    "depth = depths[names.index(dname)] # number of hidden layers # 3\n",
    "latent_dim = latent_dims[names.index(dname)] # 4\n",
    "\n",
    "x_train, x_test, x_means, x_stds, y_train, y_test, feature_names, X_dims = \\\n",
    "    get_my_COMPAS(rseed=42, separate_test=True, test_ratio=0.1, save_dir='../data/')\n",
    "\n",
    "x_train, x_test, input_dim_vec = join_compas_targets(x_train, x_test, y_train, y_test, X_dims)\n",
    "\n",
    "print('Compas', x_train.shape, x_test.shape)\n",
    "print(input_dim_vec)\n",
    "#\"\"\"\n",
    "\n",
    "optimizer_VAEAC = tfa.optimizers.RectifiedAdam(lr= lr , epsilon=1e-8)\n",
    "\n",
    "# Create new model to load in weightsinto that can then continued to be trained\n",
    "model2 = VAEAC_gauss_cat(width, depth, latent_dim, input_dim_vec, batch_size, lr, optimizer_VAEAC, save_model = True)\n",
    "\n",
    "model2.recognition_encoder = keras.models.load_model(\"./BEST_VAEAC_18/compas_recog_encoder_lr_0.0001\")\n",
    "model2.prior_encoder = keras.models.load_model(\"./BEST_VAEAC_18/compas_prior_encoder_lr_0.0001\")\n",
    "model2.decoder = keras.models.load_model(\"./BEST_VAEAC_18/compas_decoder_lr_0.0001\")\n",
    "\n",
    "\n",
    "# No mask is used to train the 2nd lvl VAE\n",
    "#masker = top_masker_tensorflow(p=1)\n",
    "\n",
    "base_network = model2\n",
    "width = 150\n",
    "depth = 2\n",
    "latent_dim = under_latent_dims[names.index(dname)]\n",
    "\n",
    "batch_size = 128\n",
    "nb_epochs = 2000\n",
    "early_stop = 200\n",
    "lr = 1e-4\n",
    "\n",
    "optimizer_under_VAEAC = tfa.optimizers.RectifiedAdam(lr = lr, epsilon = 1e-8)\n",
    "\n",
    "under_VAEAC_net = under_VAEAC(base_network, width, depth, latent_dim, batch_size, lr, \\\n",
    "                              optimizer_under_VAEAC, save_model = True)\n",
    "\n",
    "vlb_train, vlb_val, best_epoch, best_vlb, curr_epoch = train_VAE(under_VAEAC_net, x_train, x_test, nb_epochs, early_stop=early_stop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83b99b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 6 2 2 2 1 1]\n",
      "[3 6 2 2 2 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "input_dim_vec_X_dims = X_dims_to_input_dim_vec(X_dims)\n",
    "print(input_dim_vec_X_dims)\n",
    "print(input_dim_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9b68827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.clip(vlb_train[:curr_epoch], -1000, 1000), 'r')\n",
    "plt.plot(np.clip(vlb_val[:curr_epoch], -1000, 1000), 'b')\n",
    "plt.legend(['cost_train', 'cost_dev'])\n",
    "plt.ylabel('vlb')\n",
    "plt.xlabel('it')\n",
    "plt.grid(True)\n",
    "#plt.savefig( str(dname) + '_vlb_lr_' + str(model.lr) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3287999f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "827897\n"
     ]
    }
   ],
   "source": [
    "#\"\"\"\n",
    "total_parameters = 0\n",
    "for variable in under_VAEAC_net.trainable_variables:\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    #print(shape)\n",
    "    #print(len(shape))\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        #print(dim)\n",
    "        variable_parameters *= dim\n",
    "    #print(variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)\n",
    "\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41444e16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28920845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.73977529 -5.61497819 -4.62562061 ...  0.          0.\n",
      "  0.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.load(\"./COMPAS_VAEAC/compas_vlb_train_lr_0.0001.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3990338e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9883e1bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "813be207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(under_VAEAC_net.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b33d4570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model2.decoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "170d1686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO check the that the parameters in the VAEAC are not trained during the training of under_VAEAC\n",
    "\n",
    "# Encoder\n",
    "#print(model2.recognition_encoder.trainable_variables[0])\n",
    "\n",
    "# Decoder\n",
    "#print(model2.decoder.trainable_variables[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97bdd3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel2 = VAEAC_gauss_cat(width, depth, latent_dim, input_dim_vec, batch_size, lr, optimizer_VAEAC, save_model = True)\\n\\nmodel2.decoder = keras.models.load_model(\"./COMPAS_VAEAC/compas_decoder_lr_0.0001\")\\n\\nprint(model2.decoder.trainable_variables[0])\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoder\n",
    "\"\"\"\n",
    "model2 = VAEAC_gauss_cat(width, depth, latent_dim, input_dim_vec, batch_size, lr, optimizer_VAEAC, save_model = True)\n",
    "model2.recognition_encoder = keras.models.load_model(\"./COMPAS_VAEAC/compas_recog_encoder_lr_0.0001\")\n",
    "print(model2.recognition_encoder.trainable_variables[0])\n",
    "\"\"\"\n",
    "\n",
    "# Decoder\n",
    "\"\"\"\n",
    "model2 = VAEAC_gauss_cat(width, depth, latent_dim, input_dim_vec, batch_size, lr, optimizer_VAEAC, save_model = True)\n",
    "\n",
    "model2.decoder = keras.models.load_model(\"./COMPAS_VAEAC/compas_decoder_lr_0.0001\")\n",
    "\n",
    "print(model2.decoder.trainable_variables[0])\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29f22fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model2.prior_encoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f2c9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79e0896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(under_VAEAC_net.decoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78fb90c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(under_VAEAC_net.recognition_encoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88ef766",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Currently Working on ###\n",
    "\n",
    "### TODO ###\n",
    "\n",
    "# Make code work for only continous data (GaussianLogLike?)\n",
    "\n",
    "# Train VAEAC (default credit) using 7e-4 lr and save the model\n",
    "\n",
    "# Clean up the code\n",
    "\n",
    "## eval/training mode things ##\n",
    "\n",
    "# Verify if BatchNorm does what it should when training = True/False is not sent into the model\n",
    "\n",
    "# Do I need to set the models to eval mode and not training during validation data or does tf handle this?\n",
    "    # Apparently batchnorm layers do different things during training/evaluation\n",
    "    # Adding training = True affects a lot..., it is not enabled by default during train_step gradient.\n",
    "    # Setting training = True in eval affects the training_VAE still. \n",
    "    # Not using training = True/False makes the model work well, \n",
    "    # Question is if BatchNorm does the correct thing during eval\n",
    "\n",
    "##---------------------------##\n",
    "\n",
    "\n",
    "## Ask Ali \n",
    "    # If His network has the correct structure (The decoder was wrong for VAEAC)\n",
    "    # If He has thought of initlialisations\n",
    "    # hyperparameters, bias, epsilon, momentum etc.\n",
    "    # Where are the batches?\n",
    "\n",
    "# Skip connections from prior to decoder? I don't think CLUE got this to work properly...\n",
    "    # Memory layer is used in Tf2 github\n",
    "\n",
    "# Not sure what the TF equivalence of affine and track_running_stats is in Torch BatchNorm1D \n",
    "    \n",
    "# Why does train_step only print things inside it twice for the first batch and then never for any other batch?\n",
    "    \n",
    "### TO IMPLEMENT ###\n",
    "\n",
    "\n",
    "### DONE ###\n",
    "\"\"\"\n",
    "\n",
    "√ under_VAEAC updates the VAEAC recognition_encoder parameters. Need to freeze them somehow...\n",
    "    # Maybe calculate proposal_params_VAEAC before calling train_step_under_VAEAC to make it work?\n",
    "    # Yup, that did the trick\n",
    "\n",
    "√ Something wrong with the number of trainable parameters in my under_VAEAC? Seems to be way more than in Torch?\n",
    "    # The VAEAC recognition_encoder was being trained in under_VAEAC\n",
    "\n",
    "√ Make VAEAC work for COMPAS\n",
    "    # How to load COMPAS?\n",
    "\n",
    "√ Train the VAEAC for COMPAS\n",
    "\n",
    "√ Fix the under_VAEAC code  (i.e. get eval to work there just as in VAEAC)\n",
    "\n",
    "√ update_train VAE with the train_VAEAC code (Add Shuffle among many things)\n",
    "\n",
    "√ Plot the loss graph over train and validation set\n",
    "\n",
    "√ Save the vlb_train & vlb_val after training\n",
    "\n",
    "√ rec_los: Should the target not be flattened but instead just x_batch?\n",
    "    # Don't think so, the final values look fairly similar.\n",
    "    # I think it is fine since the program seems to be doing what it should\n",
    "\n",
    "√ How do the batches work in the network? How can we send a 64x31 batch to encoder? It should only take 31 as input\n",
    "    # the keras.input((31, )) means it expect features with dimension 31 and unspecified batch_size. \n",
    "    # When a batch with 64,31 size comes it in will treat each row as a sample\n",
    "\n",
    "√ Should I have 7e-4 or 1e-4 learning rate?\n",
    "    # 7e-4 for comparing and making sure the model works as intended but 1e-4 for the real training\n",
    "\n",
    "√ Remove reparametrize?\n",
    "\n",
    "√ Add lr to print epoch in VAEAC training\n",
    "\n",
    "√ Is something wrong with the trainable variables? Should I before training use tf.Variable to make them trainable?\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/Variable\n",
    "    # Seems right, it is the exact same amount of trainable parameters in torch and tf\n",
    "\n",
    "√ Have I forgotten to do a tf.reduce_sum over regularisation? \n",
    "    # Don't think so... their sum(-1) on reg_cost does nothing\n",
    "\n",
    "√ Something might be wrong with how the training is done with batches... \n",
    "    # Probably not, input(shape,) makes it so that it expects one dimension to be of shape \n",
    "    # without specifying the batch size\n",
    "\n",
    "√ vlb_val is calculated using eval not fit\n",
    "\n",
    "√ Add shuffling of the training data \n",
    "\n",
    "√ Verify the number of trainable parameters in CLUE vs TF \n",
    "    # Exact same amount for under_VAEAC and VAEAC\n",
    "\n",
    "√ implement under_VAEAC vlb (MSELoss and KL-divergence)\n",
    "\n",
    "√ rsample instead of sample? (rsample for derivatives in torch, tf does not care)\n",
    "\n",
    "√ Save the model during training (best vlb)\n",
    "\n",
    "√ Fix so that training with 2nd lvl VAE is done without the mask\n",
    "\n",
    "√ 1e-4 can at times get very poor vlb at the start but then recover. Why is this?   \n",
    "    # Seems to have been solved by initialising the dense weights and bias weights the same way as Torch\n",
    "    # I get like -19 or -20, -22 every single first epoch now\n",
    "\n",
    "√ Validation vlb not low enough? \n",
    "    # Not terrible but indeed not as low, seems to go towards the right values at least but it happens slowly\n",
    "\n",
    "√ Need to initialise the weights in keras dense for the neurons and bias the same way as nn.linear\n",
    "\n",
    "√ Need to make the layers in a skip connection sequential? nn.sequential in torch\n",
    "    # The russian doll effect does exactly this!\n",
    "    \n",
    "√ What activation is used in dense/nn.linear?\n",
    "    # None\n",
    "    \n",
    "√ Is keras dense and torch nn.linear the same thing?\n",
    "    # How are the weights inited in each? (Different ways by default but I made them init the same way)\n",
    "    # They are basically the same, input to neural network nodes (bias = True add a bias node)\n",
    "    # Google images for keras dense and nn.linear and you see that it is just a normal feed forward process.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
