{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "310825f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import uniform, binomial\n",
    "from sklearn.model_selection import KFold\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# To remove WARNINGS from saving the models without compiling them first\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "#print(tf.__version__)\n",
    "#print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e185160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.config.list_physical_devices())\n",
    "\n",
    "#Working with CPU for now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeb23f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import seed\n",
    "#from .UCI_loader import unnormalise_cat_vars\n",
    "\n",
    "\n",
    "# TODO return mean and std for variables + train test split\n",
    "\n",
    "\"\"\"\n",
    "def check_data_file(fname):\n",
    "    files = os.listdir(\".\")  # get the current directory listing\n",
    "    print\n",
    "    \"Looking for file '%s' in the current directory...\" % fname\n",
    "\n",
    "    if fname not in files:\n",
    "        print\n",
    "        \"'%s' not found! Downloading from GitHub...\" % fname\n",
    "        addr = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
    "        try:\n",
    "            response = urllib2.urlopen(addr)\n",
    "        except:\n",
    "            response = urllib3.urlopen(addr)\n",
    "        data = response.read()\n",
    "        fileOut = open(fname, \"w\")\n",
    "        fileOut.write(data)\n",
    "        fileOut.close()\n",
    "        print\n",
    "        \"'%s' download and saved locally..\" % fname\n",
    "    else:\n",
    "        print\n",
    "        \"File found in current directory..\"\n",
    "\"\"\"\n",
    "\n",
    "def check_data_file(fname):\n",
    "    files = os.listdir(\".\")  # get the current directory listing\n",
    "    print\n",
    "    \"Looking for file '%s' in the current directory...\" % fname\n",
    "\n",
    "    if fname not in files:\n",
    "        print\n",
    "        \"'%s' not found! Downloading from GitHub...\" % fname\n",
    "        addr = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
    "        \n",
    "        response = urllib.request.urlopen(addr)\n",
    "\n",
    "        data = response.read()\n",
    "        fileOut = open(fname, \"wb\")\n",
    "        fileOut.write(data)\n",
    "        fileOut.close()\n",
    "        print\n",
    "        \"'%s' download and saved locally..\" % fname\n",
    "    else:\n",
    "        print\n",
    "        \"File found in current directory..\"\n",
    "\n",
    "def get_my_COMPAS(rseed=0, separate_test=True, test_ratio=0.2, save_dir='../data/'):\n",
    "    \"\"\"\n",
    "        The adult dataset can be obtained from: https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\n",
    "        The code will look for the data file in the present directory, if it is not found, it will download them from GitHub.\n",
    "    \"\"\"\n",
    "\n",
    "    SEED = rseed\n",
    "    seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    their_FEATURES_CLASSIFICATION = [\"age_cat\", \"race\", \"sex\", \"priors_count\", \"c_charge_degree\"]\n",
    "    FEATURES_CLASSIFICATION = [\"age_cat\", \"race\", \"sex\", \"c_charge_degree\", \"is_recid\", \"priors_count\",\n",
    "                               \"time_served\"]  # features to be used for classification\n",
    "    CONT_VARIABLES = [\"priors_count\",\n",
    "                      \"time_served\"]  # continuous features, will need to be handled separately from categorical features, categorical features will be encoded using one-hot\n",
    "    CLASS_FEATURE = \"two_year_recid\"  # the decision variable\n",
    "\n",
    "\n",
    "    COMPAS_INPUT_FILE = save_dir + \"compas-scores-two-years.csv\"\n",
    "    check_data_file(COMPAS_INPUT_FILE)\n",
    "\n",
    "    # load the data and get some stats\n",
    "    df = pd.read_csv(COMPAS_INPUT_FILE)\n",
    "    df = df.dropna(subset=[\"days_b_screening_arrest\"])  # dropping missing vals\n",
    "\n",
    "    # convert to np array\n",
    "    data = df.to_dict('list')\n",
    "    for k in data.keys():\n",
    "        data[k] = np.array(data[k])\n",
    "\n",
    "    dates_in = data['c_jail_in']\n",
    "    dates_out = data['c_jail_out']\n",
    "    # this measures time in Jail\n",
    "    time_served = []\n",
    "    for i in range(len(dates_in)):\n",
    "        di = datetime.datetime.strptime(dates_in[i], '%Y-%m-%d %H:%M:%S')\n",
    "        do = datetime.datetime.strptime(dates_out[i], '%Y-%m-%d %H:%M:%S')\n",
    "        time_served.append((do - di).days)\n",
    "    time_served = np.array(time_served)\n",
    "    time_served[time_served < 0] = 0\n",
    "    data[\"time_served\"] = time_served\n",
    "\n",
    "    \"\"\" Filtering the data \"\"\"\n",
    "\n",
    "    # These filters are the same as propublica (refer to https://github.com/propublica/compas-analysis)\n",
    "    # If the charge date of a defendants Compas scored crime was not within 30 days from when the person was arrested, we assume that because of data quality reasons, that we do not have the right offense.\n",
    "    idx = np.logical_and(data[\"days_b_screening_arrest\"] <= 30, data[\"days_b_screening_arrest\"] >= -30)\n",
    "\n",
    "    # We coded the recidivist flag -- is_recid -- to be -1 if we could not find a compas case at all.\n",
    "    idx = np.logical_and(idx, data[\"is_recid\"] != -1)\n",
    "\n",
    "    # In a similar vein, ordinary traffic offenses -- those with a c_charge_degree of 'O' -- will not result in Jail time are removed (only two of them).\n",
    "    idx = np.logical_and(idx, data[\"c_charge_degree\"] != \"O\")  # F: felony, M: misconduct\n",
    "\n",
    "    # We filtered the underlying data from Broward county to include only those rows representing people who had either recidivated in two years, or had at least two years outside of a correctional facility.\n",
    "    idx = np.logical_and(idx, data[\"score_text\"] != \"NA\")\n",
    "\n",
    "    # select the examples that satisfy this criteria\n",
    "    for k in data.keys():\n",
    "        data[k] = data[k][idx]\n",
    "\n",
    "    y = data[CLASS_FEATURE]\n",
    "    \"\"\" Feature normalization and one hot encoding \"\"\"\n",
    "\n",
    "    print\n",
    "    \"\\nNumber of people recidivating within two years\"\n",
    "    print\n",
    "    pd.Series(y).value_counts()\n",
    "    print\n",
    "    \"\\n\"\n",
    "\n",
    "    X = []  # empty array with num rows same as num examples, will hstack the features to it\n",
    "    X_dims = []\n",
    "\n",
    "    feature_names = []\n",
    "    for attr in FEATURES_CLASSIFICATION:\n",
    "        vals = data[attr]\n",
    "        if attr in CONT_VARIABLES:\n",
    "            vals = [float(v) for v in vals]\n",
    "            # vals = preprocessing.scale(vals, axis=0, with_mean=True, with_std=True)  # 0 mean and 1 variance\n",
    "            vals = np.reshape(vals, (len(y), -1))  # convert from 1-d arr to a 2-d arr with one col\n",
    "            X_dims.append(1)\n",
    "\n",
    "        else:  # for binary categorical variables, the label binarizer uses just one var instead of two\n",
    "            enc = preprocessing.OneHotEncoder(categories='auto', handle_unknown='error')\n",
    "            enc.fit(vals.reshape(-1, 1))\n",
    "            vals = enc.transform(vals.reshape(-1, 1)).todense()\n",
    "            X_dims += [vals.shape[1]]*vals.shape[1]\n",
    "\n",
    "        # add to learnable features\n",
    "        X.append(vals)\n",
    "\n",
    "        if attr in CONT_VARIABLES:  # continuous feature, just append the name\n",
    "            feature_names.append(attr)\n",
    "        else:  # categorical features\n",
    "            if vals.shape[1] == 1:  # binary features that passed through lib binarizer\n",
    "                feature_names.append(attr)\n",
    "            else:\n",
    "                for k in enc.categories_:  # non-binary categorical features, need to add the names for each cat\n",
    "                    feature_names.append(attr + \"_\" + str(k))\n",
    "\n",
    "    X = np.array(np.concatenate(list(X), axis=1))\n",
    "    X_dims = np.array(X_dims)\n",
    "\n",
    "    if separate_test:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=rseed, shuffle=True)\n",
    "\n",
    "        x_means, x_stds = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "        x_means[X_dims>1] = 0\n",
    "        x_stds[X_dims>1] = 1\n",
    "        x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "        x_train = ((X_train - x_means) / x_stds).astype(np.float32)\n",
    "        x_test = ((X_test - x_means) / x_stds).astype(np.float32)\n",
    "\n",
    "        return x_train, x_test, x_means, x_stds, y_train, y_test, feature_names, X_dims\n",
    "    else:\n",
    "        x_means, x_stds = X.mean(axis=0), X.std(axis=0)\n",
    "        x_means[:,X_dims>1] = 0\n",
    "        x_stds[:,X_dims>1] = 1\n",
    "        x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "        x_train = ((X - x_means) / x_stds).astype(np.float32)\n",
    "\n",
    "        return x_train, x_means, x_stds, y, feature_names, X_dims\n",
    "\n",
    "def join_compas_targets(x_train, x_test, y_train, y_test, X_dims):\n",
    "    # output from get method is onehot so we need to flatten and append 2\n",
    "    input_dim_vec = X_dims_to_input_dim_vec(X_dims)\n",
    "    input_dim_vec = np.append(input_dim_vec, 2)\n",
    "    enc = preprocessing.OneHotEncoder(categories='auto', handle_unknown='error')\n",
    "    enc.fit(y_train.reshape(-1, 1))\n",
    "\n",
    "    vals_train = np.array(enc.transform(y_train.reshape(-1, 1)).todense()).astype(np.float32)\n",
    "    vals_test = np.array(enc.transform(y_test.reshape(-1, 1)).todense()).astype(np.float32)\n",
    "\n",
    "    x_train = np.concatenate([x_train, vals_train], axis=1)\n",
    "    x_test = np.concatenate([x_test, vals_test], axis=1)\n",
    "    return x_train, x_test, input_dim_vec\n",
    "\n",
    "def X_dims_to_input_dim_vec(X_dims):\n",
    "    \"\"\"This is for our cat_Gauss VAE model\"\"\"\n",
    "    input_dim_vec = []\n",
    "    i = 0\n",
    "    while i < len(X_dims):\n",
    "        input_dim_vec.append(X_dims[i])\n",
    "        i += X_dims[i]\n",
    "    return np.array(input_dim_vec)\n",
    "\n",
    "#\"\"\"\n",
    "def input_dim_vec_to_X_dims(input_dim_vec):\n",
    "    # This is for our cat_Gauss VAE model\n",
    "    X_dims = []\n",
    "    for i in input_dim_vec:\n",
    "        for ii in range(i):\n",
    "            X_dims.append(i)\n",
    "    return np.array(X_dims)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6079933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20a7336c",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a59efb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default_credit\n"
     ]
    }
   ],
   "source": [
    "# For Default credit\n",
    "\n",
    "input_dim_vec = [1, 2, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ,1 ,1 ,1 ,1 ,1, 2]\n",
    "width = 350\n",
    "depth = 3\n",
    "latent_dim = 8\n",
    "\n",
    "# For all tabular data sets\n",
    "names = ['wine', 'default_credit', 'compas', 'lsat']\n",
    "widths = [350, 350, 350, 350] # Bigger than VAE because the task of modelling all conditionals is more complex\n",
    "depths = [3, 3, 3, 3] # We go deeper because we are using residual models\n",
    "latent_dims = [6, 8, 4, 4]\n",
    "under_latent_dims = [6, 8, 4, 4] # following the original paper we set dim(u) = dim(z) with d>r [r is true manifold dim]\n",
    "under_latent_dims2 = [4, 6, 3, 3] # following the original paper we set dim(u) = dim(z) with d>r [r is true manifold dim]\n",
    "\n",
    "dname = 'default_credit'\n",
    "print(dname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33809880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets in UCI\n",
    "def load_UCI(dset_name, splits=10, seed=0, separate_targets=True, save_dir='data/'):\n",
    "    mkdir(save_dir)\n",
    "\n",
    "    if dset_name == 'wine':\n",
    "        if not os.path.isfile(save_dir+'winequality-red.csv'):\n",
    "            urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\",\n",
    "                               filename=save_dir+'winequality-red.csv')\n",
    "        data = pd.read_csv(save_dir+'winequality-red.csv', header=1, delimiter=';').values\n",
    "        y_idx = [-1]\n",
    "\n",
    "    elif dset_name == 'default_credit':\n",
    "        if not os.path.isfile(save_dir + 'default of credit card clients.xls'):\n",
    "            urllib.request.urlretrieve(\n",
    "                \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\",\n",
    "                filename=save_dir + 'default of credit card clients.xls')\n",
    "        data = pd.read_excel(save_dir + 'default of credit card clients.xls', header=[0, 1], index_col=0, # delimiter=\"\\s+\"\n",
    "                             ).values\n",
    "        y_idx = [-1]  # OK\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise Exception('Dataset name doesnt match any known datasets.')\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    data = data[np.random.permutation(np.arange(len(data)))] #Shuffle the data\n",
    "    \n",
    "    kf = KFold(n_splits=splits)\n",
    "    for j, (train_index, test_index) in enumerate(kf.split(data)):\n",
    "\n",
    "        # Not sure what separate targets is\n",
    "        if separate_targets:\n",
    "            x_idx = list(range(data.shape[1]))\n",
    "            for e in y_idx:\n",
    "                x_idx.remove(x_idx[e])\n",
    "\n",
    "            x_idx = np.array(x_idx)\n",
    "            y_idx = np.array(y_idx)\n",
    "            x_train, y_train = data[train_index, :], data[train_index, :]\n",
    "            x_train, y_train = x_train[:, x_idx], y_train[:, y_idx]\n",
    "            x_test, y_test = data[test_index, :], data[test_index, :]\n",
    "            x_test, y_test = x_test[:, x_idx], y_test[:, y_idx]\n",
    "\n",
    "            x_means, x_stds = x_train.mean(axis=0), x_train.std(axis=0)\n",
    "            y_means, y_stds = y_train.mean(axis=0), y_train.std(axis=0)\n",
    "\n",
    "            y_stds[y_stds < 1e-10] = 1\n",
    "            x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "            x_train = ((x_train - x_means) / x_stds).astype(np.float32)\n",
    "            y_train = ((y_train - y_means) / y_stds).astype(np.float32)\n",
    "\n",
    "            x_test = ((x_test - x_means) / x_stds).astype(np.float32)\n",
    "            y_test = ((y_test - y_means) / y_stds).astype(np.float32)\n",
    "\n",
    "            return x_train, x_test, x_means, x_stds, y_train, y_test, y_means, y_stds\n",
    "\n",
    "        else:\n",
    "            x_train, x_test = data[train_index, :], data[test_index, :]\n",
    "            x_means, x_stds = x_train.mean(axis=0), x_train.std(axis=0)\n",
    "\n",
    "            x_stds[x_stds < 1e-10] = 1\n",
    "\n",
    "            x_train = ((x_train - x_means) / x_stds).astype(np.float32)\n",
    "            x_test = ((x_test - x_means) / x_stds).astype(np.float32)\n",
    "\n",
    "            return x_train, x_test, x_means, x_stds\n",
    "\n",
    "def mkdir(paths):\n",
    "    if not isinstance(paths, (list, tuple)):\n",
    "        paths = [paths]\n",
    "    for path in paths:\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path, mode=0o777)\n",
    "\n",
    "def unnormalise_cat_vars(x, x_means, x_stds, input_dim_vec):\n",
    "    \"\"\"\n",
    "    Converts a feature vector with continous values into a vector with continous and discrete values for those \n",
    "    which come from a categorical class.\n",
    "    \"\"\"\n",
    "    input_dim_vec = np.array(input_dim_vec)\n",
    "    unnorm_x = np.multiply(x, x_stds) + x_means\n",
    "\n",
    "    fixed_unnorm = unnorm_x.round()\n",
    "    fixed_unnorm -= fixed_unnorm.min(axis=0).reshape([1, fixed_unnorm.shape[1]])  # this sets all mins to 0\n",
    "    for idx, dims in enumerate(input_dim_vec):\n",
    "        if dims > 1:\n",
    "            vec = fixed_unnorm[:, idx]\n",
    "            vec[vec > dims - 1] = dims - 1\n",
    "            fixed_unnorm[:, idx] = vec\n",
    "\n",
    "    x[:, input_dim_vec > 1] = fixed_unnorm[:, input_dim_vec > 1]\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c633c97",
   "metadata": {},
   "source": [
    "## Recognition (Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5836060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The arguments sent to the different keras layers are there to mimic the Torch layers in CLUE.\n",
    "\"\"\"\n",
    "\n",
    "def create_recognition_encoder(width, depth, latent_dim, input_dim_vec):\n",
    "    # Tensorflow network as one big Russian doll\n",
    "    nb_inputs = sum(input_dim_vec)\n",
    "    inputs = keras.Input(shape=(nb_inputs,))\n",
    "    #inputs = keras.Input(shape=(None,nb_inputs))\n",
    "    input = layers.Dense(width, use_bias=True, \\\n",
    "                         kernel_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/nb_inputs), math.sqrt(1/nb_inputs)), \\\n",
    "                         bias_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/nb_inputs), math.sqrt(1/nb_inputs))) \\\n",
    "                         (inputs)\n",
    "\n",
    "    for i in range(depth-1):\n",
    "\n",
    "        x = layers.LeakyReLU(alpha=0.01)(input)\n",
    "        x = layers.BatchNormalization(momentum=0.1, epsilon=1e-5)(x)\n",
    "        x = layers.Dense(width, use_bias=True, \\\n",
    "                         kernel_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width)), \\\n",
    "                         bias_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width))) \\\n",
    "                         (x)\n",
    "\n",
    "        # Skip connection \n",
    "        x = x + input\n",
    "\n",
    "        input = x\n",
    "\n",
    "    # Final layers\n",
    "    x = layers.LeakyReLU(alpha=0.01)(x)\n",
    "    x = layers.BatchNormalization(momentum=0.1, epsilon=1e-5)(x)\n",
    "    outputs = layers.Dense(latent_dim*2, use_bias=True, \\\n",
    "                         kernel_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width)), \\\n",
    "                         bias_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width))) \\\n",
    "                         (x)\n",
    "\n",
    "    recognition_encoder = keras.Model(inputs=inputs, outputs=outputs, name=\"recognition_encoder_model\")\n",
    "    return recognition_encoder\n",
    "#recognition_encoder.summary()\n",
    "\n",
    "#keras.utils.plot_model(encoder, \"encoder.png\")\n",
    "#keras.utils.plot_model(recognition_encoder, \"recognition.png\", show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06253187",
   "metadata": {},
   "source": [
    "## Prior network (Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8644fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The arguments sent to the different keras layers are there to mimic the Torch layers in CLUE.\n",
    "\"\"\"\n",
    "\n",
    "def create_prior_encoder(width, depth, latent_dim, input_dim_vec):\n",
    "    nb_inputs = sum(input_dim_vec)*2\n",
    "    inputs = keras.Input(shape=(nb_inputs,))\n",
    "    #inputs = keras.Input(shape=(None,nb_inputs))\n",
    "    input = layers.Dense(width, use_bias=True, \\\n",
    "                         kernel_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/nb_inputs), math.sqrt(1/nb_inputs)), \\\n",
    "                         bias_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/nb_inputs), math.sqrt(1/nb_inputs))) \\\n",
    "                         (inputs)\n",
    "\n",
    "    for i in range(depth-1):\n",
    "\n",
    "        x = layers.LeakyReLU(alpha=0.01)(input)\n",
    "        x = layers.BatchNormalization(momentum=0.1, epsilon=1e-5)(x)\n",
    "        x = layers.Dense(width, use_bias=True, \\\n",
    "                         kernel_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width)), \\\n",
    "                         bias_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width))) \\\n",
    "                         (x)\n",
    "\n",
    "        x = x + input\n",
    "\n",
    "        input = x\n",
    "\n",
    "    x = layers.LeakyReLU(alpha=0.01)(x)\n",
    "    x = layers.BatchNormalization(momentum=0.1, epsilon=1e-5)(x)\n",
    "    outputs = layers.Dense(latent_dim*2, use_bias=True, \\\n",
    "                         kernel_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width)), \\\n",
    "                         bias_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width))) \\\n",
    "                         (x)\n",
    "\n",
    "    prior_encoder = keras.Model(inputs=inputs, outputs=outputs, name=\"prior_encoder_model\")\n",
    "    return prior_encoder\n",
    "#prior_encoder.summary()\n",
    "\n",
    "#keras.utils.plot_model(encoder, \"encoder.png\")\n",
    "#keras.utils.plot_model(prior_encoder, \"prior.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8761740b",
   "metadata": {},
   "source": [
    "## Generator (Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15e47705",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The arguments sent to the different keras layers are there to mimic the Torch layers in CLUE.\n",
    "\"\"\"\n",
    "\n",
    "def create_decoder(width, depth, latent_dim, input_dim_vec):\n",
    "    nb_inputs = latent_dim\n",
    "    inputs = keras.Input(shape=(nb_inputs,))\n",
    "    input = layers.Dense(width, use_bias=True, \\\n",
    "                         kernel_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/nb_inputs), math.sqrt(1/nb_inputs)), \\\n",
    "                         bias_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/nb_inputs), math.sqrt(1/nb_inputs))) \\\n",
    "                         (inputs)\n",
    "\n",
    "    for i in range(depth-1):\n",
    "\n",
    "        x = layers.LeakyReLU(alpha=0.01)(input)\n",
    "        x = layers.BatchNormalization(momentum=0.1, epsilon=1e-5)(x)\n",
    "        x = layers.Dense(width, use_bias=True, \\\n",
    "                         kernel_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width)), \\\n",
    "                         bias_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width))) \\\n",
    "                         (x)\n",
    "        x = x + input\n",
    "\n",
    "        input = x\n",
    "\n",
    "    x = layers.LeakyReLU(alpha=0.01)(x)\n",
    "    x = layers.BatchNormalization(momentum=0.1, epsilon=1e-5)(x)\n",
    "    outputs = layers.Dense(sum(input_dim_vec), use_bias=True, \\\n",
    "                         kernel_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width)), \\\n",
    "                         bias_initializer = tf.keras.initializers.RandomUniform(-math.sqrt(1/width), math.sqrt(1/width))) \\\n",
    "                         (x)\n",
    "\n",
    "    decoder = keras.Model(inputs=inputs, outputs=outputs, name=\"decoder_model\")\n",
    "    return decoder\n",
    "\n",
    "#decoder.summary()\n",
    "\n",
    "#keras.utils.plot_model(model, \"decoder_model.png\")\n",
    "#keras.utils.plot_model(decoder, \"generator.png\", show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f25722",
   "metadata": {},
   "source": [
    "## Masker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ddccccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class top_masker_tensorflow:\n",
    "    \"\"\"\n",
    "    Returned mask is sampled from component-wise independent Bernoulli\n",
    "    distribution with probability of component to be unobserved p.\n",
    "    Such mask induces the type of missingness which is called\n",
    "    in literature \"missing completely at random\" (MCAR).\n",
    "    If some value in batch is missed, it automatically becomes unobserved.\n",
    "    \"\"\"\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            - batch is a numpy array with as many rows as batch_size and as many columns as features\n",
    "        \n",
    "        Returned:\n",
    "       \n",
    "            - mask is a float32 tensor\n",
    "        \n",
    "        The mask seems to be random\n",
    "        \"\"\"\n",
    "        # Generate one uniform number for each row (1xrow numpy matrix)\n",
    "        pp = uniform(low=0.0, high=self.p, size=batch.shape[0]) \n",
    "        pp = np.expand_dims(pp, axis=1) # Put the number in 1x1 matrices in a 1x#row matrix\n",
    "        pp = np.repeat(pp, batch.shape[1], axis=1) # Repeat the number across each row\n",
    "        nan_mask = tf.math.is_nan(batch) # If nan => should be unobserved i.e. boolean True\n",
    "        \n",
    "        # Generate Bernoulli samples (0 or 1) from pp i.e. for each sample in batch determine if a feature is\n",
    "        # observed or hidden.\n",
    "        bernoulli_mask_numpy = binomial(1, pp, size=None) \n",
    "        bernoulli_mask = tf.convert_to_tensor(tf.cast(bernoulli_mask_numpy, tf.bool))\n",
    "        mask = tf.math.logical_or(bernoulli_mask, nan_mask) # Logical or between bernoulli and nan mask\n",
    "        \n",
    "        # Logical not to invert the mask (This is done in CLUE)\n",
    "        # Mask is converted to a boolean tensor with floats for element wise multiplication with the batch\n",
    "        # which is done in apply mask\n",
    "        # (True => 0, False => 1)\n",
    "        \n",
    "        # TODO: The logical_not might be unnecessary as the probability of getting a true or false is equal.\n",
    "        #      To mirror the Torch code however I did this but it can perhaps be removed later...\n",
    "        \n",
    "        return tf.cast(tf.math.logical_not(mask), dtype=tf.float32)\n",
    "        #return tf.cast(mask, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510e8c65",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02f82868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_cat_to_flat_mask(mask, input_dim_vec):\n",
    "    output = []\n",
    "    for idx, dim in enumerate(input_dim_vec):\n",
    "        if dim == 1:\n",
    "            \"\"\"\n",
    "            tf.expand_dims (axis=1) takes mask[:, idx] (batch_size,) and \n",
    "            converts it into (64,1) e.g. [1,2,3] => [[1];[2];[3]] i.e. same as torch unsqueeze(1)\n",
    "            \"\"\"\n",
    "            output.append(tf.expand_dims(mask[:, idx], axis=1))\n",
    "\n",
    "        elif dim > 1: \n",
    "            \"\"\"\n",
    "            tf.expand_dims (read comment above)\n",
    "            tf.ones([mask.shape[0], dim]) creates an array of batch_size x dim with ones\n",
    "            oh_vec will be mask.shape[0] x dim and contain 0 or 1 on rows depending on if mask is 0 or 1.\n",
    "            \"\"\"\n",
    "            oh_vec = tf.ones([mask.shape[0], dim]) * tf.expand_dims(mask[:, idx], axis=1)\n",
    "\n",
    "            output.append(oh_vec)\n",
    "        else:\n",
    "            raise ValueError('Error, invalid dimension value')\n",
    "    return tf.concat(output, axis=1)\n",
    "\n",
    "def gauss_cat_to_flat(x, input_dim_vec):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - x: numpy array\n",
    "        - input_dim_vec: list e.g. [1, 2, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ,1 ,1 ,1 ,1 ,1, 2] credit\n",
    "    Returns:\n",
    "        - numpy array \n",
    "        \n",
    "    Example:\n",
    "        \n",
    "        x:\n",
    "             [-0.52121574  0.          2.          1.          1.2496392   0.01383046\n",
    "              0.1105278   1.8173771   0.18815508  0.2341654   1.9953084   0.2038664\n",
    "              0.31341553  0.31455126  0.32473356  0.4501966   0.45570025  0.0774322\n",
    "             -0.2517514  -0.1535475   0.03951775 -0.31174627 -0.12532774  0.        ]\n",
    "        \n",
    "        input_dim_vec:\n",
    "            [1, 2, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ,1 ,1 ,1 ,1 ,1, 2]\n",
    "        \n",
    "        return:\n",
    "             [-0.52121574  1.          0.          0.          0.          1.\n",
    "              0.          0.          1.          0.          1.2496392   0.01383046\n",
    "              0.1105278   1.8173771   0.18815508  0.2341654   1.9953084   0.2038664\n",
    "              0.31341553  0.31455126  0.32473356  0.4501966   0.45570025  0.0774322\n",
    "             -0.2517514  -0.1535475   0.03951775 -0.31174627 -0.12532774  1.\n",
    "              0.        ]\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    for idx, dim in enumerate(input_dim_vec):\n",
    "        if dim == 1:\n",
    "            output.append(tf.expand_dims(x[:, idx], axis=1))\n",
    "        elif dim > 1:\n",
    "            oh_vec = tf.one_hot(x[:, idx], dim) # Returns one hot encoding 0 with dim 2 -> 1 0, 1 -> 0 1\n",
    "            output.append(oh_vec)\n",
    "        else:\n",
    "            raise ValueError('Error, invalid dimension value')\n",
    "    return tf.concat(output, axis=1).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abe98e6",
   "metadata": {},
   "source": [
    "## CLASS VAEAC and loss and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fc4120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEAC_gauss_cat(tf.keras.Model):\n",
    "    def __init__(self, width, depth, latent_dim, input_dim_vec, batch_size, lr, optimizer, save_model):\n",
    "        super(VAEAC_gauss_cat, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.input_dim_vec = input_dim_vec\n",
    "        self.recognition_encoder = create_recognition_encoder(width, depth, latent_dim, input_dim_vec)\n",
    "        self.prior_encoder = create_prior_encoder(width, depth, latent_dim, input_dim_vec)\n",
    "        self.decoder = create_decoder(width, depth, latent_dim, input_dim_vec)\n",
    "        self.sigma_mu = 1e4\n",
    "        self.sigma_sigma = 1e-4\n",
    "        self.vlb_scale = 1 / len(self.input_dim_vec)\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer\n",
    "        self.save_model = save_model\n",
    "        self.lr = lr\n",
    "\n",
    "    # Inspiration taken from \n",
    "    # https://github.com/joocxi/tf2-VAEAC/blob/d2b1bbc258ec77ee0975ea7eb68e63c4efcda6f0/model/vaeac.py\n",
    "    def prior_regularizer(self, prior):\n",
    "\n",
    "        mu = tf.reshape(prior.mean(), (self.batch_size, -1))\n",
    "        sigma = tf.reshape(prior.scale, (self.batch_size, -1))\n",
    "\n",
    "        mu_regularizer = -tf.reduce_sum(tf.square(mu), -1) / (2 * self.sigma_mu ** 2)\n",
    "        sigma_regularizer = tf.reduce_sum((tf.math.log(sigma) - sigma), -1) * self.sigma_sigma\n",
    "        return mu_regularizer + sigma_regularizer\n",
    "\n",
    "    def apply_mask(self, x, mask):\n",
    "        return x * mask\n",
    "\n",
    "    def reconstruction_loss(self, x, y):\n",
    "        log_prob_vec = []\n",
    "        \n",
    "        cum_dims = 0\n",
    "        reshape_dim = self.batch_size\n",
    "        for idx, dims in enumerate(self.input_dim_vec):\n",
    "            if dims == 1:\n",
    "                # Gaussian_case\n",
    "                log_prob_vec.append(tf.expand_dims(-(x[:, cum_dims] - y[:, cum_dims])**2, 1))\n",
    "                \n",
    "                cum_dims += 1\n",
    "\n",
    "            elif dims > 1:\n",
    "                # if x.shape[1] == y.shape[1]:\n",
    "                #    raise Exception('Input and target seem to be in flat format. Need integer cat targets.'\n",
    "\n",
    "                cce = tf.keras.losses.CategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE, from_logits = True)\n",
    "                cat_cross_entropy = -cce(tf.cast(y[:, cum_dims:cum_dims + dims], dtype=tf.int64), x[:, cum_dims:cum_dims + dims])\n",
    "                #cat_cross_entropy = -tf.nn.softmax_cross_entropy_with_logits(labels=tf.cast(y[:, cum_dims:cum_dims + dims], dtype=tf.int64), logits=x[:, cum_dims:cum_dims + dims])\n",
    "                log_prob_vec.append(tf.expand_dims(cat_cross_entropy, 1))\n",
    "                cum_dims += dims\n",
    "                \n",
    "            else:\n",
    "                raise ValueError('Error, invalid dimension value')\n",
    "        \n",
    "        log_prob_vec = tf.reshape(log_prob_vec, [reshape_dim, len(self.input_dim_vec)])\n",
    "        log_prob_vec = tf.reduce_sum(log_prob_vec, axis= -1)     # Do I want this? \n",
    "                                                                 # Yes vlb in original code does this when return\n",
    "        return log_prob_vec\n",
    "\n",
    "def eval(model, x_batch, x_flat, x_masked, mask):\n",
    "\n",
    "    x_flat = tf.convert_to_tensor(x_flat)\n",
    "\n",
    "    prior_params = model.prior_encoder(x_masked)\n",
    "    #prior_params = model.prior_encoder(x_masked, training = True)\n",
    "    #prior_params = model.prior_encoder(x_masked, training = False)\n",
    "\n",
    "    proposal_params = model.recognition_encoder(x_flat)\n",
    "    #proposal_params = model.recognition_encoder(x_flat, training = True)\n",
    "    #proposal_params = model.recognition_encoder(x_flat, training = False)\n",
    "\n",
    "    # Essentially CLUEs normal_parse_params\n",
    "    prior_distribution = tfd.Normal(\n",
    "      loc=prior_params[..., :model.latent_dim],\n",
    "      scale=tf.clip_by_value(\n",
    "        tf.nn.softplus(prior_params[..., model.latent_dim:]),\n",
    "        1e-3,\n",
    "        tf.float32.max),\n",
    "      name=\"priors\")\n",
    "\n",
    "    proposal_distribution = tfd.Normal(\n",
    "      loc=proposal_params[..., :model.latent_dim],\n",
    "      scale=tf.clip_by_value(\n",
    "        tf.nn.softplus(proposal_params[..., model.latent_dim:]),\n",
    "        1e-3,\n",
    "        tf.float32.max),\n",
    "      name=\"proposal\")\n",
    "\n",
    "    z_sample = proposal_distribution.loc\n",
    "\n",
    "    rec_params = model.decoder(z_sample)\n",
    "    #rec_params = model.decoder(z_sample, training = True)\n",
    "    #rec_params = model.decoder(z_sample, training = False)\n",
    "    \n",
    "    regularizer = model.prior_regularizer(prior_distribution)\n",
    "\n",
    "    rec_loss = model.reconstruction_loss(rec_params, x_flat)\n",
    "\n",
    "    # True distribution, Estimated distribution in this order\n",
    "    kl_divergence = tf.reduce_sum(\n",
    "      tf.reshape(\n",
    "        tfd.kl_divergence(proposal_distribution, prior_distribution),\n",
    "        (model.batch_size, -1)), -1)\n",
    "\n",
    "    vlb = tf.reduce_mean(-kl_divergence + rec_loss + regularizer) # For comparing\n",
    "    return vlb, kl_divergence, rec_loss, regularizer\n",
    "\n",
    "def compute_loss_VAEAC(model, x_batch, x_flat, x_masked, mask):\n",
    "    \n",
    "    prior_params = model.prior_encoder(x_masked) \n",
    "    proposal_params = model.recognition_encoder(x_flat)\n",
    "    #prior_params = model.prior_encoder(x_masked, training = True) \n",
    "    #proposal_params = model.recognition_encoder(x_flat, training = True)\n",
    "    #prior_params = model.prior_encoder(x_masked, training = False) \n",
    "    #proposal_params = model.recognition_encoder(x_flat, training = False)\n",
    "\n",
    "    # Essentially CLUEs normal_parse_params\n",
    "    proposal_distribution = tfd.Normal(\n",
    "      loc=proposal_params[..., :model.latent_dim],\n",
    "      scale=tf.clip_by_value(\n",
    "        tf.nn.softplus(proposal_params[..., model.latent_dim:]),\n",
    "        1e-3,\n",
    "        tf.float32.max),\n",
    "      name=\"proposal\")\n",
    "\n",
    "    prior_distribution = tfd.Normal(\n",
    "      loc=prior_params[..., :model.latent_dim],\n",
    "      scale=tf.clip_by_value(\n",
    "        tf.nn.softplus(prior_params[..., model.latent_dim:]),\n",
    "        1e-3,\n",
    "        tf.float32.max),\n",
    "      name=\"priors\")\n",
    "\n",
    "    regularizer = model.prior_regularizer(prior_distribution)\n",
    "\n",
    "    latent = proposal_distribution.sample()\n",
    "\n",
    "    generative_params = model.decoder(latent)\n",
    "    #generative_params = model.decoder(latent, training = True)\n",
    "    #generative_params = model.decoder(latent, training = False)\n",
    "    \n",
    "    rec_loss = model.reconstruction_loss(generative_params, x_flat)\n",
    "    \n",
    "    # True distribution, Estimated distribution in this order\n",
    "    kl_divergence = tf.reduce_sum(\n",
    "      tf.reshape(\n",
    "        tfd.kl_divergence(proposal_distribution, prior_distribution),\n",
    "        (model.batch_size, -1)), -1)\n",
    "\n",
    "    vlb = tf.reduce_mean(-kl_divergence + rec_loss + regularizer) # For comparing\n",
    "    loss = tf.reduce_mean((kl_divergence - rec_loss - regularizer) * model.vlb_scale) \n",
    "    return loss, vlb, kl_divergence, rec_loss, regularizer\n",
    "\n",
    "@tf.function # Converts all numpy arrays to tensors\n",
    "def train_step_VAEAC(model, x_batch, x_flat, x_masked, mask):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, vlb, kl_divergence, rec_loss, regularizer = compute_loss_VAEAC(model, x_batch, x_flat, x_masked, mask)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, vlb, kl_divergence, rec_loss, regularizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6d5c7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "def train_VAEAC(model, x_train, x_test, masker, nb_epochs, early_stop = None, flatten=False):\n",
    "    \n",
    "    vlb_train = np.zeros(nb_epochs)\n",
    "    vlb_val = np.zeros(nb_epochs)\n",
    "    best_vlb = -np.inf\n",
    "    best_epoch = 0\n",
    "    \n",
    "    overall_batch_size = model.batch_size\n",
    "    \n",
    "    test_data = []\n",
    "    for x in batch(x_test, n = overall_batch_size):\n",
    "        test_data.append(x)\n",
    "    \n",
    "    epoch = 0\n",
    "    for epoch in range(0, nb_epochs):\n",
    "        \n",
    "        # Shuffle the training data and sort it into batches every epoch\n",
    "        train_data = []\n",
    "        np.random.shuffle(x_train)\n",
    "        for x in batch(x_train, n = overall_batch_size):\n",
    "            train_data.append(x)\n",
    "        \n",
    "        tic = time.time()\n",
    "\n",
    "        ## Training\n",
    "        nb_samples = 0\n",
    "        \n",
    "        for x_batch in train_data:\n",
    "\n",
    "            model.batch_size = x_batch.shape[0] # TODO: FIX THIS: Very ugly solution now to make sure batches \n",
    "                                                # that do not have the full size\n",
    "                \n",
    "            mask = masker(x_batch) #tensor with floats\n",
    "            \n",
    "            # If data is not already flattened (default credit)\n",
    "            if flatten:\n",
    "                x_batch_flat = gauss_cat_to_flat(x_batch, model.input_dim_vec) # numpy\n",
    "                mask_flat = gauss_cat_to_flat_mask(mask, model.input_dim_vec)\n",
    "            \n",
    "            # If data is already flattened (COMPAS from join_compas_targets)\n",
    "            else:\n",
    "                x_batch_flat = x_batch\n",
    "                mask_flat = mask\n",
    "\n",
    "            # Mask flattened batch\n",
    "            x_batch_flat_masked = model.apply_mask(tf.convert_to_tensor(x_batch_flat), mask_flat)\n",
    "            \n",
    "            # Concat the mask flattened batch with the flattened mask\n",
    "            x_batch_flat_masked_concat = tf.concat([x_batch_flat_masked, mask_flat], axis=1)\n",
    "            \n",
    "            loss, vlb, kl_divergence, rec_loss, regularizer = train_step_VAEAC(model, x_batch, x_batch_flat, x_batch_flat_masked_concat, mask_flat)\n",
    "\n",
    "            vlb_train[epoch] += vlb.numpy() * x_batch.shape[0]\n",
    "            nb_samples += x_batch.shape[0]\n",
    "\n",
    "        vlb_train[epoch] /= nb_samples\n",
    "        toc = time.time()\n",
    "        print(\"Epoch_\" + str(epoch) + \", vlb: \" + str(vlb_train[epoch]) + \", took: \" + str(toc-tic))\n",
    "        \n",
    "        ## Validation\n",
    "        nb_samples = 0\n",
    "        for x_batch in test_data:\n",
    "            model.batch_size = x_batch.shape[0] # TODO: FIX THIS: Very ugly solution now to make sure batches \n",
    "                                                # that do not have the full size\n",
    "\n",
    "            mask = masker(x_batch) #tensor with floats\n",
    "            \n",
    "            # If data is not already flattened (default credit)\n",
    "            if flatten:\n",
    "                x_batch_flat = gauss_cat_to_flat(x_batch, model.input_dim_vec) # numpy\n",
    "                mask_flat = gauss_cat_to_flat_mask(mask, model.input_dim_vec)\n",
    "            \n",
    "            # If data is already flattened (COMPAS from join_compas_targets)\n",
    "            else:\n",
    "                x_batch_flat = x_batch\n",
    "                mask_flat = mask\n",
    "            \n",
    "            # Mask flattened batch\n",
    "            x_batch_flat_masked = model.apply_mask(tf.convert_to_tensor(x_batch_flat), mask_flat)\n",
    "            \n",
    "            # Concat the mask flattened batch with the flattened mask\n",
    "            x_batch_flat_masked_concat = tf.concat([x_batch_flat_masked, mask_flat], axis=1)\n",
    "            \n",
    "            vlb, kl_divergence, rec_loss, regularizer = eval(model, x_batch, x_batch_flat, x_batch_flat_masked_concat, mask_flat)\n",
    "\n",
    "            vlb_val[epoch] += vlb.numpy() * x_batch.shape[0]\n",
    "            nb_samples += x_batch.shape[0]\n",
    "\n",
    "        vlb_val[epoch] /= nb_samples\n",
    "    \n",
    "        \n",
    "        if vlb_val[epoch] > best_vlb:\n",
    "            best_vlb = vlb_val[epoch]\n",
    "            best_epoch = epoch\n",
    "            if(model.save_model):\n",
    "                \n",
    "                folder = './tf_vaeac_vlb_plots/'\n",
    "                #open text file\n",
    "                text_file = open(str(dname) + \"_best_epoch_VAEAC_lr_\" + str(model.lr) + \".txt\", \"w\")\n",
    "\n",
    "                #write string to file\n",
    "                text_file.write(str(epoch))\n",
    "\n",
    "                #close file\n",
    "                text_file.close()\n",
    "\n",
    "                model.recognition_encoder.save(folder + str(dname) + \"_recog_encoder_lr_\" + str(model.lr))\n",
    "                model.prior_encoder.save(folder + str(dname) + \"_prior_encoder_lr_\" + str(model.lr))\n",
    "                model.decoder.save(folder + str(dname) + \"_decoder_lr_\" + str(model.lr))\n",
    "\n",
    "        print(\"Validation vlb: \" + str(vlb_val[epoch]) + \", Best vlb: \" + str(best_vlb) + \"\\n\")\n",
    "\n",
    "        if early_stop is not None and (epoch - best_epoch) > early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    \n",
    "    np.save(folder + str(dname) + \"_vlb_train_lr_\" + str(model.lr), vlb_train)\n",
    "    np.save(folder + str(dname) + \"_vlb_val_lr_\" + str(model.lr), vlb_val)\n",
    "    return vlb_train, vlb_val, best_epoch, best_vlb, epoch\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c94ecff",
   "metadata": {},
   "source": [
    "## Train VAEAC (COMPAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d62a7519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compas (5554, 19) (618, 19)\n",
      "[3 6 2 2 2 1 1 2]\n",
      "compas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow_addons/optimizers/rectified_adam.py:120: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "2021-12-13 11:17:01.166531: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_0, vlb: -8.143677273112576, took: 10.626796960830688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 11:17:13.273375: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation vlb: -8.124127190475711, Best vlb: -8.124127190475711\n",
      "\n",
      "Epoch_1, vlb: -7.613284927501411, took: 1.0337178707122803\n",
      "Validation vlb: -7.7779282319892955, Best vlb: -7.7779282319892955\n",
      "\n",
      "Epoch_2, vlb: -7.219140895545419, took: 1.014375925064087\n",
      "Validation vlb: -7.432279981841547, Best vlb: -7.432279981841547\n",
      "\n",
      "Epoch_3, vlb: -6.82585625258718, took: 1.014449119567871\n",
      "Validation vlb: -6.86813160979632, Best vlb: -6.86813160979632\n",
      "\n",
      "Epoch_4, vlb: -6.3325713332454034, took: 1.342561960220337\n",
      "Validation vlb: -5.953317009515361, Best vlb: -5.953317009515361\n",
      "\n",
      "Epoch_5, vlb: -5.807810050873693, took: 1.593484878540039\n",
      "Validation vlb: -5.2756770760496075, Best vlb: -5.2756770760496075\n",
      "\n",
      "Epoch_6, vlb: -5.363670950104188, took: 1.4641640186309814\n",
      "Validation vlb: -4.739931446063094, Best vlb: -4.739931446063094\n",
      "\n",
      "Epoch_7, vlb: -5.097824575749069, took: 1.2327730655670166\n",
      "Validation vlb: -4.47055318980541, Best vlb: -4.47055318980541\n",
      "\n",
      "Epoch_8, vlb: -4.901342311607748, took: 1.1121530532836914\n",
      "Validation vlb: -4.279278102430325, Best vlb: -4.279278102430325\n",
      "\n",
      "Epoch_9, vlb: -4.690525168088172, took: 1.0359764099121094\n",
      "Validation vlb: -4.173233323884242, Best vlb: -4.173233323884242\n",
      "\n",
      "Epoch_10, vlb: -4.619533179945597, took: 1.044771671295166\n",
      "Validation vlb: -4.086861734637165, Best vlb: -4.086861734637165\n",
      "\n",
      "Epoch_11, vlb: -4.48119632091117, took: 1.1245219707489014\n",
      "Validation vlb: -4.045274221395598, Best vlb: -4.045274221395598\n",
      "\n",
      "Epoch_12, vlb: -4.412518318325313, took: 1.017195701599121\n",
      "Validation vlb: -3.860253638048388, Best vlb: -3.860253638048388\n",
      "\n",
      "Epoch_13, vlb: -4.3536333859202925, took: 1.043431043624878\n",
      "Validation vlb: -3.6681314664365403, Best vlb: -3.6681314664365403\n",
      "\n",
      "Epoch_14, vlb: -4.282139815398159, took: 1.1296560764312744\n",
      "Validation vlb: -3.6510470384147173, Best vlb: -3.6510470384147173\n",
      "\n",
      "Epoch_15, vlb: -4.172672879494134, took: 1.035933017730713\n",
      "Validation vlb: -3.471943152375206, Best vlb: -3.471943152375206\n",
      "\n",
      "Epoch_16, vlb: -4.0932895720842115, took: 1.04762601852417\n",
      "Validation vlb: -3.393547759472745, Best vlb: -3.393547759472745\n",
      "\n",
      "Epoch_17, vlb: -4.043039096530753, took: 1.5873279571533203\n",
      "Validation vlb: -3.237803241581593, Best vlb: -3.237803241581593\n",
      "\n",
      "Epoch_18, vlb: -3.981936598665473, took: 1.4247419834136963\n",
      "Validation vlb: -3.2253221377585697, Best vlb: -3.2253221377585697\n",
      "\n",
      "Epoch_19, vlb: -3.927089148045101, took: 1.0971009731292725\n",
      "Validation vlb: -3.194704014892331, Best vlb: -3.194704014892331\n",
      "\n",
      "Epoch_20, vlb: -3.8840258336985847, took: 1.023465871810913\n",
      "Validation vlb: -2.9946671774472233, Best vlb: -2.9946671774472233\n",
      "\n",
      "Epoch_21, vlb: -3.8502487075985616, took: 1.2374539375305176\n",
      "Validation vlb: -3.0399427691709646, Best vlb: -2.9946671774472233\n",
      "\n",
      "Epoch_22, vlb: -3.8064554389277765, took: 1.322605848312378\n",
      "Validation vlb: -3.0251254857936725, Best vlb: -2.9946671774472233\n",
      "\n",
      "Epoch_23, vlb: -3.8348529665084636, took: 1.0240318775177002\n",
      "Validation vlb: -2.958949770356459, Best vlb: -2.958949770356459\n",
      "\n",
      "Epoch_24, vlb: -3.769347521225926, took: 1.0253512859344482\n",
      "Validation vlb: -2.99344702291643, Best vlb: -2.958949770356459\n",
      "\n",
      "Epoch_25, vlb: -3.7036520268876734, took: 1.0416409969329834\n",
      "Validation vlb: -2.9746319244594637, Best vlb: -2.958949770356459\n",
      "\n",
      "Epoch_26, vlb: -3.6931081205381093, took: 1.086148977279663\n",
      "Validation vlb: -3.0704071760949194, Best vlb: -2.958949770356459\n",
      "\n",
      "Epoch_27, vlb: -3.6501885645244463, took: 1.0878472328186035\n",
      "Validation vlb: -2.87769419784299, Best vlb: -2.87769419784299\n",
      "\n",
      "Epoch_28, vlb: -3.6349445433336753, took: 1.399505853652954\n",
      "Validation vlb: -2.8559615295681753, Best vlb: -2.8559615295681753\n",
      "\n",
      "Epoch_29, vlb: -3.6238297274269815, took: 1.6071579456329346\n",
      "Validation vlb: -2.973505854220838, Best vlb: -2.8559615295681753\n",
      "\n",
      "Epoch_30, vlb: -3.631272568220194, took: 1.450577974319458\n",
      "Validation vlb: -2.9452725521569114, Best vlb: -2.8559615295681753\n",
      "\n",
      "Epoch_31, vlb: -3.5643841990215726, took: 1.4197959899902344\n",
      "Validation vlb: -2.893706052434483, Best vlb: -2.8559615295681753\n",
      "\n",
      "Epoch_32, vlb: -3.581122041907635, took: 1.7501916885375977\n",
      "Validation vlb: -2.871216267829574, Best vlb: -2.8559615295681753\n",
      "\n",
      "Epoch_33, vlb: -3.554448569039883, took: 1.2605717182159424\n",
      "Validation vlb: -2.741089394177434, Best vlb: -2.741089394177434\n",
      "\n",
      "Epoch_34, vlb: -3.542226019204414, took: 1.2926278114318848\n",
      "Validation vlb: -2.6737184902610904, Best vlb: -2.6737184902610904\n",
      "\n",
      "Epoch_35, vlb: -3.493519639041641, took: 1.2623920440673828\n",
      "Validation vlb: -2.7900249826869534, Best vlb: -2.6737184902610904\n",
      "\n",
      "Epoch_36, vlb: -3.514711991462254, took: 1.2940051555633545\n",
      "Validation vlb: -2.780716872138113, Best vlb: -2.6737184902610904\n",
      "\n",
      "Epoch_37, vlb: -3.469779124481454, took: 1.2447638511657715\n",
      "Validation vlb: -2.757505911453642, Best vlb: -2.6737184902610904\n",
      "\n",
      "Epoch_38, vlb: -3.5164430642840583, took: 1.1112010478973389\n",
      "Validation vlb: -2.8345729155062087, Best vlb: -2.6737184902610904\n",
      "\n",
      "Epoch_39, vlb: -3.483709665696141, took: 1.0249710083007812\n",
      "Validation vlb: -2.6623314815817527, Best vlb: -2.6623314815817527\n",
      "\n",
      "Epoch_40, vlb: -3.465657798858439, took: 1.0295679569244385\n",
      "Validation vlb: -2.7125474673644625, Best vlb: -2.6623314815817527\n",
      "\n",
      "Epoch_41, vlb: -3.460061628010619, took: 1.0880157947540283\n",
      "Validation vlb: -2.710516564668575, Best vlb: -2.6623314815817527\n",
      "\n",
      "Epoch_42, vlb: -3.450889171951316, took: 1.085693120956421\n",
      "Validation vlb: -2.7579398348107693, Best vlb: -2.6623314815817527\n",
      "\n",
      "Epoch_43, vlb: -3.424012946704759, took: 1.216799259185791\n",
      "Validation vlb: -2.7594896265603963, Best vlb: -2.6623314815817527\n",
      "\n",
      "Epoch_44, vlb: -3.440367383754292, took: 1.2298626899719238\n",
      "Validation vlb: -2.7325577288383807, Best vlb: -2.6623314815817527\n",
      "\n",
      "Epoch_45, vlb: -3.4120063737093824, took: 1.0922250747680664\n",
      "Validation vlb: -2.697301044433248, Best vlb: -2.6623314815817527\n",
      "\n",
      "Epoch_46, vlb: -3.406563305794699, took: 1.1466619968414307\n",
      "Validation vlb: -2.704504377247832, Best vlb: -2.6623314815817527\n",
      "\n",
      "Epoch_47, vlb: -3.3528266640760336, took: 1.0379610061645508\n",
      "Validation vlb: -2.752368504175476, Best vlb: -2.6623314815817527\n",
      "\n",
      "Epoch_48, vlb: -3.3892101215552115, took: 1.1073729991912842\n",
      "Validation vlb: -2.5984885638585755, Best vlb: -2.5984885638585755\n",
      "\n",
      "Epoch_49, vlb: -3.3587741216481657, took: 1.0198557376861572\n",
      "Validation vlb: -2.630496836788832, Best vlb: -2.5984885638585755\n",
      "\n",
      "Epoch_50, vlb: -3.2975362849656222, took: 1.0345377922058105\n",
      "Validation vlb: -2.52262814839681, Best vlb: -2.52262814839681\n",
      "\n",
      "Epoch_51, vlb: -3.281020440426841, took: 1.1865720748901367\n",
      "Validation vlb: -2.728591683613058, Best vlb: -2.52262814839681\n",
      "\n",
      "Epoch_52, vlb: -3.2904313845217206, took: 1.0259919166564941\n",
      "Validation vlb: -2.673279087906131, Best vlb: -2.52262814839681\n",
      "\n",
      "Epoch_53, vlb: -3.293861330864796, took: 1.1006519794464111\n",
      "Validation vlb: -2.6322924334640256, Best vlb: -2.52262814839681\n",
      "\n",
      "Epoch_54, vlb: -3.2999368866957215, took: 1.0831868648529053\n",
      "Validation vlb: -2.5954282546120555, Best vlb: -2.52262814839681\n",
      "\n",
      "Epoch_55, vlb: -3.304759921124396, took: 1.0384230613708496\n",
      "Validation vlb: -2.527476936482303, Best vlb: -2.52262814839681\n",
      "\n",
      "Epoch_56, vlb: -3.2452203613586206, took: 1.0197060108184814\n",
      "Validation vlb: -2.595249378951236, Best vlb: -2.52262814839681\n",
      "\n",
      "Epoch_57, vlb: -3.2241857765302155, took: 1.0279450416564941\n",
      "Validation vlb: -2.6226424098400622, Best vlb: -2.52262814839681\n",
      "\n",
      "Epoch_58, vlb: -3.2446609288615034, took: 1.045168161392212\n",
      "Validation vlb: -2.571220709668008, Best vlb: -2.52262814839681\n",
      "\n",
      "Epoch_59, vlb: -3.2118536311600505, took: 1.1476678848266602\n",
      "Validation vlb: -2.555851947142468, Best vlb: -2.52262814839681\n",
      "\n",
      "Epoch_60, vlb: -3.230155399045714, took: 1.081421136856079\n",
      "Validation vlb: -2.574773019957311, Best vlb: -2.52262814839681\n",
      "\n",
      "Epoch_61, vlb: -3.193879190433957, took: 1.0385208129882812\n",
      "Validation vlb: -2.5599306023236617, Best vlb: -2.52262814839681\n",
      "\n",
      "Epoch_62, vlb: -3.1754813171094125, took: 1.017388105392456\n",
      "Validation vlb: -2.6329538837605697, Best vlb: -2.52262814839681\n",
      "\n",
      "Epoch_63, vlb: -3.1856843691143495, took: 0.9857888221740723\n",
      "Validation vlb: -2.5315507183568764, Best vlb: -2.52262814839681\n",
      "\n",
      "Epoch_64, vlb: -3.135700590712671, took: 1.020989179611206\n",
      "Validation vlb: -2.462376917064383, Best vlb: -2.462376917064383\n",
      "\n",
      "Epoch_65, vlb: -3.1521599877425825, took: 1.090587854385376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation vlb: -2.572693498389235, Best vlb: -2.462376917064383\n",
      "\n",
      "Epoch_66, vlb: -3.1891672301301006, took: 1.0536541938781738\n",
      "Validation vlb: -2.5925952037947075, Best vlb: -2.462376917064383\n",
      "\n",
      "Epoch_67, vlb: -3.1259088682793523, took: 1.0474879741668701\n",
      "Validation vlb: -2.6697755916990507, Best vlb: -2.462376917064383\n",
      "\n",
      "Epoch_68, vlb: -3.1271276756461077, took: 1.028730869293213\n",
      "Validation vlb: -2.6046917361348965, Best vlb: -2.462376917064383\n",
      "\n",
      "Epoch_69, vlb: -3.0812281068383593, took: 1.033923864364624\n",
      "Validation vlb: -2.408326956060712, Best vlb: -2.408326956060712\n",
      "\n",
      "Epoch_70, vlb: -3.102765930870861, took: 1.0437211990356445\n",
      "Validation vlb: -2.539379981729205, Best vlb: -2.408326956060712\n",
      "\n",
      "Epoch_71, vlb: -3.129172302897903, took: 1.0385541915893555\n",
      "Validation vlb: -2.3741589541574126, Best vlb: -2.3741589541574126\n",
      "\n",
      "Epoch_72, vlb: -3.0838224541853347, took: 1.086198091506958\n",
      "Validation vlb: -2.421703411923257, Best vlb: -2.3741589541574126\n",
      "\n",
      "Epoch_73, vlb: -3.0918045778003234, took: 1.0665559768676758\n",
      "Validation vlb: -2.4485843397652833, Best vlb: -2.3741589541574126\n",
      "\n",
      "Epoch_74, vlb: -3.114392243145952, took: 1.5514349937438965\n",
      "Validation vlb: -2.484001284664117, Best vlb: -2.3741589541574126\n",
      "\n",
      "Epoch_75, vlb: -3.0386306771624203, took: 1.3987441062927246\n",
      "Validation vlb: -2.4754772356027153, Best vlb: -2.3741589541574126\n",
      "\n",
      "Epoch_76, vlb: -3.1019247934721252, took: 1.3138298988342285\n",
      "Validation vlb: -2.4683000393284176, Best vlb: -2.3741589541574126\n",
      "\n",
      "Epoch_77, vlb: -3.047538705210399, took: 1.716468095779419\n",
      "Validation vlb: -2.552364579296421, Best vlb: -2.3741589541574126\n",
      "\n",
      "Epoch_78, vlb: -3.039991250345507, took: 1.376816749572754\n",
      "Validation vlb: -2.491870089256262, Best vlb: -2.3741589541574126\n",
      "\n",
      "Epoch_79, vlb: -3.0505442588416027, took: 1.8707029819488525\n",
      "Validation vlb: -2.615078047255482, Best vlb: -2.3741589541574126\n",
      "\n",
      "Epoch_80, vlb: -3.06059328300042, took: 1.281325101852417\n",
      "Validation vlb: -2.4577137652338514, Best vlb: -2.3741589541574126\n",
      "\n",
      "Epoch_81, vlb: -3.0344226963021232, took: 1.3089320659637451\n",
      "Validation vlb: -2.4263600901878384, Best vlb: -2.3741589541574126\n",
      "\n",
      "Epoch_82, vlb: -2.9989323846118503, took: 1.397803783416748\n",
      "Validation vlb: -2.416336365116453, Best vlb: -2.3741589541574126\n",
      "\n",
      "Epoch_83, vlb: -3.043794559668327, took: 1.20210599899292\n",
      "Validation vlb: -2.3753820692451253, Best vlb: -2.3741589541574126\n",
      "\n",
      "Epoch_84, vlb: -3.0068560839128615, took: 1.770310878753662\n",
      "Validation vlb: -2.468763700195115, Best vlb: -2.3741589541574126\n",
      "\n",
      "Epoch_85, vlb: -3.0625631789488033, took: 1.7696330547332764\n",
      "Validation vlb: -2.3969620747859426, Best vlb: -2.3741589541574126\n",
      "\n",
      "Epoch_86, vlb: -3.0440942894266283, took: 1.5209109783172607\n",
      "Validation vlb: -2.46501868905373, Best vlb: -2.3741589541574126\n",
      "\n",
      "Epoch_87, vlb: -3.0070102571110353, took: 1.7151639461517334\n",
      "Validation vlb: -2.4791116598740364, Best vlb: -2.3741589541574126\n",
      "\n",
      "Epoch_88, vlb: -3.0226891730215466, took: 1.6246099472045898\n",
      "Validation vlb: -2.3472195396917153, Best vlb: -2.3472195396917153\n",
      "\n",
      "Epoch_89, vlb: -2.9949202792525162, took: 1.8244061470031738\n",
      "Validation vlb: -2.443667625532181, Best vlb: -2.3472195396917153\n",
      "\n",
      "Epoch_90, vlb: -2.9991725235479874, took: 2.0804901123046875\n",
      "Validation vlb: -2.4783125625844913, Best vlb: -2.3472195396917153\n",
      "\n",
      "Epoch_91, vlb: -2.980704943224557, took: 2.1292688846588135\n",
      "Validation vlb: -2.239757697944888, Best vlb: -2.239757697944888\n",
      "\n",
      "Epoch_92, vlb: -2.962189210160928, took: 1.4075100421905518\n",
      "Validation vlb: -2.4552061503759095, Best vlb: -2.239757697944888\n",
      "\n",
      "Epoch_93, vlb: -2.9583107049747643, took: 1.0483717918395996\n",
      "Validation vlb: -2.402938730122588, Best vlb: -2.239757697944888\n",
      "\n",
      "Epoch_94, vlb: -2.9270260813084272, took: 1.3580880165100098\n",
      "Validation vlb: -2.4572633292682733, Best vlb: -2.239757697944888\n",
      "\n",
      "Epoch_95, vlb: -2.9670681423706506, took: 1.1585328578948975\n",
      "Validation vlb: -2.2899722389418717, Best vlb: -2.239757697944888\n",
      "\n",
      "Epoch_96, vlb: -2.918010942876704, took: 1.4935617446899414\n",
      "Validation vlb: -2.3782863593795924, Best vlb: -2.239757697944888\n",
      "\n",
      "Epoch_97, vlb: -2.9829526889397164, took: 1.138174057006836\n",
      "Validation vlb: -2.2508403737953953, Best vlb: -2.239757697944888\n",
      "\n",
      "Epoch_98, vlb: -2.9570565018501047, took: 1.318138837814331\n",
      "Validation vlb: -2.3416087650558324, Best vlb: -2.239757697944888\n",
      "\n",
      "Epoch_99, vlb: -2.9572799639346403, took: 1.412226915359497\n",
      "Validation vlb: -2.211620816906679, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_100, vlb: -2.925658845111626, took: 1.2408461570739746\n",
      "Validation vlb: -2.3049256354088152, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_101, vlb: -2.9115777698231122, took: 0.9968457221984863\n",
      "Validation vlb: -2.497561377614833, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_102, vlb: -2.953719490073248, took: 1.0053157806396484\n",
      "Validation vlb: -2.4170017026388915, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_103, vlb: -2.8909232268712914, took: 1.0105228424072266\n",
      "Validation vlb: -2.325049491376167, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_104, vlb: -2.899412224951261, took: 1.0066862106323242\n",
      "Validation vlb: -2.3609635961094333, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_105, vlb: -2.894150566702744, took: 1.151200771331787\n",
      "Validation vlb: -2.407557274531392, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_106, vlb: -2.9023965433599455, took: 1.343836784362793\n",
      "Validation vlb: -2.351139453236725, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_107, vlb: -2.9004515949066567, took: 1.3731498718261719\n",
      "Validation vlb: -2.319253892188705, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_108, vlb: -2.856606288483386, took: 1.2560019493103027\n",
      "Validation vlb: -2.3541579817491054, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_109, vlb: -2.8811504422738308, took: 1.4279999732971191\n",
      "Validation vlb: -2.4894847985610222, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_110, vlb: -2.8954429853979873, took: 1.4785380363464355\n",
      "Validation vlb: -2.371827386343749, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_111, vlb: -2.923383093584258, took: 1.457000970840454\n",
      "Validation vlb: -2.3141988384685086, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_112, vlb: -2.9008208856249524, took: 1.4490199089050293\n",
      "Validation vlb: -2.422426999193951, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_113, vlb: -2.8434837625144325, took: 1.3772878646850586\n",
      "Validation vlb: -2.32163926621471, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_114, vlb: -2.844645763671265, took: 1.1864707469940186\n",
      "Validation vlb: -2.292506525817427, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_115, vlb: -2.8849587973648583, took: 1.4973649978637695\n",
      "Validation vlb: -2.455736634800735, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_116, vlb: -2.896922695409234, took: 1.445465087890625\n",
      "Validation vlb: -2.3289177602934608, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_117, vlb: -2.869455870853983, took: 1.5112271308898926\n",
      "Validation vlb: -2.3772061997632763, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_118, vlb: -2.896834200906118, took: 1.381140947341919\n",
      "Validation vlb: -2.484753879528601, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_119, vlb: -2.8922689903713965, took: 1.2869000434875488\n",
      "Validation vlb: -2.3744566432866465, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_120, vlb: -2.8379931561501284, took: 1.3588001728057861\n",
      "Validation vlb: -2.2216988921551257, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_121, vlb: -2.8460474428960563, took: 1.3962581157684326\n",
      "Validation vlb: -2.342487558192034, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_122, vlb: -2.887448918188395, took: 1.2681679725646973\n",
      "Validation vlb: -2.274786368928681, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_123, vlb: -2.858090407478324, took: 1.2066891193389893\n",
      "Validation vlb: -2.286890555353998, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_124, vlb: -2.854708852036462, took: 1.3597848415374756\n",
      "Validation vlb: -2.41490626952409, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_125, vlb: -2.892213784150181, took: 1.5008010864257812\n",
      "Validation vlb: -2.436170331096958, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_126, vlb: -2.9210776831118976, took: 1.1189329624176025\n",
      "Validation vlb: -2.4803033577199893, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_127, vlb: -2.8595076330196614, took: 1.0472080707550049\n",
      "Validation vlb: -2.284071585121278, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_128, vlb: -2.8436130685775365, took: 1.0606889724731445\n",
      "Validation vlb: -2.253435515662999, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_129, vlb: -2.835799527331398, took: 1.0207509994506836\n",
      "Validation vlb: -2.217190220132229, Best vlb: -2.211620816906679\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_130, vlb: -2.900885070361892, took: 1.0055890083312988\n",
      "Validation vlb: -2.3861975932198436, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_131, vlb: -2.858475547322791, took: 1.021245002746582\n",
      "Validation vlb: -2.3561581091587596, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_132, vlb: -2.8880919856393255, took: 1.1064281463623047\n",
      "Validation vlb: -2.294793871228363, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_133, vlb: -2.850180958516743, took: 1.007883071899414\n",
      "Validation vlb: -2.33888923003064, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_134, vlb: -2.8852135842336697, took: 1.008061170578003\n",
      "Validation vlb: -2.4238403730793676, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_135, vlb: -2.8943113125954594, took: 1.0143098831176758\n",
      "Validation vlb: -2.2594069879031875, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_136, vlb: -2.8393421938919876, took: 1.0200722217559814\n",
      "Validation vlb: -2.438940160868623, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_137, vlb: -2.7899504105393134, took: 1.0329208374023438\n",
      "Validation vlb: -2.3489490496687906, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_138, vlb: -2.805349504857515, took: 1.0280570983886719\n",
      "Validation vlb: -2.308525720460515, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_139, vlb: -2.8216484389718937, took: 1.0146381855010986\n",
      "Validation vlb: -2.289459018645549, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_140, vlb: -2.8055294982640917, took: 1.1644742488861084\n",
      "Validation vlb: -2.295876345973956, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_141, vlb: -2.8066260471247837, took: 1.0106947422027588\n",
      "Validation vlb: -2.407095703106482, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_142, vlb: -2.885617312571107, took: 1.0076816082000732\n",
      "Validation vlb: -2.259738639334645, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_143, vlb: -2.822884089748422, took: 1.0335259437561035\n",
      "Validation vlb: -2.3056619329359926, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_144, vlb: -2.7961339407959644, took: 1.0391030311584473\n",
      "Validation vlb: -2.3974350578962405, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_145, vlb: -2.8605560845164546, took: 1.0270178318023682\n",
      "Validation vlb: -2.2575805966522315, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_146, vlb: -2.840597548328547, took: 1.0341992378234863\n",
      "Validation vlb: -2.3164592499100274, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_147, vlb: -2.838480891535769, took: 1.035841941833496\n",
      "Validation vlb: -2.3273735216134575, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_148, vlb: -2.8201555017520077, took: 1.071044921875\n",
      "Validation vlb: -2.3196307825810702, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_149, vlb: -2.8024140638029316, took: 1.1071560382843018\n",
      "Validation vlb: -2.2979012222351765, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_150, vlb: -2.7897886797211604, took: 1.0255367755889893\n",
      "Validation vlb: -2.2932280313621445, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_151, vlb: -2.8082971147895073, took: 1.1345148086547852\n",
      "Validation vlb: -2.405614351377518, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_152, vlb: -2.787821762588413, took: 1.0330753326416016\n",
      "Validation vlb: -2.405938213311353, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_153, vlb: -2.7971168511369737, took: 1.0458660125732422\n",
      "Validation vlb: -2.2725527170792366, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_154, vlb: -2.7596418402544405, took: 1.035017967224121\n",
      "Validation vlb: -2.3796154441956947, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_155, vlb: -2.793915681887068, took: 1.1420490741729736\n",
      "Validation vlb: -2.288044932590719, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_156, vlb: -2.8218874190409147, took: 1.0561439990997314\n",
      "Validation vlb: -2.3136617504663066, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_157, vlb: -2.7839893753853984, took: 1.028374195098877\n",
      "Validation vlb: -2.278811834390881, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_158, vlb: -2.781911842334515, took: 1.020503044128418\n",
      "Validation vlb: -2.397362604110372, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_159, vlb: -2.784716402406868, took: 1.0182719230651855\n",
      "Validation vlb: -2.299993885373606, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_160, vlb: -2.7659853243080885, took: 1.0336508750915527\n",
      "Validation vlb: -2.279914110609629, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_161, vlb: -2.7228567095424445, took: 1.0203049182891846\n",
      "Validation vlb: -2.3129492292126406, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_162, vlb: -2.8224374668435908, took: 1.0187110900878906\n",
      "Validation vlb: -2.2606888149162714, Best vlb: -2.211620816906679\n",
      "\n",
      "Epoch_163, vlb: -2.765594100454191, took: 1.030045986175537\n",
      "Validation vlb: -2.1788636264677574, Best vlb: -2.1788636264677574\n",
      "\n",
      "Epoch_164, vlb: -2.8120015775036973, took: 1.0354928970336914\n",
      "Validation vlb: -2.194456741647813, Best vlb: -2.1788636264677574\n",
      "\n",
      "Epoch_165, vlb: -2.7816174777240086, took: 1.092040777206421\n",
      "Validation vlb: -2.244448822293081, Best vlb: -2.1788636264677574\n",
      "\n",
      "Epoch_166, vlb: -2.832726815470612, took: 1.0327401161193848\n",
      "Validation vlb: -2.1994275047555325, Best vlb: -2.1788636264677574\n",
      "\n",
      "Epoch_167, vlb: -2.7702174505227584, took: 1.0203938484191895\n",
      "Validation vlb: -2.3092866869806086, Best vlb: -2.1788636264677574\n",
      "\n",
      "Epoch_168, vlb: -2.829826470540932, took: 1.021960973739624\n",
      "Validation vlb: -2.1728960194634004, Best vlb: -2.1728960194634004\n",
      "\n",
      "Epoch_169, vlb: -2.761411423128545, took: 1.0243923664093018\n",
      "Validation vlb: -2.362610811168708, Best vlb: -2.1728960194634004\n",
      "\n",
      "Epoch_170, vlb: -2.771764134691051, took: 1.0657539367675781\n",
      "Validation vlb: -2.243106379478109, Best vlb: -2.1728960194634004\n",
      "\n",
      "Epoch_171, vlb: -2.83357823775061, took: 1.0199310779571533\n",
      "Validation vlb: -2.1005778821926673, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_172, vlb: -2.7235520848448345, took: 1.0469329357147217\n",
      "Validation vlb: -2.2915034286412608, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_173, vlb: -2.776473370910249, took: 1.043091058731079\n",
      "Validation vlb: -2.254453464233374, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_174, vlb: -2.777033037790773, took: 1.0401461124420166\n",
      "Validation vlb: -2.3601306711585774, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_175, vlb: -2.7205163403985995, took: 1.1207079887390137\n",
      "Validation vlb: -2.2308976696532907, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_176, vlb: -2.8182363736788316, took: 1.14593505859375\n",
      "Validation vlb: -2.4228754521959424, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_177, vlb: -2.7837997377455386, took: 1.1795320510864258\n",
      "Validation vlb: -2.2174001473053373, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_178, vlb: -2.7725424700432386, took: 1.3271911144256592\n",
      "Validation vlb: -2.2156804996786765, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_179, vlb: -2.7564826737130166, took: 1.2735722064971924\n",
      "Validation vlb: -2.227842925050112, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_180, vlb: -2.746762899773817, took: 1.505730152130127\n",
      "Validation vlb: -2.2822269482905813, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_181, vlb: -2.7972891239712983, took: 1.4366419315338135\n",
      "Validation vlb: -2.232499659640118, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_182, vlb: -2.8035682648001794, took: 1.2166907787322998\n",
      "Validation vlb: -2.132934505499682, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_183, vlb: -2.782105273122104, took: 1.3799901008605957\n",
      "Validation vlb: -2.164744732063565, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_184, vlb: -2.7056366761983366, took: 1.4233388900756836\n",
      "Validation vlb: -2.364652551107808, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_185, vlb: -2.80565170474219, took: 1.0929820537567139\n",
      "Validation vlb: -2.205582486001419, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_186, vlb: -2.7543990923682693, took: 1.0048418045043945\n",
      "Validation vlb: -2.3279451293852724, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_187, vlb: -2.749425331121904, took: 1.025803804397583\n",
      "Validation vlb: -2.3263360775018587, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_188, vlb: -2.6883839900690227, took: 1.0242640972137451\n",
      "Validation vlb: -2.2265400168965166, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_189, vlb: -2.7662231777913715, took: 1.0174989700317383\n",
      "Validation vlb: -2.3157416763429115, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_190, vlb: -2.7749877346905607, took: 1.0021250247955322\n",
      "Validation vlb: -2.3414290175083, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_191, vlb: -2.767891236162031, took: 1.065492868423462\n",
      "Validation vlb: -2.43444986868059, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_192, vlb: -2.748545065430973, took: 1.0244431495666504\n",
      "Validation vlb: -2.2787343653274585, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_193, vlb: -2.710114614200283, took: 1.018306016921997\n",
      "Validation vlb: -2.3091016602747647, Best vlb: -2.1005778821926673\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_194, vlb: -2.7737869362115086, took: 1.004788875579834\n",
      "Validation vlb: -2.323445492191994, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_195, vlb: -2.736610116742264, took: 1.025810956954956\n",
      "Validation vlb: -2.2075765094324993, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_196, vlb: -2.758740878766245, took: 1.0019550323486328\n",
      "Validation vlb: -2.1251150130454004, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_197, vlb: -2.7671789467914265, took: 1.0137951374053955\n",
      "Validation vlb: -2.1576135004222587, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_198, vlb: -2.752205258442594, took: 1.051887035369873\n",
      "Validation vlb: -2.2692606487706257, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_199, vlb: -2.745787642609099, took: 1.0017552375793457\n",
      "Validation vlb: -2.354532142287319, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_200, vlb: -2.746240592234676, took: 1.1241087913513184\n",
      "Validation vlb: -2.2018776271721308, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_201, vlb: -2.7502114995160376, took: 1.0277519226074219\n",
      "Validation vlb: -2.299014032851531, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_202, vlb: -2.7479747189950032, took: 1.0644299983978271\n",
      "Validation vlb: -2.303499936285914, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_203, vlb: -2.739862182735324, took: 1.067795991897583\n",
      "Validation vlb: -2.3380356112730154, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_204, vlb: -2.70633025722277, took: 1.021251916885376\n",
      "Validation vlb: -2.144695425496518, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_205, vlb: -2.7594737767057107, took: 1.0268349647521973\n",
      "Validation vlb: -2.3124365397641573, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_206, vlb: -2.7408583262948025, took: 1.0028049945831299\n",
      "Validation vlb: -2.288605673799237, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_207, vlb: -2.7720944458175403, took: 1.0075430870056152\n",
      "Validation vlb: -2.258980330525864, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_208, vlb: -2.7557119802042953, took: 1.1265208721160889\n",
      "Validation vlb: -2.3802918739689205, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_209, vlb: -2.7383436759169855, took: 1.0413792133331299\n",
      "Validation vlb: -2.245222637182686, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_210, vlb: -2.706252980326679, took: 1.0119318962097168\n",
      "Validation vlb: -2.1274699196460563, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_211, vlb: -2.7634219187474485, took: 1.010455846786499\n",
      "Validation vlb: -2.2940978456083623, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_212, vlb: -2.7068860616708, took: 1.0340957641601562\n",
      "Validation vlb: -2.1706056834038794, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_213, vlb: -2.7326002876493285, took: 1.052443027496338\n",
      "Validation vlb: -2.2367524869233657, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_214, vlb: -2.7379505157814124, took: 1.0583198070526123\n",
      "Validation vlb: -2.359435440267174, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_215, vlb: -2.689057761660402, took: 1.0323331356048584\n",
      "Validation vlb: -2.354725620121632, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_216, vlb: -2.701654966846833, took: 1.048577070236206\n",
      "Validation vlb: -2.3372922807835455, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_217, vlb: -2.7850033508859866, took: 1.0467381477355957\n",
      "Validation vlb: -2.2928986719125297, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_218, vlb: -2.746310334090582, took: 1.031041145324707\n",
      "Validation vlb: -2.380878686133326, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_219, vlb: -2.718706434477908, took: 1.0105681419372559\n",
      "Validation vlb: -2.180181419965133, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_220, vlb: -2.718641808361813, took: 1.02708101272583\n",
      "Validation vlb: -2.513461377628413, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_221, vlb: -2.7652971468943677, took: 1.0342810153961182\n",
      "Validation vlb: -2.287125646875128, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_222, vlb: -2.750490983699267, took: 1.0185892581939697\n",
      "Validation vlb: -2.2193301881401286, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_223, vlb: -2.7446467157830257, took: 1.0327579975128174\n",
      "Validation vlb: -2.2268477111186797, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_224, vlb: -2.6966925772308747, took: 1.0194017887115479\n",
      "Validation vlb: -2.3580263039054996, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_225, vlb: -2.7168708924213503, took: 1.1333248615264893\n",
      "Validation vlb: -2.171536021248037, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_226, vlb: -2.705632711512861, took: 1.014219045639038\n",
      "Validation vlb: -2.220829694788047, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_227, vlb: -2.720674988860315, took: 1.035710096359253\n",
      "Validation vlb: -2.2238224169968785, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_228, vlb: -2.7170026084266636, took: 1.1612112522125244\n",
      "Validation vlb: -2.1956875277954397, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_229, vlb: -2.705401570323907, took: 1.2873318195343018\n",
      "Validation vlb: -2.244849154092733, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_230, vlb: -2.7292395118848, took: 1.5003159046173096\n",
      "Validation vlb: -2.1785666282894542, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_231, vlb: -2.7448346958256558, took: 1.5492827892303467\n",
      "Validation vlb: -2.277147767613235, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_232, vlb: -2.6843243637744325, took: 1.881438970565796\n",
      "Validation vlb: -2.307120816221515, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_233, vlb: -2.6623326292302396, took: 1.2145462036132812\n",
      "Validation vlb: -2.3695629430048673, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_234, vlb: -2.7048457742037857, took: 1.4875779151916504\n",
      "Validation vlb: -2.237058494083318, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_235, vlb: -2.6780610266545715, took: 1.4194920063018799\n",
      "Validation vlb: -2.118979290465321, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_236, vlb: -2.663563929232583, took: 1.2844040393829346\n",
      "Validation vlb: -2.1784235910304544, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_237, vlb: -2.711267072518266, took: 1.7475190162658691\n",
      "Validation vlb: -2.1771243212678284, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_238, vlb: -2.6890379428691817, took: 1.3002660274505615\n",
      "Validation vlb: -2.204389954847811, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_239, vlb: -2.76547661650125, took: 1.1818430423736572\n",
      "Validation vlb: -2.2227219215874534, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_240, vlb: -2.7325341656004563, took: 1.3622591495513916\n",
      "Validation vlb: -2.392332695062878, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_241, vlb: -2.701743809748434, took: 1.2722508907318115\n",
      "Validation vlb: -2.2056966487643788, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_242, vlb: -2.68280556195925, took: 1.5669777393341064\n",
      "Validation vlb: -2.2895588465878878, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_243, vlb: -2.697091830734044, took: 1.4516339302062988\n",
      "Validation vlb: -2.3182120917298645, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_244, vlb: -2.7308747012021946, took: 1.6359002590179443\n",
      "Validation vlb: -2.2121138294923655, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_245, vlb: -2.663299759214715, took: 1.229956865310669\n",
      "Validation vlb: -2.2921667199304574, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_246, vlb: -2.68453267301464, took: 1.1596558094024658\n",
      "Validation vlb: -2.322319488309348, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_247, vlb: -2.7385924196432168, took: 1.1486830711364746\n",
      "Validation vlb: -2.336366736773148, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_248, vlb: -2.672782373720251, took: 1.0571730136871338\n",
      "Validation vlb: -2.2467065791096115, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_249, vlb: -2.7298268585012764, took: 1.1347589492797852\n",
      "Validation vlb: -2.2827366370599247, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_250, vlb: -2.710830734580551, took: 1.125640869140625\n",
      "Validation vlb: -2.291692307080266, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_251, vlb: -2.7297412817710844, took: 1.0439469814300537\n",
      "Validation vlb: -2.3467305956534967, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_252, vlb: -2.689233924915862, took: 1.061716079711914\n",
      "Validation vlb: -2.2485709757480805, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_253, vlb: -2.760548450125178, took: 1.1020638942718506\n",
      "Validation vlb: -2.1772018890936398, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_254, vlb: -2.6966900469203368, took: 1.4722843170166016\n",
      "Validation vlb: -2.3786676979373573, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_255, vlb: -2.718952022871183, took: 1.3994650840759277\n",
      "Validation vlb: -2.449782595279533, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_256, vlb: -2.6539896690978564, took: 1.1403470039367676\n",
      "Validation vlb: -2.1930910040259746, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_257, vlb: -2.7200501228692424, took: 1.425205945968628\n",
      "Validation vlb: -2.304431372861646, Best vlb: -2.1005778821926673\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_258, vlb: -2.6990252037378024, took: 1.584928035736084\n",
      "Validation vlb: -2.2792608112964814, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_259, vlb: -2.674328719019933, took: 1.2494871616363525\n",
      "Validation vlb: -2.2913824358418537, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_260, vlb: -2.7152174328744603, took: 1.2568492889404297\n",
      "Validation vlb: -2.233265845906773, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_261, vlb: -2.7106288539706522, took: 1.0614190101623535\n",
      "Validation vlb: -2.3433976628633766, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_262, vlb: -2.676379377738065, took: 1.0816171169281006\n",
      "Validation vlb: -2.3398534395162343, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_263, vlb: -2.7061051256930524, took: 1.390618085861206\n",
      "Validation vlb: -2.1922986576858077, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_264, vlb: -2.6782198076759864, took: 1.1437709331512451\n",
      "Validation vlb: -2.3254009057017204, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_265, vlb: -2.6910109556876027, took: 1.0534710884094238\n",
      "Validation vlb: -2.2201714808886877, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_266, vlb: -2.7092850428929123, took: 1.1031999588012695\n",
      "Validation vlb: -2.275770774551194, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_267, vlb: -2.677856838613008, took: 1.266956090927124\n",
      "Validation vlb: -2.360934875543835, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_268, vlb: -2.65466435238059, took: 1.282569169998169\n",
      "Validation vlb: -2.173519608272318, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_269, vlb: -2.6934424641753085, took: 1.119602918624878\n",
      "Validation vlb: -2.3153840870533173, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_270, vlb: -2.6867610287829446, took: 1.2027347087860107\n",
      "Validation vlb: -2.2621870735316603, Best vlb: -2.1005778821926673\n",
      "\n",
      "Epoch_271, vlb: -2.6909274425616485, took: 1.2622201442718506\n",
      "Validation vlb: -2.095097526377459, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_272, vlb: -2.682282440088016, took: 1.0460460186004639\n",
      "Validation vlb: -2.244947134098189, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_273, vlb: -2.674712661963639, took: 1.3077149391174316\n",
      "Validation vlb: -2.3144486884083175, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_274, vlb: -2.704722263334808, took: 1.1899909973144531\n",
      "Validation vlb: -2.3127803366546877, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_275, vlb: -2.680590017690992, took: 1.5641179084777832\n",
      "Validation vlb: -2.2611261108546583, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_276, vlb: -2.688963998338495, took: 1.3152210712432861\n",
      "Validation vlb: -2.2655908660209683, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_277, vlb: -2.6475210818275943, took: 1.3043649196624756\n",
      "Validation vlb: -2.266568808879667, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_278, vlb: -2.7204515317553533, took: 1.490920066833496\n",
      "Validation vlb: -2.304811925949788, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_279, vlb: -2.7008453340980276, took: 1.343878984451294\n",
      "Validation vlb: -2.316122751019919, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_280, vlb: -2.691084387493855, took: 1.5285248756408691\n",
      "Validation vlb: -2.142300811786096, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_281, vlb: -2.680166295590399, took: 1.2246448993682861\n",
      "Validation vlb: -2.2094549638939522, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_282, vlb: -2.6837840187579642, took: 1.2319231033325195\n",
      "Validation vlb: -2.1591363254102687, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_283, vlb: -2.7222476964551507, took: 1.2369298934936523\n",
      "Validation vlb: -2.268729141229179, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_284, vlb: -2.711012773991146, took: 1.4605751037597656\n",
      "Validation vlb: -2.2482670950658115, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_285, vlb: -2.675526914040219, took: 1.292098045349121\n",
      "Validation vlb: -2.370774443867137, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_286, vlb: -2.7056784852185523, took: 1.2250759601593018\n",
      "Validation vlb: -2.3197708770295176, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_287, vlb: -2.6755812365587213, took: 1.0611250400543213\n",
      "Validation vlb: -2.2236438345369014, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_288, vlb: -2.683376310538765, took: 1.0386297702789307\n",
      "Validation vlb: -2.184059693203775, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_289, vlb: -2.678054776945516, took: 1.0538349151611328\n",
      "Validation vlb: -2.3593412595273606, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_290, vlb: -2.703678902508245, took: 1.056152105331421\n",
      "Validation vlb: -2.2420362670058958, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_291, vlb: -2.7019790497279486, took: 1.0621728897094727\n",
      "Validation vlb: -2.2745706695568986, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_292, vlb: -2.6376679046659706, took: 1.0642309188842773\n",
      "Validation vlb: -2.1937265581297645, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_293, vlb: -2.6464956832696176, took: 1.0583040714263916\n",
      "Validation vlb: -2.392357006042135, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_294, vlb: -2.663917610702817, took: 1.473912239074707\n",
      "Validation vlb: -2.175537148725639, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_295, vlb: -2.688816144219997, took: 1.2252659797668457\n",
      "Validation vlb: -2.282449103481947, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_296, vlb: -2.668836358128411, took: 1.3026561737060547\n",
      "Validation vlb: -2.186250207493606, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_297, vlb: -2.6516503648102003, took: 1.2058629989624023\n",
      "Validation vlb: -2.359884968081724, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_298, vlb: -2.6598294687047894, took: 1.0082979202270508\n",
      "Validation vlb: -2.103915878869955, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_299, vlb: -2.6232229656223605, took: 0.9856870174407959\n",
      "Validation vlb: -2.2302280521701454, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_300, vlb: -2.6856644042842372, took: 1.0521390438079834\n",
      "Validation vlb: -2.368750636245826, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_301, vlb: -2.674414941205968, took: 1.0371379852294922\n",
      "Validation vlb: -2.4071861108144126, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_302, vlb: -2.696275247873314, took: 1.1775181293487549\n",
      "Validation vlb: -2.2616946635508612, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_303, vlb: -2.6802426838900573, took: 1.0072338581085205\n",
      "Validation vlb: -2.310496118847992, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_304, vlb: -2.667765085414712, took: 1.0317611694335938\n",
      "Validation vlb: -2.1429877832869497, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_305, vlb: -2.631684343807123, took: 1.0345730781555176\n",
      "Validation vlb: -2.316663999001957, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_306, vlb: -2.678095912521247, took: 1.0116820335388184\n",
      "Validation vlb: -2.218994679188651, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_307, vlb: -2.6290772318367828, took: 1.0068588256835938\n",
      "Validation vlb: -2.22211978736433, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_308, vlb: -2.681011488766476, took: 1.015641689300537\n",
      "Validation vlb: -2.3440254386574706, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_309, vlb: -2.6950642556907205, took: 0.9943349361419678\n",
      "Validation vlb: -2.1781687265846723, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_310, vlb: -2.7052696587415235, took: 1.110008955001831\n",
      "Validation vlb: -2.1590934879957278, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_311, vlb: -2.649030509937741, took: 0.991692066192627\n",
      "Validation vlb: -2.2520050169194787, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_312, vlb: -2.682563666692946, took: 0.9987521171569824\n",
      "Validation vlb: -2.1050888681874693, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_313, vlb: -2.666290962571586, took: 0.9939818382263184\n",
      "Validation vlb: -2.215389173779287, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_314, vlb: -2.710643364879181, took: 1.0159218311309814\n",
      "Validation vlb: -2.3249679730548056, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_315, vlb: -2.694451310113475, took: 1.0041320323944092\n",
      "Validation vlb: -2.192550099011764, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_316, vlb: -2.6599611616151813, took: 1.0007660388946533\n",
      "Validation vlb: -2.2873787903091283, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_317, vlb: -2.652858975468499, took: 0.9994261264801025\n",
      "Validation vlb: -2.305818786127282, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_318, vlb: -2.6561013375592393, took: 1.0020840167999268\n",
      "Validation vlb: -2.2106622978321555, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_319, vlb: -2.737206521290088, took: 1.1146023273468018\n",
      "Validation vlb: -2.177477486311039, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_320, vlb: -2.671514680413922, took: 1.0053880214691162\n",
      "Validation vlb: -2.260340481514298, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_321, vlb: -2.6616778596940556, took: 1.0435779094696045\n",
      "Validation vlb: -2.2748280979668825, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_322, vlb: -2.6460054363756247, took: 1.0544118881225586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation vlb: -2.226803779602051, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_323, vlb: -2.6156751293907674, took: 1.00626802444458\n",
      "Validation vlb: -2.207189868183198, Best vlb: -2.095097526377459\n",
      "\n",
      "Epoch_324, vlb: -2.643940920961989, took: 1.0031208992004395\n",
      "Validation vlb: -2.0710240760667427, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_325, vlb: -2.6658416879576317, took: 1.5268621444702148\n",
      "Validation vlb: -2.2239512347866417, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_326, vlb: -2.625059878263793, took: 1.2710750102996826\n",
      "Validation vlb: -2.235206635638734, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_327, vlb: -2.6569728769756367, took: 1.5881741046905518\n",
      "Validation vlb: -2.3542422769910694, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_328, vlb: -2.6127151854827244, took: 1.3620309829711914\n",
      "Validation vlb: -2.3119302741146397, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_329, vlb: -2.6683585915069252, took: 1.2845978736877441\n",
      "Validation vlb: -2.2909984858676453, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_330, vlb: -2.638242399001405, took: 1.286046028137207\n",
      "Validation vlb: -2.2858070416743703, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_331, vlb: -2.686163581701161, took: 1.2531330585479736\n",
      "Validation vlb: -2.25706150153694, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_332, vlb: -2.6736960507229073, took: 1.274916172027588\n",
      "Validation vlb: -2.25442248949341, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_333, vlb: -2.633820631626831, took: 1.2970800399780273\n",
      "Validation vlb: -2.2944737699811126, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_334, vlb: -2.620367684894901, took: 1.0977962017059326\n",
      "Validation vlb: -2.291210650626124, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_335, vlb: -2.654080193378447, took: 1.0396432876586914\n",
      "Validation vlb: -2.2151161011754503, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_336, vlb: -2.6648955874018934, took: 1.1101369857788086\n",
      "Validation vlb: -2.1115104453849174, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_337, vlb: -2.667353352497583, took: 1.3037490844726562\n",
      "Validation vlb: -2.0941752469269588, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_338, vlb: -2.6180306290567112, took: 1.0937879085540771\n",
      "Validation vlb: -2.3002771621383125, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_339, vlb: -2.6577341164192965, took: 1.410304069519043\n",
      "Validation vlb: -2.184986115273534, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_340, vlb: -2.6690001455107137, took: 1.327772855758667\n",
      "Validation vlb: -2.1811272889665028, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_341, vlb: -2.6468773174938383, took: 1.0345122814178467\n",
      "Validation vlb: -2.287386283133794, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_342, vlb: -2.638537266548907, took: 1.1679539680480957\n",
      "Validation vlb: -2.166085071548289, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_343, vlb: -2.652024394967035, took: 1.238799810409546\n",
      "Validation vlb: -2.200953285670975, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_344, vlb: -2.647643942690428, took: 1.2067739963531494\n",
      "Validation vlb: -2.3920341618238528, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_345, vlb: -2.6902991747486964, took: 1.5310111045837402\n",
      "Validation vlb: -2.2981189416064414, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_346, vlb: -2.592461139811515, took: 1.183631181716919\n",
      "Validation vlb: -2.301816661766818, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_347, vlb: -2.6220562068257864, took: 1.1562418937683105\n",
      "Validation vlb: -2.30190602166753, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_348, vlb: -2.657609866856241, took: 1.2775611877441406\n",
      "Validation vlb: -2.2509913074160086, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_349, vlb: -2.6614556842284705, took: 1.3506419658660889\n",
      "Validation vlb: -2.2613015267455463, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_350, vlb: -2.60207599686941, took: 1.3788561820983887\n",
      "Validation vlb: -2.285273423086864, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_351, vlb: -2.657622066469642, took: 1.207374095916748\n",
      "Validation vlb: -2.215908116121508, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_352, vlb: -2.677801865315669, took: 1.3757917881011963\n",
      "Validation vlb: -2.219702194037947, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_353, vlb: -2.6227416378326196, took: 1.2742400169372559\n",
      "Validation vlb: -2.2293361707026906, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_354, vlb: -2.6488579665751693, took: 1.2323877811431885\n",
      "Validation vlb: -2.287502848986283, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_355, vlb: -2.653777829167823, took: 1.2499210834503174\n",
      "Validation vlb: -2.3284794396952906, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_356, vlb: -2.6461197510871433, took: 1.351078987121582\n",
      "Validation vlb: -2.0969487424807256, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_357, vlb: -2.6510261361187726, took: 1.3375139236450195\n",
      "Validation vlb: -2.1257909821846726, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_358, vlb: -2.684627065308282, took: 1.155919075012207\n",
      "Validation vlb: -2.3254705586479707, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_359, vlb: -2.643707316214548, took: 1.3387532234191895\n",
      "Validation vlb: -2.1810314007175777, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_360, vlb: -2.6316862827502954, took: 1.5358819961547852\n",
      "Validation vlb: -2.1743280683134754, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_361, vlb: -2.6408007370725914, took: 1.413827896118164\n",
      "Validation vlb: -2.224926665377077, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_362, vlb: -2.6107315202818784, took: 1.1537158489227295\n",
      "Validation vlb: -2.1464169264611304, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_363, vlb: -2.6808682433297664, took: 1.156369924545288\n",
      "Validation vlb: -2.1053417701165653, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_364, vlb: -2.635197653373798, took: 1.2442841529846191\n",
      "Validation vlb: -2.2195503889164105, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_365, vlb: -2.661592150661728, took: 1.1201038360595703\n",
      "Validation vlb: -2.286477927636946, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_366, vlb: -2.6597516361569, took: 1.094405174255371\n",
      "Validation vlb: -2.146270765841586, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_367, vlb: -2.6924390344685, took: 1.1155040264129639\n",
      "Validation vlb: -2.188246089663706, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_368, vlb: -2.63352196784083, took: 1.0673580169677734\n",
      "Validation vlb: -2.136469041645334, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_369, vlb: -2.6580338943421005, took: 1.114736795425415\n",
      "Validation vlb: -2.2419290338133533, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_370, vlb: -2.632813162626789, took: 1.5315821170806885\n",
      "Validation vlb: -2.2196768487541423, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_371, vlb: -2.651029222252818, took: 1.5306313037872314\n",
      "Validation vlb: -2.1320169851617905, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_372, vlb: -2.634028625265059, took: 1.1687428951263428\n",
      "Validation vlb: -2.308905403205106, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_373, vlb: -2.5993212020607532, took: 1.1818230152130127\n",
      "Validation vlb: -2.197105937791102, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_374, vlb: -2.573472520710111, took: 1.1319260597229004\n",
      "Validation vlb: -2.3769763310750327, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_375, vlb: -2.656594977689639, took: 1.0284810066223145\n",
      "Validation vlb: -2.2883718013763428, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_376, vlb: -2.6059237219336913, took: 1.0615110397338867\n",
      "Validation vlb: -2.1852646051487103, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_377, vlb: -2.5995788629165775, took: 1.6505279541015625\n",
      "Validation vlb: -2.103913653629883, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_378, vlb: -2.6149906433869234, took: 2.0982470512390137\n",
      "Validation vlb: -2.216072556270365, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_379, vlb: -2.6433292729930993, took: 1.884263038635254\n",
      "Validation vlb: -2.0876854523100126, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_380, vlb: -2.6285440730327405, took: 1.2192161083221436\n",
      "Validation vlb: -2.150181143029222, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_381, vlb: -2.594435336995906, took: 1.512770175933838\n",
      "Validation vlb: -2.107901430438637, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_382, vlb: -2.648885655669287, took: 1.7419979572296143\n",
      "Validation vlb: -2.391604703606911, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_383, vlb: -2.6352594407033183, took: 1.5882949829101562\n",
      "Validation vlb: -2.218460542098604, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_384, vlb: -2.6276057353756346, took: 1.2191390991210938\n",
      "Validation vlb: -2.1351557811872857, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_385, vlb: -2.6357509707820728, took: 1.1926169395446777\n",
      "Validation vlb: -2.2303989952050367, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_386, vlb: -2.6349028598330713, took: 1.5250880718231201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation vlb: -2.1647028190032565, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_387, vlb: -2.63271919986435, took: 1.275181770324707\n",
      "Validation vlb: -2.137008253038894, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_388, vlb: -2.6400663213932476, took: 1.5941989421844482\n",
      "Validation vlb: -2.1273287426692384, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_389, vlb: -2.66769706769345, took: 1.5299131870269775\n",
      "Validation vlb: -2.1786637614845845, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_390, vlb: -2.5927085446332656, took: 1.4170849323272705\n",
      "Validation vlb: -2.138993789848772, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_391, vlb: -2.641358558677451, took: 1.1876118183135986\n",
      "Validation vlb: -2.30265198787825, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_392, vlb: -2.631248827585008, took: 1.0837297439575195\n",
      "Validation vlb: -2.1976220592325943, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_393, vlb: -2.6976118252609633, took: 1.0626769065856934\n",
      "Validation vlb: -2.1381614632591073, Best vlb: -2.0710240760667427\n",
      "\n",
      "Epoch_394, vlb: -2.604936245924112, took: 1.0368001461029053\n",
      "Validation vlb: -2.0344274661301793, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_395, vlb: -2.640226010328409, took: 1.0535869598388672\n",
      "Validation vlb: -2.2195113245337525, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_396, vlb: -2.611501509910614, took: 1.0092816352844238\n",
      "Validation vlb: -2.2815236844676985, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_397, vlb: -2.6493998603250724, took: 1.0309741497039795\n",
      "Validation vlb: -2.219049679037051, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_398, vlb: -2.630490725273102, took: 1.0844557285308838\n",
      "Validation vlb: -2.2081621873725967, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_399, vlb: -2.602798406537763, took: 1.0489192008972168\n",
      "Validation vlb: -2.083766675689845, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_400, vlb: -2.6465027823394953, took: 1.0359508991241455\n",
      "Validation vlb: -2.148511755427882, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_401, vlb: -2.591998865074677, took: 1.1919972896575928\n",
      "Validation vlb: -2.174962069224385, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_402, vlb: -2.658408502988091, took: 1.0189390182495117\n",
      "Validation vlb: -2.1808232134599903, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_403, vlb: -2.575977035250473, took: 1.0473051071166992\n",
      "Validation vlb: -2.247574317030922, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_404, vlb: -2.622718624043559, took: 1.0136868953704834\n",
      "Validation vlb: -2.1715626269096697, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_405, vlb: -2.6110479896086605, took: 1.0089716911315918\n",
      "Validation vlb: -2.1294080015139287, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_406, vlb: -2.604347841319825, took: 1.1138339042663574\n",
      "Validation vlb: -2.176784337145611, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_407, vlb: -2.592759336118523, took: 1.1142420768737793\n",
      "Validation vlb: -2.2471041316739178, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_408, vlb: -2.6246502258642828, took: 1.0537090301513672\n",
      "Validation vlb: -2.1791073982769618, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_409, vlb: -2.6399881189831906, took: 1.2031402587890625\n",
      "Validation vlb: -2.2409561590855174, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_410, vlb: -2.643295857817911, took: 1.1318302154541016\n",
      "Validation vlb: -2.1040707757172075, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_411, vlb: -2.6466533559064964, took: 1.0668449401855469\n",
      "Validation vlb: -2.285282555521499, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_412, vlb: -2.619371328930826, took: 1.1087207794189453\n",
      "Validation vlb: -2.164752160846994, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_413, vlb: -2.627891166973423, took: 1.0454919338226318\n",
      "Validation vlb: -2.064892993775772, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_414, vlb: -2.6375801353777404, took: 1.0861599445343018\n",
      "Validation vlb: -2.2296375965994923, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_415, vlb: -2.5979649061602057, took: 1.3608529567718506\n",
      "Validation vlb: -2.2391476708322666, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_416, vlb: -2.653954855091588, took: 1.0349678993225098\n",
      "Validation vlb: -2.103253937847792, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_417, vlb: -2.668016850197105, took: 1.110030174255371\n",
      "Validation vlb: -2.1248720602695994, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_418, vlb: -2.6227957907021797, took: 1.0076520442962646\n",
      "Validation vlb: -2.2097523150706366, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_419, vlb: -2.6003741616175935, took: 1.0263118743896484\n",
      "Validation vlb: -2.2094224780122826, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_420, vlb: -2.6349318044667283, took: 1.0232219696044922\n",
      "Validation vlb: -2.1753600260972203, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_421, vlb: -2.6051034368453885, took: 1.2734570503234863\n",
      "Validation vlb: -2.1902241799437885, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_422, vlb: -2.6143131091262437, took: 1.045546054840088\n",
      "Validation vlb: -2.1126970527241533, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_423, vlb: -2.617558026940521, took: 1.0387811660766602\n",
      "Validation vlb: -2.257773851113798, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_424, vlb: -2.606622815518487, took: 0.9985079765319824\n",
      "Validation vlb: -2.1531268308077816, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_425, vlb: -2.6114083594197544, took: 1.1455020904541016\n",
      "Validation vlb: -2.3021329629768448, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_426, vlb: -2.5873283665086624, took: 1.0030760765075684\n",
      "Validation vlb: -2.2079728695181196, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_427, vlb: -2.6490595361333895, took: 0.9952890872955322\n",
      "Validation vlb: -2.401475762857974, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_428, vlb: -2.6223652563723547, took: 0.9988529682159424\n",
      "Validation vlb: -2.3132823809836673, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_429, vlb: -2.642163632797774, took: 1.0005910396575928\n",
      "Validation vlb: -2.2741030909868507, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_430, vlb: -2.6598031328528915, took: 0.9770321846008301\n",
      "Validation vlb: -2.232625204382591, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_431, vlb: -2.658180387405599, took: 1.0213451385498047\n",
      "Validation vlb: -2.2340678906363576, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_432, vlb: -2.5722749906346913, took: 1.0022618770599365\n",
      "Validation vlb: -2.2374230958883046, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_433, vlb: -2.6060807070459098, took: 1.0084218978881836\n",
      "Validation vlb: -2.2970684438847417, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_434, vlb: -2.634539368687493, took: 1.0952084064483643\n",
      "Validation vlb: -2.2212025525114685, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_435, vlb: -2.653040317910414, took: 1.0007390975952148\n",
      "Validation vlb: -2.23794477657207, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_436, vlb: -2.5932576735327904, took: 1.0160160064697266\n",
      "Validation vlb: -2.2092223275440794, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_437, vlb: -2.6500292857173195, took: 1.0318560600280762\n",
      "Validation vlb: -2.122275508337422, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_438, vlb: -2.5873562217316786, took: 1.0104031562805176\n",
      "Validation vlb: -2.0579654811655432, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_439, vlb: -2.55955815341003, took: 1.0138797760009766\n",
      "Validation vlb: -2.283958364844708, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_440, vlb: -2.63227665471567, took: 1.0174672603607178\n",
      "Validation vlb: -2.0628749352057003, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_441, vlb: -2.6097140056347734, took: 1.0207798480987549\n",
      "Validation vlb: -2.148276190155918, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_442, vlb: -2.610216829387842, took: 1.0984652042388916\n",
      "Validation vlb: -2.1309514419932194, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_443, vlb: -2.6412406657125866, took: 1.0042941570281982\n",
      "Validation vlb: -2.1317575780319165, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_444, vlb: -2.601729467698984, took: 1.0168209075927734\n",
      "Validation vlb: -2.055510930258865, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_445, vlb: -2.630271048265951, took: 1.0134739875793457\n",
      "Validation vlb: -2.1864604402128545, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_446, vlb: -2.591970572765242, took: 1.0327396392822266\n",
      "Validation vlb: -2.2253223646034317, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_447, vlb: -2.608784264182289, took: 0.9961588382720947\n",
      "Validation vlb: -2.1288293989730884, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_448, vlb: -2.6375166162725057, took: 0.9885768890380859\n",
      "Validation vlb: -2.290144896430105, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_449, vlb: -2.6223768577500133, took: 1.0205230712890625\n",
      "Validation vlb: -2.2288779980927993, Best vlb: -2.0344274661301793\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_450, vlb: -2.6497188527076676, took: 1.0068390369415283\n",
      "Validation vlb: -2.0524721566141615, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_451, vlb: -2.609728722508794, took: 1.0628020763397217\n",
      "Validation vlb: -2.1960930770269105, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_452, vlb: -2.6227756510718416, took: 1.0557119846343994\n",
      "Validation vlb: -2.1089672579348666, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_453, vlb: -2.6059467486009606, took: 1.0118489265441895\n",
      "Validation vlb: -2.179175054371164, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_454, vlb: -2.57984144703965, took: 1.0611701011657715\n",
      "Validation vlb: -2.1312250901966032, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_455, vlb: -2.614088868596891, took: 1.3670988082885742\n",
      "Validation vlb: -2.2081479486138305, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_456, vlb: -2.6212734381071647, took: 1.2765507698059082\n",
      "Validation vlb: -2.2906304578565084, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_457, vlb: -2.6500805344648573, took: 1.1313199996948242\n",
      "Validation vlb: -2.3015068256353484, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_458, vlb: -2.6622906180221397, took: 1.1443469524383545\n",
      "Validation vlb: -2.109969409924109, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_459, vlb: -2.6387210780010837, took: 1.0093040466308594\n",
      "Validation vlb: -2.1505262319324085, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_460, vlb: -2.6089696684972306, took: 1.04939603805542\n",
      "Validation vlb: -2.145902730500428, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_461, vlb: -2.601377305943287, took: 1.0365581512451172\n",
      "Validation vlb: -2.209989547729492, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_462, vlb: -2.6207318764433483, took: 1.4064438343048096\n",
      "Validation vlb: -2.1505870711070436, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_463, vlb: -2.6563753333931395, took: 1.0404019355773926\n",
      "Validation vlb: -2.1275773966582463, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_464, vlb: -2.634325231971782, took: 1.040154218673706\n",
      "Validation vlb: -2.114720429417385, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_465, vlb: -2.61333718976401, took: 1.0406899452209473\n",
      "Validation vlb: -2.065698524509047, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_466, vlb: -2.6591911437831683, took: 1.235172986984253\n",
      "Validation vlb: -2.2274143460499043, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_467, vlb: -2.603049765399003, took: 1.0555269718170166\n",
      "Validation vlb: -2.198885155341386, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_468, vlb: -2.606109017041419, took: 1.0044407844543457\n",
      "Validation vlb: -2.0716296658161, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_469, vlb: -2.5392852063912215, took: 1.0166661739349365\n",
      "Validation vlb: -2.0612817404725403, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_470, vlb: -2.624952986981313, took: 1.0397109985351562\n",
      "Validation vlb: -2.260393075572634, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_471, vlb: -2.602125171881852, took: 1.0549471378326416\n",
      "Validation vlb: -2.1589314420632175, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_472, vlb: -2.602376499406116, took: 1.0183610916137695\n",
      "Validation vlb: -2.216854264049468, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_473, vlb: -2.590130767849739, took: 1.0204508304595947\n",
      "Validation vlb: -2.17426183771547, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_474, vlb: -2.6048330222697493, took: 1.033714771270752\n",
      "Validation vlb: -2.196018486732804, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_475, vlb: -2.5791623438696747, took: 1.0575249195098877\n",
      "Validation vlb: -2.182577880840857, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_476, vlb: -2.5731161743949804, took: 1.0178611278533936\n",
      "Validation vlb: -2.142112870046622, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_477, vlb: -2.6000159085548136, took: 1.0149681568145752\n",
      "Validation vlb: -2.1249631357810257, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_478, vlb: -2.5882173176072767, took: 1.0818498134613037\n",
      "Validation vlb: -2.25340163591996, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_479, vlb: -2.5887626777418147, took: 1.1542949676513672\n",
      "Validation vlb: -2.1839938063451774, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_480, vlb: -2.550213254953315, took: 1.2105767726898193\n",
      "Validation vlb: -2.143544524618723, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_481, vlb: -2.6408774535090194, took: 1.3092892169952393\n",
      "Validation vlb: -2.065283747552668, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_482, vlb: -2.6152760267343544, took: 1.2370729446411133\n",
      "Validation vlb: -2.1540533778736894, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_483, vlb: -2.6264881688483035, took: 1.045403003692627\n",
      "Validation vlb: -2.326021214519118, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_484, vlb: -2.627496264416149, took: 1.0191330909729004\n",
      "Validation vlb: -2.1942335664261505, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_485, vlb: -2.618410290038796, took: 1.0135002136230469\n",
      "Validation vlb: -2.1876432023773686, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_486, vlb: -2.6416763449728298, took: 1.0428528785705566\n",
      "Validation vlb: -2.1300259207444667, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_487, vlb: -2.6607103267933425, took: 1.0329480171203613\n",
      "Validation vlb: -2.279461862971482, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_488, vlb: -2.58186291841281, took: 1.0220699310302734\n",
      "Validation vlb: -2.3498169388200085, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_489, vlb: -2.589073153507121, took: 1.0388078689575195\n",
      "Validation vlb: -2.10344621085812, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_490, vlb: -2.659749126537352, took: 1.0705740451812744\n",
      "Validation vlb: -2.1696082504050245, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_491, vlb: -2.582531652821454, took: 1.046069860458374\n",
      "Validation vlb: -2.2440766130836263, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_492, vlb: -2.5810486361153657, took: 1.0090088844299316\n",
      "Validation vlb: -2.3225874623048655, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_493, vlb: -2.609902253114881, took: 1.0295259952545166\n",
      "Validation vlb: -2.302490045337615, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_494, vlb: -2.640486700654159, took: 1.0453572273254395\n",
      "Validation vlb: -2.213343257657147, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_495, vlb: -2.586344860164133, took: 1.0349738597869873\n",
      "Validation vlb: -2.2244723113223572, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_496, vlb: -2.6434603660365084, took: 1.0242743492126465\n",
      "Validation vlb: -2.2218206484340928, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_497, vlb: -2.5790576732197295, took: 1.0273699760437012\n",
      "Validation vlb: -2.0958104789449945, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_498, vlb: -2.5951311298956923, took: 1.0415639877319336\n",
      "Validation vlb: -2.07621720809381, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_499, vlb: -2.648770073312712, took: 1.1219151020050049\n",
      "Validation vlb: -2.287079680698975, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_500, vlb: -2.5562499273497292, took: 1.0123748779296875\n",
      "Validation vlb: -2.3150576441804955, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_501, vlb: -2.598351351511663, took: 1.0194149017333984\n",
      "Validation vlb: -2.2216462317408094, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_502, vlb: -2.615072313383581, took: 1.0495259761810303\n",
      "Validation vlb: -2.148097321053539, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_503, vlb: -2.5915023069996432, took: 1.0730197429656982\n",
      "Validation vlb: -2.139480455793609, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_504, vlb: -2.644462107925223, took: 1.2983109951019287\n",
      "Validation vlb: -2.161675951627466, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_505, vlb: -2.595370978127721, took: 1.479914903640747\n",
      "Validation vlb: -2.14329191010361, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_506, vlb: -2.596632642715064, took: 1.3440139293670654\n",
      "Validation vlb: -2.1070022883924464, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_507, vlb: -2.574957825101276, took: 1.3011996746063232\n",
      "Validation vlb: -2.302535699794979, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_508, vlb: -2.6169353042268737, took: 1.6728723049163818\n",
      "Validation vlb: -2.252807076694896, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_509, vlb: -2.6672326604110284, took: 1.1482138633728027\n",
      "Validation vlb: -2.176069451767264, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_510, vlb: -2.6140704910489867, took: 1.3770020008087158\n",
      "Validation vlb: -2.2883879991796796, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_511, vlb: -2.5755032445614314, took: 1.43998384475708\n",
      "Validation vlb: -2.107184349140303, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_512, vlb: -2.6044046232328597, took: 1.1400580406188965\n",
      "Validation vlb: -2.215886464782517, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_513, vlb: -2.604467829569321, took: 1.1725749969482422\n",
      "Validation vlb: -2.229404725688947, Best vlb: -2.0344274661301793\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_514, vlb: -2.5703367161845234, took: 0.9815318584442139\n",
      "Validation vlb: -2.347469156999804, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_515, vlb: -2.581190898000314, took: 0.9991428852081299\n",
      "Validation vlb: -2.1250271496263524, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_516, vlb: -2.6306640046682555, took: 1.0163929462432861\n",
      "Validation vlb: -2.115908705686674, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_517, vlb: -2.582507304849923, took: 0.983618974685669\n",
      "Validation vlb: -2.25680347785209, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_518, vlb: -2.604461818107135, took: 0.9897992610931396\n",
      "Validation vlb: -2.0863074615935293, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_519, vlb: -2.6215743835684115, took: 1.0453870296478271\n",
      "Validation vlb: -2.1609505958927486, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_520, vlb: -2.607004549248338, took: 1.0106327533721924\n",
      "Validation vlb: -2.042822377195636, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_521, vlb: -2.6157803974179084, took: 1.1256177425384521\n",
      "Validation vlb: -2.339710972455713, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_522, vlb: -2.5809387473867744, took: 1.0126211643218994\n",
      "Validation vlb: -2.1505590573097897, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_523, vlb: -2.6155962060585614, took: 1.0443847179412842\n",
      "Validation vlb: -2.199301134035425, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_524, vlb: -2.6345961564386497, took: 0.9888839721679688\n",
      "Validation vlb: -2.192459261533126, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_525, vlb: -2.5962099676987314, took: 1.0419600009918213\n",
      "Validation vlb: -2.285184741405993, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_526, vlb: -2.5997239362004425, took: 1.0418288707733154\n",
      "Validation vlb: -2.147286616482781, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_527, vlb: -2.5790481288354945, took: 1.014815092086792\n",
      "Validation vlb: -2.0702239749501055, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_528, vlb: -2.660001571804317, took: 1.1551852226257324\n",
      "Validation vlb: -2.1641430862513173, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_529, vlb: -2.618144572017259, took: 1.0214288234710693\n",
      "Validation vlb: -2.2694305625162463, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_530, vlb: -2.565204468055041, took: 1.023193120956421\n",
      "Validation vlb: -2.1129773048907037, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_531, vlb: -2.6143297298288535, took: 1.0222713947296143\n",
      "Validation vlb: -2.253065050998552, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_532, vlb: -2.6176729882239953, took: 0.9986567497253418\n",
      "Validation vlb: -2.168300232840973, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_533, vlb: -2.6145360384821936, took: 0.9963829517364502\n",
      "Validation vlb: -2.2916983678502945, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_534, vlb: -2.5872452939033166, took: 1.0136070251464844\n",
      "Validation vlb: -2.223188225891212, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_535, vlb: -2.534136705532493, took: 1.1595361232757568\n",
      "Validation vlb: -2.183599850892249, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_536, vlb: -2.584669621762587, took: 1.245866060256958\n",
      "Validation vlb: -2.378404141243993, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_537, vlb: -2.6290813746708177, took: 1.283823013305664\n",
      "Validation vlb: -2.195056149103109, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_538, vlb: -2.592470127341985, took: 1.2376682758331299\n",
      "Validation vlb: -2.1776141626549386, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_539, vlb: -2.5449238953334548, took: 1.0048229694366455\n",
      "Validation vlb: -2.122042284042704, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_540, vlb: -2.5670885862873147, took: 1.0034558773040771\n",
      "Validation vlb: -2.185634498457307, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_541, vlb: -2.5682797553000967, took: 1.0408029556274414\n",
      "Validation vlb: -2.1375384477349932, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_542, vlb: -2.5878870132633107, took: 0.9988720417022705\n",
      "Validation vlb: -2.3264144809500684, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_543, vlb: -2.585272593939523, took: 0.9850080013275146\n",
      "Validation vlb: -2.118992407345077, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_544, vlb: -2.5820720473081034, took: 0.9912519454956055\n",
      "Validation vlb: -2.2560258189451345, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_545, vlb: -2.5812657064012714, took: 1.0719571113586426\n",
      "Validation vlb: -2.2090379863880987, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_546, vlb: -2.6405807713359906, took: 1.1952567100524902\n",
      "Validation vlb: -2.1832227915236095, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_547, vlb: -2.633378471279041, took: 1.3742458820343018\n",
      "Validation vlb: -2.255278216982351, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_548, vlb: -2.6365964975037675, took: 1.2528481483459473\n",
      "Validation vlb: -2.2318912324010363, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_549, vlb: -2.6566266901938658, took: 1.3981578350067139\n",
      "Validation vlb: -2.1408453651230697, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_550, vlb: -2.6208403140553305, took: 1.3085176944732666\n",
      "Validation vlb: -2.29718449123469, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_551, vlb: -2.5693089416828094, took: 1.1991379261016846\n",
      "Validation vlb: -2.218893747113669, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_552, vlb: -2.58451335053243, took: 1.2952308654785156\n",
      "Validation vlb: -2.304003484040788, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_553, vlb: -2.6108591573720017, took: 1.162559986114502\n",
      "Validation vlb: -2.1500224704495525, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_554, vlb: -2.6226931487994527, took: 1.0429468154907227\n",
      "Validation vlb: -2.308464393646586, Best vlb: -2.0344274661301793\n",
      "\n",
      "Epoch_555, vlb: -2.56672339710693, took: 1.023832082748413\n",
      "Validation vlb: -1.9865416589292508, Best vlb: -1.9865416589292508\n",
      "\n",
      "Epoch_556, vlb: -2.573225983278675, took: 1.1698901653289795\n",
      "Validation vlb: -2.141585638222185, Best vlb: -1.9865416589292508\n",
      "\n",
      "Epoch_557, vlb: -2.576734887896831, took: 1.0325591564178467\n",
      "Validation vlb: -2.1645462933481703, Best vlb: -1.9865416589292508\n",
      "\n",
      "Epoch_558, vlb: -2.6318070618076894, took: 1.0400328636169434\n",
      "Validation vlb: -1.9862191167849939, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_559, vlb: -2.574950522469839, took: 1.0519518852233887\n",
      "Validation vlb: -2.3440417494974475, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_560, vlb: -2.620119088667092, took: 1.061939001083374\n",
      "Validation vlb: -2.205429526980255, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_561, vlb: -2.6235948434870235, took: 1.1138160228729248\n",
      "Validation vlb: -2.1846310868618173, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_562, vlb: -2.593486408385434, took: 1.0802578926086426\n",
      "Validation vlb: -2.1090095224503944, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_563, vlb: -2.550858661172885, took: 1.1100330352783203\n",
      "Validation vlb: -2.198133990216795, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_564, vlb: -2.5551540788919067, took: 1.040343999862671\n",
      "Validation vlb: -2.161725646855376, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_565, vlb: -2.5706754361806174, took: 1.0361089706420898\n",
      "Validation vlb: -2.2036634502287438, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_566, vlb: -2.587305472486947, took: 1.0360751152038574\n",
      "Validation vlb: -2.1791347253669815, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_567, vlb: -2.561256232259941, took: 1.0380480289459229\n",
      "Validation vlb: -2.147285788576194, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_568, vlb: -2.582023858079646, took: 1.0421769618988037\n",
      "Validation vlb: -2.179750667035001, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_569, vlb: -2.6208761110123775, took: 1.1174569129943848\n",
      "Validation vlb: -2.1580430882648356, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_570, vlb: -2.602690487745901, took: 1.0609087944030762\n",
      "Validation vlb: -2.2385136579618483, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_571, vlb: -2.590982889845769, took: 1.0294270515441895\n",
      "Validation vlb: -2.1697932659229413, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_572, vlb: -2.6244861132642714, took: 1.0577940940856934\n",
      "Validation vlb: -2.2048219627547034, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_573, vlb: -2.60847832008708, took: 1.0444459915161133\n",
      "Validation vlb: -2.2756382562581776, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_574, vlb: -2.5680171050826206, took: 1.0333569049835205\n",
      "Validation vlb: -2.136622339390628, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_575, vlb: -2.607805384377331, took: 1.0758922100067139\n",
      "Validation vlb: -2.1978275914794034, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_576, vlb: -2.5786803531955806, took: 1.0531771183013916\n",
      "Validation vlb: -2.202289765706726, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_577, vlb: -2.5587495175204693, took: 1.1418468952178955\n",
      "Validation vlb: -2.2409998668436093, Best vlb: -1.9862191167849939\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_578, vlb: -2.6106504397208545, took: 1.0255250930786133\n",
      "Validation vlb: -2.0190378307910413, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_579, vlb: -2.6162249737177388, took: 1.030886173248291\n",
      "Validation vlb: -2.34572311589633, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_580, vlb: -2.5897828821746143, took: 1.0200870037078857\n",
      "Validation vlb: -2.0116894391748126, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_581, vlb: -2.5726728959610705, took: 1.0230939388275146\n",
      "Validation vlb: -2.15884376498102, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_582, vlb: -2.545243451524047, took: 1.0407569408416748\n",
      "Validation vlb: -2.1770386410376785, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_583, vlb: -2.6061039701141726, took: 1.046983242034912\n",
      "Validation vlb: -2.202388391525614, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_584, vlb: -2.594226649308059, took: 1.0442781448364258\n",
      "Validation vlb: -2.2998112613715014, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_585, vlb: -2.6031620898262218, took: 1.0729179382324219\n",
      "Validation vlb: -2.2823327228089365, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_586, vlb: -2.5901442645563684, took: 1.0437917709350586\n",
      "Validation vlb: -2.2524494407246416, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_587, vlb: -2.5732327717261474, took: 1.0540258884429932\n",
      "Validation vlb: -2.1991560752337804, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_588, vlb: -2.5714102931532623, took: 1.0462110042572021\n",
      "Validation vlb: -2.2620395595587572, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_589, vlb: -2.535488084017651, took: 1.0254359245300293\n",
      "Validation vlb: -2.216509815944437, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_590, vlb: -2.581070181702726, took: 1.0294272899627686\n",
      "Validation vlb: -2.1519397230981623, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_591, vlb: -2.5854940623055698, took: 1.1296958923339844\n",
      "Validation vlb: -2.1844975342642527, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_592, vlb: -2.5702698496034517, took: 1.058786153793335\n",
      "Validation vlb: -2.1867413374212568, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_593, vlb: -2.5490122627859972, took: 1.0399870872497559\n",
      "Validation vlb: -2.1973482381950302, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_594, vlb: -2.6486835600791494, took: 1.0392730236053467\n",
      "Validation vlb: -2.250398531700801, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_595, vlb: -2.5253876269529054, took: 1.0279159545898438\n",
      "Validation vlb: -2.2322699475828496, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_596, vlb: -2.599861101491184, took: 1.0251169204711914\n",
      "Validation vlb: -2.193334739956655, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_597, vlb: -2.5716795516091366, took: 1.0151238441467285\n",
      "Validation vlb: -2.2065828070285636, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_598, vlb: -2.553992084088324, took: 1.0671939849853516\n",
      "Validation vlb: -2.223170134241913, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_599, vlb: -2.577586385213861, took: 1.0177960395812988\n",
      "Validation vlb: -2.1320601504983254, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_600, vlb: -2.597687908072329, took: 1.0110089778900146\n",
      "Validation vlb: -2.2674223142148606, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_601, vlb: -2.5819956497790617, took: 1.0147497653961182\n",
      "Validation vlb: -2.110591954783714, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_602, vlb: -2.570608782089997, took: 1.1284959316253662\n",
      "Validation vlb: -2.231592199948999, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_603, vlb: -2.652501035260347, took: 1.0111680030822754\n",
      "Validation vlb: -2.3200708793590756, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_604, vlb: -2.605115143137524, took: 1.4482543468475342\n",
      "Validation vlb: -2.16118527616112, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_605, vlb: -2.577861441096085, took: 1.4129180908203125\n",
      "Validation vlb: -2.180078583627843, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_606, vlb: -2.5441649430597435, took: 1.0550730228424072\n",
      "Validation vlb: -2.219739111496021, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_607, vlb: -2.6072040426675303, took: 1.0498747825622559\n",
      "Validation vlb: -2.172694743258282, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_608, vlb: -2.5577120874526478, took: 1.0565598011016846\n",
      "Validation vlb: -2.2037290423433373, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_609, vlb: -2.5778989373757595, took: 1.1471340656280518\n",
      "Validation vlb: -2.2567980983882276, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_610, vlb: -2.545873122975944, took: 1.0559601783752441\n",
      "Validation vlb: -2.30084034000014, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_611, vlb: -2.5555333851651834, took: 1.024026870727539\n",
      "Validation vlb: -2.2038222782820176, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_612, vlb: -2.5604349351161066, took: 1.0144240856170654\n",
      "Validation vlb: -2.089077869279485, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_613, vlb: -2.583346782676771, took: 1.0317480564117432\n",
      "Validation vlb: -2.2949355597634917, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_614, vlb: -2.613316308005405, took: 1.0371809005737305\n",
      "Validation vlb: -2.231705690278976, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_615, vlb: -2.5961983058793447, took: 1.0251038074493408\n",
      "Validation vlb: -2.246889473165123, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_616, vlb: -2.614934933026746, took: 1.0591809749603271\n",
      "Validation vlb: -2.07380047353726, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_617, vlb: -2.6006999659718564, took: 1.0416240692138672\n",
      "Validation vlb: -2.1322213183714736, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_618, vlb: -2.604269965672862, took: 1.0425260066986084\n",
      "Validation vlb: -2.2856757471090767, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_619, vlb: -2.5979403127300085, took: 1.0419538021087646\n",
      "Validation vlb: -2.073836636774748, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_620, vlb: -2.572637192585677, took: 1.0383827686309814\n",
      "Validation vlb: -2.1517537635506936, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_621, vlb: -2.5898112332086836, took: 1.0324301719665527\n",
      "Validation vlb: -2.2115854862052644, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_622, vlb: -2.607314159426279, took: 1.0449316501617432\n",
      "Validation vlb: -2.009237644741836, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_623, vlb: -2.6250566759683007, took: 1.0201451778411865\n",
      "Validation vlb: -2.217219899773212, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_624, vlb: -2.6221145278565268, took: 1.0322659015655518\n",
      "Validation vlb: -2.090858629992093, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_625, vlb: -2.568046861902003, took: 1.0411350727081299\n",
      "Validation vlb: -2.1676385248363212, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_626, vlb: -2.540941869006296, took: 1.11940598487854\n",
      "Validation vlb: -2.176275114025499, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_627, vlb: -2.5717923273746597, took: 1.0268831253051758\n",
      "Validation vlb: -2.2845170598199838, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_628, vlb: -2.5720929181700263, took: 1.0240838527679443\n",
      "Validation vlb: -2.1857481650935795, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_629, vlb: -2.577580200840253, took: 1.0290889739990234\n",
      "Validation vlb: -2.2018454429786956, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_630, vlb: -2.5830551520242166, took: 1.0238258838653564\n",
      "Validation vlb: -2.1268169378385573, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_631, vlb: -2.6029863596220086, took: 1.029062032699585\n",
      "Validation vlb: -2.1927520773557396, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_632, vlb: -2.632531517480061, took: 1.0300171375274658\n",
      "Validation vlb: -2.2828154394155953, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_633, vlb: -2.567191465921783, took: 1.0529930591583252\n",
      "Validation vlb: -2.1962699905568344, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_634, vlb: -2.604309543156993, took: 1.1457386016845703\n",
      "Validation vlb: -2.249809453402522, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_635, vlb: -2.638009711482948, took: 1.113590955734253\n",
      "Validation vlb: -2.2442238276830384, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_636, vlb: -2.6145206544310833, took: 1.04768705368042\n",
      "Validation vlb: -2.1984419645229205, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_637, vlb: -2.5895879236021444, took: 1.0286529064178467\n",
      "Validation vlb: -2.097507409293289, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_638, vlb: -2.592633578946447, took: 1.0457351207733154\n",
      "Validation vlb: -2.145953764807445, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_639, vlb: -2.5920690051248103, took: 1.0200262069702148\n",
      "Validation vlb: -2.125719898341157, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_640, vlb: -2.5940924625753885, took: 1.0457210540771484\n",
      "Validation vlb: -2.1159236045331244, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_641, vlb: -2.586593242957088, took: 1.0740840435028076\n",
      "Validation vlb: -2.092533415961034, Best vlb: -1.9862191167849939\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_642, vlb: -2.5486921798766495, took: 1.1192729473114014\n",
      "Validation vlb: -2.100238334013806, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_643, vlb: -2.5977557569173415, took: 1.0059447288513184\n",
      "Validation vlb: -2.2526850816115593, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_644, vlb: -2.5669501166056934, took: 1.0916800498962402\n",
      "Validation vlb: -2.305056446964301, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_645, vlb: -2.599904046970432, took: 1.220870018005371\n",
      "Validation vlb: -2.213280989128409, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_646, vlb: -2.5539547642401152, took: 1.0562870502471924\n",
      "Validation vlb: -2.2057195562375016, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_647, vlb: -2.575128866091966, took: 1.045531988143921\n",
      "Validation vlb: -2.2747843882798375, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_648, vlb: -2.5906253056771726, took: 1.0151541233062744\n",
      "Validation vlb: -2.2938333529870487, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_649, vlb: -2.5975773325059155, took: 1.0364418029785156\n",
      "Validation vlb: -2.156198030150824, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_650, vlb: -2.5706343656827704, took: 1.1244699954986572\n",
      "Validation vlb: -2.2558684657692525, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_651, vlb: -2.5525924010288814, took: 1.02642822265625\n",
      "Validation vlb: -2.0554562224539352, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_652, vlb: -2.5589741587338364, took: 1.066910982131958\n",
      "Validation vlb: -2.280343702695902, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_653, vlb: -2.588384804942001, took: 1.0590178966522217\n",
      "Validation vlb: -2.2230748586284306, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_654, vlb: -2.576447806858012, took: 1.0474579334259033\n",
      "Validation vlb: -2.0831506202136043, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_655, vlb: -2.5300074119039007, took: 1.0193018913269043\n",
      "Validation vlb: -2.179886808673155, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_656, vlb: -2.538680429290343, took: 1.0411663055419922\n",
      "Validation vlb: -2.1844635310682277, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_657, vlb: -2.507498966346853, took: 1.0554940700531006\n",
      "Validation vlb: -2.2958613614819967, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_658, vlb: -2.557704424145499, took: 1.0956082344055176\n",
      "Validation vlb: -2.3169132683269416, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_659, vlb: -2.545492862942496, took: 1.04581880569458\n",
      "Validation vlb: -1.990261383041209, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_660, vlb: -2.5556283035765976, took: 1.012916088104248\n",
      "Validation vlb: -2.37398533450747, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_661, vlb: -2.542228312384194, took: 1.0256900787353516\n",
      "Validation vlb: -2.109864317483501, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_662, vlb: -2.6420506330372664, took: 1.027405023574829\n",
      "Validation vlb: -2.2263128958087908, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_663, vlb: -2.5747561702893114, took: 1.038334846496582\n",
      "Validation vlb: -2.2393250365087516, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_664, vlb: -2.579997376653845, took: 1.0149800777435303\n",
      "Validation vlb: -2.0703746913706214, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_665, vlb: -2.5700017477824697, took: 1.0296480655670166\n",
      "Validation vlb: -2.365174900752441, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_666, vlb: -2.5387506831838453, took: 1.0391631126403809\n",
      "Validation vlb: -2.095230357160846, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_667, vlb: -2.542074940038921, took: 1.1300780773162842\n",
      "Validation vlb: -2.1633113078700688, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_668, vlb: -2.5896486798163836, took: 1.0448272228240967\n",
      "Validation vlb: -2.2416717303995175, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_669, vlb: -2.607712360178776, took: 1.0294950008392334\n",
      "Validation vlb: -2.2208111702817157, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_670, vlb: -2.6043813973826215, took: 1.0133848190307617\n",
      "Validation vlb: -2.1222076956122438, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_671, vlb: -2.592136937077199, took: 1.0321640968322754\n",
      "Validation vlb: -2.1697001557519906, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_672, vlb: -2.5273542148327715, took: 1.1394648551940918\n",
      "Validation vlb: -2.188715267335713, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_673, vlb: -2.6210428936944234, took: 1.0260040760040283\n",
      "Validation vlb: -2.105327377813148, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_674, vlb: -2.5498881087613956, took: 1.2768750190734863\n",
      "Validation vlb: -2.190605580228046, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_675, vlb: -2.565867632037236, took: 1.2038021087646484\n",
      "Validation vlb: -2.202115078960036, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_676, vlb: -2.5749780235421027, took: 1.0334899425506592\n",
      "Validation vlb: -2.153662072802053, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_677, vlb: -2.546730462964765, took: 1.0318632125854492\n",
      "Validation vlb: -2.1598301940751305, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_678, vlb: -2.5551957914114083, took: 1.0302789211273193\n",
      "Validation vlb: -2.251478023899412, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_679, vlb: -2.5672509060689, took: 1.049095869064331\n",
      "Validation vlb: -2.121435562769572, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_680, vlb: -2.556460175876528, took: 1.0119779109954834\n",
      "Validation vlb: -2.1042370611024133, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_681, vlb: -2.595039526936301, took: 1.022953987121582\n",
      "Validation vlb: -2.397270270535861, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_682, vlb: -2.5308789555271796, took: 1.03519606590271\n",
      "Validation vlb: -2.0329546025655802, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_683, vlb: -2.58813141282617, took: 1.1889212131500244\n",
      "Validation vlb: -2.199862674602027, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_684, vlb: -2.622254060848952, took: 1.050311803817749\n",
      "Validation vlb: -2.1288737340266652, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_685, vlb: -2.559402290247394, took: 1.0299391746520996\n",
      "Validation vlb: -2.296863336779153, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_686, vlb: -2.5673390280054247, took: 1.0502400398254395\n",
      "Validation vlb: -2.1929344814572134, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_687, vlb: -2.5582239983276707, took: 1.0312249660491943\n",
      "Validation vlb: -2.312221811813058, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_688, vlb: -2.5285060169630698, took: 1.0512239933013916\n",
      "Validation vlb: -2.018609489052041, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_689, vlb: -2.553342720223366, took: 1.029923915863037\n",
      "Validation vlb: -2.2984167549602423, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_690, vlb: -2.6189756960073822, took: 1.0412988662719727\n",
      "Validation vlb: -2.21946498028283, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_691, vlb: -2.5727876090814537, took: 1.1032800674438477\n",
      "Validation vlb: -2.151609315069748, Best vlb: -1.9862191167849939\n",
      "\n",
      "Epoch_692, vlb: -2.577816069533939, took: 1.2725448608398438\n",
      "Validation vlb: -1.9736776421371016, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_693, vlb: -2.517551028389359, took: 1.04561185836792\n",
      "Validation vlb: -2.2159884384920683, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_694, vlb: -2.57679468528547, took: 1.0626928806304932\n",
      "Validation vlb: -2.207365163321634, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_695, vlb: -2.578574972355327, took: 1.0481560230255127\n",
      "Validation vlb: -2.2023190082469806, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_696, vlb: -2.596397991012145, took: 1.2750232219696045\n",
      "Validation vlb: -2.2006618004400753, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_697, vlb: -2.5483228350698757, took: 1.034419059753418\n",
      "Validation vlb: -2.235497051843933, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_698, vlb: -2.596159594669418, took: 1.0559508800506592\n",
      "Validation vlb: -2.0534496778037554, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_699, vlb: -2.5900565181913504, took: 1.0141139030456543\n",
      "Validation vlb: -2.1962494360204654, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_700, vlb: -2.612379884393601, took: 1.121964931488037\n",
      "Validation vlb: -2.300727885903664, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_701, vlb: -2.5573061549667835, took: 1.348344087600708\n",
      "Validation vlb: -2.219813571392911, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_702, vlb: -2.6207136505148934, took: 1.030472993850708\n",
      "Validation vlb: -2.21280827761468, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_703, vlb: -2.5931570475849264, took: 1.0743658542633057\n",
      "Validation vlb: -2.162961283933769, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_704, vlb: -2.611100420747509, took: 1.0236809253692627\n",
      "Validation vlb: -2.1523659946849043, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_705, vlb: -2.562282955728077, took: 1.0475761890411377\n",
      "Validation vlb: -2.040826858054473, Best vlb: -1.9736776421371016\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_706, vlb: -2.5773261051020007, took: 1.0410189628601074\n",
      "Validation vlb: -2.071245242476849, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_707, vlb: -2.597263336954333, took: 1.0380628108978271\n",
      "Validation vlb: -2.272795858506632, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_708, vlb: -2.5679710760449694, took: 1.0346417427062988\n",
      "Validation vlb: -2.3534888387883752, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_709, vlb: -2.539570269242954, took: 1.0171740055084229\n",
      "Validation vlb: -2.1617093313859117, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_710, vlb: -2.5737111438810287, took: 1.0360298156738281\n",
      "Validation vlb: -2.290390151989884, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_711, vlb: -2.543811332934788, took: 1.140392780303955\n",
      "Validation vlb: -2.271189682692, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_712, vlb: -2.5855282416736904, took: 1.1895008087158203\n",
      "Validation vlb: -2.1565519964810713, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_713, vlb: -2.5725268765581055, took: 1.2782719135284424\n",
      "Validation vlb: -2.036341613550402, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_714, vlb: -2.53386199143794, took: 1.9209392070770264\n",
      "Validation vlb: -2.1027881304423013, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_715, vlb: -2.5161748720580484, took: 1.555600881576538\n",
      "Validation vlb: -2.202810873105688, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_716, vlb: -2.565337897291448, took: 1.3424890041351318\n",
      "Validation vlb: -2.1142694687766164, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_717, vlb: -2.5845889877749295, took: 1.7441580295562744\n",
      "Validation vlb: -2.1789682706197104, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_718, vlb: -2.542667311217487, took: 1.4269812107086182\n",
      "Validation vlb: -2.3041375439529665, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_719, vlb: -2.6032103153712294, took: 1.3987128734588623\n",
      "Validation vlb: -2.1802420616149902, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_720, vlb: -2.5248533505778714, took: 1.3463737964630127\n",
      "Validation vlb: -2.2487935410348343, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_721, vlb: -2.544582763162928, took: 1.4371089935302734\n",
      "Validation vlb: -2.134335475446337, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_722, vlb: -2.5930677803034055, took: 1.916410207748413\n",
      "Validation vlb: -2.16961985305675, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_723, vlb: -2.514328058305987, took: 1.230668067932129\n",
      "Validation vlb: -2.160415884746317, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_724, vlb: -2.5766294606091695, took: 1.0469038486480713\n",
      "Validation vlb: -2.4326148256900626, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_725, vlb: -2.5327384625573273, took: 1.0474650859832764\n",
      "Validation vlb: -2.1953598997739525, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_726, vlb: -2.6293724892935306, took: 1.0093331336975098\n",
      "Validation vlb: -2.1970041859111356, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_727, vlb: -2.5714062492668006, took: 1.048367977142334\n",
      "Validation vlb: -2.346233818909111, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_728, vlb: -2.54155286748417, took: 1.0253660678863525\n",
      "Validation vlb: -2.1577332856971467, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_729, vlb: -2.5223289332803653, took: 0.9925880432128906\n",
      "Validation vlb: -2.13437190063563, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_730, vlb: -2.564406694076368, took: 1.0189568996429443\n",
      "Validation vlb: -2.1940164589187474, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_731, vlb: -2.5974400356857306, took: 1.0011601448059082\n",
      "Validation vlb: -2.296239690873229, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_732, vlb: -2.5707610002901644, took: 1.0270068645477295\n",
      "Validation vlb: -2.2385417913541823, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_733, vlb: -2.5899159142470505, took: 1.0312941074371338\n",
      "Validation vlb: -2.3830729364191443, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_734, vlb: -2.545060008216943, took: 0.9915659427642822\n",
      "Validation vlb: -2.261611154550102, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_735, vlb: -2.5982595014623655, took: 1.0368766784667969\n",
      "Validation vlb: -2.16690378513151, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_736, vlb: -2.5601326051615194, took: 1.558466911315918\n",
      "Validation vlb: -2.08259391823247, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_737, vlb: -2.5643660276108693, took: 1.7936780452728271\n",
      "Validation vlb: -2.18538435145875, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_738, vlb: -2.524731002782891, took: 1.1515922546386719\n",
      "Validation vlb: -2.2211073687161442, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_739, vlb: -2.633103486312822, took: 1.153991937637329\n",
      "Validation vlb: -2.11387137147601, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_740, vlb: -2.5540481109090276, took: 1.1908419132232666\n",
      "Validation vlb: -2.199652804525925, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_741, vlb: -2.554309793972995, took: 1.2468762397766113\n",
      "Validation vlb: -2.2197555739516965, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_742, vlb: -2.610464938819344, took: 1.3230400085449219\n",
      "Validation vlb: -2.1604627768198648, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_743, vlb: -2.5646775683182197, took: 1.8053982257843018\n",
      "Validation vlb: -2.289866698212608, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_744, vlb: -2.5763458463842595, took: 1.388970136642456\n",
      "Validation vlb: -2.2734667413828826, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_745, vlb: -2.6054046531783017, took: 1.435723066329956\n",
      "Validation vlb: -2.23788773820624, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_746, vlb: -2.5917107528347225, took: 1.2683050632476807\n",
      "Validation vlb: -2.0479693208311756, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_747, vlb: -2.5386388010801837, took: 1.0465331077575684\n",
      "Validation vlb: -2.1419720348802587, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_748, vlb: -2.559968529049767, took: 1.0410668849945068\n",
      "Validation vlb: -2.2920934984213326, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_749, vlb: -2.531521210448966, took: 1.0648887157440186\n",
      "Validation vlb: -2.241270990819221, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_750, vlb: -2.58170287249369, took: 1.0764241218566895\n",
      "Validation vlb: -2.2182823539166003, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_751, vlb: -2.5844393689125016, took: 1.205915927886963\n",
      "Validation vlb: -2.113124750964464, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_752, vlb: -2.535787835668908, took: 1.1992387771606445\n",
      "Validation vlb: -2.1181592883415594, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_753, vlb: -2.553851596219616, took: 1.0465688705444336\n",
      "Validation vlb: -2.1412830719284255, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_754, vlb: -2.542434286032386, took: 1.0142958164215088\n",
      "Validation vlb: -2.134569037307813, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_755, vlb: -2.5584912968059648, took: 1.0514512062072754\n",
      "Validation vlb: -2.2175240470367727, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_756, vlb: -2.561598912820483, took: 1.03147292137146\n",
      "Validation vlb: -2.335738303591904, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_757, vlb: -2.540497642478627, took: 1.0438590049743652\n",
      "Validation vlb: -2.2145037998273533, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_758, vlb: -2.6210390001825177, took: 1.0396161079406738\n",
      "Validation vlb: -2.059636015722281, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_759, vlb: -2.5918566861253955, took: 1.0425310134887695\n",
      "Validation vlb: -2.2130135448233594, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_760, vlb: -2.558558926463857, took: 1.0316081047058105\n",
      "Validation vlb: -2.1390998594969224, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_761, vlb: -2.5579720857034367, took: 1.0345070362091064\n",
      "Validation vlb: -2.1975182584188517, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_762, vlb: -2.5896608087713444, took: 1.0266318321228027\n",
      "Validation vlb: -2.280633705719389, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_763, vlb: -2.5577785968780518, took: 1.0376760959625244\n",
      "Validation vlb: -2.1074879019005786, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_764, vlb: -2.534882952851875, took: 1.0432310104370117\n",
      "Validation vlb: -2.088171460481909, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_765, vlb: -2.5104296304443823, took: 1.0866999626159668\n",
      "Validation vlb: -2.181798562262822, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_766, vlb: -2.578795587067815, took: 1.119555950164795\n",
      "Validation vlb: -2.3188910931831037, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_767, vlb: -2.558756407705794, took: 1.0361337661743164\n",
      "Validation vlb: -2.1620254431727637, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_768, vlb: -2.556421305663645, took: 1.0738499164581299\n",
      "Validation vlb: -2.0947174555275434, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_769, vlb: -2.5533943183996457, took: 1.0163347721099854\n",
      "Validation vlb: -2.190003560198935, Best vlb: -1.9736776421371016\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_770, vlb: -2.546787801130581, took: 1.0432300567626953\n",
      "Validation vlb: -2.2516806534578886, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_771, vlb: -2.5378308477357088, took: 1.0721678733825684\n",
      "Validation vlb: -2.1963079245730897, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_772, vlb: -2.5591516530809963, took: 1.0414369106292725\n",
      "Validation vlb: -2.1149994779173222, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_773, vlb: -2.57279804322116, took: 1.0564029216766357\n",
      "Validation vlb: -2.167155591415356, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_774, vlb: -2.571687562971352, took: 1.1819219589233398\n",
      "Validation vlb: -2.225624188636113, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_775, vlb: -2.60387894838544, took: 1.0478880405426025\n",
      "Validation vlb: -2.1887815843508083, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_776, vlb: -2.5535921025370625, took: 1.0444610118865967\n",
      "Validation vlb: -2.1793854093860268, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_777, vlb: -2.520596256863955, took: 1.0419011116027832\n",
      "Validation vlb: -2.1019181977583754, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_778, vlb: -2.554647000607802, took: 1.0293810367584229\n",
      "Validation vlb: -2.2315391861505107, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_779, vlb: -2.6156100762332564, took: 1.0257339477539062\n",
      "Validation vlb: -2.1613808719857226, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_780, vlb: -2.5973465334480514, took: 1.0674118995666504\n",
      "Validation vlb: -2.2224406532485124, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_781, vlb: -2.5189549756384944, took: 1.0364809036254883\n",
      "Validation vlb: -2.143156397304103, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_782, vlb: -2.5265408750485294, took: 1.1371550559997559\n",
      "Validation vlb: -1.9977066235249097, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_783, vlb: -2.538548247523131, took: 1.0660228729248047\n",
      "Validation vlb: -2.3091506163279214, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_784, vlb: -2.5421802219745278, took: 1.0574970245361328\n",
      "Validation vlb: -2.186945211540148, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_785, vlb: -2.5997416594494664, took: 1.04463529586792\n",
      "Validation vlb: -2.1606890036450235, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_786, vlb: -2.533715016300145, took: 1.0273668766021729\n",
      "Validation vlb: -2.2775933480185597, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_787, vlb: -2.5505130357951793, took: 1.0138869285583496\n",
      "Validation vlb: -2.149147399420877, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_788, vlb: -2.5492479322967143, took: 1.0424950122833252\n",
      "Validation vlb: -2.144315998916873, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_789, vlb: -2.5309494899080778, took: 1.4626469612121582\n",
      "Validation vlb: -2.265656564613762, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_790, vlb: -2.5975421165278116, took: 1.4326467514038086\n",
      "Validation vlb: -2.1762941585775333, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_791, vlb: -2.585604515271432, took: 1.2523789405822754\n",
      "Validation vlb: -2.1719875551736085, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_792, vlb: -2.5269082605645603, took: 1.1485440731048584\n",
      "Validation vlb: -2.21229339957623, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_793, vlb: -2.543393870502399, took: 1.0414791107177734\n",
      "Validation vlb: -2.314955197491692, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_794, vlb: -2.5620699450400477, took: 1.0284829139709473\n",
      "Validation vlb: -2.06757434980769, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_795, vlb: -2.5521260432558264, took: 1.0286600589752197\n",
      "Validation vlb: -2.279481021331738, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_796, vlb: -2.5719405274018565, took: 1.0318231582641602\n",
      "Validation vlb: -2.1855173365583695, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_797, vlb: -2.5520432739752335, took: 1.0675437450408936\n",
      "Validation vlb: -2.235084338095582, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_798, vlb: -2.5522358223479302, took: 1.0230190753936768\n",
      "Validation vlb: -2.149442191262847, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_799, vlb: -2.5653480931413575, took: 1.0437090396881104\n",
      "Validation vlb: -2.3072395000642945, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_800, vlb: -2.5738092484472466, took: 1.0690581798553467\n",
      "Validation vlb: -2.246161073928512, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_801, vlb: -2.5374442878540444, took: 1.0218539237976074\n",
      "Validation vlb: -2.3566722337482044, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_802, vlb: -2.5463479363665034, took: 1.0442559719085693\n",
      "Validation vlb: -2.0797793433118406, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_803, vlb: -2.515924271895038, took: 1.0194909572601318\n",
      "Validation vlb: -2.286235371839653, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_804, vlb: -2.5812478119579203, took: 1.1206021308898926\n",
      "Validation vlb: -2.1917935054279067, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_805, vlb: -2.599609754134478, took: 1.0351440906524658\n",
      "Validation vlb: -2.2561229752877, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_806, vlb: -2.523016048928667, took: 1.0802359580993652\n",
      "Validation vlb: -2.3073117956760245, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_807, vlb: -2.575262109281178, took: 1.0273349285125732\n",
      "Validation vlb: -2.119528883097627, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_808, vlb: -2.5738370793733742, took: 1.0367217063903809\n",
      "Validation vlb: -2.0854804716449724, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_809, vlb: -2.5342028782356545, took: 1.0251379013061523\n",
      "Validation vlb: -2.189925409057765, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_810, vlb: -2.5646658682076247, took: 1.0312910079956055\n",
      "Validation vlb: -2.2166394863313843, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_811, vlb: -2.535005108589829, took: 1.0191559791564941\n",
      "Validation vlb: -2.2223035409612564, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_812, vlb: -2.5657687457637217, took: 1.0443718433380127\n",
      "Validation vlb: -2.1660864411048517, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_813, vlb: -2.5184675298408505, took: 1.0248630046844482\n",
      "Validation vlb: -2.178393022913763, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_814, vlb: -2.602074132018867, took: 1.1407990455627441\n",
      "Validation vlb: -2.1771255295639285, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_815, vlb: -2.5377190385227113, took: 1.0234038829803467\n",
      "Validation vlb: -2.1397440483654973, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_816, vlb: -2.577271263162395, took: 1.0403077602386475\n",
      "Validation vlb: -2.099875477911199, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_817, vlb: -2.5284912491771956, took: 1.0461900234222412\n",
      "Validation vlb: -2.1633817285395747, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_818, vlb: -2.559128765069112, took: 1.0682849884033203\n",
      "Validation vlb: -2.1687799329510784, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_819, vlb: -2.5792293054060753, took: 1.0202128887176514\n",
      "Validation vlb: -2.2244134134459266, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_820, vlb: -2.5366870982637497, took: 1.0501549243927002\n",
      "Validation vlb: -2.089072933860581, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_821, vlb: -2.5521634650994516, took: 1.053347110748291\n",
      "Validation vlb: -2.1866535881962204, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_822, vlb: -2.564584126092995, took: 1.137267827987671\n",
      "Validation vlb: -2.226439844057398, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_823, vlb: -2.524396032234469, took: 1.042849063873291\n",
      "Validation vlb: -2.2477483024103355, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_824, vlb: -2.5887783785968708, took: 1.1344094276428223\n",
      "Validation vlb: -2.2124202687377683, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_825, vlb: -2.542594832742133, took: 1.0308780670166016\n",
      "Validation vlb: -2.1993869053121524, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_826, vlb: -2.5294263760391913, took: 1.03385591506958\n",
      "Validation vlb: -2.3618179270364705, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_827, vlb: -2.536041090717666, took: 1.0289559364318848\n",
      "Validation vlb: -2.173340913933072, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_828, vlb: -2.5889019906885724, took: 1.1733520030975342\n",
      "Validation vlb: -2.169182180200966, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_829, vlb: -2.4970398350160328, took: 1.1378650665283203\n",
      "Validation vlb: -2.273611404749182, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_830, vlb: -2.5886715543669334, took: 1.138066291809082\n",
      "Validation vlb: -2.09213054681673, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_831, vlb: -2.590630522210434, took: 1.011974811553955\n",
      "Validation vlb: -2.2147044541380554, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_832, vlb: -2.543973930284708, took: 1.0506467819213867\n",
      "Validation vlb: -2.279049738711138, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_833, vlb: -2.5107963112648073, took: 1.024960994720459\n",
      "Validation vlb: -2.1817305288654314, Best vlb: -1.9736776421371016\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_834, vlb: -2.5664249915719504, took: 1.028838872909546\n",
      "Validation vlb: -2.1854711481668416, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_835, vlb: -2.5831400644105247, took: 1.0234389305114746\n",
      "Validation vlb: -2.1863011048449668, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_836, vlb: -2.5563868482979157, took: 1.028433084487915\n",
      "Validation vlb: -2.2138004800648363, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_837, vlb: -2.5614174466619057, took: 1.0626888275146484\n",
      "Validation vlb: -2.212507406098943, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_838, vlb: -2.568433450115727, took: 1.0737552642822266\n",
      "Validation vlb: -2.258931457031892, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_839, vlb: -2.5765392059124244, took: 1.0239040851593018\n",
      "Validation vlb: -2.306581571264174, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_840, vlb: -2.592260145136552, took: 1.0190849304199219\n",
      "Validation vlb: -2.227310592688403, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_841, vlb: -2.537590939920499, took: 1.0274789333343506\n",
      "Validation vlb: -2.2783622163013346, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_842, vlb: -2.5480775349767764, took: 1.084190845489502\n",
      "Validation vlb: -2.227347871632252, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_843, vlb: -2.5662984045280757, took: 1.0145938396453857\n",
      "Validation vlb: -2.2249183701080026, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_844, vlb: -2.5749588736450666, took: 1.0361106395721436\n",
      "Validation vlb: -2.182687141362903, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_845, vlb: -2.5238874422203175, took: 1.0521891117095947\n",
      "Validation vlb: -2.213723137154934, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_846, vlb: -2.5760265859632043, took: 1.0185198783874512\n",
      "Validation vlb: -2.155590582434028, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_847, vlb: -2.509243260710884, took: 1.0961828231811523\n",
      "Validation vlb: -2.2919272427420014, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_848, vlb: -2.5003157128178475, took: 1.0714900493621826\n",
      "Validation vlb: -2.1394890627814727, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_849, vlb: -2.576411717222198, took: 1.0199189186096191\n",
      "Validation vlb: -2.238728068021509, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_850, vlb: -2.5418447403329116, took: 1.0045239925384521\n",
      "Validation vlb: -2.120128480361889, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_851, vlb: -2.562463866149001, took: 1.0210671424865723\n",
      "Validation vlb: -2.253729668249976, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_852, vlb: -2.5854530715667647, took: 1.0197248458862305\n",
      "Validation vlb: -2.090752049171423, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_853, vlb: -2.5994209297964543, took: 1.0142390727996826\n",
      "Validation vlb: -2.094254879118169, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_854, vlb: -2.5222960783931305, took: 1.020853042602539\n",
      "Validation vlb: -2.1737646284998426, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_855, vlb: -2.5674751419620976, took: 1.134796142578125\n",
      "Validation vlb: -2.267484616307379, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_856, vlb: -2.5069039491255762, took: 1.0205507278442383\n",
      "Validation vlb: -2.0776181197860866, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_857, vlb: -2.555540885777623, took: 1.066377878189087\n",
      "Validation vlb: -2.2143206380331786, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_858, vlb: -2.57215506711966, took: 1.0139670372009277\n",
      "Validation vlb: -2.23478534229365, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_859, vlb: -2.5646839690457415, took: 1.0329339504241943\n",
      "Validation vlb: -2.165834559591843, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_860, vlb: -2.5353755641850713, took: 1.0289018154144287\n",
      "Validation vlb: -2.125974201075853, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_861, vlb: -2.5191997010200455, took: 1.0056171417236328\n",
      "Validation vlb: -2.0713208238669583, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_862, vlb: -2.5476299196771466, took: 1.0289411544799805\n",
      "Validation vlb: -2.184864988219005, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_863, vlb: -2.575032424875245, took: 1.1074259281158447\n",
      "Validation vlb: -2.355673479030819, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_864, vlb: -2.5920454410069445, took: 1.020693063735962\n",
      "Validation vlb: -2.150132740199759, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_865, vlb: -2.546586017708234, took: 1.0308268070220947\n",
      "Validation vlb: -2.1528459374572853, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_866, vlb: -2.5083099328917826, took: 1.0525240898132324\n",
      "Validation vlb: -2.2109691857520044, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_867, vlb: -2.54171673290842, took: 1.0288019180297852\n",
      "Validation vlb: -2.0683420881098527, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_868, vlb: -2.568874737492645, took: 1.0161888599395752\n",
      "Validation vlb: -2.227913824099939, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_869, vlb: -2.5660795781695676, took: 1.0469541549682617\n",
      "Validation vlb: -2.288821510512466, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_870, vlb: -2.578604695090382, took: 1.0229449272155762\n",
      "Validation vlb: -2.140483206915624, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_871, vlb: -2.50583965391146, took: 1.0809552669525146\n",
      "Validation vlb: -2.1906160745034327, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_872, vlb: -2.5502100048367393, took: 1.156879186630249\n",
      "Validation vlb: -2.0892379866448807, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_873, vlb: -2.5017292560499094, took: 1.2003111839294434\n",
      "Validation vlb: -2.2856831157091753, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_874, vlb: -2.587823540262065, took: 1.205132007598877\n",
      "Validation vlb: -2.172978111069565, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_875, vlb: -2.5118065838509858, took: 1.4059371948242188\n",
      "Validation vlb: -2.2147426211718217, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_876, vlb: -2.524097597508882, took: 1.4329800605773926\n",
      "Validation vlb: -2.097706074081964, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_877, vlb: -2.520320454389018, took: 1.277674913406372\n",
      "Validation vlb: -2.2244169534602984, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_878, vlb: -2.576605128778328, took: 1.5103297233581543\n",
      "Validation vlb: -2.2771479450769023, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_879, vlb: -2.606927264711176, took: 1.3224289417266846\n",
      "Validation vlb: -2.116600732587302, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_880, vlb: -2.5237036718925556, took: 1.2023429870605469\n",
      "Validation vlb: -2.057797418828921, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_881, vlb: -2.5816505451532077, took: 1.364241123199463\n",
      "Validation vlb: -2.13883920549189, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_882, vlb: -2.5687535623873914, took: 1.1066770553588867\n",
      "Validation vlb: -2.204415813618879, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_883, vlb: -2.5461624583023506, took: 1.0258870124816895\n",
      "Validation vlb: -2.3491456701531765, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_884, vlb: -2.5290692034581603, took: 1.0904200077056885\n",
      "Validation vlb: -2.1334332571060526, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_885, vlb: -2.5564285428737796, took: 1.3381590843200684\n",
      "Validation vlb: -2.2545442488587017, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_886, vlb: -2.553767511816303, took: 1.3082749843597412\n",
      "Validation vlb: -2.2023593121747753, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_887, vlb: -2.553783961067021, took: 1.062150001525879\n",
      "Validation vlb: -2.167833293525918, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_888, vlb: -2.5249162425143195, took: 1.0331940650939941\n",
      "Validation vlb: -2.1296074575590858, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_889, vlb: -2.5604521898558814, took: 1.1298880577087402\n",
      "Validation vlb: -2.2832931368867944, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_890, vlb: -2.5227751802273435, took: 1.2867350578308105\n",
      "Validation vlb: -2.2014798354176643, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_891, vlb: -2.5143730224874496, took: 1.1482319831848145\n",
      "Validation vlb: -2.254664106276429, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_892, vlb: -2.546459659724773, took: 1.4240550994873047\n",
      "Validation vlb: -2.1632855802677984, Best vlb: -1.9736776421371016\n",
      "\n",
      "Epoch_893, vlb: -2.5892818652342924, took: 1.2553110122680664\n",
      "Validation vlb: -2.2320555753306666, Best vlb: -1.9736776421371016\n",
      "\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "masker = top_masker_tensorflow(p=1)\n",
    "\n",
    "x_train, x_test, x_means, x_stds, y_train, y_test, feature_names, X_dims = \\\n",
    "    get_my_COMPAS(rseed=42, separate_test=True, test_ratio=0.1, save_dir='../data/')\n",
    "\n",
    "x_train, x_test, input_dim_vec = join_compas_targets(x_train, x_test, y_train, y_test, X_dims)\n",
    "\n",
    "print('Compas', x_train.shape, x_test.shape)\n",
    "print(input_dim_vec)\n",
    "\n",
    "dname = 'compas'\n",
    "print(dname)\n",
    "\n",
    "width = widths[names.index(dname)] # 350\n",
    "depth = depths[names.index(dname)] # number of hidden layers # 3\n",
    "latent_dim = latent_dims[names.index(dname)] # 4\n",
    "\n",
    "batch_size = 128\n",
    "nb_epochs = 2000\n",
    "early_stop = 200\n",
    "lr = 1e-4\n",
    "\n",
    "optimizer = tfa.optimizers.RectifiedAdam(lr = lr, epsilon = 1e-8)\n",
    "\n",
    "model = VAEAC_gauss_cat(width, depth, latent_dim, input_dim_vec, batch_size, lr, optimizer, save_model = True)\n",
    "\n",
    "vlb_train, vlb_val, best_epoch, best_vlb, curr_epoch = train_VAEAC(model, x_train, x_test, masker, nb_epochs, early_stop=early_stop, flatten = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9551adf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_vlb:  -1.9736776421371016\n",
      "best epoch:  692\n"
     ]
    }
   ],
   "source": [
    "print(\"best_vlb: \", best_vlb)\n",
    "print(\"best epoch: \", best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f3ac60b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEGCAYAAABsLkJ6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAA0G0lEQVR4nO3deXgUVfo24OeQBKIQAdlkEYMLiiBhiSAgGARR0Rk/HXUYF1BHUQcHd2aQQR11FMdxRFDBFRUdxA39qeACphVHwAkIgux7IgiEJRAga7/fH29XuqvTnXQ63akk9dzXVVd3LV11+nT1eatOnVNlRAREROQ+DZxOABEROYMBgIjIpRgAiIhcigGAiMilGACIiFwq0ekEVEXLli0lNTU1qs8ePnwYjRs3jm2C6jDmR3nMEzvmh11dzo+lS5fmikir4Ol1KgCkpqYiKysrqs96PB5kZGTENkF1GPOjPOaJHfPDri7nhzFmW6jprAIiInIpBgAiIpdiACAicikGACIil2IAICJyKQYAIiKXciQAGGOeMsasNcb8ZIyZY4xp5kQ6iIjczKkzgK8AdBOR7gDWAxjvUDqI6i0RwOt1OhVVt3s38MEHTqfCHRwJACLypYiU+EYXA+jgRDqI6quVK4EGDYCEBOCTT0Ivs28fsHNn1dddUgJ8913VPnP11cCFF0a27CWXAFdeCeTlVT1t1fXmm8DMmTW/XafUhp7ANwGYHW6mMWY0gNEA0KZNG3g8nqg2kp+fH/Vn6yPmhx4hf/RRe1xwwa9o0qS01ufJ1q3HomPHI2gQwWHbp5+2BXA6AODFF3cgJWW9bf4337TCww93BQBkZnpCriNcfrz2WipmzkzFCy8sRZcuhyJK+3vvZQBA2PzNzGyFtm0L8Je/dMfBg0kAgAULvsfxxxcBANauTcFJJx1GQoJgwYI2uPDCXyPKh6Iig6QkgTERJROjRmk6TzyxfDpr+/4RFRGJywBgPoBVIYbLApaZAGAOABPJOnv37i3RyszMjPqz9VFdyY9Fi0Ty8uKz7i+/FAFEbrlFxwPzZPFikf/7v/CfPXJEpFcvkT/8QeTss0W+/lokKUnk1Vfty733nsiTT4o8/LB/Wna2yEsvlV9nfr5Ibm7o7WVlaVr/+U+RNWtE1q2r+Lu99ZYuD4iMGVN+/vnn++eHE7yPFBWJ/PSTSKdO+rmPP644DXPnivTsKVJc7N/Wd9+J7N9ffllrfuDwzDMixx0nsnu3jp9zjsikSfr+jTcq3raISEGBLjt+vH/axIn6W4UTnCc//yzy7bf6fsGCTPF6K99uZYqLRQ4c0N9w61aRH38UWb7cP3/LFpHf/laXiRUAWRKqnA41sSYGADcAWATg2Eg/wwBQMa9X5IMPREpLK182nvnh9Yo88YTIr79WvNz69SJ799qnrVolUlKi7w8d0j30oov88z0ekeefD73NqVO1sNi9W2TnTvv8r78W6dJFZOVK/zSrkExMFDnmGJG5c78REU23VRCsXh067R98YC+s2re3Fx75+SJHj9qXsfTooeO//iqyfbsWAiKahmbNpCwNR47ofBGRhg31M9dcU3nBLSLy/vv+5W6/XQvDQIMG+eeHKtS8XpGvvvKUjf/73yItW9q/T0aGf/niYpEpU/R3twrn1q1DF+yALr9li/7GeXnhlwM0iFrvx4/X14kTQ3/vl1/W+UeP+n/HhAT/d7LWc9NNui5rXzp4UGThwvJ5Gzhu5f8ll4i88IKu7513RB59VGT6dF1m+3aRsWP1+4XyxBPhv+eoUboeK99mzgy9jmjUqgAA4CIAqwG0qsrnGAAqNmOG/qLTpun4vn3+oxfLZ59pwReYH++8owVvOMuWaSF8+HBk6fjf/zQdF1+s4yUloQsZQKRdO//4unU67YEHdHzrVh23CkXrM6EKvy1bdHrXrv5l1q8XmTfPfwRpDYsWiTz+uMhf/mKf/tRTy+XVV8v/Ma++WuTYY+3be/758H/k667T1/vus08vKNA/uDX+wgv+90eO+N9bBVerVvrapYu9MLfeW3+H2bNFLrtMJC1NzyAOHdJpgdvu3t2f9t27RU45xT8vP9/+3bxekdRUsQWHcN/1hx9EZs0Seftt+/TS0vIBI3Do0EFfzz1XpFGj8MsB9kLzb3/zv1+4UGTFCk3fmjUiOTkizZv75y9d6n/foIGerYVaf0KCBt/AaaH2t+DPffpp+c8MH67vFywov4++8UbF3zN4uPFGke+/1/9sddW2ALARQDaA5b5heiSfYwComFWgPfSQjo8YoePbtvmXsXauL77wlC0bqlA9ckSPokT0SA8Q+fzzyNLx3Xe6fJ8+/tPw5s3LBwFruw0b6h/n/vt1vG9fnW9Vexx/vI4HHlGvWSOyZ49/XStXhv8jnXxy1f544YbA7x9YEEU6TJkSfp4V7Ko6FBbaxy+/PPyyDRpogRtq2zt3ivy//6fBZ84c/7znntPAX1k6XnnFPp6bG5s8B0SGDfO/P+us8vMzM/X1lFNEGjf2Tx8woHrbDczLtLTy8y+80D4u4g+sn3xi39d/97vqpeXf/9aDnGjVqgAQ7cAAENqQIVrXau0s//iHTu/TR8fffNO/rLXMpEkryu1kn3wictVVWmcd+EeyjuROPdW+3eXLdfrEiXrK+803Wkhan0tPF9mxwz/+wQd6FPb3v1e+w19yiUjHjv7ti4hs3mxfJvCofNGi6v3BIh0s114b2/V+9lnNpD/U8K9/6dEmoMGyW7eqr8MqbP/0J321zkZreqjsbCKew8MP28enTtV95c03Y7P+r76KvoxgAKgFAWDsWJFHHgk978cf9ZRSROu5n3gisnXu319+R7n1VpFLL7VPmzfPXgcaarj11sh2xLZttXrpjjv800IdEffurXXo1d3xrTOAwDpaaxDRi4Xdu8fmTxbJnzA4HcnJsd9OTX2fWA4tWuiFcyfTEC4ANG0an+0FVsmFGqyz2lgMlV34rwgDgEMB4MkntQ57/Xr/DxlK4Dzr/YED2upCRFulzJ2rVTOBli2LfAf66aeK5199dfh5p51mH2/ZUqRNm4rXd8IJ9jrv6gz//a//jCZwmDUruvUFf9fbbgu9nDH26wqhhsDrBq+/7n//r3/5j4itoVmzytOWnCxy0kmxyTdrWLJE66Vjtb62bctP69YtdJAON2Rni1xwQeh5wfn0xz/63w8eXPX0WnXzsRo6ddL/duAZbryH4P9+VTAAOBQAQv2QgXJz9eKVNe/RR/3vrQtlzz3nn3b55f761sBqn0gGqyVJuCH4zxhY4F50Uc3t6IGDdSG0KsNLL4WebgWjCy7Q6q7AeatWiQwfvsN2xHbJJVrvunhxxdsL/J0D6/JF9Kzr2291/P339WKp9bsGD7//vf93Nya2+WgJrqYIHC6+WFvHTJsW2fqCp3k85avpwg2zZ+s6tm0LPX/ZMpENG/zjgdUoVj6FG154wf5ZQOvQwy3/73/r77Rpk711VEVDYaH9f/zUU+WXqewahNWQIDGxar9hdOUQA0DUn62OUD/kxo1aAH30UWQ/fLTDccdV7/MjRvjrhiv708VjuO46kXffrfrnrNYfwRcmrVY3t9zi7wPQtate/Cwp8e8j33yj83bsCP87Nm6s10tuv13np6VpfpeU+JcJFNzcNVS6n31WX1u31uqUwHmBLZZSUjRfjj++/DrCrdtSWqoX/oOD2u23+y/6i2jrGmte4IXjPn30CFxEZMIE7U9w6aUir72m04KrGS+9VOSGG8KnR0T7NgTPP3jQ/l3mz/e/D7zOFGqYM8d/4dpq3RN49hvYrPTPf7anpV8/nd6rl/ZzCLeNiv7rI0aIPP20BlNAq/OsAygRTcu2bf4zssDqvkGDNF+ts86HH9ZrYRs2hN5mpBgAHAgAwUch8RpGjAj9p9i4MfJ1WG3TA4crrvA3Jxw6tGppatFCpHNnkeuv19c//zmyz40cqa933aWFyc6d/nmbNmkrldmztemqNf3QIXtLkd27/b9BYNO7yZP1dcwYXff06fYmkBXtI2vW+Nezfr09OIjoRXDrqHDpUpEvvqh43wj13QsKRPr31++2dq0eJASmObjwCWw6al2oF9HrSYMGiTz4YPnPWKw+Fg0aaDCx+l4ESk4ukSFD9HrP7NmR10G/+67/bEDE3q8iVHqso+fzztOqpcBGCytW6NnApk26TFKSf3pwU1drWLxYlzl8WH/nXbvseS6iZ3xA+WbS69bptS2r6XLnzvZOdSNH6hlDKMHfLTdXmwIfPar7RnDHLo9Hlx8woHwLuZtv1nlW/4LqYgCIYwDYsUN/aKsAyMvTTiZJSVUrNCMZQl10sjp+BU/ftav8tJkz/fXL99zjn37vveWXffxxf1PMwJ6jO3dqy5/AKqjZs/VIsUEDHQ88mhSxF2Chhs8+03raoiJ/5ycRfy/SUaPs67PanV91lY4fOaLXCaxCJ/jzgP9I8557Qv+Ole0j4QrTaOza5S/Upk3TAjkUq39DVpae0Xz4Yeg07dunderBxozRi+TBSkv1us6sWeHT+PXXmRF9l0gMHGj/vQM9/bSUBf1wiop0mTvvtE+31teunb8PQGCz50BZWVoNFw1rf6uoJU5V9w/rt3300fLzrKbU1bnwa08bA0DUn62MVdBbPffuvDO6wj2ww0+4wepkFerPtHChFp7W9KNHddqyZf6LdkuXamH52Wd66DNmjP559u61N9176y0tJEpL9ZTUKqyCd3BrmnUEeeCA7rzBfvlFL7T+8IP/M4HVOxXZu7f8Eercufq5SZMq/33699fC07pgHOo2DCI1GwBiZfRoPfuJh1j+Z1au1AC/aFH5TlJWVZ3VfyWcQ4fK7wfWb1JY6O/TEtzrOVZmzlxc4fw1a/TWEVWRnR1Zz/3qYgCIUwAIbIZ51126g4ZrilZZfbZV3w7YLyYmJWkb/OnT9Yj2oov09H7JEi0Ig1l1ioGnlWeeqdOsWxuEyw/r4mPwkbSITgvewaMpFH/5RYOSiFYRBd8/JxJer1azVOXP4/VqXXK4+7lUto9Mnaqfd4uaqjYtKdGzlOCzxkgE7n9eb/hbMMRCbWhKHq1wAaA23A20znriCSA72z8+eTJQWKhDKB18N71OTQXatgUWLbLPP+88YMYMfX/SScCCBcDw4cCSJUBamn+5efMqTldmJrBlC2x3QPzgA+CZZ4DOnSv+rIi+NmlSfl6nTuWnnX028L//VbzOYO3a6QDo7XejYQwwbFjVPzNkSHTbA4A77oj+sxReQgJw113RfXbJEuCbb/S9MUAiS7Qq4SMhq+GBB4Bp0+zTrPHTT7dP/+gj/87ZsiXQsaO+X7wY6NdP33fsCPTq5f/M+ecDBQX2wj8Sxx8P9O5tn3bGGcCLL+qfLRIpKZEt5/FEd095cpEjR4CvvqraZ6wjkVBKS/WPAaDPGQdx/9gwR1wVOXQIWLu2ap8J2G5Y//kPMH068NNP9uk7dgAPPggcPgzk5mrUsr7jN9/otDvv9B9FBiopAQ4cqFpaI8R4GaEZM4DnnweuvRZo3Rq45pryy/TsCfz4o77PzASSk/WopEkTLfyLi4Hrrwf+9jddx/DhQN++wMKFwH//CwwapA/ayM+v2e8W6MILgS++ABo1imz5Y4/VgSqQmwts21Y+KsfCjBlA//66QzVvHl3amjYFNm0Ctm7VwmfePC3EAGDvXiApSQurpk3tn/3pJz2dPe44/zSrVsa6WX9pKdC4sb7/z390p//qKz11HD8eeO453YHefhs49VQgK0v/NPfeq09mueYa/7qys4HVq4FHHwXWrQPS04HPP9d5F18MnHsucOONwMsv6zbvvRfYuBF46CHgtNOAPn30zzhwoH52/Xpg82ZgwwYtYO+5R/+k6el6Cr5jh677lVeA5s2RMXSojp92mub1Dz/okdLIkfr7rlqleWhZsUKX+/57YMQInXbwIPDss/o+LU3zbuFCe77efbceDV5wgebla68BJ5yg6470jxmpUPVCtXVw8hpAJBdxA5vdRVOfWZPC5cfhw6Hr/+u0jRvDN7MJ8N9I76W9eLEuN3WqXi0PVFpa/krl6adL2ZXyH37QizTXXWf/jNerO83Ikdr+8/BhvViydaveyzpwhyos1GnBN5n55RddT3a2Xml98UW96vrCC9p5oHt3bcO6fLlekZ84UT8XqglYRoasePxx+zSrTeKyZf4eXx076oUlq92rdb9mYyJv+1vRcMwxeie1Bx6o+mcj7WFVVwbrXjFRAC8CZ0b92dLSyn+b5cvt7bJru7p8QatMVpa2ocvMtF/9C3xvXaVv2FB7AL38sk7/8UeRxx7Tq9r5+f57OJ9wgr8514cfaq+wd97Rq7+9e/tvlRnYU2vqVO31c/CgXqHv3l3X+dln9p5hwcPhw/77Lkcy9OpV+TIDB4a/v0Isht/8Jvy8wLbCwUPv3vFLU3WGE090Pg0VDcOGac/ARx4J/7SgCDAAVKPAi+QeKlYbbGu8tnM0AOzapW31vv1WOxvs3q1t+AIfiySi862nY7RurX3nR4zQXk7LlpW/Yc7Gjf4uvn/6kxbeoe5hEcm9nK0uodEO8br7WLgh+O5/wUNwQ/zg4Yoryj8gIXAIvB9J4PCvf9nHA++M17atntHMmKG/pzX9rrvsnwm8UVaoGz4Fb+/VV7X5WKR5M2mSDoFHaH/7m561lJT4u35bw4ABmo5u3fSujPn5IoWF8t/33tMed1dcodOOPVab/M2dqz3T7r5bu5V37qzduj/+WA8sZs/Ws8bly7WHV6dOekZaUKDVBpMna8+/zEyRceP83Zerc//nIAwAVSjwvF7dL95/X4/+gx/88Nvf6u0EAqdZNQcrV4ZumlnbRJwfXq8e9b77rr8rY2am9gYrLdVeb7fe6j86OXpUT/23btUj3PR0+zP4SkvLF46Bd5rr2lV7Rl15ZXx60lU2hLtbW3WPFFNT9UjiyJHKe8UFD2PHhk7DWWdpNU7TptobMXBH7dLFfkT+889aAHXrps9WnDtXC7elS7UBe3BvwsD369bpfvD003p7y19+0SPTCRP8PbQAvR9GRQL/LN9+q9U6Ho/O279f7zeyebN/GyKavl69QveWsp4t2aePBnyLdeOf998v/2i4iRNDt+XdvVsDQgXtfDMzM+1tiIuKomt3WtlzJfft0xuExRADQIQFnlXd066dvk6bZr/tceDj5QoL/e3ma7Xt2/XPG7DjlcuPo0f1Rv3WfRS++06/YOBNWAD7Y5UGDCj/KKVwQ0pK9QrQ6gyjR+uR16xZ2g04+JaV554r0quXFKWkaNBasECPGK3vOm+e5snLL2tX5y5dtBv0jTdqUPR4NN9uv10L2rfe0luljhunnw98dqKIVhUtWuTvedeggVYXPfOMHhGuXu3P96FD7Z/Nz9dg3KxZ+Ycle71a73/22eG7w1bmzTe1t6GIrB8zRrdVmaIiPVqt7OHNmzZV/Oi5WLH+nDFWl6tNGQAi/PGs7tnhhuCDnFDPn61Rq1bZj7C//lrveZCVpePWBUhAj7Duu09ERDxffaWFcvApPKD3V4h3odyqlf1xZOFO/Q8f1tPlCRO0EN+9W3+k7GwtHCdN0vGXXtJqoZEj9Yzluuv0DMK6EUywPXv81Qi+wqLcPuL1Rn8Xrlg8PXzPnurdA7ia6nKBFw91OT8YACL88ebNq7jceuWVqJMQWz/+aK/TLCkpfy9e68Jm8JCcLIcCHwobyWA9X7KiAv03v9FqhYULNY1z5+oR98iRWu0wdKgeXX76qb+A9Hr9DxtevFiPhAMfAhtPJSV6uu1Tl//g8cD8sKvL+REuALAfQBCr6W+wc88FPv5YO1nVqDVrtK1x+/ZAXh5wzDHAlCnAuHH25UJ1gXzrrdDrLChAwpEjFW93wABtn/zqq9pO+9JLgXfeATIytO22x6OdYu67TzsOhOqWe/HFOlTEGH9Hgr599XX4cN1eaWnFn62uhITo2s4T1RMMAEH27LGP79gBtGgBNGzoTHpw5pn28WHDgC+/rPp67rxTv0RJCXDeeVhy3HHIGDAAeOEF7aDUoQMwfz7w+ONa6A8cqIXzzTf717FkCdC1q3ay6dpVp117rXZSibVu3WK/TiKyYQAIkJ2tBX5SknYIBPSePXGRl6eVHM8+qz0gjdEemGPGAJ99Bvz8M2D1PAxkFf69egHLlmmB/cAD9mU6dgS2b9fuxbNnA23aaK/LwJsDeTwaEAJvwnLLLTqE06dP+WnxKPyJqEYwAPhs3gyccoq+79hR790T2Ks7ZkT0KLxvX+3OHiyw2mbOHH0dM0bvQwEAzZoBv/6qXcL37tXTk5tv1u7m69bp/Sduu02jmDF6mwAiohAYAHwyM/3vzz5b7+vTs2cMN7B7tx5x33CDXkyoiptv1gBwwQUaFKz7gbRooa+tWgGXXRbDxBKRGzgSAIwxjwK4DIAXwG4AN4hImMuvNSMvz//+d7+L8crfeEML/kAXX+y/r/NDDwE5OXrkPmwYsH+/3jXuoov0qnOPHlo3dcIJ9mocIqJqcOoM4CkRmQgAxpixAB4EcJtDacH48cCkSf5xqzFKTOTklC/8AWDuXL0dbUGBFvCWDRuAXbvKX/yN28UIInIrR54HICIHA0YbAxAn0gFoi8bAwv+ss4CTT67mSr/5Rgv9l18GTjzRPu/pp/UWtYDepD+w8Ae0Wie48CciigOjfQQc2LAx/wAwEkAegMEisifMcqMBjAaANm3a9H7nnXei2l5+fj6ahHjM1eDBGbbxzExPVOsHgJTVq5GUn4/uf/mLbfqWm25CzpVXInnnThyudnSJjXD54WbMEzvmh11dzo/BgwcvFZH04OlxCwDGmPkAQrURnCAiHwcsNx5Asog8VNk609PTJSsrK6r0eDweZGRkhEinfTzq7Cgo0E5awS6+WKt7aplw+eFmzBM75oddXc4PY0zIABC3awAiEqIRe0hvA5gLoNIAEA8nnKC9fFes0FaVVXb0qNYZ/fqrf9p55+kDdA8eBN59N2ZpJSKKJadaAZ0mIht8o5cBqOLDOWPn0CFt9z9rVhQf3rVLO2IFFv6AdrIiIqrlnGoFNMkYczq0Geg2ONQCqLRUn9GckhL6VjqVGjhQW+1Y/vpXffAzEVEd4EgAEJFYt7SPivXw9cBnWkfszTf9hf+ZZ+qtG4iI6hBHmoHWFuvX62vr1lX4kAjwpz8Bo0bpuDHA5MmxThoRUdy5OgBMmqS3zqnsjsU2H38MTJum7595BvB69RYNRER1jKvvBbR6tRb+1i11KnT4MPDPfwLffqvjDz4I/PnPcU0fEVE8uToA7N8PtGwZ4cJTpwKPPKLve/cG/v73uKWLiKgmuDYAiGgAiOgJX3l5ekO3Zs30CVih7otPRFTHuDYAHDkCFBVF+ETAK6/UG7cNHw5MmBD3tBER1QTXXgTet09fKz0DKC3VRyUCwJAhcU0TEVFNcm0AsJ72FXyzTpvSUn8PsSuusD8+kYiojnNtANi8WV+tx0CW88YbepMgy3vvAQ1cm11EVA+5tkTLydHXjh1DzHz2Wb2f/+LFwL33alt/Fv5EVM+4tlQ7cEA7gVmP17UJrOp57DE+hpGI6iXXBoC8PKBp0xAz7rxTX9u1A5Ys0ShBRFQPubYZ6IED2qy/3MQpU/T9ypURdhIgIqqbeAYQ6PHH9fXOO1n4E1G959oAsHt3UAD48Ufgqaf0/bhxjqSJiKgmuTIAbN8OLF8O9O0bMHHWLG3zn5ur9f9ERPWcKwPA7t36WhYAjhwBpk8HBg2K8NagRER1nysDQF6evpY9CWzNGn048G2OPJmSiMgRrgwABw/qa1kAWL1aX7t1cyQ9REROcGUAsM4AmjYFUFwMjBypE0491bE0ERHVNFcGANsZwNKl/hlJSY6kh4jICa4MALZrAIsW6Yj1nF8iIpdwZQA4eBA45hjfAf+iRcBJJ/ECMBG5jqMBwBhzrzFGjDGRPpk3JvLyAi4AL1oE9OtXk5snIqoVHAsAxpgTAQwDsL2mt33woO8CcE6ODgwARORCTp4BPANgHACp6Q2XnQFY9f8MAETkQo7cDdQYcxmAX0RkhankXvvGmNEARgNAmzZt4PF4otpmfn5+2Wezs3uiUSMvst99F+0aNsR3Bw5AolxvXRWYH6SYJ3bMD7v6mB9GJD4H4MaY+QBOCDFrAoAHAAwTkTxjzFYA6SKSW9k609PTJSsrK6r0eDweZGRkAAA6dwa6dwfe/6Wf3v9n4cKo1lmXBeYHKeaJHfPDri7nhzFmqYikB0+PWxWQiAwVkW7BA4DNADoBWOEr/DsAWGaMCRUsYm7PHmDDBiC9RwmwbBmrf4jItWq8CkhEVgJobY1X5QwgFtau1dderXOAoiKgZ8+a2CwRUa3jun4A1sPgT0S2vgn5VHgiovrP8UdCikhqTW4v21fudyjcpG9OPLEmN09EVGu48gzguOOAlD2bgQYN+PAXInIt1wWA7GzfQX92NtC2rbYCIiJyIdcFgJwcoEMHBEQCIiJ3cl0A2LMHaN0awJYtehM4IiKXcl0AyM8HUo4p0QDQpYvTySEicowrA0CTor2ACAMAEbmaqwJASQlQWAg0yd+lExgAiMjFXBUADh/W1yZ5v2gT0M6dnU0QEZGDXBUA8vP1tfHe7cDJJwONGjmbICIiB7kyADTZswU44wxnE0NE5DB3BoBDO7UTGBGRi7kzABzcAbSs0ccQExHVOq4KAGUXgb15DABE5HquCgBlF4FxmAGAiFzPlQGgCfJ5F1Aicj33BoDTT3c2MUREDnNnAEgoANq3dzYxREQOc10ASDClaHhcsvYEJiJyMVeVgoWFQHJCMcxxKU4nhYjIca4KAMXFQJIpAZo0cTopRESOc18AQDGQwjMAIiIGACIil3JVACgqYgAgIrJEFACMMb2MMWONMX82xvSq7kaNMQ8bY34xxiz3DcOru85IFBcDSVLEawBERIggABhjHgTwBoAWAFoCmGGM+VsMtv2MiPTwDXNjsL5KFRcDDaWQAYCICEBiBMtcCyBNRAoAwBgzCcByAI/FMV1xUVwMJHkZAIiIgMgCwA4AyQAKfOONAPwSg23fYYwZCSALwL0isj/UQsaY0QBGA0CbNm3g8Xii2lh+fj527cxFkhRhy5492BbleuqL/Pz8qPOyvmKe2DE/7OpjfoQNAMaYqQAEQB6An40xX/nGLwDwQ2UrNsbMB3BCiFkTAEwD8KhvfY8CeBrATaHWIyIvAXgJANLT0yUjI6OyTYfk8XiQ0rgpDqMYnc46C52iXE994fF4EG1e1lfMEzvmh119zI+KzgCyfK9LAcwJmO6JZMUiMjSS5YwxLwP4NJJlq6u4wKutgFgFREQUPgCIyBvx2qgxpq2I7PSNXg5gVby2Fai40ItjUAw0blwTmyMiqtUqqgJaCa2iCUlEuldju/80xvTwrX8rgFursa6IFRd6cRzPAIiIAFRcBXSp7/VKAIsB5MRqoyJyfazWVRXFRawCIiKyVFQFtA0AjDFNoBdh9wGYDeA9EdlVM8mLrWKrJzADABFR5R3BROTvItIVwBgAbQF842vhU+cUFwsDABGRT1XuBbQbwK8A9gJoHZ/kxFdxsWEAICLyieRWEH8yxngALIDeDuKWal4AdkxJCZCIErYCIiJCZD2BTwRwl4gsj3Na4q6k1BcAeAZARFR5ABCR8TWRkJpQWmqQaEqBhg2dTgoRkeNc9TyAEq9BQqIBjHE6KUREjnNVACj1GiQ2CNu3jYjIVVwVAEq8DZCQ4HQqiIhqB1cFgFJhACAisrguACQmeJ1OBhFRreCqAKBVQLwATEQEuCgAeL2AoAESE3kRmIgIcFUA0CN/ngEQESnXBYDESPo+ExG5gGsCQGmp7wyAAYCICICrAoC+sgqIiEi5JgCUVQElOZwQIqJawjUBwF8F5JqvTERUIdeUhrwITERk55oAUHYGkOSar0xEVCHXlIY8AyAisnNNAOAZABGRnWOloTHmz8aYtcaYn40x/4z39sqagTIAEBEBiOyZwDFnjBkM4DIAaSJSaIxpHe9t+puBsh8AERHg3BnA7QAmiUghAIjI7nhvsKwKqCEfCEBEBDh0BgCgM4CBxph/ACgAcJ+I/C/UgsaY0QBGA0CbNm3g8Xii2mB+vsa6vQdyo15HfZKfn898CMI8sWN+2NXH/IhbADDGzAdwQohZE3zbPR7AOQDOBvCuMeZkESl3r2YReQnASwCQnp4uGRkZUaVn7c8/AADatT8B0a6jPvF4PMyHIMwTO+aHXX3Mj7gFABEZGm6eMeZ2AB/6CvwfjDFeAC0B7Ilbeor1SWCJDXkRmIgIcO4awEcABgOAMaYzgIYAcuO5QW+RBgBeAyAiUk5dA3gNwGvGmFUAigCMClX9E0vWGQCbgRIRKUcCgIgUAbiuRrdZpB0BEhvxDICICHBTT+BiPcFgFRARkXJNACi7CMwzACIiAC4KAF7rGkAj3g2OiAhwUQAQXxUQzwCIiJRrAkApm4ESEdm4JgBIie8icDIfCkxEBLgpAFgXgZN5DYCICHBRAPBazUB5EZiICICrAoDvDOAYVgEREQFuCgAl+sprAEREyj0BgGcAREQ27gkAPAMgIrJxTwCwegIzABARAXBTANCbgSLx2IbOJoSIqJZwTQAoZRUQEZGNawKA1ROYZwBERMo1AaDUVwXEMwAiIuWaAFDWCoiPhCQiAuCyANAApTDG6ZQQEdUO7gkApYIElDqdDCKiWsM1AaC01CARJU4ng4io1nBNAPCWAAnG63QyiIhqDdcEgFKvQaJhFRARkcWRm+MbY2YDON032gzAARHpEc9tlpQ0QJJhFRARkcWRACAiv7feG2OeBpAX722WlDZAEs8AiIjKOPp4LGOMAXA1gPPjva2S0gZIasAzACIii9PPRxwIYJeIbAi3gDFmNIDRANCmTRt4PJ6oNlRSCjRASdSfr2/y8/OZF0GYJ3bMD7v6mB9GROKzYmPmAzghxKwJIvKxb5lpADaKyNORrDM9PV2ysrKiSs/lzb7A2uIzsObwSVF9vr7xeDzIyMhwOhm1CvPEjvlhV5fzwxizVETSg6fH7QxARIZWkqBEAFcA6B2vNAQqKU1AUgM2AyUisjjZDHQogLUiklMTGyv2JiApgReBiYgsTgaAEQBm1dTGSrwJSErgGQARkcWxi8AickNNbq/U24ABgIgogGt6AhdLAhIT4nPBm4ioLnK6GWiNKfEm4lgGAKJapbi4GDk5OSgoKHA6KZVq2rQp1qxZ43QyKpScnIwOHTogKSmyB1+5JgAUSwKSEhkAiGqTnJwcpKSkIDU1FaaWP6zj0KFDSElJcToZYYkI9u7di5ycHHTq1Cmiz7imCqhEEhkAiGqZgoICtGjRotYX/nWBMQYtWrSo0tmUawJAMQMAUa3Ewj92qpqX7ggAIihBIhJdU+FFRFQ5dwSAkhIUIwlJSTwDICKyuCMAFBX5AoDTCSGi+mT58uWYO3duhct4PB58//33VV53VlYWxo4dG23SIuKOSpGiIhShIZKSWNdIVGvddRewfHls19mjBzB5cmzXGWD58uXIysrC8OHDwy7j8XjQpEkT9O/fv9y8kpISJIapm05PT0d6ern7t8WUa84ACpCMY5LZE5iI7N588010794daWlpuP7667F161acf/756N69O4YMGYLt27cDAObMmYNu3bohLS0NgwYNQlFRER588EHMnj0bPXr0wOzZs8ute+vWrZg+fTqeeeYZ9OjRAwsXLsQNN9yA2267DX379sW4cePwww8/oF+/fujZsyf69++PdevWAdDAcemllwIAHn74Ydx0003IyMjAySefjClTpsTku7vmDKAQzZDcyOmEEFFYcTxSD+fnn3/GY489hu+//x4tW7bEvn37MGrUqLLhtddew9ixY/HRRx/hySefxJdffon27dvjwIEDaNiwIR555BFkZWXhueeeC7n+1NRU3HbbbWjSpAnuu+8+AMCrr76KnJwcfP/990hISMDBgwexcOFCJCYmYv78+XjggQfwwQcflFvX2rVrkZmZiUOHDuH000/H7bffHnGHr3BcEQC8BUUoQiMkN+JFYCLy+/rrr3HVVVehZcuWAIDjjz8eixYtwocffggAuP766zFu3DgAwDnnnIMbbrgBV199Na644opqbfeqq65CQkICACAvLw+jRo3Chg0bYIxBcXFxyM9ccsklaNSoERo1aoTWrVtj165d6NChQ7XS4YoqoMJ8zdDkYxxOCBHVWZMnT8Zjjz2G7Oxs9O7dG3v37o16XY0bNy57P3HiRAwePBirVq3CJ598ErYjV6NG/iqMhIQElJRU/xG3rggABfmaUcnJvAhMRH7nn38+3nvvvbLCfN++fejfvz/eeecdAMDbb7+NgQMHAgA2b96Mvn374pFHHkGrVq2QnZ2NlJQUHDp0qMJtVLZMXl4e2rdvDwB4/fXXY/CtIueOAHBYHwSTfAwDABH5de3aFRMmTMB5552HtLQ03HPPPZg6dSpmzJiB7t27Y+bMmXj22WcB6JH6WWedhW7duqF///5IS0vD4MGDsXr16rAXgQHgN7/5DebMmVN2ETjYuHHjMH78ePTs2TMmR/VVIiJ1Zujdu7dEY/N7WQKIvH7fyqg+Xx9lZmY6nYRah3liVxP5sXr16rhvI1YOHjzodBIiEipPAWRJiDLVVWcAjY5xxdclIoqIK1oBlVUBHcsAQETxMWPGjLLqIsuAAQPw/PPPO5SiyrkjABxhACCi+Lrxxhtx4403Op2MKnFFiVhwWHsAJzdOcDglRES1hysCQOFRBgAiomCuCAAFR7UHMAMAEZGfuwJAE1dc8iAiiogjAcAY08MYs9gYs9wYk2WM6RPP7TEAEFE8RPI8gGCpqanIzc2NU4qqxqkS8Z8A/i4i84wxw33jGfHaWEGh9gBOTuETYYhqqzr4OICIngdQmzlVBSQAjvO9bwpgRzw3Zt1biQGAiILF83kAALB3714MGzYMXbt2xc033wztmKveeust9OnTBz169MCtt96K0tJSTJ8+Hffff3/ZMq+//jruuOOOuHx3E5iYmmKM6QLgCwAGGoT6i8i2MMuOBjAaANq0adPbuklTVcy9fy+eyvodvvj0azRs7IrLHpXKz89HkyZNnE5GrcI8sauJ/GjatClOPfXUuG6jImvWrME111yD+fPno0WLFti3bx9uu+02XHbZZbj22msxc+ZMzJ07F7NmzULfvn0xZ84ctGvXDgcOHECzZs3w9ttvY9myZXj66afDbuP+++9HixYt8Ne//hWff/45rr76amzZsgW5ubmYOHEi3n77bSQlJeHuu+/G2WefjWHDhmHIkCFYsWIFAOCKK67A/fffj379+kX0nTZu3Ii8vDzbtMGDBy8VkfKPFwt1f4hYDADmA1gVYrgMwBQAv/MtdzWA+ZGsM9p7AT006GsBREpLvFF9vj7ifW/KY57YueFeQFOmTJEHHnjANq1FixZSVFQkIiJFRUXSokULERG56aabZOjQofLSSy9Jbm6uiIjMmDFDxowZU+E20tLSZNOmTWXjzZs3lz179sjUqVOlbdu2kpaWJmlpadK5c2d56KGHRETkggsukEWLFklubq6kpqaK1xt52VWVewHF7RqAiAwNN88Y8yaAO32j7wF4JV7pAICCIoOGKESDBD4SjIiiM3nyZKxevRqfffYZevfujaVLl1ZrfSKCUaNG4Yknnig3b8SIEXj33Xdxxhln4PLLL4cx8bmTsVP1ITsAnOd7fz6ADfHcWEFhAyQj9EMWiMi9auJ5AIMGDcJ//vMfAMC8efOwf/9+AMCQIUPw/vvvY/fu3WXb3rZNa8Ivv/xyfPzxx5g1axZGjBgR+y/u41QroFsAPGuMSQRQAF8df7wUFhskm8J4boKI6qDA5wEkJCSgZ8+emDp1Km688UY89dRTaNWqFWbMmAFAnwewZcsWiAiGDBmCtLQ0dOzYEZMmTUKPHj0wfvx4/P73vy+3jYceegh/+MMf0LVrV/Tv3x8dO3YEAJx55pl47LHHMGzYMHi9XiQlJeH555/HSSedhObNm6NLly5YvXo1+vSJXyt5Ry4CRys9PV2ysrKq/LmSF1/Ftg8+xSlfzolDquomj8eDjIwMp5NRqzBP7GoiP9asWYMuXbrEdRuxcujQIaSkpDidjEqFylNjTMiLwK7oGZV46x+RffopOMXphBAR1SKuCABERPHG5wEQEVWRiMStlUtNqg3PA6hqlT57RRGRY5KTk7F3794qF1xUnohg7969SE5OjvgzPAMgIsd06NABOTk52LNnj9NJqVRBQUGVClcnJCcno0OHDhEvzwBARI5JSkpCp06dnE5GRDweD3r27Ol0MmKKVUBERC7FAEBE5FIMAERELlWnegIbY/YACHnb6Ai0BFA7HsNTOzA/ymOe2DE/7OpyfpwkIq2CJ9apAFAdxpisUF2h3Yr5UR7zxI75YVcf84NVQERELsUAQETkUm4KAC85nYBahvlRHvPEjvlhV+/ywzXXAIiIyM5NZwBERBSAAYCIyKVcEQCMMRcZY9YZYzYaY/7qdHpqgjHmRGNMpjFmtTHmZ2PMnb7pxxtjvjLGbPC9NvdNN8aYKb48+skY08vZbxAfxpgEY8yPxphPfeOdjDFLfN97tjGmoW96I9/4Rt/8VEcTHgfGmGbGmPeNMWuNMWuMMf3cvH8YY+72/VdWGWNmGWOS6/v+Ue8DgDEmAcDzAC4GcCaAPxhjznQ2VTWiBMC9InImgHMAjPF9778CWCAipwFY4BsHNH9O8w2jAUyr+STXiDsBrAkYfxLAMyJyKoD9AP7om/5HAPt905/xLVffPAvgcxE5A0AaNF9cuX8YY9oDGAsgXUS6AUgAMAL1ff8QkXo9AOgH4IuA8fEAxjudLgfy4WMAFwBYB6Ctb1pbAOt8718E8IeA5cuWqy8DgA7QQu18AJ8CMNCenYnB+wqALwD0871P9C1nnP4OMcyLpgC2BH8nt+4fANoDyAZwvO/3/hTAhfV9/6j3ZwDw/7CWHN801/CdnvYEsARAGxHZ6Zv1K4A2vvduyKfJAMYB8PrGWwA4ICIlvvHA71yWH775eb7l64tOAPYAmOGrEnvFGNMYLt0/ROQXAP8CsB3ATujvvRT1fP9wQwBwNWNMEwAfALhLRA4GzhM9fHFFO2BjzKUAdovIUqfTUkskAugFYJqI9ARwGP7qHgCu2z+aA7gMGhjbAWgM4CJHE1UD3BAAfgFwYsB4B9+0es8YkwQt/N8WkQ99k3cZY9r65rcFsNs3vb7n0wAAvzXGbAXwDrQa6FkAzYwx1oORAr9zWX745jcFsLcmExxnOQByRGSJb/x9aEBw6/4xFMAWEdkjIsUAPoTuM/V6/3BDAPgfgNN8V/MbQi/s/J/DaYo7o0/ZfhXAGhH5d8Cs/wMwyvd+FPTagDV9pK+1xzkA8gKqAuo8ERkvIh1EJBW6D3wtItcCyARwpW+x4Pyw8ulK3/L15mhYRH4FkG2MOd03aQiA1XDp/gGt+jnHGHOs779j5Uf93j+cvghREwOA4QDWA9gEYILT6amh73wu9PT9JwDLfcNwaD3lAgAbAMwHcLxveQNtLbUJwEpoawjHv0ec8iYDwKe+9ycD+AHARgDvAWjkm57sG9/om3+y0+mOQz70AJDl20c+AtDczfsHgL8DWAtgFYCZABrV9/2Dt4IgInIpN1QBERFRCAwAREQuxQBARORSDABERC7FAEBE5FIMAETVYIz53veaaoy5xun0EFUFAwBRNYhIf9/bVAAMAFSnMAAQVYMxJt/3dhKAgcaY5caYu51ME1Gk2BGMqBqMMfki0sQYkwHgPhG51OEkEUWMZwBERC7FAEBE5FIMAESxcQhAitOJIKoKBgCi2PgJQKkxZgUvAlNdwYvAREQuxTMAIiKXYgAgInIpBgAiIpdiACAicikGACIil2IAICJyKQYAIiKX+v9aI4w2IqmApgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.clip(vlb_train[:curr_epoch], -1000, 1000), 'r')\n",
    "plt.plot(np.clip(vlb_val[:curr_epoch], -1000, 1000), 'b')\n",
    "plt.legend(['cost_train', 'cost_dev'])\n",
    "plt.ylabel('vlb')\n",
    "plt.xlabel('it')\n",
    "plt.grid(True)\n",
    "plt.savefig('./tf_vlb_plots/' +  str(dname) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddb8b3aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a4356eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEgCAYAAACq+TSYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAABNgklEQVR4nO2dd5wV1fXAv4feQURXBBHF3hVRYywQLKixRY0lGrARazTWGBt2fxqNsSXRRBArFqKxxsZaEUVjR7HRu4Lssruw7J7fH2dm37y3772dXd7bt+V8P5/7mZl779x75r5598xt54qq4jiO4ziZaFNoARzHcZymjSsKx3EcJyuuKBzHcZysuKJwHMdxsuKKwnEcx8mKKwrHcRwnK64onFqIyOsioiKyWkT6pAm/KAhXETk+4n9mxF9FZP80945KiZPqhqa55+aUOJtnkX0bEblPRL4XkQoRWSwik0XkAhHpmuW+oWlkWSki34rILSLSPU7ZNVVEZFz4XHnMY5SIjBGRc9OEFQf5z8hX/k7+cEXhpOOx4NgWODxN+JHBcSXwdMT/6JR4v86RPEfFSVdETgQ+BE4EBgIdgT7AbsDNwKb1zLcDsDFwHokycTIzCrgSOLewYji5xhWFk44ngKrg/MhogIgMAIYEly+qakngvz7w85R0DhOR9lnyuUpVJcUVp+S3G7Bhyn21FIWI7ArcC7QHyoCTgV5AN2A48HwWOVK5H/tv7AAsC/xGiMgG9UgjZ4hIp0Lkm0tUdWjw+w4stCxO/XFF4dRCVRcCbwSXw0RkrUjwEZHzCZHzI0m8Tw8Gx97AvmsoTlQphOluIyJbpcT7E9YCAjhfVe9T1Z9UdYWqvqaqBwGfxs1UjY+B1yLeSYpCRPYXkVdF5Kegm+vToPtNInHGRLqy9hKR/4jIChGZH4RF49Z0zwRpfywilcCIIHyAiIwVkbkiskpE5gVdShumyDVIRF4WkfKgC250umfM1B0VkXdciv+RgYw/BWl/JSIXh/cAewdRN0xNI1PXk4j8XESeF5GlQVffdBG5VkQ6R+JEuyuPEJHxgQzzReQmEWmX7vmcHKKq7tzVcsBpgAbuxIj/W4FfGdAt4v924P8j0A+oDq7HpaQ7KpLumDpkEGB2EPdTrCVT615MQawI/JcD7RvwvEMjaY+L+E+M+G8S8T854p/q7ozEGxPxX5Im7p8icYsDvxVARSTOYViramGG/BYCGwZpdAC+SRNnfngeyW9cql/gn64crsyQd3HKPaluXMqzzYikeTCwOsN9b4a/Y8o7syxN3NMK/X9p6c5bFE4mniSl+0lE+gK7B34vqGpp4N8f+FnEfy7wXnB9qIh0yJDHlSmDx8tSwncH+gfn/wGmAguC62hLow/QJTj/TlUrYz5jRsTYHhgWeH2sqt8EYd2AWwP/iUBfrIvrlsDvDBHZMk2ynwHrAdsB8wK/i9IMlHfBusr6BvHfAa4C1g3CzwZ6BkcC/6uC898Cg4Lz+4C1gEMj99YbERkIXB5cLsRaid2C53gUQFUFeD2IM1MTXYmjMqQpwO2Yki/FWiO9SbQa9wB+k+bWJcDmWLdgReB3ZJp4Tg5xReGkRVUXA5OCy31EpCfwK+wrH5K7nY6K+P8nOD4THHsBtWY/xSSqDP6j9nn5bHC9pYhsG4rbwPQzMRJrEX2Eyf8e9uwhuwM9gvNfYV/rpcD5gZ9gLZRUrlHVhar6KfCvwK8nsHVKvPAreUEQfxGJMvxeVe9U1eWqeicwI/DfLyJbyBhVXaaq/8Fagg1lPxLdeteq6itqXXqfqurfG5jmZtiEA4AnVPUNVV0KXJaSbyq3qOp0tW7BTwK/gowdtSZcUTjZCGf6dAAOIfHlVkaiwobEbCcFZovINlj3R0im2U+pg9m9wgARaRPJbzlQFqT7RZp0fwhkAtg4D33WXUlUlADrxLindxq/2ZHzuZHzfinxFgXKIUo4TXlOin94HcrUNxI2L8N5RkSkbRrv6PN+FSedGESnXUfLJfp86cr568h52KLomCOZnAy4onCyMRHrQwYbs9gzOH9OVcugZhbUroG/YGMVnxJ0SQQcIiL1/TPvAawfnPfAvh4/JdHlA4GiUNUq4JXArztwUroEM1SC6bgfU46jMOW3NfB45P4lkbjnpig7Adqo6nVp0u0fOY8qh7kp8SqoTZhn/xT/finh8yNh62c4D1kZnkRmVg1ME29x5DzjGhbq17KLlmH/DOfROCGrI+e+R0Ij4YrCyYiq/gC8GlzuTuKrOtrtFGetRA/ggHpmHyfdzURkh+D8OhJjKrcEM2V6iEhXEfmFiDwPbJs2lTSoaqWq3g88EnhtD4SLC98BSoLzC4OZOx1FZH0RGYmt5UjHZSJSFHSZnRz4/QR8HkOkl4LjRiJyhoh0F5EzgI1Swt+O3DNGRHqKyCGY4k0l+vW+f9CK+1OGvMOyvTQozy4ispWInBaJtzQ49hGR9ep4nunAzOD8SBHZU0R6AVen5Os0BQo9mu6uaTvs6zw6w6QE6BwJfz/wrwT6pNy7Q+S+hwO/UaSf5RK6UdgHzILgehHQLiXdwyLxr4/4nwisypL2Dlmec2gk3riI/+YkZuZ8E8oC/C7bc0TuHxPxn58mbrpZTzPSyDcQ+7JPl99iYGAQrz3pZz3VzLiKpLk1idlp1dg4S1mGcsg66ymIc0ma8FMyPRs2yJ5p1tM7pJ/1NDROebnLrfMWhVMX/8aUQMizqloOICIbATsH/i+palJXgap+BHwZXB4cnRtfB3sDRcH546q6OiX8BexLHCItD1UdCwzGpn3OxJTGD8AU4CKS+7djoapfkehGG4QNdKOq/8BaSa8GsqwEvse669LN1gFbg/IUVhkvwr6eb4gpxwxsevB4TImuDo4PAEOCcNRmfO2PdcWtBGZhs6OeTZPm59gsqa+DuB8Ae2XI/yps0sIb2MfCyuC+FyPR7gjkSddllC7Np4FfAP/FyrAS+Ba4EdhHczB7zckNEmhmx3HygIiMwb7GATYKK3THaU54i8JxHMfJiisKx3EcJyve9eQ4juNkxVsUjuM4TlZanNXFPn366MCBAxt8/4oVK+jaNeP+Nq0OL49kvDyS8fJIpjmXxwcffLBEVdNaHWhximLgwIFMnTq1wfcXFxczdOjQ3AnUzPHySMbLIxkvj2Sac3mIyMxMYd715DiO42TFFYXjOI6TFVcUjuM4TlZcUTiO4zhZcUXhOI7jZMUVheM4jpOVjNNjReS+eqSjqnpy3dHiEWxycztmWXI9zM79BOByVU23qYvjOI6TJ7KtoxhFvB2kJIiXM0WBybUEOBgzZdwfM9/cEfh9DvNxnFaNqjkRc82F6moYNw5OOAHaty+0NC2furqeJIbLOWobt1+qql+qapWqzgTuJf2G9Y7jNJA2baBtWzjyyPThlZXwVQN3yZ4yBcrL48e/916r9Fen7j6ShrFj4eST4a9/bZhsa8Kbb8LVV9cdryURyyigiGwBvIZtTTgG26h9feAqbPOW4ar6af7EBBF5HKhQ1RPShI0GRgMUFRUNfvTRR1OjxKa0tJRu3bo1+P6WhpcHfPBBLzp3rmarrZY3+fJYvLgDnTpV0717jNoWGDZsaM35pEnFSWGlpW05+GDbJv3hh9+lb9/avb6ZymPRoo4cffTPGDFiPhdfHE/THHDAnlRUtOXZZ9+ka9eqWuHTpnVn4cJOvPvu2vz3v7bT6nHHzeTUU78HYO7cTnTooKyzzkpee20ddt55KT161F0Oq1cLItC2bTwDqcOGDQVqlxc07//LsGHDPlDVndMGxtkGD5iE7ZnbI8W/J7aFYnGcdIJ7xpF9K8xr09xzLrYj2IC60h88eLCuCZMmTVqj+1sazaE8Zs9W/fzz/KUfdtCoJpdHSYnqTTeprl6d+d4bblDt31/1+ONVL7tM9Te/UR0+PDnOJ5+o/uUvqqeeqrpkiflVVZnf0qXJcaurVWfNyi5rUZHqsmWqxcXxny18vigff5wIe/319Penez8+/VT14Yftvq23zp7/ggWqG2+sOm2aaufOds/bb6t+/XV2WUN38smqHTuqTpmS8Js7145Dh9b9/Kp2/w47JK7vv1/1nnsyxw/zqa6260WLVCdOtPNXX52kVVXx8q2LH35QnTfPynPGDNVoUZeWqh5+uOr06bnJS1UVmKqZ6u1MAUmRbOvGKmBEiv+IQFGUxUknuKcb0CeL65IS/w/Ylo/bxEnfFUXdfPSR6ldfxYubz/J45RXVZ57JHmf58tpKYMECcyGpFd3SpaoXXWSVZSrFxZZvVZXqZ58lh/3wg+o++1jlHyVMv0sX1TFjPlVVUw677mr+552XqDRSSVe5gd1fVmZyRP3D8njlFbs+8UTV8nLV9983/z/8wfxnz7bKYtEic2Vlqo88kkhnn33smK4MQiork/P+6afk8A8+SIQ9/3z6NF56KaGNioutck591lWrEvEnTFC9+27VSy6xZ7/77sxlNHeu6uLFVlmuXp0+zmab2fHooxN+b7xhx86d08v82WcW/sEHyb9R6m92882qZ56pesUV9gzV1aovvJAILyuz+LvtZtc//qi6/vplut569v6deWZYRqp33GF+qvZ7nn66/W7peOgh1Q4dkp8zVKKXXaZ6yy2qRx1l14cfnj6NhpALRfF1oChWAS8HrYKXg+sq4Js46dTXAZcDc4DN497jiqJuUv8Yr71mFXLIsmWqF15oX8xheXzzjerYsdnTPe64uiv+THJUV6f/Mv/FLxIVayb5U6+PPNKuH388c54XXmjHKVNU33rLKuKNN06EX3ut6oMPJr6MQ7fOOuX6wAOqZ5yR7H/ccXb83/8SeZWVZa4E+/Wz4yGHJPs//LDJsu22dr3bbtZCAPsNwnhvvaW61lqZ02/XLnFeWmpf6Icdprrjjlbpr1hhv3PqfaHCq6hQHT8+4f/YY7XL8pZbLCz8y2SS5ZlnrGVVUZHs/+abqnfdlfm+qBs1Kr3/jjva8Ze/TPj985+J83/8w/JRtY+LTz9VveqqRPjbbyfO11tPdaedMsuw+ebJ1z/+aOn26WPXc+bUvufHH2uX7wMP2Pkpp9Qu07feilceodt+e9WpUy3NNSUXiuL4QCFUB8eqlOvj4qRTHwfcDMwEBtXnPlcU2Yl+RaraVzSo/vzniTh33GF+F12ketxxM1REdd11zS/61VldnegaKS1NTrcuqquT4x90kJ2nNqVFzP/cc+347be18wmvwyb/DjvY9d/+Zt06UerzJ2yI69IlkVf45Voft+eemcOee65hMk2ZojpsWLy4RUVW5nvvnez/97/bb3bKKdbaif5+PXrY13GmNLt3t2O0Agfrrgl/q4a6sPLeYIPaflF31lkJ5Xnppbn7vTfd1LquIL3iPuus5OslS1RvvNHOf/3r5HfzvvvWTJbx42u3CuvDGisKS4PhwFvAykBBrATeAH4RN4165LVhMF6xEiiNuM/rutcVRXoefrj2n1LVuqGi16rWNw7pK5cePaz5vc02qn/9q/l17ZpofoPql18m573DDtZNM22adSU89pj9waL5huf9+1u3xL/+Vfcf4+qrEwoMEl94gwYlxwv9o/nk04Vf5U89lfBr23bN0/373xtH/nRu333tyzy8/vOf659GWIGHX/+piqOx3K9+VbhyvPnm5Os//MHel/ffz036Bx7Y8DoiJ4qi5gabUrsu0Ka+9zaGa+6K4pNPVLfbLrlyC6mqUr3tNvt6X7lS9ZxzVL/7Ll66u+xS+6W6/HLVddZJfsl+/FH1gguyv4zrrx/vpR0/PvnLE6ypnBovtUuioW7aNHvWqPIA1aefVv3wQ9VjjslNPnW5k082ZdmzZ8JvzJg1TzfanQSqu+/eOM+Ta/fOO4XNP1uXXT7cbbdlDz/88Nzl1aNHw+ueXCuKImBAqqtvOvlyzVFRrFhhv8SDDyZ+8EceqR3v3/+2sN//3sYLQPV3v7OvkbIyG/R77TVrJaSyxx7xXrQ99qi7Qs32R9tkk+TrE0+sO88nnqhduTfEXXNN+j7vyy9vWHrbbJN8veGGmeNeeWX2tKIDw//3f4nzY45JHoQG1dtvjyffaaeteZlF3fjx1t+dyzTTudRB9Gzu4YcTA/t1ueOPT77u1av+soVjR7lwXbqojhxpH0udOuW/XEG1TZuG10O5GKNYG3gYqEgZowjd6jjpNIZrjooiOkgZuvvuS47z8cc2GyIMD/v0Dz7Yjqlfl9OnJyq61P7mXLtoq2T//RvnD5HqwgHfuC6TYooO4IYKPHQ776x6wQXT9PzzVQcMSPi/8IL9RtnyW7Ag0eXy+OMJ/3AKa3m5Xf/859bfH22NRF10dk9Dun+yuXB86N13s8e7916rAMNBXLBZPKnx5s2z54n63XZb3WUVuuOOs0kM1dXpW7EnnGBjZOH1f/6THD58eOa0TzihthwDB6putFH6+CedZHLMm2fPH0f+KVOS/8MTJ9aO86c/ZU/jmWfq9xtmmsYch1woionBuEQmVxUnncZwzVFRfPVV7R/8rLOs++KVVxJ97uFsnoa6gQPX7P5M7r33EucnnNDwdDJVjqlu7bUT5z16qI4bl1xpZXLROP362df8rrva9Nbonzs8V02c77+/dV+F70d1tbWs7rwz8TuOG1c7z/HjrZJbtSpRwcyalZitE239lZRYl2JIOmV2xRWJ83Stp3DwH1RvvdXWbYTX4ZTLZ59NX17R7s5HHqldSXXsmFjnoao6f34ibMKExHlY2VZU2Pvbq5dNjIjO8ommu/vu6SvM6LToefNqh999t4UNHmzXixcnwi64IDETLXXMClTPPtvu3XFH+/IHm8UVjp098EBCyaV+pUfHVqLjUKkunH4bJQw79FD7jz/7bMLv2mvtOHeurZv49NPEZJOo69XLfvtTTrHr4cNVt9pK9b//zVbL1E0uFMWyoOXwMXAjtjr7yqiLk05juOaoKFJnRuTLff55bb/Bg22gck3SjY5DpPuyzOa22Ub12GOtyyqcbVWXC+fOQ2I9SHg9darqF1/YtMfvvkv2j85COuOMRPlHK5iFCxPnqtaVF/3DZ3s/onP9n3oqeaqsqpVTOPf+hx9McWRbnJVuSujUqTZr55RTrCK+++7krpxwHCNUOFVViYry0EPt+PHH1sI591zVvfZK3JtOljDsmWesnFI58MB5Ne/AQw+ZMo2z4GzqVFvYNn263Zs6lgXJM+CiLYfddrPFieGkgQULarfqVE3x3Xqr6m9/Wzvt88+3OCtXWvktXGjphetAvvwyscblmmuSZS8ttS7VsDyOOsoU0/nnW/z11rPzdNO9w/wrK+26stImb8yZY/lHFXEYHv2fRQnXdOy7b93lHYdcKIq5gaLoEyd+IV1TVhR/+5tVPCGPPRa/cqyvi35Zgo1rqKr+7GfJ/jvtZN0IkDzLKfxS2muvRTV/tGilEnWqifNwfcFf/2qVWHSM4uqrVUeMMMUAFhYldfFZqvv8c5s++v779lUe/tlUVffbLyFLlKiMqtZi+OKL5EVg0Xip03ZTqev9CO/99tus0WJRUWHdkoMH22/zww+Z426zjY3FTJ1qrY4o4Rf3u+/WXmSoqvroozYTKB2nnWaD85l47bVJGRcb1peXX07+vWfOTIStXJn9dwnp2NFazlGiiuJ3v7PjJZekv3/+fHt3G/JMYavnvPMyx+nfv+5nSCWTMgg/ap56qn7pZc5nzRXFlYGi2DdO/EK6pqoool/zqnVXitlc377ZwzfdNPFChu7aay3fxYttpWjYVbDTTrbYbuzYhIwdO1rchQutPMK1C6+/nrwgqGtX+3Oo2tfN2LGJGR5PPJF49nPOMb/oiufiYvsyS+Xaa+3Fv+Yau+fMMxP5ZVtlXFGRfg55nMpF1aYEjxiRuCe6HiJKXEUR7UIqNM8+a91OazLHPhO5/L+UlakecIC9Z+PHJ4eFCrx37+xplJfbuxAl7A69//7EArtwEV6uefjhybU+QqLMn28y1IeFC+258k02RZHNzHiUNsBy4GkReRpbqZ1kbUtVW5k9xfqx9daJ85Ur4f7708fbZRd4773saRUVwfz58Mc/wscfwwsvmP9ee0HfvnDjjWbh8s47YeJEePRROPNMi9OnD+y7L3z3nV3vuCN07w6jRkFJiflVBfbY1l0XvvgCNt7Yqr+QqVNh552hXz/LD2DECDtWV8Pmm8P++yfir7++HefNS/jtvXf6Z7v0UjsecggMHmzpDB9u1kJ79MhcJh07mkvls8/s+eri3HPNAXz6qZVTQ3j3XZg0CTp0aNj9+eCgg+yda+p07gzPP58+TAQefhh23TV7Gp061fYLf4v27WH33c0ibru4NV896du3IqvZ8/XWM1cf1l13zWTKBXGL63Ko2Zvi1xniuKJIw5w5cPzxyX7bb5+5Iom+REuWpK+wDj0UPvoIysrg8cdh2DCruO++OxFn4ECz1Q9w/vm109h4Y1MmO0dsRXbvDhdeCIcdlv2ZQtnT/dnatEkojZC99rLjNttkTzeKCBxwgJ0ffri5hhBV0HGpj5yp7Lpr3ZWZ0zCOPbZh9914I3TtCkccYdf5UhItmfpshdqoe1K0FG66CV5/Pdnvq6/sq3XgwGT/iy6Cf/4zcb322nbs0MFs+4O98Iceaudbb23X772XrCTisscetb/AbrrJvrqyUV1tx7h/uN12sxbMSSfVX0anlaBqTeNVq+p3T7Spm8qKFQD06VbBX68tqX8rr7oaPvywnjcl8s3I22/DZZdZd0D4ZwL78rv6apg1y8rhmWcSz/fhh7BwIVx5pblly2rL+uOP9Zc1JnF160Z5k6AFUVlplffZZ1vFfs89tZv8v/wlPPusne+9t1WgS5aYMujZ0/wnTIDvv7fzZ5+FLbaAQYNg2jTo1ctaHbNnJ7p0Gpveve04fHj8ezbyNyg7qvDKK1aobXK8lf0bb9huQFts0bCXZsUKq4jatIFHHoF99oHTT4cHH7TwlSutkurSJfESh8yZY3mnfhWF6YUcdRQ8+aTtoHTXXVYxbrstXHMNnHIKbLklvP8+fPut9XdOngy33GLNhHHjbPelUNZXXrHm8i23WHpPPGFhI0dCaan9MV98EV5+2fo0f/wRLrnE+ng23tjKafvt4U9/sp2RJk+GuXNh003hwAMtrQ03tL7RxYst7I47YPvt2e7CC61vtmtXa+qHf/aRI+348suJPtjrrrP7Ro2Cl15KNHlefx1ee83ON9oI1lqrtsK6+mr7Uhs1yp79H/9IlHe/fvF/27hkGrxorq6Qg9nff1/3QPSrrybOjztujURtFDKVx1dfJc86avYsW2Y/YB288fzz8UaFv/nGRiGnTrUR+dQ5o6mFF66m/Ne/bF7oH/+o2r59Il5oXreqyibRhzarv/7a5odOmWIT8EOqq21+8D/+kfwCPvqoha1aZTMOnn7aRrvfecdWThYVqT75pE2xuuIKm6Pbq5fNjghXd0bc55deajZnQr/DD7f0Z840W+ih/6xZttCkqspGq0P/005LNtsbdW3a1P2HCt3BB5tZ2FQ7J7mYHZJv161b7tLKtpFGHZCDWU+/rcvFSacxXCEVRV1G2446yuKdfHLydVOm0Lav1pjQKNakSbaKKSSsdEPCpb9PPWXzTJcvt3vPO88WRCxalLAPDTa/sqLC5pu+8IJVgl98YfMYo8u7QzdhghnT+uijhJnQb7+1+dKLFtW2Nx6tYEePjl9RpM5/zlQx5XpZd9SlM98aul69TAGmC8tm47vQLnWDiMZycaxJ7rtvwkRvpnnOMciFokg1L+4mPNJQ1+952mkW78UX7Tq66KupUjBFUV5uX+QrVtgX+bx59rX9r38lx1u1KtlOxP3329floYfawpVUuwnjxiVvxPD117VNeoYVWnRZczqXzUZEHNe7d+NWOpDeKmPUZfq6B1sSf+edmcMzlVddSumQQ2wV4apVidWBqYabFixIGBKLthq22KJ2emPG2Pzq0Dha1KXOGw/dqFG20Ce64vI3v7HFHcuWWSsyGj+05PfrX9vCoIoK1ZUr9a2JE23B0dZbW1ojRli8l16yd+2002x13lpr2SYd48ZZ6/GWW2yB0Mcfq15/vRmHmjPHluuPHWsfO/Pm2a5M55+vOmSIpfvOO4n/wmWX2SrKBpIrReEmPCKEq0n/9z/7kFRNfo823zx5gxRILMSprrY6rKRkjURtFGIrimnTzO7D1Kl2PXeuLboIJ5XffXfyyqAHHkiYl73qqoQWDUndNCBcNQa2ouryy20l1ZouK2+oy/RVHNdFjUVF3c032/LcadPql15oaTCdXNdfb90rjz5aezelcLk2WKW1YoXqEUdYC+v22+03euYZazmFC1/C+NHVicXFdv3qq2YY6eOPbXeoAw+0llnU4FjqQocoRxyhNYrhgw/svfnb3xLh556b2DLujTfMb/58W4Dxm9+Y/FF++slaj+edZ11kIaF9/XPOSSyLDhk/PjnPkKVLbel5ll2CJk2alJzW6tUNW1RT14q/iorc7oOqmhNFsXeK+wVwEvA/bJ+Io+Kk0xiusRTFZZcl/99Sd7cKjb2tWpX4qJ0wYY1Eyz933llrj9Ra5TFxYmJfzBkzrC8+dZ/KVJval1wSr7KL9nU3tjvySDOd+8QT1jf/2GOJvU5Dd+qpdnzhBeuDv+EG6xMGU1qrVllFMnCg9fUfdph9wc6caV1fJSW2a82DD1qX0oABVlGFhpe++UZTCt/GGcKdcf76V/uNLr/cKuL7709sARitWKqqrFurR4/0e5i+/74ZWEq3bV0cJk82s66q+r9bb7U/Q12sXm0KcPbs7PF++ineZt+5IJvCaiDNuat2jRVFxpuhF2ZR9oE1SSeXrrEURV31Tuqq448/XiOx1ozVq82SWfjHWLTIBkRDi3ZRi25gOw3Nn6+qqt+fcIJVdtdfnxynrm6ZXLl99kkMombapu3++60SmjzZbHx8+qkpvOnTratq++1tUPj2262lM2qUbRd3wQW2GfHf/56+3FatskoLar4w074fs2cnjDjVh1zYvigrS2+EqZFozhVjPmjO5ZFPRbFeoCiWrUk6uXRNRVE0CX74wayb/e1vJtQNN1jff9ScZtR0aqqrr33yOJtPDB1qpkJvu82+fKdPt4p+hx1sLGLAAJuFM3Fi8u7zYZfCDz9YF9aNNybsdufLHkPI4sU1lXpzrgjygZdHMs25PHLR9fRaGvcOZtajGpgbJ53GcIVUFB072mzDdDaM8kp5eaK/NuyTzcUONF26aFVdffHTplk3x+mnW4slHFycMcMq9Oef15pWSq4pK7OuolxZpYtBc64I8oGXRzLNuTyyKYq4C+6GQo0JjyjhquwHYqbTItCUkjjiCFsjVFRUGHm44w5b1h1y8MG2qrMhnHGG2fJYZx049ljemD6doW3bmtGnn/0MBgyAzTYzo0y//KWtBITE0vBJk2wB04YbmgNbRZoP40edOycWKTmOkzfiKopZ1FYUK4E5wBPAvbkUqimzcqUtDgUzdfHOO7YYNW9KYtYsW8n50ENwwQXwhz+YMaVNNoHLLzcDOA89lHxPqCT22stW5R57rK2oTccdd5g1wJkzzSreFlskh0+fDnvuaS5k0aLM8qazepa6WtdxnGZFLEWhqgPzLEezYffdE6vpR4+GX/8afvvbPGRUWWnL/Q86KOF35512/Pe/E37vvGPHESPMLEFIcbHZCFm+3L68//EPsxUzfjxssIGZKBgyxKzvOY7jZKFedhRFpCvwM2AdYAnwjqrWYQGrZRE1uTJkCGy1VQ4TVzXjTytWWEVeH4YMsabO11/DBx/ATjuZf2ibu31761K62o38Oo5TP2IrChH5HfB/QNS6f6mI/FFV/5ZrwUTkPmBfoCewAngBOF9Vl+Y6r4bQrl2OlURZmRkSS2XAAOt+Ov546yI6/niLO3KkdRP96lc2ZnDYYXDxxWaAzbt6HMfJIbEUhYgcDKRTBt2BO0Vkrqr+J6eSwa3A2aq6QkR6AX8H7gKOy3E+sZg50+rjkJxbQ73wwtp+kybZ2MDTT9tOPlG73ptuaptJZNslxXEcJwfEbVFcEBznAfdgg9j9gVOC4wVAThWFqn6W4lUNbJ7LPOKyaFFtK8l33LGGia5aZYPH22xj4wbhhhNgCuGZZ2yMQcRaDan87GdrKIDjOE48RFPneqaLJLIc6ApsH63ARWQb4BOgVFWzbFTZQOFE/ghcCnQDyoHjVXVimnijgdEARUVFgx999NEG51laWkq3bt2S/J58sh933rlpzfXvfvctxxwzu0Hpt1u+nB5ffkmvjz5iQMpMpLeeeYZOCxZQtsEGVKfb17MApCuP1oyXRzJeHsk05/IYNmzYB6q6c9rATAssog4ow6zE9knxXwf70i+Lk05wzzhsqm0md22aezYCrgG2qyv9fCy4e/DB5DVmcUzbZCSTGeioCewmRHNeQJQPvDyS8fJIpjmXB1kW3MXdSuvr4PiwiOwpIgNFZA8g2OKKb2KmA3BWoGAyuetTb1DV74FngOdFJMfbf9VNSYkdH3/cjkOHNiCRW2+1bqTJkxN+F15oW+JNmpRYnOY4jtPEiDtGMR64GRgeuCgK3B83Q1UtxSzO1pd2QD+sC6ykAfc3mFBRjBgB5eW195nOiqptan3++cn+jz1m2z/edFPO5HQcx8kHcb/O/wI8jpnsSHUTg/CcISLrishvg9lOiMhmwE3AW6raqEoCbM2aiM1erZeSANswfrfdEtdDhsBbb5mScBzHaQbEXZldDRwtIncD+wN9sAV3L6lqcR7kUmAUcJuIdAzyegG4Mg951UlJia1Vq/ci5hUrkldWl5RAMx3ochyn9VKvldmq+jrwep5kieazGNscqUkwfXoDbDl9+SVsuWXi+rDDXEk4jtMsyaooApMdQ4FOwOuqukREtgLOBDYGFgD3qOrkzKk0bz76yHqP/vjHetykmqwkqqvdppLjOM2WjIpCRDbFWg/ht/RyETkdW3AXtTVxnIjsq6pv5E/MwvHll3b8zW9i3vDcczZ4HfLVV64kHMdp1mRrUVyJ7WAX0hN4iMQeFCHtgYuBFqkolgaWpfr0iRG5vNz2aAh5/fVkux+O4zjNkGyKYi9sUPlD4BVszGBI4HczNiV2FHAhsEtepSwgP/5ox7XWihH54YftOHIk9OvnZjYcx2kRZFMUYZfTfqq6VER6Y7OPAMaoaoWIjMEURa/8iVhYli6FLl2gTosaCxfCKafY+Z/+5C0Jx3FaDNnWUbQH0MCst6r+GAaoakVwLI+RTrPmhx+gd+8YEaNbj266aeZ4juM4zYw6p8cG+0LU6ddSmTnTNoTLykMPwamn2vnnn/vgteM4LYo46yhGRs41jV+L5ttvbevptMyYYTvGjR1r108/nePdjBzHcQpPXYqiVX8aq8LcubbJXC1KSpJ3L/ryS9i8INtlOI7j5JVsimJYo0nRRFmxAqqqoFevNIGTJiXOTznFlYTjOC2WjIoiMNfRqvnpJzvW2oL6m2/g0EPt/M9/hjPPbFS5HMdxGpN62XpqbSxbZsdaLYqzz7bjRRfVNh/uOI7Twmix01pzQdoWxdKl8OKLdn755Y0uk+M4TmPjiiILixbZMUlRnHyyHc87z63BOo7TKnBFkYX//Md0QY0h2KVLbQrsRRfBzTcXVDbHcZzGIs6COwG6B5crVLUqvyI1HRYuhC22iLQo7rjDTIYfdhi0cR3rOE7rIE5t1w5YCvwIDMqvOE2Ln36CHj0iHlOmwDbbuLE/x3FaFXUqClWtBOZii+/m5V2iJsTy5SmK4osvYNttCyaP4zhOIYjbf3IbpijOyJ8oTY+ffop0Oz38sJnsiO5c5ziO0wqIu45iW6z76QYROR74DKiIhKuqnpxr4QpNUosi3OJu/fULJo/jOE4hiKsoRpIwCLh14FJpUYpC1RRFz57YAHabNnY85phCi+Y4jtOo1GdldqsyELhihemFHj2wsYnqahg3Drp2retWx3GcFkUsRaGqBZsLKiJdgU+ADVW10UyOhKuye/QAJk+2C5/t5DhOK6Q5LAa4Efi+sTNdvtyOPXtiimLttX3nOsdxWiWxFYWIDBKRh0VkvohUBH4XicgVIjIwH8KJyF7AnsD/5SP9bCS1KN59F3bbzXeucxynVRKrK0dENgHeBdbCxirCge0NsCmzbYErcymYiHQB7gV+A2Q1qiQio4HRAEVFRRQXFzc439LSUoqLi3n//bWA7Znx6VswbRrf7b47s9Yg3eZKWB6O4eWRjJdHMi22PFS1Tgc8AlQDC4JjVeC/a3D9Xpx0gnvGYYomk7s2iPdX4M/B+VBgdZz0Bw8erGvCpEmTVFX1wQdVQfWzv79pJ6++ukbpNlfC8nAML49kvDySac7lAUzVDPVq3MHh4UElPhz4NOL/cXDcqNYdmTkLuCBLeJmI7AEcAOxQj3RzyjvvmEHAzWa/alNjd9mlUKI4juMUlLiKIlyfPD3FP5wrGtvetqqWAqXZ4ojIPli31iyzSUh7oK2ILAFOVNVn4ubXUKZNg+22g/bTPrFBbDcp7jhOKyXuYPbs4Dg0xf+ylPBccSuwKdai2AE4BagKzl/JcV5pmTMHNtgAmD0bBgxojCwdx3GaJHEVxdPYIPZ/Qg8RWQz8HuuSeiqXQqnqclWdEzpgceA/R1XLc5lX+vxNUfTvjymKDTbId5aO4zhNlriK4mrgC6BjxG9tTHl8CVybY7mSUNVibcTFdkuXQnk59F9vtW1K4YrCcZxWTNyV2T+JyG7AH4ARwDrAEuBF4DZVXZ4/ERuf2UFHWv/OP1jzwhWF4zitmNhf6cEg9DWBa9HMmWPHDdoG22+4onAcpxUTd8HdNOCN0KlqrgevmxSLF9txnZLv7GTDDQsnjOM4ToGJ26LYHNgMm32EiMwC3iShOFKnzTZrSoPJu91nfwHt28OgVrUDrOM4ThJxFcUzwG7Y2ATAhsAAzLwGIrJQVVvMjj6houj2XbCGol2jjaM7juM0OeIOZh8KNTafdo+4rbGZT0X5ErAQrFhhi7E7ffUxbL9docVxHMcpKPWxHtsW6BW4noFrkeZUS0uhWzdFvvvW98h2HKfVE3cw+w1gMNAp4v0VMBaYHLgWQ2kpdOu0GpZXu6JwHKfVE7fzfY/gWI2NV/xFVd/Ij0iFp7QUurWrsIsttiisMI7jOAUmbtfTI9guc22AQ4FJIrJMRP4rImNEZL+8SVgASkuhW9tAUfTtW1hhHMdxCkzcwexwdlMR8LPA7Y6ZHd8Hs/fUYqYGlZZC1zaBSam11y6sMI7jOAWmPoPZnYEtArc5Zt1VIq7FsGIFdKPU9kHt0KHQ4jiO4xSUuIPZHwLbYFue1ngHx2oSGxi1CEpLYaCWQJ8+hRbFcRyn4MTtLtohcr4KeJ/Eyux3WppRwNJS6LZqCQxoMWsIHcdxGkxcRfESphjeBKao6sr8iVR4SkuhW8V82HzzQoviOI5TcOIOZo/ItyBNidJSpVvlIhg4sNCiOI7jFJz6DGZvICL3icgcEVkpInOD6xa1T2hlpVBZKXRlBfTsWfcNjuM4LZy4g9kDgCnAuiQGsfsCI4EDRWQXVZ2VHxEbl8pK052dqIDu3QssjeM4TuGJ26K4CjP8J8AM4C1sAZ5gFmWvyodwhWD1atOD7al0ReE4jkN8RbEftqjuLFXdWFX3UtVBwFmYsmgxK7NXr7YicUXhOI5jxFUU4YKC8Sn+41PCmz3eonAcx0kmrqIINgflhBT/44PjktyIU3hcUTiO4yQTV1G8jHUx3Ski34hIsYh8A9yFdUm9lGvBgjxWikhpxP0y1/mkkqQounXLd3aO4zhNnriK4gqsVSHAxsCewEbB9RLgyrxIB9eoareIezZP+dRQVWVF0oFVrigcx3GIqShUdTYwBBuTWABUAQuD6xYzNRa8ReE4jpNKbNPggTIYlT9R0nKuiJwHzAceBP6sqpWpkURkNDAaoKioiOLi4gZnWFJiRdJOqiiePBmkRRnGrTelpaVrVJ4tDS+PZLw8kmmx5aGqDXbA/ljrYnU97hmHjWtkctcG8X4GrIVZrN0N+Ba4oa70Bw8erGvC7bd/oKD6UpdD1yidlsKkSZMKLUKTwssjGS+PZJpzeQBTNUO9movNhur7yX0WcEGW8DIAVY3uw/2uiFwB3AhcUs/86kXNOoou7fOZjeM4TrOh0XelU9VSoLQBt1bTCBsk1YxRdG4xG/Y5juOsEbGNAjYmItJLRH4pIt3E2BEYA0zId941isJbFI7jOECWFkVMq7BFOZQlSnvgMuAhTJnND85vyFN+NYTTY9t39S1QHcdxIHvX0wxscLnRUdXF2AB2o1PTonBF4TiOA9Q9RtHq5oa6onAcx0kmm6J4gwK1KApJjaLo1rHAkjiO4zQNMioKVR3aiHI0GaqqTFG069apwJI4juM0DZrkrKdC4orCcRwnGVcUKWhlNeCKwnEcJ8QVRQrVgaJo28XHKBzHccAVRS1qFEWHtgWWxHEcp2ngiiIFrbSJXu06uQkPx3EccEVRi6rVpijadnITHo7jOJDdhMdv65OQqo5fc3EKjwaKol1H73pyHMeB7AvuxhF/wZ1iu901e6pXeYvCcRwnipvwSKG6yhRFm46uKBzHcSC7ohjWaFI0IaoroR2V0MFtPTmO40B2Ex6vN6YgTYXq1UpbqlxROI7jBNRrDqiI7ABsBtRattxSBrOrq6Adq6G9dz05juNATEUhImsDzwK7ZIjSYgazq7xF4TiOk0TcFsV1wK75FKSpUL3aWxSO4zhR4i64OwBrNYwJrhU4GHgb+Ab4Zc4lKxDVVXiLwnEcJ0JcRbFecPxL6KGqzwHHApsAh+VWrMJRvdoVheM4TpS4iqIiOJYHDhHZFKgO/I/KsVwFo7rau54cx3GixFUUC4JjH+Dr4LwYeDc4r8yhTAWlylsUjuM4ScRVFB9iq7SHAA8F532B/kH4o7kXrTBUV4m1KFxROI7jAPEVxdnAlsBbqnozcAHWmvgQuAa4KB/CicgxIvKJiKwQkQUicmk+8olSFQ5me9eT4zgOEHN6rKouAZZErm8Fbs2XUAAicgJwE3A88DrQGRiYzzzBWhTe9eQ4jpMg9spsEWmLraXYAKi1T2guV2aLSBvgRuAqVX018C4BPs1VHpnwldmO4zjJxF2ZvRMwEVMS6cj1yuzNgPWB9UTkS6A38B5wrqp+k0a+0cBogKKiIoqLixucceXq7rSliuK334a2vidFaWnpGpVnS8PLIxkvj2RaanmIat1bTojIu2Q23wGgqhqrVhWRccDILFGuA14E3gQ+Aw4BFgJ/xizabquqqzPdvPPOO+vUqVPjiJKWX/T7iJJ5lbyvQxqcRkuiuLiYoUOHFlqMJoOXRzJeHsk05/IQkQ9Uded0YXG7nrbFWg33A49h6yribmqUylnYYHgmyoBNg/O/qur3ACLyJ2Ap1tr4ooF510lVtdBWGvpojuM4LY+4imIGsAXW9bN8TTJU1VKgNFscEfkKW9iXrsbOay1eVS20bVNdd0THcZxWQtzpsVcEx9PzJUgUVa0AxgLniMgGItIRm4b7OTA9n3lXVbWhnbiicBzHCYnbojgLm3V0vYj8HvgWiI4TqKoOz7Fs52G2pT7GTIW8AxysqlU5zieJam9ROI7jJBFXUeyNdfmEK7LXi4QJeegOUtWVwBmBazSqVWjnisJxHKeGuIpiFnkeG2gqrK5qQ9s2reJRHcdxYhF3ZfbAPMvRZKhWcUXhOI4ToV57ZgOIyCbAOsASVf26rvjNjarqNrRr54rCcRwnJO6sJ0TkQBH5FvgKeAv4UkS+FZEWs7sdQJV615PjOE6UWIpCRH4OPI0Z5ZOI2wiYGIS3CKqq29CurSsKx3GckLhdT5cBbbGFck8Ac7C9KI4AugOXAgfmQ8DGpkrbuIknx3GcCHEVxa7YrKcDVfWt0FNE7gPeAHbLg2wFoUp9jMJxHCdK3DGKLsEx1cz3pynhzZ4qbestCsdxnAhxFcWM4Hh7YFJDRKQ/8NfAf2bOJSsQ1vUkhRbDcRynyRBXUTyODV4fjymN1ZhyOAHrknosH8IVAut6KrQUjuM4TYe4iuI64G2SZzyF7t0gvEVQpW1p64rCcRynhrgrsytEZBjWotgP6IPtof0S8GC2jYSaG1W09RaF4zhOhNhVYqAMxgWuxbJa2/oYheM4ToSMikJEfgugquPD82yoai73zC4YVbSlbTtXFI7jOCHZWhTjsH0gxgfn2RYXaBCv2VNFW9q1d0XhOI4TUlfXk2Q4b5mospp23qJwWizLly9n0aJFVFZW5iS9nj17Mm3atJyk1RJoquXRvn171l13XXr06NGg+7MpimEZzlssuqqSajrQrkNsW4mO02xYvnw5CxcupF+/fnTu3BmRNf8gKikpoXv37jmQrmXQFMtDVSkvL2fu3LkADVIWGRWFqr4eufw+8JtV7xyaEVXlq4AO3qJwWiSLFi2iX79+dOnSYgwpODEQEbp06UK/fv2YN29ebhVFCjOw8Ypa8UVkNlCtqhvWO/cmhikKaNveWxROy6OyspLOnTsXWgynQHTu3LnBXY71WTFQ6zNbRNoA/Wgh26SuLrdC9K4np6WSi+4mp3myJr99tumx2wE7pPilTpPdKjiuarAETQhvUTiO49QmW4vicOCKyLUAY9PEU6BFbIm6uswURbsO/tXlOI4TUtenc2jPSQOXztbTMuDiXAsmIqUpbqWIVIlIn1znFVJZZl1P7b3ryXGaLI899hjjxo3LWXrFxcWICJ999lnO0mxp1LXgrhhTBq9hiiI6TVaBpcA3qlqea8FUtVv0WkQeAtZS1SW5ziukstxMVrXv6IrCcZoqjz32GEuWLGHUqFE5SW+nnXZi8uTJDBo0KCfptUSyTY+dSbDPhIhcbV5JU2YbDRFZG9t29df5zKdGUXTynYscpzlTWVlJmzZtaBtjF7IePXqw224tZpPOvBDXeuyY8FxE1gU6pYmTzzUWJwKLgefSBYrIaGA0QFFREcXFxQ3K5KcPZgI7MHfBHIqLf2yYpC2M0tLSBpdnS6Q5l0fPnj0pKSnJaZpVVVU5TzMbp512Gk8++SSQmMXzxz/+kbfeeou1116bX/ziF/zlL39h1qxZfPbZZ5SVlXH99dczZcoUfvzxRzbccENGjhzJ6aefTps21nPw5ptvctBBB/Huu++y1VY2P6dHjx7ceOONLF68mHHjxiEiHHbYYdxwww107Ngxo3yNXR71paKiokHvbyxFEUyDvQ74HdAzTRStR1rjgJFZolynqpdF4gumBP6pqlXpblDVe4B7AHbeeWcdOnRoHFFq8cV37wKwyWYbMXTolg1Ko6VRXFxMQ8uzJdKcy2PatGk5XzXc2CuRr776aubPn8+yZcu4++67Aejfvz+TJ09mypQpzJw5k5tvvrlmgdn777/PNttsw6hRo+jevTsfffQRV155JarKJZdcAlCzALFr165Jz3LXXXfxi1/8goceeohPPvmESy65hE033ZSLLrooo3xNcWV2lE6dOrHjjjvW+7646yguIncD1mcBF2QJL0u5HgZsBPwzR/lnxLuenFbHuefCRx81+PbOVVU0eJP5HXaA226r1y2DBg2id+/eVFdX1+ouWrZsGR999BFFRUU1fsOHD2f48OGAmbLYY489KCsr4957761RFJkYOHBgzaD5/vvvz9tvv83EiROzKoqWSlxFcTzWangeOCg4vxXbCnU58GDcDFW1FCith4ynAc+q6tx63NMgKiusweKKwnGaH4MHD05SEmBdLTfccAMPPfQQs2bNSlqZvHr1atpl2aVsv/32S7reaqutmDp1am6FbibEVRQbB8eTgQUAqnqhiEwA3gPy0iknIkXAYcDB+Ug/lZoWRWff4s5pJdTziz6V8ibU1ZKqJAAuvvhi/vnPf3LllVey00470atXL55++mmuvfZaKioq6NatW5qUjF69eiVdd+jQgYqKilyL3SyIWyOGJjqWAJVAu2Am0peB/7nAX3IrGmCD2LOxLVfzzuqV3vXkOM2VdCYqHn/8cc4+++yk7qLnnks7J8bJQtwFA+HahV5A2AX0cOBC/5yjqjeq6iBVbRRbUpUV1QC06+QtCsdpqtTny768vDxpllJVVRWPPvpovkRrscStET8D+gNbYuMUZwD7BGEKFGR9Ra6pXBmMUXjXk+M0WbbYYguefvppnnrqKfr378/666+fMe6+++7LXXfdxSabbELv3r256667WLlyZSNK2zKI26K4FlMOS4FLgWdJmPR4Czg9L9I1MmGLwhWF4zRdzjjjDPbbbz9OOukkhgwZwj333JMx7h133MGee+7JmWeeyUknncQ222xT52wnpzZxF9xNBiZHvA4RkU5Ae1VtuqtL6knlykBRdGlfYEkcx8lEnz59+Pe//x0rblFRUdq4p556as350KFDSe3dTtfbPWbMGMaMGVM/YVsIDf50VtUKoEVNAahcZS+HKwrHcZwE2fajqMZ2rmsXnGcbUFZVbfb9NatX+WC24zhOKnXViJLhvEVS06Jw67GO4zg1ZFMU40m0IqLnLZYaReE9T47jODVkMzM+CmqM8v0+8C5T1dWNIFdBWBVs6OqKwnEcJ0GcPpZ22LTYH0mY8miRVKy03rXOnQssiOM4ThOiTkWhqpXYamwB5uVdogKycpUpik61dttwHMdpvcQdtb0NUxRn5E+UwlOxyoqjQ4cCC+I4jtOEiDsPdFus++kGETkeM+kRXUOhqnpyroVrbCpWtaGTVGBrCR3HcRyI36IYScLw39bA0YFf6EblWrBCUFHZlk7idmAcp6UzZswY+vTpU3NdXFyMiPDZZ59lve+CCy5g4MCB9cpr0aJFjBkzhhkzZiT5x82zKVCfBQNSh2v2VKxuS0dXFI7T6thpp52YPHkygwYNynnaixYt4qqrrqqlKPKZZ66Ja+upVaxAq6hsR8c2qwothuM4jUyPHj1qba3aEvNsKK1CAcSlYnVbOraprDui4zgFYdy4cXTo0IFly5Yl+X/++eeICK+88grPPfcc++67L+uuu25NZfzSS9n3PkvXDbRs2TKOO+44unXrRt++fbnuuutq3Td//nxOOukkNt54Yzp37syOO+7IZZddxqpgUdaMGTPYdtttARg2bBgiUrPBUro8y8rK+P3vf896661Hp06dGDJkSC3Zhw4dypFHHsnDDz/MJptsQo8ePTjggAOYM2dO/IKsJ7EVhYgUici5InK3iNyX6vImYSNSsbq9KwrHacIcdthhiEgti7ATJkygqKiIYcOG8f3333PwwQfzwAMP8OSTT7L77rtzwAEH8Pbbb9crrxNPPJEXXniBv/zlL9xzzz289NJLtTY9WrJkCb179+bWW2/lxRdf5JxzzmHs2LGcffbZAPTt25eHHnoIgLvuuovJkyczefLkWnmFnHrqqYwdO5ZLL72Uf//732ywwQYcdNBBvPXWW0nxpkyZwp133sktt9zCPffcw4cffsjo0aPr9Xz1IVbXk4hsD0wCeqYLxsx7nJRDuQpCRVU7Orb1rien9XDuufDRRw2/v6qqM20buHPwDjvUf8vuXr16MWLECCZMmMCJJ55Y4z9hwgSOPPJI2rZty1lnnVXjX11dzbBhw/j888/517/+xc9//vNY+Xz++ec89dRTPProoxx99NGAtQgGDBhAjx49auJtu+22/PnPf6653m677Vh77bU56aSTuOOOO+jYsSPbbbcdAFtttVXWrqZp06bxyCOPMHbsWEaOHAnA/vvvz3bbbcc111zDf//735q4y5cv57nnnmOttdYCYMGCBfzhD3+gvLycznlYMRy3RXEVNusp0yB2yxjMrmpPx7beonCcpszRRx/Nq6++yg8//ADARx99xPTp02sq9Dlz5jBy5Ej69etHu3btaN++PS+99BLTp0+Pncf7778PwKGHHlrj161bN/bdd9+keKrKbbfdxlZbbUXnzp3p3bs3v/nNb1i5ciWzZs2q13O9//77qCpHHXVUjV+bNm046qijarUohgwZUqMkwJQQwNy5c8kHcddR7I61GkYA/w3OewA3BH7750W6RmZlVTu6dPQWhdN6qO8XfSolJeV07949J7LE5ZBDDqF9+/Y8+eSTjB49mgkTJtC/f3/22GMPqqurOeSQQygpKeHqq69mk002oWvXrlxxxRUsWrQodh4LFiyge/fudEox07DuuusmXd92221ceOGFXHzxxey999506NCBL774gjPPPDP2vt4h8+fPp1u3bnTp0iXJv6ioiLKyMlauXFmz/3evXr2S4nQIVgnXN8+4xFUUvYLjGySsyFZg26KeBfwD2C+nkhWAiuoO9GpbVmgxHMfJQrdu3TjooIOYMGECo0eP5rHHHuOoo45CRPj666/53//+xwsvvMCIESNq7ikvL69XHuuttx4lJSVUVFQkKYtUZfP4449z5JFH1gx0l5SUMHPmzAY9V9++fSktLaWsrCxJWSxcuJAuXbrUKIlCELfraXlwlMj5fsAOwfnuOZSpYFRUd6BjO+96cpymzjHHHMPrr7/OM888w3fffccxxxwDJBRCtFKdOXNmvQeyhwwZAsDTTz9d41daWsrLL7+cFK+8vLxWBR4OXofE/dofMmQIIsITTzxR46eqPPHEE+yxxx71kj/XxG1RzAbWAvoCHwN7As8GYQoszLVgIrI58Bdg1yCPt4FzVHVGrvMKMUXRYq2oO06L4cADD6RLly787ne/Y6ONNmKXXXYBYIsttqB///6cf/75XHPNNZSUlHDllVfSr1+/eqW/9dZbc8ghh3D66aezfPly+vbty80331yrW2jffffl9ttvZ9ddd2XQoEGMGzeOb775JinOgAED6Ny5M/fffz89e/akffv27LzzzrXy3HLLLTn22GM566yzKCkpYdCgQdx77718+eWX/O1vf6tnCeWWuC2K14BFwI7An4Eqkge0/y8Psj0CLAE2ADYESoCHst6xhlRoR1cUjtMM6Ny5M4cccgjz58+vGcQGa0lMnDiRdu3aceSRR3L55ZdzySWXsPfee9c7j3HjxrHffvtx7rnncvLJJzN8+PCalkvIFVdcwbHHHstll13GscceS4cOHbj99tuT4nTq1Il7772XDz74gL333rumtZKOe++9l5EjR3L11Vdz6KGHMnPmTJ599tmCtyhENf3GdSJyHPC0qq5IE7YLcBjQAXhOVSflXDCR5cCRqvpScL0/8KSqdst2384776xTp05tUJ69ZBlHbPoO/5p+YIPub4kUFxczdOjQQovRZGjO5TFt2jS23HLLnKZZUlLS6IPZTZmmXh7Z3gER+UBVazd1yN719CBQISIvAo8Cz6pqGYCqvge8t2Yi18mNwG9FZDLWahkF/DtdRBEZDYwGmyFQXFzcoAwr2I22bSobfH9LpLS01MsjQnMuj549e1JSUpLTNKuqqnKeZnOmqZdHRUVFg97fusYoOgGHBq5cRF4AJmBKo0HzsERkHGZxNhPXqeplwIvAr4BlmKL4hAzTcFX1HuAesBZFQ774tKqalbShUyea7RdjPmjOX9D5oDmXx7Rp03L+tdvUv6Abm6ZeHp06dWLHHXes933Zxij+CLxPYhyiC1ZxTwAWi8ijInK4iNR3ztZZwDpZ3PUishbwKvAU0C1wTwFvSp42i1hZarOdOravykfyjuM4zZaMikJVb1LVXbGB5POwWUdgSqMrcBTwBLBIRB6Mm6GqlqrqkiyuDBiErd24RVXLA79bgE2BLRrwnHVSsdwW2nXoUJ2P5B3HcZotcfbMnq2qt6nqnkA/4GygGKjGlEZ34Ngcy/Ul8CNwjoh0CFot52FrOL7JemcDCVsUHbxF4bRgMk1ecVo+a/Lb18vMuKouAMYD9wFTGpxr3fmUAr/EzIMsCNw+wC+DsJyz7nptKD/ieH653+x8JO84Bad9+/b1XqHstBzKy8tp3759g+6Naz22JzYd9kiswu6QEiXny5lVdTIwNNfpZkLW6kWnJx6kpJnOaHGculh33XWZO3cu/fr1o3PnzjX7IjgtG1WlvLycuXPnUlRU1KA0MioKEekNHI4ph2FAqIrCt2s1NuD8GBmmrTqO03QIzWPPmzePysrcfNul2kJq7TTV8mjfvj1FRUVJJtLrQ7YWxUISXVOhcqjC9qV4DJioqj82KFfHcQpCjx49GlxZpKO4uLhB0y1bKi21PLIpinA7kirMauxj2MroJXmXynEcx2kyZFMUoXJ4QlXjG3J3HMdxWhQZFYWqDm1EORzHcZwmSr2mxzqO4zitD1cUjuM4TlZcUTiO4zhZybgfRXNFRBYDDdu01uiDbZjkGF4eyXh5JOPlkUxzLo8NVXWddAEtTlGsKSIyNdPmHa0RL49kvDyS8fJIpqWWh3c9OY7jOFlxReE4juNkxRVFbe4ptABNDC+PZLw8kvHySKZFloePUTiO4zhZ8RaF4ziOkxVXFI7jOE5WXFE4juM4WXFFAYhIWxG5WUQWi0iJiDwpIn0KLVc+EJH/E5HPRWS5iMwTkXuDTaqicX4rIt+KSJmITBGRwSnhO4vIe0H4tyJyfOM+RX4QkTYi8o6IqIj0j/i31vLYR0TeFZFSEVkiIndHwlpVmYjIeiIyIagjlorIayKyfSS8ZZeHqrZ6B1wKTAc2BnoCTwIvFFquPD3r9cCO2I6F6wAvAP+JhO8BrAD2AzoCF2GbWPUIwnsCi4GLg/B9gVLgZ4V+thyUzfnAK4AC/VtzeWDbEC/DdrjsCHQCdmqtZQJMBF4G1sK2gr4JmI1t6tbiy6PgAjQFh5n8ODlyPSioLDYstGyN8OwjgOWR6/uBByLXAswCRgbXJwblJZE4DwBjC/0sa1gOmwHfAjukKIrWWh6TgRszhLW6MgE+AUZHrjcP3pM+raE8Wn3Xk4j0AgYAH4R+qvotsBzYPsNtLYnhwMeR6+1JLgsF/keiLLYH/hf4h3xIMy4rEWkD3AdcgH1FR2mN5dEV2AVoJyIfBt1OxSISmqZodWUC3AwcISLriEgnYDTwltqOny2+PFq9ogC6B8efUvyXAbnbXLgJIiJHAKcB50S8u5O9LOoKb46cAyxQ1X+nCWuN5bEWVjccC4wC1gdeAp4PPqxaY5m8jW0PvQjrNvoVcGoQ1uLLwxUFlATHnin+vbBWRYtERI4C7gUOUdUPI0ElZC+LusKbFSKyCTY2cVaGKK2qPALC/8RYVf1EVVcBN2DjWrvTysokaHG+go1j9gS6ANcBb4pIEa2gPFq9olDVZVh/4k6hn4hsjGn7TwokVl4RkROBfwAHq+qklOCPSS4LwfrtP46E75Byz44kd181J/bABvU/E5ElWJcAwCcicgatrzxQ1Z+AGVgffFJQ4FpbmfQGNgLuUNXlqrpKVf+J1Z8/ozWUR6EHSZqCw2Y9fYW9DD2Ax4EXCy1Xnp7198APwJAM4XtgTevh2OyOC0iewdELm8FxYRA+nGY2gyPlebsA/SNuN6wy3Bno1trKI1IuFwJzgK2AdthMnvnYl3GrK5OgfrgD6BqUx0nAKmymZIsvj4IL0BQc1vf4Z2zDkRJsKlyfQsuVp2dVoDJ4UWtcSpzfAt8B5cB7wOCU8CGBf3kQ7/hCP1cOy2cgkVlPrbU8sJk7VwMLsP70ScAOrbVMgC2BZ4M64ids8PrQ1lIebhTQcRzHyUqrH6NwHMdxsuOKwnEcx8mKKwrHcRwnK64oHMdxnKy4onAcx3Gy4orCcRzHyYorCseJICLjgv0o0rqmIlsh5XBaH64oHMdxnKy4onCczAxTVYm6QgvkOIXAFYXj1BMRGRPpjtpLRP4jIitEZH4QJinxDxaR14PtZytE5FMRuUBE2qbE2yToXpojIqtEZKGIPC0ia6WRYTMR+W+wtebXzW5rTadZ0a7QAjhOM2cisHZw3gW4EjMWdz2AiJwO3J1yzzbYRjhDgKODeNsCb5G8R8G6wCGYIb6lKWm8GYQDbAKMF5EPVfWLNX8kx0nGWxSOk5lJKYPZT6WJ8xmwHrAdMC/wu0hEuotId+D/Ar+52I5mRcBrgd+vRWRocH4bCSVxFbbFZl9sn4yyNPlODuKMDq4F20zHcXKOKwrHWTOuUdWFqvop8K/AryewNbbJT7iD4r1qmwAtwqyyhuwnIp2BvYPrD1R1jKr+oKoLVPWu4J5ULlHVH4AHI34b5OypHCeCdz05TmaGqWpxHXFmR87nRs77AZ0yxJsTOV8H2xgnHK/4KqZsXwfHiohfx5j3Ok698BaF46wZ/SPn/SLnc7G9C9LFi54vAX4EqoLrzeNkqqqrg6OvqXDyjisKx1kzLhORomAw+uTA7yfgc2wcoTTwO1VEthWRdYDLIve/pKrlQHFwPVhErhCR3kG6p4nIujhOAXFF4TiZSR3MVhEZmBJnS2wXuE+A9QO/m1S1RFWXA5cEfv2DOIuAfQK/JzSxZ/kfgOXB+VXYdrULgL9hs6kcp2C4onCcNeMI4ClsZlI4UH1DGKiqdwKHY9NZS4GVwBfAxcCxkXifAoOB8djsqcogvWewForjFAzfCtVx6omIjMHWSwBspKozCieN4+Qfb1E4juM4WXFF4TiO42TFu54cx3GcrHiLwnEcx8mKKwrHcRwnK64oHMdxnKy4onAcx3Gy4orCcRzHycr/A/fjD8V6P9lLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vlb_train = np.load('./COMPAS_Valter_VAEAC/compas_vlb_train_lr_0.0001.npy')\n",
    "vlb_val = np.load('./COMPAS_Valter_VAEAC/compas_vlb_val_lr_0.0001.npy')\n",
    "\n",
    "# best_epoch = 693 (pos 692 in train_val)\n",
    "curr_epoch = 894\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.clip(vlb_train[:curr_epoch], -1000, 1000), 'r')\n",
    "plt.plot(np.clip(vlb_val[:curr_epoch], -1000, 1000), 'b')\n",
    "plt.legend(['train', 'validation'], fontsize =15)\n",
    "plt.ylabel('Variational Lower Bound', fontweight='bold', fontsize =15)\n",
    "plt.xlabel('Epoch', fontweight='bold', fontsize =15)\n",
    "plt.yticks(fontsize = 13)\n",
    "plt.xticks(fontsize=13)\n",
    "plt.title('VAEAC Reproduction', fontweight='bold', fontsize= 15)\n",
    "plt.grid(True)\n",
    "plt.savefig('./COMPAS_Valter_VAEAC/' +  'compas_vaeac' + '.png', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e2168a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vlb_train = np.load('./COMPAS_Valter_under_VAEAC/compas_under_vlb_train_lr_0.0001.npy')\n",
    "vlb_val = np.load('./COMPAS_Valter_under_VAEAC/compas_under_vlb_val_lr_0.0001.npy')\n",
    "\n",
    "# best_epoch = 321 (pos 320 in train_val)\n",
    "curr_epoch = 522\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.clip(vlb_train[:curr_epoch], -1000, 1000), 'r')\n",
    "plt.plot(np.clip(vlb_val[:curr_epoch], -1000, 1000), 'b')\n",
    "plt.legend(['train', 'validation'], fontsize =15)\n",
    "plt.ylabel('Variational Lower Bound', fontweight='bold', fontsize =15)\n",
    "plt.xlabel('Epoch', fontweight='bold', fontsize =15)\n",
    "plt.yticks(fontsize = 13)\n",
    "plt.xticks(fontsize=13)\n",
    "plt.title('under_VAEAC Reproduction', fontweight='bold', fontsize= 15)\n",
    "plt.grid(True)\n",
    "plt.savefig('./COMPAS_Valter_under_VAEAC/' +  'compas_under_vaeac' + '.png', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8e2b58",
   "metadata": {},
   "source": [
    "## Train VAEAC (default credit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fa76c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow_addons/optimizers/rectified_adam.py:120: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "2021-12-11 09:16:23.922815: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_31061/2988879061.py\", line 202, in train_step_VAEAC  *\n        loss, vlb, kl_divergence, rec_loss, regularizer = compute_loss_VAEAC(model, x_batch, x_flat, x_masked, mask)\n    File \"/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_31061/2988879061.py\", line 151, in compute_loss_VAEAC  *\n        prior_params = model.prior_encoder(x_masked)\n    File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"prior_encoder_model\" is incompatible with the layer: expected shape=(None, 62), found shape=(64, 48)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_31061/2727803489.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munnormalise_cat_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_means\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_stds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mvlb_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvlb_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_vlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_VAEAC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_31061/2361816603.py\u001b[0m in \u001b[0;36mtrain_VAEAC\u001b[0;34m(model, x_train, x_test, masker, nb_epochs, early_stop, flatten)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mx_batch_flat_masked_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_batch_flat_masked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_flat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_divergence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step_VAEAC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch_flat_masked_concat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mvlb_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_31061/2988879061.py\", line 202, in train_step_VAEAC  *\n        loss, vlb, kl_divergence, rec_loss, regularizer = compute_loss_VAEAC(model, x_batch, x_flat, x_masked, mask)\n    File \"/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_31061/2988879061.py\", line 151, in compute_loss_VAEAC  *\n        prior_params = model.prior_encoder(x_masked)\n    File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/anaconda3/envs/py38/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"prior_encoder_model\" is incompatible with the layer: expected shape=(None, 62), found shape=(64, 48)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "batch_size = 64\n",
    "nb_epochs = 2000 # 2000\n",
    "early_stop = 200\n",
    "lr = 7e-4        # Maybe this should be 1e-4, but it makes the performance terrible...\n",
    "\n",
    "optimizer = tfa.optimizers.RectifiedAdam(lr = lr, epsilon = 1e-8)\n",
    "\n",
    "width = 350\n",
    "depth = 3\n",
    "latent_dim = 8\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "masker = top_masker_tensorflow(p=1)\n",
    "\n",
    "x_train, x_test, x_means, x_stds = \\\n",
    "load_UCI(dset_name=dname, splits=10, seed=42, separate_targets=False, save_dir='../data/') # np.arrays\n",
    "\n",
    "model = VAEAC_gauss_cat(width, depth, latent_dim, input_dim_vec, batch_size, lr, optimizer, save_model = True)\n",
    "\n",
    "x_train = unnormalise_cat_vars(x_train, x_means, x_stds, input_dim_vec) # np.array\n",
    "x_test = unnormalise_cat_vars(x_test, x_means, x_stds, input_dim_vec) \n",
    "\n",
    "vlb_train, vlb_val, best_epoch, best_vlb, curr_epoch = train_VAEAC(model, x_train, x_test, masker, nb_epochs, early_stop=early_stop, flatten = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc9ac90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621e0c18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f7efcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad555fe9",
   "metadata": {},
   "source": [
    "## Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9b8ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e427ab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the total number of trainable variables \n",
    "\"\"\"\n",
    "total_parameters = 0\n",
    "for variable in model.trainable_variables:\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    #print(shape)\n",
    "    #print(len(shape))\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        #print(dim)\n",
    "        variable_parameters *= dim\n",
    "    #print(variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)\n",
    "\n",
    "time.sleep(10)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b0e8fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469f0d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20896d98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae708749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b7c650e",
   "metadata": {},
   "source": [
    "## Generate sample parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e71b276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55cd3a92",
   "metadata": {},
   "source": [
    "## UNDER VAEAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b107d2c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af920dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class under_VAEAC(tf.keras.Model):\n",
    "    def __init__(self, base_VAE, width, depth, latent_dim, batch_size, lr, optimizer, save_model = True):\n",
    "        super(under_VAEAC, self).__init__()\n",
    "        \n",
    "        self.base_VAEAC = base_VAE\n",
    "        self.input_dim = self.base_VAEAC.latent_dim # 8 for default credit\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.latent_dim = latent_dim # 6 for default credit\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.prior_encoder = tfd.Normal(loc=tf.zeros(latent_dim), scale=tf.ones(latent_dim))\n",
    "        \n",
    "        # self.input_dim is put in a list to make sum(input_dim_vec in recognition_encoder work)\n",
    "        self.recognition_encoder = create_recognition_encoder(width, depth, latent_dim, [self.input_dim])\n",
    "        self.decoder = create_decoder(width, depth, latent_dim, [self.input_dim])\n",
    "        \n",
    "        self.vlb_scale = 1 / self.input_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer\n",
    "        self.save_model = save_model\n",
    "\n",
    "    # Inspiration taken from \n",
    "    # https://github.com/joocxi/tf2-VAEAC/blob/d2b1bbc258ec77ee0975ea7eb68e63c4efcda6f0/model/vaeac.py\n",
    "\n",
    "    def reconstruction_loss(self, x, y):\n",
    "        log_prob_vec = []\n",
    "        reshape_dim = self.batch_size\n",
    "        for idx in range(self.input_dim):\n",
    "            # Gaussian_case\n",
    "            \n",
    "            # minus sign in front because vlb on return after calling this adds it\n",
    "            log_prob_vec.append(tf.expand_dims(-(x[:, idx] - y[:, idx])**2, 1))\n",
    "\n",
    "        log_prob_vec = tf.reshape(log_prob_vec, [reshape_dim, self.input_dim])\n",
    "        log_prob_vec = tf.math.reduce_sum(log_prob_vec, axis= -1)\n",
    "        \n",
    "        return log_prob_vec\n",
    "    \n",
    "def compute_loss_under_VAEAC(model, x_flat, proposal_params_VAEAC):\n",
    "    \n",
    "    # Essentially CLUEs normal_parse_params\n",
    "    proposal_distribution_VAEAC = tfd.Normal(\n",
    "      loc=proposal_params_VAEAC[..., :model.input_dim],\n",
    "      scale=tf.clip_by_value(\n",
    "        tf.nn.softplus(proposal_params_VAEAC[..., model.input_dim:]),\n",
    "        1e-3,\n",
    "        tf.float32.max),\n",
    "      name=\"proposal\")\n",
    "    \n",
    "    z_sample = proposal_distribution_VAEAC.sample() # tensor with dim (base_VAEAC.latent_dim,)\n",
    "    \n",
    "    proposal_params_VAE = model.recognition_encoder(z_sample) \n",
    "\n",
    "    proposal_distribution_VAE = tfd.Normal(\n",
    "      loc=proposal_params_VAE[..., :model.latent_dim],\n",
    "      scale=tf.clip_by_value(\n",
    "        tf.nn.softplus(proposal_params_VAE[..., model.latent_dim:]),\n",
    "        1e-3,\n",
    "        tf.float32.max),\n",
    "      name=\"priors\")\n",
    "\n",
    "    u_sample = proposal_distribution_VAE.sample() \n",
    "    \n",
    "    rec_params = model.decoder(u_sample) \n",
    "    \n",
    "    # True distribution, Estimated distribution in this order\n",
    "    kl_divergence = tf.reduce_sum(\n",
    "      tf.reshape(\n",
    "        tfd.kl_divergence(proposal_distribution_VAE, model.prior_encoder),\n",
    "        (model.batch_size, -1)), -1)\n",
    "    \n",
    "    rec_loss = model.reconstruction_loss(rec_params, z_sample)\n",
    "    \n",
    "    vlb = tf.reduce_mean(-kl_divergence + rec_loss) # For comparing\n",
    "    loss = tf.reduce_mean((kl_divergence - rec_loss) * model.vlb_scale) \n",
    "    return loss, vlb, kl_divergence, rec_loss\n",
    "\n",
    "@tf.function # Converts all numpy arrays to tensors\n",
    "def train_step_under_VAEAC(model, x_flat, proposal_params_VAEAC):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, vlb, kl_divergence, rec_loss = compute_loss_under_VAEAC(model, x_flat, proposal_params_VAEAC)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss, vlb, kl_divergence, rec_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e843dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "def train_VAE(model, x_train, x_test, nb_epochs, early_stop = None, flatten = False):\n",
    "    \n",
    "    vlb_train = np.zeros(nb_epochs)\n",
    "    vlb_val = np.zeros(nb_epochs)\n",
    "    best_vlb = -np.inf\n",
    "    best_epoch = 0\n",
    "    \n",
    "    overall_batch_size = model.batch_size\n",
    "    \n",
    "    test_data = []\n",
    "    for x in batch(x_test, n = overall_batch_size):\n",
    "        test_data.append(x)\n",
    "    \n",
    "    epoch = 0\n",
    "    for epoch in range(0, nb_epochs):\n",
    "        tic = time.time()\n",
    "        \n",
    "        train_data = []\n",
    "        np.random.shuffle(x_train)\n",
    "        for x in batch(x_train, n = overall_batch_size):\n",
    "            train_data.append(x)\n",
    "        \n",
    "        ## Training\n",
    "        nb_samples = 0\n",
    "        for x_batch in train_data:\n",
    "\n",
    "            model.batch_size = x_batch.shape[0] # TODO: FIX THIS: Very ugly solution now to make sure batches \n",
    "                                                # that do not have the full size\n",
    "            \n",
    "            # If data is not already flattened (default credit)\n",
    "            if flatten:\n",
    "                x_batch_flat = gauss_cat_to_flat(x_batch, model.input_dim_vec) # numpy\n",
    "            # If data is already flattened (COMPAS from join_compas_targets)\n",
    "            else:\n",
    "                x_batch_flat = x_batch\n",
    "            \n",
    "            x_batch_flat = tf.convert_to_tensor(x_batch_flat)\n",
    "    \n",
    "            proposal_params_VAEAC = model.base_VAEAC.recognition_encoder(x_batch_flat) # tensor with dim (16,)\n",
    "            \n",
    "            loss, vlb, kl_divergence, rec_loss = train_step_under_VAEAC(model, x_batch_flat, proposal_params_VAEAC)\n",
    "\n",
    "            vlb_train[epoch] += vlb.numpy() * x_batch.shape[0]\n",
    "            nb_samples += x_batch.shape[0]\n",
    "\n",
    "        vlb_train[epoch] /= nb_samples\n",
    "        toc = time.time()\n",
    "        print(\"Epoch\" + str(epoch) + \", vlb: \" + str(vlb_train[epoch]) + \", took: \" + str(toc-tic))\n",
    "        \n",
    "        ## Validation\n",
    "        nb_samples = 0\n",
    "        \n",
    "        for x_batch in test_data:\n",
    "            model.batch_size = x_batch.shape[0] # TODO: FIX THIS: Very ugly solution now to make sure batches \n",
    "                                                # that do not have the full size\n",
    "            \n",
    "            # If data is not already flattened (default credit)\n",
    "            if flatten:\n",
    "                x_batch_flat = gauss_cat_to_flat(x_batch, model.input_dim_vec) # numpy\n",
    "            # If data is already flattened (COMPAS from get_my_COMPAS)\n",
    "            else:\n",
    "                x_batch_flat = x_batch\n",
    "            \n",
    "            x_batch_flat = tf.convert_to_tensor(x_batch_flat)\n",
    "    \n",
    "            proposal_params_VAEAC = model.base_VAEAC.recognition_encoder(x_batch_flat) # tensor with dim (16,)\n",
    "            \n",
    "            # In CLUE there is actually no difference between eval and fitother than that we should not update the weights.\n",
    "            # Therefore ok to just call compute_loss_under_VAEAC directly instead of a special eval func\n",
    "            loss, vlb, kl_divergence, rec_loss = compute_loss_under_VAEAC(model, x_batch_flat, proposal_params_VAEAC)\n",
    "\n",
    "            vlb_val[epoch] += vlb.numpy() * x_batch.shape[0]\n",
    "            nb_samples += x_batch.shape[0]\n",
    "\n",
    "        vlb_val[epoch] /= nb_samples\n",
    "\n",
    "        if vlb_val[epoch] > best_vlb:\n",
    "            best_vlb = vlb_val[epoch]\n",
    "            best_epoch = epoch\n",
    "            if(model.save_model):\n",
    "                #open text file\n",
    "                text_file = open(\"./COMPAS_Valter_under_VAEAC/\" + str(dname) + \"_best_epoch_under_VAEAC_lr_\" + str(model.lr) + \".txt\", \"w\")\n",
    "\n",
    "                #write string to file\n",
    "                text_file.write(str(epoch))\n",
    "\n",
    "                #close file\n",
    "                text_file.close()\n",
    "\n",
    "                model.recognition_encoder.save(\"./COMPAS_Valter_under_VAEAC/\" + str(dname) + \"_under_recog_encoder_lr_\" + str(model.lr))\n",
    "                model.decoder.save(\"./COMPAS_Valter_under_VAEAC/\" + str(dname) + \"_under_decoder_lr_\" + str(model.lr))\n",
    "\n",
    "        print(\"Validation vlb: \" + str(vlb_val[epoch]) + \", Best vlb: \" + str(best_vlb) + \"\\n\")\n",
    "\n",
    "        if early_stop is not None and (epoch - best_epoch) > early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    \n",
    "    np.save(\"./COMPAS_Valter_under_VAEAC/\" + str(dname) + \"_under_vlb_train_lr_\" + str(model.lr), vlb_train)\n",
    "    np.save(\"./COMPAS_Valter_under_VAEAC/\" + str(dname) + \"_under_vlb_val_lr_\" + str(model.lr), vlb_val)\n",
    "    return vlb_train, vlb_val, best_epoch, best_vlb, epoch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88f64f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compas\n",
      "Compas (5554, 19) (618, 19)\n",
      "[3 6 2 2 2 1 1 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow_addons/optimizers/rectified_adam.py:120: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "2021-12-13 13:01:46.043092: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "2021-12-13 13:01:49.368327: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0, vlb: -13.974108723800818, took: 3.0440380573272705\n",
      "Validation vlb: -12.603024263598, Best vlb: -12.603024263598\n",
      "\n",
      "Epoch1, vlb: -11.512968399905025, took: 0.6395220756530762\n",
      "Validation vlb: -10.185881068405596, Best vlb: -10.185881068405596\n",
      "\n",
      "Epoch2, vlb: -9.447635804829856, took: 0.7047171592712402\n",
      "Validation vlb: -8.717284005050907, Best vlb: -8.717284005050907\n",
      "\n",
      "Epoch3, vlb: -8.138348224826712, took: 0.6325180530548096\n",
      "Validation vlb: -7.9152032318238685, Best vlb: -7.9152032318238685\n",
      "\n",
      "Epoch4, vlb: -7.414419886960286, took: 0.6474659442901611\n",
      "Validation vlb: -7.173444448551313, Best vlb: -7.173444448551313\n",
      "\n",
      "Epoch5, vlb: -6.917978245532895, took: 0.6393449306488037\n",
      "Validation vlb: -6.869564449902877, Best vlb: -6.869564449902877\n",
      "\n",
      "Epoch6, vlb: -6.5417620207622065, took: 0.6579139232635498\n",
      "Validation vlb: -6.268019654604224, Best vlb: -6.268019654604224\n",
      "\n",
      "Epoch7, vlb: -6.127180304851813, took: 0.5743939876556396\n",
      "Validation vlb: -5.999173869592858, Best vlb: -5.999173869592858\n",
      "\n",
      "Epoch8, vlb: -5.849508957249851, took: 0.6119780540466309\n",
      "Validation vlb: -5.78577704568511, Best vlb: -5.78577704568511\n",
      "\n",
      "Epoch9, vlb: -5.770039273888076, took: 0.5839810371398926\n",
      "Validation vlb: -5.864893615438715, Best vlb: -5.78577704568511\n",
      "\n",
      "Epoch10, vlb: -5.696103996452686, took: 0.5982310771942139\n",
      "Validation vlb: -5.821183581182486, Best vlb: -5.78577704568511\n",
      "\n",
      "Epoch11, vlb: -5.677383059342645, took: 0.6054039001464844\n",
      "Validation vlb: -5.64494082225565, Best vlb: -5.64494082225565\n",
      "\n",
      "Epoch12, vlb: -5.605484298978043, took: 0.5721020698547363\n",
      "Validation vlb: -5.664472296014187, Best vlb: -5.64494082225565\n",
      "\n",
      "Epoch13, vlb: -5.635580047755779, took: 0.5993797779083252\n",
      "Validation vlb: -5.659027383551242, Best vlb: -5.64494082225565\n",
      "\n",
      "Epoch14, vlb: -5.5896040767399064, took: 0.6220111846923828\n",
      "Validation vlb: -5.764832991998173, Best vlb: -5.64494082225565\n",
      "\n",
      "Epoch15, vlb: -5.5887812357563575, took: 0.8332479000091553\n",
      "Validation vlb: -5.736444703197788, Best vlb: -5.64494082225565\n",
      "\n",
      "Epoch16, vlb: -5.611193448637817, took: 0.6860208511352539\n",
      "Validation vlb: -5.682840822584035, Best vlb: -5.64494082225565\n",
      "\n",
      "Epoch17, vlb: -5.536174060202005, took: 0.5893559455871582\n",
      "Validation vlb: -5.6374129894096106, Best vlb: -5.6374129894096106\n",
      "\n",
      "Epoch18, vlb: -5.553732046945343, took: 0.6680557727813721\n",
      "Validation vlb: -5.614451294192219, Best vlb: -5.614451294192219\n",
      "\n",
      "Epoch19, vlb: -5.538018125314957, took: 0.6438109874725342\n",
      "Validation vlb: -5.599604649836963, Best vlb: -5.599604649836963\n",
      "\n",
      "Epoch20, vlb: -5.528117980895216, took: 0.5766067504882812\n",
      "Validation vlb: -5.661086287699085, Best vlb: -5.599604649836963\n",
      "\n",
      "Epoch21, vlb: -5.543980391368104, took: 0.5464940071105957\n",
      "Validation vlb: -5.723616834597294, Best vlb: -5.599604649836963\n",
      "\n",
      "Epoch22, vlb: -5.517072990295223, took: 0.544597864151001\n",
      "Validation vlb: -5.568451338215553, Best vlb: -5.568451338215553\n",
      "\n",
      "Epoch23, vlb: -5.5212095744497365, took: 0.5644938945770264\n",
      "Validation vlb: -5.59788179397583, Best vlb: -5.568451338215553\n",
      "\n",
      "Epoch24, vlb: -5.513945583993254, took: 0.5400538444519043\n",
      "Validation vlb: -5.605051054537875, Best vlb: -5.568451338215553\n",
      "\n",
      "Epoch25, vlb: -5.528576649819341, took: 0.5614180564880371\n",
      "Validation vlb: -5.530164021118559, Best vlb: -5.530164021118559\n",
      "\n",
      "Epoch26, vlb: -5.468399681631163, took: 0.6153359413146973\n",
      "Validation vlb: -5.543274501380797, Best vlb: -5.530164021118559\n",
      "\n",
      "Epoch27, vlb: -5.504070349979366, took: 0.7546279430389404\n",
      "Validation vlb: -5.516003563951906, Best vlb: -5.516003563951906\n",
      "\n",
      "Epoch28, vlb: -5.482044266847384, took: 0.7885308265686035\n",
      "Validation vlb: -5.55144938990522, Best vlb: -5.516003563951906\n",
      "\n",
      "Epoch29, vlb: -5.4924618462070445, took: 0.6430196762084961\n",
      "Validation vlb: -5.556840165147504, Best vlb: -5.516003563951906\n",
      "\n",
      "Epoch30, vlb: -5.46575279256445, took: 0.5885770320892334\n",
      "Validation vlb: -5.669591020997674, Best vlb: -5.516003563951906\n",
      "\n",
      "Epoch31, vlb: -5.5000550620711275, took: 0.5302481651306152\n",
      "Validation vlb: -5.488190553720715, Best vlb: -5.488190553720715\n",
      "\n",
      "Epoch32, vlb: -5.448007206887619, took: 0.5885298252105713\n",
      "Validation vlb: -5.588060416064216, Best vlb: -5.488190553720715\n",
      "\n",
      "Epoch33, vlb: -5.459359903579399, took: 0.5787901878356934\n",
      "Validation vlb: -5.456770639203513, Best vlb: -5.456770639203513\n",
      "\n",
      "Epoch34, vlb: -5.4665741765932685, took: 0.5642900466918945\n",
      "Validation vlb: -5.643531367230955, Best vlb: -5.456770639203513\n",
      "\n",
      "Epoch35, vlb: -5.447675715602041, took: 0.5368998050689697\n",
      "Validation vlb: -5.6373012860616045, Best vlb: -5.456770639203513\n",
      "\n",
      "Epoch36, vlb: -5.453763105820356, took: 0.5456314086914062\n",
      "Validation vlb: -5.565986986684954, Best vlb: -5.456770639203513\n",
      "\n",
      "Epoch37, vlb: -5.4673065455817556, took: 0.5586531162261963\n",
      "Validation vlb: -5.4978305591348695, Best vlb: -5.456770639203513\n",
      "\n",
      "Epoch38, vlb: -5.444320287041669, took: 0.5343301296234131\n",
      "Validation vlb: -5.507907882863264, Best vlb: -5.456770639203513\n",
      "\n",
      "Epoch39, vlb: -5.46134334525746, took: 0.6724662780761719\n",
      "Validation vlb: -5.524656451635762, Best vlb: -5.456770639203513\n",
      "\n",
      "Epoch40, vlb: -5.428262901168785, took: 0.5757491588592529\n",
      "Validation vlb: -5.485120040313325, Best vlb: -5.456770639203513\n",
      "\n",
      "Epoch41, vlb: -5.425579318649957, took: 0.5569050312042236\n",
      "Validation vlb: -5.5280180702703285, Best vlb: -5.456770639203513\n",
      "\n",
      "Epoch42, vlb: -5.462892811204789, took: 0.5604360103607178\n",
      "Validation vlb: -5.558958863749088, Best vlb: -5.456770639203513\n",
      "\n",
      "Epoch43, vlb: -5.453236815652102, took: 0.6066858768463135\n",
      "Validation vlb: -5.6094846200788675, Best vlb: -5.456770639203513\n",
      "\n",
      "Epoch44, vlb: -5.425959607358884, took: 0.5519568920135498\n",
      "Validation vlb: -5.634655381483554, Best vlb: -5.456770639203513\n",
      "\n",
      "Epoch45, vlb: -5.436899891249945, took: 0.5431790351867676\n",
      "Validation vlb: -5.489328944567338, Best vlb: -5.456770639203513\n",
      "\n",
      "Epoch46, vlb: -5.415525866876973, took: 0.542464017868042\n",
      "Validation vlb: -5.531690250322657, Best vlb: -5.456770639203513\n",
      "\n",
      "Epoch47, vlb: -5.425190997716116, took: 0.5331499576568604\n",
      "Validation vlb: -5.520006402024945, Best vlb: -5.456770639203513\n",
      "\n",
      "Epoch48, vlb: -5.420906043542045, took: 0.5362730026245117\n",
      "Validation vlb: -5.494553940967449, Best vlb: -5.456770639203513\n",
      "\n",
      "Epoch49, vlb: -5.439248059265554, took: 0.5342669486999512\n",
      "Validation vlb: -5.4988095104501475, Best vlb: -5.456770639203513\n",
      "\n",
      "Epoch50, vlb: -5.423023733682327, took: 0.5372810363769531\n",
      "Validation vlb: -5.461936203793028, Best vlb: -5.456770639203513\n",
      "\n",
      "Epoch51, vlb: -5.389503081153098, took: 0.5514123439788818\n",
      "Validation vlb: -5.537465632540508, Best vlb: -5.456770639203513\n",
      "\n",
      "Epoch52, vlb: -5.481845020861518, took: 0.540869951248169\n",
      "Validation vlb: -5.497659413174132, Best vlb: -5.456770639203513\n",
      "\n",
      "Epoch53, vlb: -5.429733121313549, took: 0.537445068359375\n",
      "Validation vlb: -5.440325988534971, Best vlb: -5.440325988534971\n",
      "\n",
      "Epoch54, vlb: -5.434402359875579, took: 0.5359039306640625\n",
      "Validation vlb: -5.4571636147483655, Best vlb: -5.440325988534971\n",
      "\n",
      "Epoch55, vlb: -5.386980147768707, took: 0.5354549884796143\n",
      "Validation vlb: -5.542504133144243, Best vlb: -5.440325988534971\n",
      "\n",
      "Epoch56, vlb: -5.452765120161837, took: 0.5389461517333984\n",
      "Validation vlb: -5.481626468954734, Best vlb: -5.440325988534971\n",
      "\n",
      "Epoch57, vlb: -5.4018397044483, took: 0.5424258708953857\n",
      "Validation vlb: -5.503678985398178, Best vlb: -5.440325988534971\n",
      "\n",
      "Epoch58, vlb: -5.435298793814703, took: 0.604421854019165\n",
      "Validation vlb: -5.493102215640367, Best vlb: -5.440325988534971\n",
      "\n",
      "Epoch59, vlb: -5.431900227546349, took: 0.5453979969024658\n",
      "Validation vlb: -5.464326821484612, Best vlb: -5.440325988534971\n",
      "\n",
      "Epoch60, vlb: -5.4139237003786596, took: 0.5482747554779053\n",
      "Validation vlb: -5.3734168543398955, Best vlb: -5.3734168543398955\n",
      "\n",
      "Epoch61, vlb: -5.407320295601043, took: 0.5415101051330566\n",
      "Validation vlb: -5.4598199264131315, Best vlb: -5.3734168543398955\n",
      "\n",
      "Epoch62, vlb: -5.376352307089894, took: 0.5453450679779053\n",
      "Validation vlb: -5.4700034937812285, Best vlb: -5.3734168543398955\n",
      "\n",
      "Epoch63, vlb: -5.415603907107449, took: 0.5439691543579102\n",
      "Validation vlb: -5.507591062379115, Best vlb: -5.3734168543398955\n",
      "\n",
      "Epoch64, vlb: -5.434503033987944, took: 0.5356671810150146\n",
      "Validation vlb: -5.505835412775428, Best vlb: -5.3734168543398955\n",
      "\n",
      "Epoch65, vlb: -5.395479342385424, took: 0.5554728507995605\n",
      "Validation vlb: -5.435308388521756, Best vlb: -5.3734168543398955\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch66, vlb: -5.434958294479376, took: 0.5572819709777832\n",
      "Validation vlb: -5.551028186835132, Best vlb: -5.3734168543398955\n",
      "\n",
      "Epoch67, vlb: -5.410896418203671, took: 0.5435190200805664\n",
      "Validation vlb: -5.364662849401579, Best vlb: -5.364662849401579\n",
      "\n",
      "Epoch68, vlb: -5.436508031555934, took: 0.5518178939819336\n",
      "Validation vlb: -5.528605609264188, Best vlb: -5.364662849401579\n",
      "\n",
      "Epoch69, vlb: -5.406671770279211, took: 0.5562231540679932\n",
      "Validation vlb: -5.420719861212672, Best vlb: -5.364662849401579\n",
      "\n",
      "Epoch70, vlb: -5.427052634887915, took: 0.5579991340637207\n",
      "Validation vlb: -5.486631640338589, Best vlb: -5.364662849401579\n",
      "\n",
      "Epoch71, vlb: -5.421849726428649, took: 0.5449390411376953\n",
      "Validation vlb: -5.481140161409347, Best vlb: -5.364662849401579\n",
      "\n",
      "Epoch72, vlb: -5.370235202893706, took: 0.5438971519470215\n",
      "Validation vlb: -5.3987418995706005, Best vlb: -5.364662849401579\n",
      "\n",
      "Epoch73, vlb: -5.402454812706139, took: 0.6013917922973633\n",
      "Validation vlb: -5.425106293946794, Best vlb: -5.364662849401579\n",
      "\n",
      "Epoch74, vlb: -5.4261396629242835, took: 0.539405107498169\n",
      "Validation vlb: -5.439516632302293, Best vlb: -5.364662849401579\n",
      "\n",
      "Epoch75, vlb: -5.395562189793819, took: 0.547142744064331\n",
      "Validation vlb: -5.506478684619792, Best vlb: -5.364662849401579\n",
      "\n",
      "Epoch76, vlb: -5.372001029795179, took: 0.5422821044921875\n",
      "Validation vlb: -5.453282296078877, Best vlb: -5.364662849401579\n",
      "\n",
      "Epoch77, vlb: -5.426206653823001, took: 0.532620906829834\n",
      "Validation vlb: -5.486791623063072, Best vlb: -5.364662849401579\n",
      "\n",
      "Epoch78, vlb: -5.3863008538295265, took: 0.6398899555206299\n",
      "Validation vlb: -5.345817379195328, Best vlb: -5.345817379195328\n",
      "\n",
      "Epoch79, vlb: -5.430918307367915, took: 0.6205260753631592\n",
      "Validation vlb: -5.452211230318137, Best vlb: -5.345817379195328\n",
      "\n",
      "Epoch80, vlb: -5.388625764829631, took: 0.6881709098815918\n",
      "Validation vlb: -5.523875344532593, Best vlb: -5.345817379195328\n",
      "\n",
      "Epoch81, vlb: -5.412826548388505, took: 0.5511879920959473\n",
      "Validation vlb: -5.626713252761989, Best vlb: -5.345817379195328\n",
      "\n",
      "Epoch82, vlb: -5.378118901532633, took: 0.6032528877258301\n",
      "Validation vlb: -5.402624449683625, Best vlb: -5.345817379195328\n",
      "\n",
      "Epoch83, vlb: -5.388042566198473, took: 0.640625\n",
      "Validation vlb: -5.50667873864035, Best vlb: -5.345817379195328\n",
      "\n",
      "Epoch84, vlb: -5.40799984899321, took: 0.6888997554779053\n",
      "Validation vlb: -5.3606958188671125, Best vlb: -5.345817379195328\n",
      "\n",
      "Epoch85, vlb: -5.369389842730183, took: 0.5871732234954834\n",
      "Validation vlb: -5.441226186104191, Best vlb: -5.345817379195328\n",
      "\n",
      "Epoch86, vlb: -5.4054389662737465, took: 0.5516948699951172\n",
      "Validation vlb: -5.430207448484056, Best vlb: -5.345817379195328\n",
      "\n",
      "Epoch87, vlb: -5.394438038895703, took: 0.5924251079559326\n",
      "Validation vlb: -5.409186531424909, Best vlb: -5.345817379195328\n",
      "\n",
      "Epoch88, vlb: -5.407371365599037, took: 0.5945322513580322\n",
      "Validation vlb: -5.545485414733393, Best vlb: -5.345817379195328\n",
      "\n",
      "Epoch89, vlb: -5.356431816995337, took: 0.5709221363067627\n",
      "Validation vlb: -5.511505988037702, Best vlb: -5.345817379195328\n",
      "\n",
      "Epoch90, vlb: -5.388305681576864, took: 0.5572881698608398\n",
      "Validation vlb: -5.552410639605476, Best vlb: -5.345817379195328\n",
      "\n",
      "Epoch91, vlb: -5.3561902169834426, took: 0.5784082412719727\n",
      "Validation vlb: -5.4203293038031815, Best vlb: -5.345817379195328\n",
      "\n",
      "Epoch92, vlb: -5.4123707672740045, took: 0.6390399932861328\n",
      "Validation vlb: -5.574631795914042, Best vlb: -5.345817379195328\n",
      "\n",
      "Epoch93, vlb: -5.4028754167347275, took: 0.7141339778900146\n",
      "Validation vlb: -5.398017747502497, Best vlb: -5.345817379195328\n",
      "\n",
      "Epoch94, vlb: -5.404361510731825, took: 0.5950100421905518\n",
      "Validation vlb: -5.411684486858282, Best vlb: -5.345817379195328\n",
      "\n",
      "Epoch95, vlb: -5.386167795313834, took: 0.5619871616363525\n",
      "Validation vlb: -5.455732910378465, Best vlb: -5.345817379195328\n",
      "\n",
      "Epoch96, vlb: -5.382747635379471, took: 0.5761373043060303\n",
      "Validation vlb: -5.411812921172207, Best vlb: -5.345817379195328\n",
      "\n",
      "Epoch97, vlb: -5.357469037062494, took: 0.5509722232818604\n",
      "Validation vlb: -5.414749881596241, Best vlb: -5.345817379195328\n",
      "\n",
      "Epoch98, vlb: -5.369910689193566, took: 0.5480589866638184\n",
      "Validation vlb: -5.409985198172165, Best vlb: -5.345817379195328\n",
      "\n",
      "Epoch99, vlb: -5.418410591225767, took: 0.5547809600830078\n",
      "Validation vlb: -5.337577846057978, Best vlb: -5.337577846057978\n",
      "\n",
      "Epoch100, vlb: -5.369326188489607, took: 0.547692060470581\n",
      "Validation vlb: -5.388491760179835, Best vlb: -5.337577846057978\n",
      "\n",
      "Epoch101, vlb: -5.3557610352089995, took: 0.8758590221405029\n",
      "Validation vlb: -5.432365667472765, Best vlb: -5.337577846057978\n",
      "\n",
      "Epoch102, vlb: -5.396955747676488, took: 0.7316708564758301\n",
      "Validation vlb: -5.2658716312889915, Best vlb: -5.2658716312889915\n",
      "\n",
      "Epoch103, vlb: -5.38796867927631, took: 0.6730587482452393\n",
      "Validation vlb: -5.376994429282772, Best vlb: -5.2658716312889915\n",
      "\n",
      "Epoch104, vlb: -5.368426900224201, took: 0.6112360954284668\n",
      "Validation vlb: -5.396857596523939, Best vlb: -5.2658716312889915\n",
      "\n",
      "Epoch105, vlb: -5.365475138100009, took: 0.571394681930542\n",
      "Validation vlb: -5.419951168850401, Best vlb: -5.2658716312889915\n",
      "\n",
      "Epoch106, vlb: -5.317353932514953, took: 0.6246237754821777\n",
      "Validation vlb: -5.421813768862135, Best vlb: -5.2658716312889915\n",
      "\n",
      "Epoch107, vlb: -5.380947159055244, took: 0.6331079006195068\n",
      "Validation vlb: -5.530379244424765, Best vlb: -5.2658716312889915\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_55946/2068241469.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m                               optimizer_under_VAEAC, save_model = False)\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mvlb_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvlb_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_vlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_VAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munder_VAEAC_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/38/m6rs_5bd7zsf3gjy2hg8y57r0000gn/T/ipykernel_55946/3724644588.py\u001b[0m in \u001b[0;36mtrain_VAE\u001b[0;34m(model, x_train, x_test, nb_epochs, early_stop, flatten)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mx_batch_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mproposal_params_VAEAC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_VAEAC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecognition_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch_flat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# tensor with dim (16,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_divergence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step_under_VAEAC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_params_VAEAC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py38/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py38/lib/python3.8/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1081\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1082\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1083\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py38/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_call_info_injected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py38/lib/python3.8/site-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mone\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \"\"\"\n\u001b[0;32m--> 451\u001b[0;31m     return self._run_internal_graph(\n\u001b[0m\u001b[1;32m    452\u001b[0m         inputs, training=training, mask=mask)\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py38/lib/python3.8/site-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py38/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py38/lib/python3.8/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1081\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1082\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1083\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py38/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_call_info_injected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py38/lib/python3.8/site-packages/keras/layers/core/dense.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m             self.kernel, ids, weights, combiner='sum')\n\u001b[1;32m    198\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m     \u001b[0;31m# Broadcast kernel to inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, name)\u001b[0m\n\u001b[1;32m   3698\u001b[0m             a, b, adj_x=adjoint_a, adj_y=adjoint_b, Tout=output_type, name=name)\n\u001b[1;32m   3699\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3700\u001b[0;31m         return gen_math_ops.mat_mul(\n\u001b[0m\u001b[1;32m   3701\u001b[0m             a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[1;32m   3702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   6011\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6012\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6013\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   6014\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6015\u001b[0m         transpose_b)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dname = 'compas'\n",
    "print(dname)\n",
    "\n",
    "input_dim_vec = [3, 6, 2, 2, 2, 1, 1, 2]\n",
    "batch_size = 128\n",
    "nb_epochs = 2000\n",
    "early_stop = 200\n",
    "lr = 1e-4\n",
    "width = widths[names.index(dname)] # 350\n",
    "depth = depths[names.index(dname)] # number of hidden layers # 3\n",
    "latent_dim = latent_dims[names.index(dname)] # 4\n",
    "\n",
    "x_train, x_test, x_means, x_stds, y_train, y_test, feature_names, X_dims = \\\n",
    "    get_my_COMPAS(rseed=42, separate_test=True, test_ratio=0.1, save_dir='../data/')\n",
    "\n",
    "x_train, x_test, input_dim_vec = join_compas_targets(x_train, x_test, y_train, y_test, X_dims)\n",
    "\n",
    "print('Compas', x_train.shape, x_test.shape)\n",
    "print(input_dim_vec)\n",
    "#\"\"\"\n",
    "\n",
    "optimizer_VAEAC = tfa.optimizers.RectifiedAdam(lr = lr, epsilon = 1e-8)\n",
    "\n",
    "# Create new model to load in weightsinto that can then continued to be trained\n",
    "model2 = VAEAC_gauss_cat(width, depth, latent_dim, input_dim_vec, batch_size, lr, optimizer_VAEAC, save_model = False)\n",
    "\n",
    "model2.recognition_encoder = keras.models.load_model(\"./Valter_VAEAC/compas_recog_encoder_lr_0.0001\")\n",
    "model2.prior_encoder = keras.models.load_model(\"./Valter_VAEAC/compas_prior_encoder_lr_0.0001\")\n",
    "model2.decoder = keras.models.load_model(\"./Valter_VAEAC/compas_decoder_lr_0.0001\")\n",
    "\n",
    "\n",
    "# No mask is used to train the 2nd lvl VAE\n",
    "#masker = top_masker_tensorflow(p=1)\n",
    "\n",
    "base_network = model2\n",
    "width = 150\n",
    "depth = 2\n",
    "latent_dim = under_latent_dims[names.index(dname)]\n",
    "\n",
    "batch_size = 128\n",
    "nb_epochs = 2000\n",
    "early_stop = 200\n",
    "lr = 1e-4\n",
    "\n",
    "optimizer_under_VAEAC = tf.keras.optimizers.Adam(lr = lr, epsilon = 1e-8)\n",
    "#optimizer_under_VAEAC = tfa.optimizers.RectifiedAdam(lr = lr, epsilon = 1e-8)\n",
    "\n",
    "under_VAEAC_net = under_VAEAC(base_network, width, depth, latent_dim, batch_size, lr, \\\n",
    "                              optimizer_under_VAEAC, save_model = False)\n",
    "\n",
    "vlb_train, vlb_val, best_epoch, best_vlb, curr_epoch = train_VAE(under_VAEAC_net, x_train, x_test, nb_epochs, early_stop=early_stop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fd6628",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best_vlb: \", best_vlb)\n",
    "print(\"best epoch: \", best_epoch)\n",
    "plt.figure()\n",
    "plt.plot(np.clip(vlb_train[:curr_epoch], -1000, 1000), 'r')\n",
    "plt.plot(np.clip(vlb_val[:curr_epoch], -1000, 1000), 'b')\n",
    "plt.legend(['cost_train', 'cost_dev'])\n",
    "plt.ylabel('vlb')\n",
    "plt.xlabel('it')\n",
    "plt.grid(True)\n",
    "plt.savefig( str(dname) + '_vlb_lr_' + str(model.lr) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7187415a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eddd68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf0a1b6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For Default credit\n",
    "\"\"\"\n",
    "input_dim_vec = [1, 2, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ,1 ,1 ,1 ,1 ,1, 2]\n",
    "width = 350\n",
    "depth = 3\n",
    "latent_dim = 8\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "batch_size = 128\n",
    "nb_epochs = 2000 # 2000\n",
    "early_stop = 200\n",
    "lr = 1e-4        # Maybe this should be 1e-4, but it makes the performance terrible...\n",
    "\n",
    "x_train, x_test, x_means, x_stds = \\\n",
    "load_UCI(dset_name=dname, splits=10, seed=42, separate_targets=False, save_dir='../data/') # np.arrays\n",
    "\n",
    "x_train = unnormalise_cat_vars(x_train, x_means, x_stds, input_dim_vec) # np.array\n",
    "x_test = unnormalise_cat_vars(x_test, x_means, x_stds, input_dim_vec) \n",
    "\"\"\"\n",
    "\n",
    "# For COMPAS\n",
    "#\"\"\"\n",
    "dname = 'compas'\n",
    "print(dname)\n",
    "\n",
    "input_dim_vec = [3, 6, 2, 2, 2, 1, 1, 2]\n",
    "batch_size = 128\n",
    "nb_epochs = 2000\n",
    "early_stop = 200\n",
    "lr = 1e-4\n",
    "width = widths[names.index(dname)] # 350\n",
    "depth = depths[names.index(dname)] # number of hidden layers # 3\n",
    "latent_dim = latent_dims[names.index(dname)] # 4\n",
    "\n",
    "x_train, x_test, x_means, x_stds, y_train, y_test, feature_names, X_dims = \\\n",
    "    get_my_COMPAS(rseed=42, separate_test=True, test_ratio=0.1, save_dir='../data/')\n",
    "\n",
    "x_train, x_test, input_dim_vec = join_compas_targets(x_train, x_test, y_train, y_test, X_dims)\n",
    "\n",
    "print('Compas', x_train.shape, x_test.shape)\n",
    "print(input_dim_vec)\n",
    "#\"\"\"\n",
    "\n",
    "optimizer_VAEAC = tfa.optimizers.RectifiedAdam(lr= lr , epsilon=1e-8)\n",
    "\n",
    "# Create new model to load in weightsinto that can then continued to be trained\n",
    "model2 = VAEAC_gauss_cat(width, depth, latent_dim, input_dim_vec, batch_size, lr, optimizer_VAEAC, save_model = False)\n",
    "\n",
    "model2.recognition_encoder = keras.models.load_model(\"./Valter_VAEAC/compas_recog_encoder_lr_0.0001\")\n",
    "model2.prior_encoder = keras.models.load_model(\"./Valter_VAEAC/compas_prior_encoder_lr_0.0001\")\n",
    "model2.decoder = keras.models.load_model(\"./Valter_VAEAC/compas_decoder_lr_0.0001\")\n",
    "\n",
    "\n",
    "# No mask is used to train the 2nd lvl VAE\n",
    "#masker = top_masker_tensorflow(p=1)\n",
    "\n",
    "base_network = model2\n",
    "width = 150\n",
    "depth = 2\n",
    "latent_dim = under_latent_dims[names.index(dname)]\n",
    "\n",
    "batch_size = 128\n",
    "nb_epochs = 2000\n",
    "early_stop = 200\n",
    "lr = 1e-4\n",
    "\n",
    "optimizer_under_VAEAC = tfa.optimizers.RectifiedAdam(lr = lr, epsilon = 1e-8)\n",
    "\n",
    "under_VAEAC_net = under_VAEAC(base_network, width, depth, latent_dim, batch_size, lr, \\\n",
    "                              optimizer_under_VAEAC, save_model = False)\n",
    "\n",
    "vlb_train, vlb_val, best_epoch, best_vlb, curr_epoch = train_VAE(under_VAEAC_net, x_train, x_test, nb_epochs, early_stop=early_stop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83b99b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_vlb:  -5.184708448675458\n",
      "best epoch:  320\n"
     ]
    }
   ],
   "source": [
    "print(\"best_vlb: \", best_vlb)\n",
    "print(\"best epoch: \", best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9b68827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"best_vlb: \", best_vlb)\n",
    "print(\"best epoch: \", best_epoch)\n",
    "plt.figure()\n",
    "plt.plot(np.clip(vlb_train[:curr_epoch], -1000, 1000), 'r')\n",
    "plt.plot(np.clip(vlb_val[:curr_epoch], -1000, 1000), 'b')\n",
    "plt.legend(['cost_train', 'cost_dev'])\n",
    "plt.ylabel('vlb')\n",
    "plt.xlabel('it')\n",
    "plt.grid(True)\n",
    "plt.savefig( str(dname) + '_vlb_lr_' + str(model.lr) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3287999f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "827897\n"
     ]
    }
   ],
   "source": [
    "#\"\"\"\n",
    "total_parameters = 0\n",
    "for variable in under_VAEAC_net.trainable_variables:\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    #print(shape)\n",
    "    #print(len(shape))\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        #print(dim)\n",
    "        variable_parameters *= dim\n",
    "    #print(variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)\n",
    "\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41444e16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28920845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.73977529 -5.61497819 -4.62562061 ...  0.          0.\n",
      "  0.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.load(\"./COMPAS_VAEAC/compas_vlb_train_lr_0.0001.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3990338e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9883e1bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "813be207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(under_VAEAC_net.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b33d4570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model2.decoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "170d1686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO check the that the parameters in the VAEAC are not trained during the training of under_VAEAC\n",
    "\n",
    "# Encoder\n",
    "#print(model2.recognition_encoder.trainable_variables[0])\n",
    "\n",
    "# Decoder\n",
    "#print(model2.decoder.trainable_variables[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97bdd3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel2 = VAEAC_gauss_cat(width, depth, latent_dim, input_dim_vec, batch_size, lr, optimizer_VAEAC, save_model = True)\\n\\nmodel2.decoder = keras.models.load_model(\"./COMPAS_VAEAC/compas_decoder_lr_0.0001\")\\n\\nprint(model2.decoder.trainable_variables[0])\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoder\n",
    "\"\"\"\n",
    "model2 = VAEAC_gauss_cat(width, depth, latent_dim, input_dim_vec, batch_size, lr, optimizer_VAEAC, save_model = True)\n",
    "model2.recognition_encoder = keras.models.load_model(\"./COMPAS_VAEAC/compas_recog_encoder_lr_0.0001\")\n",
    "print(model2.recognition_encoder.trainable_variables[0])\n",
    "\"\"\"\n",
    "\n",
    "# Decoder\n",
    "\"\"\"\n",
    "model2 = VAEAC_gauss_cat(width, depth, latent_dim, input_dim_vec, batch_size, lr, optimizer_VAEAC, save_model = True)\n",
    "\n",
    "model2.decoder = keras.models.load_model(\"./COMPAS_VAEAC/compas_decoder_lr_0.0001\")\n",
    "\n",
    "print(model2.decoder.trainable_variables[0])\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29f22fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model2.prior_encoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f2c9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79e0896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(under_VAEAC_net.decoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78fb90c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(under_VAEAC_net.recognition_encoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88ef766",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Currently Working on ###\n",
    "\n",
    "### TODO ###\n",
    "\n",
    "# Make code work for only continous data (GaussianLogLike?)\n",
    "\n",
    "# Train VAEAC (default credit) using 7e-4 lr and save the model\n",
    "\n",
    "# Clean up the code\n",
    "\n",
    "## eval/training mode things ##\n",
    "\n",
    "# Verify if BatchNorm does what it should when training = True/False is not sent into the model\n",
    "\n",
    "# Do I need to set the models to eval mode and not training during validation data or does tf handle this?\n",
    "    # Apparently batchnorm layers do different things during training/evaluation\n",
    "    # Adding training = True affects a lot..., it is not enabled by default during train_step gradient.\n",
    "    # Setting training = True in eval affects the training_VAE still. \n",
    "    # Not using training = True/False makes the model work well, \n",
    "    # Question is if BatchNorm does the correct thing during eval\n",
    "\n",
    "##---------------------------##\n",
    "\n",
    "\n",
    "## Ask Ali \n",
    "    # If His network has the correct structure (The decoder was wrong for VAEAC)\n",
    "    # If He has thought of initlialisations\n",
    "    # hyperparameters, bias, epsilon, momentum etc.\n",
    "    # Where are the batches?\n",
    "\n",
    "# Skip connections from prior to decoder? I don't think CLUE got this to work properly...\n",
    "    # Memory layer is used in Tf2 github\n",
    "\n",
    "# Not sure what the TF equivalence of affine and track_running_stats is in Torch BatchNorm1D \n",
    "    \n",
    "# Why does train_step only print things inside it twice for the first batch and then never for any other batch?\n",
    "    \n",
    "### TO IMPLEMENT ###\n",
    "\n",
    "\n",
    "### DONE ###\n",
    "\"\"\"\n",
    "\n",
    " under_VAEAC updates the VAEAC recognition_encoder parameters. Need to freeze them somehow...\n",
    "    # Maybe calculate proposal_params_VAEAC before calling train_step_under_VAEAC to make it work?\n",
    "    # Yup, that did the trick\n",
    "\n",
    " Something wrong with the number of trainable parameters in my under_VAEAC? Seems to be way more than in Torch?\n",
    "    # The VAEAC recognition_encoder was being trained in under_VAEAC\n",
    "\n",
    " Make VAEAC work for COMPAS\n",
    "    # How to load COMPAS?\n",
    "\n",
    " Train the VAEAC for COMPAS\n",
    "\n",
    " Fix the under_VAEAC code  (i.e. get eval to work there just as in VAEAC)\n",
    "\n",
    " update_train VAE with the train_VAEAC code (Add Shuffle among many things)\n",
    "\n",
    " Plot the loss graph over train and validation set\n",
    "\n",
    " Save the vlb_train & vlb_val after training\n",
    "\n",
    " rec_los: Should the target not be flattened but instead just x_batch?\n",
    "    # Don't think so, the final values look fairly similar.\n",
    "    # I think it is fine since the program seems to be doing what it should\n",
    "\n",
    " How do the batches work in the network? How can we send a 64x31 batch to encoder? It should only take 31 as input\n",
    "    # the keras.input((31, )) means it expect features with dimension 31 and unspecified batch_size. \n",
    "    # When a batch with 64,31 size comes it in will treat each row as a sample\n",
    "\n",
    " Should I have 7e-4 or 1e-4 learning rate?\n",
    "    # 7e-4 for comparing and making sure the model works as intended but 1e-4 for the real training\n",
    "\n",
    " Remove reparametrize?\n",
    "\n",
    " Add lr to print epoch in VAEAC training\n",
    "\n",
    " Is something wrong with the trainable variables? Should I before training use tf.Variable to make them trainable?\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/Variable\n",
    "    # Seems right, it is the exact same amount of trainable parameters in torch and tf\n",
    "\n",
    " Have I forgotten to do a tf.reduce_sum over regularisation? \n",
    "    # Don't think so... their sum(-1) on reg_cost does nothing\n",
    "\n",
    " Something might be wrong with how the training is done with batches... \n",
    "    # Probably not, input(shape,) makes it so that it expects one dimension to be of shape \n",
    "    # without specifying the batch size\n",
    "\n",
    " vlb_val is calculated using eval not fit\n",
    "\n",
    " Add shuffling of the training data \n",
    "\n",
    " Verify the number of trainable parameters in CLUE vs TF \n",
    "    # Exact same amount for under_VAEAC and VAEAC\n",
    "\n",
    " implement under_VAEAC vlb (MSELoss and KL-divergence)\n",
    "\n",
    " rsample instead of sample? (rsample for derivatives in torch, tf does not care)\n",
    "\n",
    " Save the model during training (best vlb)\n",
    "\n",
    " Fix so that training with 2nd lvl VAE is done without the mask\n",
    "\n",
    " 1e-4 can at times get very poor vlb at the start but then recover. Why is this?   \n",
    "    # Seems to have been solved by initialising the dense weights and bias weights the same way as Torch\n",
    "    # I get like -19 or -20, -22 every single first epoch now\n",
    "\n",
    " Validation vlb not low enough? \n",
    "    # Not terrible but indeed not as low, seems to go towards the right values at least but it happens slowly\n",
    "\n",
    " Need to initialise the weights in keras dense for the neurons and bias the same way as nn.linear\n",
    "\n",
    " Need to make the layers in a skip connection sequential? nn.sequential in torch\n",
    "    # The russian doll effect does exactly this!\n",
    "    \n",
    " What activation is used in dense/nn.linear?\n",
    "    # None\n",
    "    \n",
    " Is keras dense and torch nn.linear the same thing?\n",
    "    # How are the weights inited in each? (Different ways by default but I made them init the same way)\n",
    "    # They are basically the same, input to neural network nodes (bias = True add a bias node)\n",
    "    # Google images for keras dense and nn.linear and you see that it is just a normal feed forward process.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
